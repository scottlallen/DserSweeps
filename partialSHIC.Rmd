---
title: "partialS/HIC"
author: "Scott L. Allen"
date: "2023-06-21"
output: html_document
---

# Map DsGRP

```{shell, get DNAseq}

# Dell2 @biol-6nt80t2.staff.net.uq.edu.au

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP
mkdir -p rawData
cd rawData
rsync -ahPv uqsalle3@data.qriscloud.org.au:/data/Q1013/sci-data05/sequence_data_raw/Adam_DS100/* ./

```

## conda

```{shell}

module load anaconda3
source ~/conda-init

# conda create -n bwa
# conda activate bwa
# conda install -c bioconda bwa samtools picard

conda create -n bwa
conda activate bwa
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

conda install qualimap multiqc bwa samtools picard r-base 

```

## index ref

```{shell}

# Bunya

# interactive session

salloc\
 --nodes=1\
 --ntasks-per-node=1 \
 --cpus-per-task=8\
 --mem=16G\
 --job-name=uqsalle3_TEST\
 --time=01:00:00\
 --partition=general\
 --account=a_chenoweth\
 srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

module load anaconda3
source ~/conda-init
conda activate bwa

cd /scratch/project_mnt/S0032/partialSHIC_DsGRP/mapping

rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome_2/dovetail_files/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta ./
  
bwa index drosophila_06Jul2018_A8VGg_noSpecialChar.fasta

# Dell 2

rsync -ahPv /home/uqsalle3/shared/Reference_genome_2/dovetail_files/drosophila_06Jul2018_A8VGg.fasta ./

bwa index drosophila_06Jul2018_A8VGg.fasta

```

## bwa

```{shell, test script bunya}

salloc\
 --nodes=1\
 --ntasks-per-node=1 \
 --cpus-per-task=8\
 --mem=16G\
 --job-name=uqsalle3_TEST\
 --time=01:00:00\
 --partition=general\
 --account=a_chenoweth\
 srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=20G
#SBATCH --job-name=DsGRP_map_line100
#SBATCH --time=4:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_map_line100.output
#SBATCH -e /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_map_line100.error

module load anaconda3
source ~/conda-init
conda activate bwa

cd /scratch/project_mnt/S0032/partialSHIC_DsGRP/mapping

refName='drosophila_06Jul2018_A8VGg_noSpecialChar.fasta'
genomeName='line100'
DNAfastqPath=$(grep '/100/' fastq_paths.txt | sed 's/100\/.*/100\//')
DNAfastq1=$(grep '/100/.*1.fq.gz' fastq_paths.txt)
DNAfastq2=$(grep '/100/.*2.fq.gz' fastq_paths.txt)
gffPath="/scratch/project_mnt/S0032/partialSHIC_DsGRP/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_noSpecialChar.gff"
cpu=8

echo -e "\nMapping genome ${genomeName}...\n"
echo "started mapping genome ${genomeName} at: $(date)" > ${genomeName}_mapping.log
srun bwa mem -t $cpu $refName $DNAfastq1 $DNAfastq2 > ./${genomeName}.sam
srun samtools view -@ $cpu -bT $refName ${genomeName}.sam -o ${genomeName}.bam
srun samtools sort -@ $cpu -o ${genomeName}_noSpecialChar_sorted.bam ${genomeName}.bam
srun samtools index -@ $cpu ${genomeName}_noSpecialChar_sorted.bam
rm ${genomeName}.sam ${genomeName}.bam
srun qualimap bamqc -bam ${genomeName}_noSpecialChar_sorted.bam -gff ${gffPath} -nt ${cpu}
echo "ended at: $(date)" >> ${genomeName}_mapping.log
echo -e "\nMapping genome ${genomeName} complete\n"

```

```{shell, create fastq and sample lists Bunya}

# get paths
find /scratch/project_mnt/S0032/partialSHIC_DsGRP/rawData/batch_1 -type f -name '*.fq.gz' | grep -v 'SN' | grep -v 'DT' > fastq_paths_batch_1.txt
find /scratch/project_mnt/S0032/partialSHIC_DsGRP/rawData/batch_2 -type f -name '*pairend*.fq.gz' > fastq_paths_batch_2.txt
find /scratch/project_mnt/S0032/partialSHIC_DsGRP/rawData/batch_3 -type f -name '*.fq.gz' > fastq_paths_batch_3.txt
find /scratch/project_mnt/S0032/partialSHIC_DsGRP/rawData/batch_4 -type f -name '*.fq.gz' > fastq_paths_batch_4.txt
cat fastq_paths_batch_1.txt fastq_paths_batch_2.txt fastq_paths_batch_3.txt fastq_paths_batch_4.txt > fastq_paths.txt
rm fastq_paths_batch_1.txt fastq_paths_batch_2.txt fastq_paths_batch_3.txt fastq_paths_batch_4.txt

# get samples
sed 's/.*clean_reads\///' fastq_paths.txt | 
  sed 's/.*batch_4\///' | 
  sed 's/\/.*//' | 
  sort |
  uniq > samples.txt

```

```{shell, template script Bunya}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=20G
#SBATCH --job-name=DsGRP_map_line[SAMPLE]
#SBATCH --time=4:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_map_line[SAMPLE].output
#SBATCH -e /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_map_line[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate bwa

cd /scratch/project_mnt/S0032/partialSHIC_DsGRP/mapping

refName='drosophila_06Jul2018_A8VGg_noSpecialChar.fasta'
genomeName='line[SAMPLE]'
DNAfastqPath=$(grep '/[SAMPLE]/' fastq_paths.txt | sed 's/[SAMPLE]\/.*/[SAMPLE]\//')
DNAfastq1=$(grep '/[SAMPLE]/.*1.fq.gz' fastq_paths.txt)
DNAfastq2=$(grep '/[SAMPLE]/.*2.fq.gz' fastq_paths.txt)
gffPath="/scratch/project_mnt/S0032/partialSHIC_DsGRP/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_noSpecialChar.gff"
cpu=8

echo -e "\nMapping genome ${genomeName}...\n"
echo "started mapping genome ${genomeName} at: $(date)" > ${genomeName}_mapping.log
srun bwa mem -t $cpu $refName $DNAfastq1 $DNAfastq2 > ./${genomeName}.sam
srun samtools view -@ $cpu -bT $refName ${genomeName}.sam -o ${genomeName}.bam
srun samtools sort -@ $cpu -o ${genomeName}_noSpecialChar_sorted.bam ${genomeName}.bam
srun samtools index -@ $cpu ${genomeName}_noSpecialChar_sorted.bam
rm ${genomeName}.sam ${genomeName}.bam
srun qualimap bamqc -bam ${genomeName}_noSpecialChar_sorted.bam -gff ${gffPath} -nt ${cpu}
echo "ended at: $(date)" >> ${genomeName}_mapping.log
echo -e "\nMapping genome ${genomeName} complete\n"

```

```{shell, create scripts Bunya}

while read p; do
  echo "$p"
  cp template_bwa.txt bwa_line${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" bwa_line${p}.slurm.sh
done < samples.txt

chmod 755 ./*.slurm.sh

```

# GATK genotyping

https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-

## conda

```{shell}

module load anaconda3
source ~/conda-init
conda create -n gatk
conda activate gatk
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

conda install qualimap r-base picard gatk4 samtools vcftools bcftools bedtools

module load anaconda3
source ~/conda-init
conda activate gatk

```

## data pre-processing for variant discovery

https://gatk.broadinstitute.org/hc/en-us/articles/360035535912-Data-pre-processing-for-variant-discovery

### duplicate removal

https://gatk.broadinstitute.org/hc/en-us/articles/13832748517275-MarkDuplicates-Picard-

* Saved as template_MarkDuplicates.pbs

```{shell, test duplicate removal Bunya}

salloc\
 --nodes=1\
 --ntasks-per-node=1 \
 --cpus-per-task=8\
 --mem=16G\
 --job-name=uqsalle3_TEST\
 --time=01:00:00\
 --partition=general\
 --account=a_chenoweth\
 srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=20G
#SBATCH --job-name=DsGRP_MarkDuplicates_line100
#SBATCH --time=1:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_MarkDuplicates_line100.output
#SBATCH -e /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_MarkDuplicates_line100.error

module load anaconda3
source ~/conda-init
conda activate gatk

cd /scratch/project_mnt/S0032/partialSHIC_DsGRP/MarkDuplicates

genomeName="line100"
BAMPath="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/${genomeName}_noSpecialChar_sorted.bam"
outBAM="${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outMetrics="${genomeName}_MarkDuplicatesMetrics.txt"
gffPath="/scratch/project_mnt/S0032/partialSHIC_DsGRP/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_noSpecialChar.gff"
cpu=8

# MarkDuplicates

echo -e "\nMarkDuplicates\n"

srun picard MarkDuplicates\
 INPUT=${BAMPath}\
 OUTPUT=./${outBAM}\
 REMOVE_DUPLICATES=TRUE\
 METRICS_FILE=./${outMetrics}

# Qualimap

srun qualimap bamqc -bam ${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam -gff ${gffPath} -nt ${cpu}

```

```{shell, template duplicate removal Bunya}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=20G
#SBATCH --job-name=DsGRP_MarkDuplicates_line[SAMPLE]
#SBATCH --time=1:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_MarkDuplicates_line[SAMPLE].output
#SBATCH -e /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_MarkDuplicates_line[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate gatk

cd /scratch/project_mnt/S0032/partialSHIC_DsGRP/MarkDuplicates

genomeName="line[SAMPLE]"
BAMPath="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/${genomeName}_noSpecialChar_sorted.bam"
outBAM="${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outMetrics="${genomeName}_MarkDuplicatesMetrics.txt"
gffPath="/scratch/project_mnt/S0032/partialSHIC_DsGRP/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_noSpecialChar.gff"
cpu=8

# MarkDuplicates

echo -e "\nMarkDuplicates\n"

srun picard MarkDuplicates\
 INPUT=${BAMPath}\
 OUTPUT=./${outBAM}\
 REMOVE_DUPLICATES=TRUE\
 METRICS_FILE=./${outMetrics}

# Qualimap

srun qualimap bamqc -bam ${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam -gff ${gffPath} -nt ${cpu}

```

```{shell, create scripts Bunya}

while read p; do
  echo "$p"
  cp template_MarkDuplicates.slurm.sh MarkDuplicates_line${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" MarkDuplicates_line${p}.slurm.sh
done < /scratch/project_mnt/S0032/partialSHIC_DsGRP/mapping/samples.txt

chmod 755 ./*.slurm.sh

```

### recalibrate base quality scores

* Evidence that this is needed or helpful is not clear
* https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1279-z#Abs1

Tian, S., et al. (2016). "Impact of post-alignment processing in variant discovery from whole exome data." BMC Bioinformatics 17(1).

## HaplotypeCaller in GVCF mode

https://gatk.broadinstitute.org/hc/en-us/articles/13832687299739-HaplotypeCaller

### index ref

```{shell}

# Dell 2

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller

module load anaconda3
source ~/conda-init
conda activate gatk

refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
dict="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar"

picard CreateSequenceDictionary\
 R=$refPath\
 O=${dict}.dict

# Bunya

salloc\
 --nodes=1\
 --ntasks-per-node=1 \
 --cpus-per-task=8\
 --mem=16G\
 --job-name=uqsalle3_TEST\
 --time=01:00:00\
 --partition=general\
 --account=a_chenoweth\
 srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l


cd /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller

module load anaconda3
source ~/conda-init
conda activate gatk

refPath="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
dict="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar"

picard CreateSequenceDictionary\
 R=$refPath\
 O=${dict}.dict

```

### call genotypes

#### AddReplaceReadGroups

```{shell, test AddOrReplaceReadGroups Bunya}

salloc\
 --nodes=1\
 --ntasks-per-node=1 \
 --cpus-per-task=8\
 --mem=16G\
 --job-name=uqsalle3_TEST\
 --time=01:00:00\
 --partition=general\
 --account=a_chenoweth\
 srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=16G
#SBATCH --job-name=DsGRP_HaplotypeCaller_line100
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_MarkDuplicates_line100.output
#SBATCH -e /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_MarkDuplicates_line100.error

module load anaconda3
source ~/conda-init
conda activate gatk

cd /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller

refPath="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line100"
BAMPath1="/scratch/project/chenoase/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="/scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=8

# add read groups (2 minutes)

srun gatk AddOrReplaceReadGroups \
I=${BAMPath1} \
O=${outBAM1} \
RGLB=lib \
RGPL=ILLUMINA \
RGPU=unit1 \
RGSM=${genomeName}

# index bam file

srun samtools index -@ ${cpu} ${outBAM1}

```

```{shell, template AddOrReplaceReadGroups Bunya}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=16G
#SBATCH --job-name=DsGRP_HaplotypeCaller_line[SAMPLE]
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_MarkDuplicates_line[SAMPLE].output
#SBATCH -e /home/uqsalle3/partialSHIC_DsGRP/mapping/slurm_DsGRP_MarkDuplicates_line[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate gatk

cd /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller

refPath="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line[SAMPLE]"
BAMPath1="/scratch/project/chenoase/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="/scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=8

# add read groups (2 minutes)

srun gatk AddOrReplaceReadGroups \
I=${BAMPath1} \
O=${outBAM1} \
RGLB=lib \
RGPL=ILLUMINA \
RGPU=unit1 \
RGSM=${genomeName}

# index bam file

srun samtools index -@ ${cpu} ${outBAM1}

```

```{shell, create scripts Bunya}

while read p; do
  echo "$p"
  cp template_AddOrReplaceReadGroups.txt AddOrReplaceReadGroups_line${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" AddOrReplaceReadGroups_line${p}.slurm.sh
done < /scratch/project_mnt/S0032/partialSHIC_DsGRP/mapping/samples.txt

chmod 755 ./*.slurm.sh

```

#### HaplotypeCaller

##### regions for parallel

```{shell, regions file for parallel}

salloc\
 --nodes=1\
 --ntasks-per-node=1 \
 --cpus-per-task=1\
 --mem=16G\
 --job-name=uqsalle3_temp\
 --time=00:15:00\
 --partition=general\
 --account=a_chenoweth\
 srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

module load anaconda3
source ~/conda-init
conda activate gatk

cd /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller

refPath="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"

# create regions file for parallel processing

samtools faidx ${refPath}
awk 'BEGIN {FS="\t"}; {print $1 FS "0" FS $2}' ${refPath}.fai > ${refPath}.bed
seqnames=$(cut -f 1 ${refPath}.bed)
cut -f 1 ${refPath}.bed > seqnames.txt

```

##### genotype

```{shell, test HaplotypeCaller Dell 2}

# module load parallel
module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller

refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line100"
BAMPath1="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=38

# add read groups (2 minutes)

gatk AddOrReplaceReadGroups \
I=${BAMPath1} \
O=${outBAM1} \
RGLB=lib \
RGPL=ILLUMINA \
RGPU=unit1 \
RGSM=${genomeName}

# samples_file="../mapping/samples.txt"
samples_file="./samples_temp.txt"

for sample in $(cat "$samples_file"); do
    refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
    refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
    genomeName="line${sample}"
    BAMPath1="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
    outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
    BAMPath2="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
    cpu=38
    gatk AddOrReplaceReadGroups I=${BAMPath1} O=${outBAM1} RGLB=lib RGPL=ILLUMINA RGPU=unit1 RGSM=${genomeName}
done

# index bam file

samtools index -@ ${cpu} ${outBAM1}

samples_file="../mapping/samples.txt"

for sample in $(cat "$samples_file"); do
    refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
    refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
    genomeName="line$sample"
    BAMPath1="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
    outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
    BAMPath2="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
    cpu=38
    samtools index -@ ${cpu} ${outBAM1}
done

# create regions file for parallel processing

samtools faidx ${refPath}
awk 'BEGIN {FS="\t"}; {print $1 FS "0" FS $2}' ${refPath}.fai > ${refPath}.bed
seqnames=$(cut -f 1 ${refPath}.bed)
seqnames32=$(head -n 32 ${refPath}.bed | cut -f 1)
seq718=$(grep 'ScA8VGg_718_' ${refPath}.bed | cut -f 1)
# cut -f 1 ${refPath}.bed > seqnames.txt

# run HaplotypCaller in parallel

# parallel --max-procs ${cpu} --memfree 4G "gatk HaplotypeCaller -R ${refPath} -I ${BAMPath2} --intervals {} -O ${genomeName}_{}.g.vcf.gz -ERC GVCF" ::: $seqnames
parallel --max-procs ${cpu} --memfree 4G "gatk HaplotypeCaller -R ${refPath} -I ${BAMPath2} --intervals {} -O ${genomeName}_{}.g.vcf.gz -ERC GVCF" ::: $seqnames32

samples_file="../mapping/samples.txt"

for sample in $(cat "$samples_file"); do
    refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
    refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
    genomeName="line$sample"
    BAMPath1="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
    outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
    BAMPath2="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
    cpu=38
    parallel --max-procs ${cpu} --memfree 4G "gatk HaplotypeCaller -R ${refPath} -I ${BAMPath2} --intervals {} -O ${genomeName}_{}.g.vcf.gz -ERC GVCF" ::: $seqnames32
done

# scf718

refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line$sample"
BAMPath1="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=38

for sample in $(cat "$samples_file"); do
    genomeName="line$sample"
    echo $genomeName
done > genomeNames.txt

cat genomeNames.txt | parallel --max-procs ${cpu} --memfree 4G "gatk HaplotypeCaller -R ${refPath} -I {}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam --intervals ${seq718} -O {}_${seq718}.g.vcf.gz --output-mode EMIT_ALL_CONFIDENT_SITES -ERC BP_RESOLUTION"

# gather vcf files

find ./ -type f -name "${genomeName}_*.g.vcf.gz" | sed 's/\.\///' > temp_${genomeName}.list
sort -V temp_${genomeName}.list > input_vcfs_${genomeName}.list
rm temp_${genomeName}.list

gatk GatherVcfs \
-I input_vcfs_${genomeName}.list \
-O ${genomeName}.g.vcf.gz \
--REFERENCE_SEQUENCE ${refPath}

gatk IndexFeatureFile \
-I ${genomeName}.g.vcf.gz

gatk ValidateVariants \
-V ${genomeName}.g.vcf.gz

samples_file="../mapping/samples.txt"

for sample in $(cat "$samples_file"); do
    refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
    refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
    genomeName="line$sample"
    BAMPath1="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
    outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
    BAMPath2="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
    cpu=38
    find ./ -type f -name "${genomeName}_*.g.vcf.gz" | sed 's/\.\///' > temp_${genomeName}.list
    sort -V temp_${genomeName}.list > input_vcfs_${genomeName}.list
    rm temp_${genomeName}.list
    gatk GatherVcfs -I input_vcfs_${genomeName}.list -O ${genomeName}.g.vcf.gz --REFERENCE_SEQUENCE ${refPath}
    gatk IndexFeatureFile -I ${genomeName}.g.vcf.gz
done

# clean up

rm -Rv ${outBAM1}
rm -Rv ${outBAM1}.bai
rm -Rv ./${genomeName}_*.g.vcf.gz
rm -Rv ./${genomeName}_*.g.vcf.gz.tbi
rm input_vcfs_${genomeName}.list

```

```{shell, test HaplotypeCaller Bunya}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=32
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=10G
#SBATCH --job-name=DsGRP_HaplotypeCaller_line100
#SBATCH --time=12:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/slurm_DsGRP_MarkDuplicates_line100.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/slurm_DsGRP_MarkDuplicates_line100.error

module load parallel
module load anaconda3
source ~/conda-init
conda activate gatk

cd /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller

refPath="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line100"
BAMPath1="/scratch/project/chenoase/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="/scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=32

# Parallel should launch one instances of srun per SLURM task
MY_PARALLEL_OPTS="-N 1 --delay .2 -j ${SLURM_NTASKS} --joblog ${genomeName}-parallel-${SLURM_JOBID}.log"

# srun itself should launch 1 instance of our program and not oversubscribe resources
MY_SRUN_OPTS="-N 1 -n 1 --exclusive"

# Use parallel to launch srun with these options
# parallel $MY_PARALLEL_OPTS srun $MY_SRUN_OPTS ./a.out ::: {0..1023}

# run HaplotypCaller in parallel
seqnames=$(cut -f 1 ${refPath}.bed)
parallel $MY_PARALLEL_OPTS srun $MY_SRUN_OPTS "gatk HaplotypeCaller -R ${refPath} -I ${BAMPath2} --intervals {} -O ${genomeName}_{}.g.vcf.gz --output-mode EMIT_ALL_CONFIDENT_SITES -ERC BP_RESOLUTION" ::: $seqnames

```

```{shell, template HaplotypeCaller Bunya}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=10G
#SBATCH --job-name=DsGRP_HaplotypeCaller_line[SAMPLE]
#SBATCH --time=04:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/slurm_DsGRP_HaplotypeCaller_line[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/slurm_DsGRP_HaplotypeCaller_line[SAMPLE].error

module load parallel
module load anaconda3
source ~/conda-init
conda activate gatk

cd /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller

refPath="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line[SAMPLE]"
BAMPath1="/scratch/project/chenoase/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="/scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=8

# Parallel should launch one instances of srun per SLURM task
MY_PARALLEL_OPTS="-N 1 --delay .2 -j ${SLURM_NTASKS} --joblog ${genomeName}-parallel-${SLURM_JOBID}.log"

# srun itself should launch 1 instance of our program and not oversubscribe resources
MY_SRUN_OPTS="-N 1 -n 1 --exclusive"

# run HaplotypCaller in parallel
seqnames=$(cut -f 1 ${refPath}.bed)
parallel $MY_PARALLEL_OPTS srun $MY_SRUN_OPTS "gatk HaplotypeCaller -R ${refPath} -I ${BAMPath2} --intervals {} -O ${genomeName}_{}.g.vcf.gz --output-mode EMIT_ALL_CONFIDENT_SITES -ERC BP_RESOLUTION" ::: $seqnames

```

```{shell, create scripts Bunya}

while read p; do
  echo "$p"
  cp template_HaplotypeCaller.txt HaplotypeCaller_line${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" HaplotypeCaller_line${p}.slurm.sh
done < /scratch/project_mnt/S0032/partialSHIC_DsGRP/mapping/samples.txt

chmod 755 ./*.slurm.sh

```

##### gather VCFs

```{shell, test GatherVcfs Bunya}

salloc\
 --nodes=1\
 --ntasks-per-node=1 \
 --cpus-per-task=1\
 --mem=16G\
 --job-name=uqsalle3_temp\
 --time=01:00:00\
 --partition=general\
 --account=a_chenoweth\
 srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=4G
#SBATCH --job-name=DsGRP_GatherVcfs_line100
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/slurm_DsGRP_GatherVcfs_line100.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/slurm_DsGRP_GatherVcfs_line100.error

# module load parallel
module load anaconda3
source ~/conda-init
conda activate gatk

cd /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller

refPath="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line100"
BAMPath1="/scratch/project/chenoase/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="/scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=1

# gather vcf files

find ./ -type f -name "${genomeName}_*.g.vcf.gz" | sed 's/\.\///' > temp_${genomeName}.list
sort -V temp_${genomeName}.list > input_vcfs_${genomeName}.list
rm temp_${genomeName}.list

gatk GatherVcfs\
 -I input_vcfs_${genomeName}.list\
 -O ${genomeName}.g.vcf.gz\
 --REFERENCE_SEQUENCE ${refPath}

gatk IndexFeatureFile\
 -I ${genomeName}.g.vcf.gz

# gatk ValidateVariants\
#  -V ${genomeName}.g.vcf.gz

# clean up

# find ./ -type f -name "${genomeName}_ScA8VGg_*.g.vcf.gz" -exec rm -v {} \;
# find ./ -type f -name "${genomeName}_ScA8VGg_*.g.vcf.gz.tbi" -exec rm -v {} \;
# find ./ -type f -name "input_vcfs_${genomeName}.list" -exec rm -v {} \;

```

```{shell, template GatherVcfs Bunya}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=4G
#SBATCH --job-name=DsGRP_GatherVcfs_line[SAMPLE]
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/slurm_DsGRP_GatherVcfs_line[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/slurm_DsGRP_GatherVcfs_line[SAMPLE].error

# module load parallel
module load anaconda3
source ~/conda-init
conda activate gatk

cd /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller

refPath="/scratch/project/chenoase/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line[SAMPLE]"
BAMPath1="/scratch/project/chenoase/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="/scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=1

# gather vcf files

find ./ -type f -name "${genomeName}_*.g.vcf.gz" | sed 's/\.\///' > temp_${genomeName}.list
sort -V temp_${genomeName}.list > input_vcfs_${genomeName}.list
rm temp_${genomeName}.list

gatk GatherVcfs\
 -I input_vcfs_${genomeName}.list\
 -O ${genomeName}.g.vcf.gz\
 --REFERENCE_SEQUENCE ${refPath}

gatk IndexFeatureFile\
 -I ${genomeName}.g.vcf.gz

# gatk ValidateVariants\
#  -V ${genomeName}.g.vcf.gz

# clean up

# find ./ -type f -name "${genomeName}_ScA8VGg_*.g.vcf.gz" -exec rm -v {} \;
# find ./ -type f -name "${genomeName}_ScA8VGg_*.g.vcf.gz.tbi" -exec rm -v {} \;
# find ./ -type f -name "input_vcfs_${genomeName}.list" -exec rm -v {} \;

```

```{shell, create scripts Bunya}

while read p; do
  echo "$p"
  cp template_GatherVcfs.txt GatherVcfs_line${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" GatherVcfs_line${p}.slurm.sh
done < /scratch/project_mnt/S0032/partialSHIC_DsGRP/mapping/samples.txt

chmod 755 ./*.slurm.sh

```

##### CombineGVCFs

https://gatk.broadinstitute.org/hc/en-us/articles/13832686645787-GenomicsDBImport

```{shell, test consolidate GVCFs Dell 2}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /scratch/project/chenoase/partialSHIC_DsGRP/HaplotypeCaller

refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"

# Create a samples mapping file

samples_file="../mapping/samples.txt"

while IFS= read -r sample; do
    printf "line%s\tline%s.g.vcf.gz\tline%s.g.vcf.gz.tbi\n" "$sample" "$sample" "$sample"
done < "$samples_file" > map_file.txt

cut -f 2 map_file.txt > variants.list

# gatk CombineGVCFs\
#  -R $refPath\
#  --variant variants.list\
#  -O cohort.g.vcf.gz

gatk CombineGVCFs\
 -R $refPath\
 --variant variants.list\
 --convert-to-base-pair-resolution true\
 -O cohort.basepair.vcf.gz

# scf718

while IFS= read -r sample; do
    printf "line%s_ScA8VGg_718_HRSCAF_1046.g.vcf.gz\n" "$sample"
done < "$samples_file" > variants.list

gatk CombineGVCFs\
 -R $refPath\
 --variant variants.list\
 --convert-to-base-pair-resolution true\
 -O cohort.basepair.vcf.gz

```

```{shell, consolidate GVCFs Dell 2}

# on Bunya
cd /scratch/project_mnt/S0032/partialSHIC_DsGRP/HaplotypeCaller
find ./ -type f -name '*.vcf.gz' | sed 's/\.\///' > rsync_file_list.txt
find ./ -type f -name '*.vcf.gz.tbi' | sed 's/\.\///' > rsync_file_list_2.txt

# on Dell 2
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya
rsync -ahPv uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/HaplotypeCaller/rsync_file_list.txt ./
rsync -ahPv --files-from=rsync_file_list.txt uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/HaplotypeCaller/ ./
rsync -ahPv uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/HaplotypeCaller/rsync_file_list_2.txt ./
rsync -ahPv --files-from=rsync_file_list_2.txt uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/HaplotypeCaller/ ./
  
# Create a samples mapping file
  
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya

samples_file="../../mapping/samples.txt"
samples=$(cat ../../mapping/samples.txt)
scaffolds=$(ls | grep 'line100_*' | grep -v 'tbi' | grep -v 'line100.g.vcf.gz' | sed 's/line100_//; s/.g.vcf.gz//')

for scf in $scaffolds; do
    for sample in $samples; do
        printf "line%s_${scf}\tline%s_${scf}.g.vcf.gz\tline%s_${scf}.g.vcf.gz.tbi\n" "$sample" "$sample" "$sample" >> map_${scf}_file.txt
    done
done

for scf in $scaffolds; do
    cut -f 2 map_${scf}_file.txt > variants_${scf}.list
done

refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"

for scf in $scaffolds; do
    gatk CombineGVCFs\
        -R $refPath\
        --variant variants_${scf}.list\
        --convert-to-base-pair-resolution true\
        -O cohort.${scf}.basepair.vcf.gz
done

# transfer to Bunya
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya
find ./ -type f -name 'cohort.*' | sed 's/\.\///' > rsync_file_list.txt
rsync -ahPv --files-from=rsync_file_list.txt ./ uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/HaplotypeCaller/

```

##### joint-call cohort

https://gatk.broadinstitute.org/hc/en-us/articles/13832766863259-GenotypeGVCFs

```{shell, test GenotypeGVCFs Dell 2}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller

refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"

gatk --java-options "-Xmx4g" GenotypeGVCFs \
   -R ${refPath} \
   -V cohort.basepair.vcf.gz \
   --include-non-variant-sites true \
   -O output_Default_allSites.vcf.gz

```

```{shell, GenotypeGVCFs Dell 2}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya

refPath="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"

contigs=$(find ./ -type f -name 'cohort*.vcf.gz' | sed 's/\.\/cohort\.//' | sed 's/\.basepair\.vcf\.gz//')

find ./ -type f -name 'cohort*.vcf.gz' | sed 's/\.\/cohort\.//' | sed 's/\.basepair\.vcf\.gz//' > contigs.txt

# gatk --java-options "-Xmx4g" GenotypeGVCFs \
#    -R ${refPath} \
#    -V cohort.ScA8VGg_718_HRSCAF_1046.basepair.vcf.gz \
#    --include-non-variant-sites true \
#    -O output_ScA8VGg_718_HRSCAF_1046_Default_allSites.vcf.gz

while IFS= read -r contig; do
  # Run the GATK command for each contig
  gatk --java-options "-Xmx120g" GenotypeGVCFs -R "$refPath" -V "cohort.${contig}.basepair.vcf.gz" --include-non-variant-sites true -O "GenotypeGVCFs_${contig}_Default_allSites.vcf.gz"
done < contigs.txt

# transfer to Bunya
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya
find ./ -type f -name 'GenotypeGVCFs_ScA8VGg_*' | sed 's/\.\///' > rsync_file_list.txt
rsync -ahPv --files-from=rsync_file_list.txt ./ uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/GenotypeGVCFs/

# gather vcf files

find ./ -type f -name "GenotypeGVCFs_*.vcf.gz" | sed 's/\.\///' > temp.list
sort -V temp.list > input_vcfs.list
rm temp.list

gatk GatherVcfs\
 -I input_vcfs.list\
 -O GenotypeGVCFs_Default_allSites.vcf.gz\
 --REFERENCE_SEQUENCE ${refPath}

nohup gatk IndexFeatureFile\
 -I GenotypeGVCFs_Default_allSites.vcf.gz > nohup_IndexFeatureFile_GenotypeGVCFs_Default_allSites.log &

nohup gatk GatherVcfs\
 -I input_vcfs_top6.list\
 -O GenotypeGVCFs_Default_allSites_top6.vcf.gz\
 --REFERENCE_SEQUENCE ${refPath} > nohup_GatherVcfs_GenotypeGVCFs_Default_allSites_top6.log &

nohup gatk IndexFeatureFile\
 -I GenotypeGVCFs_Default_allSites_top6.vcf.gz > nohup_IndexFeatureFile_GenotypeGVCFs_Default_allSites_top6.log &

nohup gatk GatherVcfs\
 -I input_vcfs_NOTtop6.list\
 -O GenotypeGVCFs_Default_allSites_NOTtop6.vcf.gz\
 --REFERENCE_SEQUENCE ${refPath} > nohup_GatherVcfs_GenotypeGVCFs_Default_allSites_NOTtop6.log &

nohup gatk IndexFeatureFile\
 -I GenotypeGVCFs_Default_allSites_NOTtop6.vcf.gz > nohup_IndexFeatureFile_GenotypeGVCFs_Default_allSites_NOTtop6.log &

# transfer to Bunya
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya
find ./ -type f -name 'GenotypeGVCFs_Default_allSites.*' | sed 's/\.\///' > rsync_file_list.txt
rsync -ahPv --files-from=rsync_file_list.txt ./ uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/GenotypeGVCFs/

```

##### filter variants

https://gatk.broadinstitute.org/hc/en-us/articles/13832652896539-FilterVcf-Picard-

https://gatk.broadinstitute.org/hc/en-us/articles/13832765070875-VariantRecalibrator

```{shell, filter Dell 2 OLD}

# /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller
# 
# zcat output_Default_allSites.vcf.gz | less -S
# 
# # SNP
# zcat output_Default_allSites.vcf.gz |\
#  grep -P 'ScA8VGg_718_HRSCAF_1046\t1265\t'
# 
# # SNP QUAL < 30
# zcat output_Default_allSites.vcf.gz |\
#  grep -P 'ScA8VGg_718_HRSCAF_1046\t1265\t'
# 
# # INDEL
# zcat output_Default_allSites.vcf.gz |\
#  grep -P 'ScA8VGg_718_HRSCAF_1046\t1584\t'
# 
# # SNP and INDEL
# zcat output_Default_allSites.vcf.gz |\
#  grep -P 'ScA8VGg_718_HRSCAF_1046\t4029\t'
# 
# # SNP no REF
# zcat output_Default_allSites.vcf.gz | awk -v OFS='\t' '$5 ~ /,/ {print $1,$2,$3,$4,$5}' | head
# 
# zcat output_Default_allSites.vcf.gz | awk 'length($5) > 1 && /[01]\/[012]/ {print $1,$2,$3,$4,$5}' | head
# 
# # low QUAL SNP
# zcat output_Default_allSites.vcf.gz | awk -v OFS='\t' '$6 ~ /^[0-9]+$/ && $6 < 30 {print $1,$2,$3,$4,$5,$6,$7}'
# 
# zcat output_Default_allSites.vcf.gz |\
#  grep -P 'ScA8VGg_718_HRSCAF_1046\t4719\t'
# 
# zcat output_Default_allSites.vcf.gz |\
#  grep -P 'ScA8VGg_718_HRSCAF_1046\t4440612\t'
# 
# 
# 
# Hard-filter on RGQ for REF and QUAL for SNPs and INDELs

# zcat output_Default_allSites.vcf.gz | \
# while read -r line; do
#     if [[ $line =~ ^# ]]; then
#         echo "$line"
#     else
#         IFS=$'\t' read -ra fields <<< "$line"
#         if [[ ${fields[6]} =~ LowQual ]]; then
#             ${fields[3]}="N"
#             for i in {9..119}; do
#                 IFS=: read -ra arr <<< "${fields[$i]}"
#                 if [[ ${arr[1]} != "0/0" ]]; then
#                     fields[$i]="./.:${fields[$i]#*:}"
#                 fi
#             done
#         elif [[ ${fields[7]} =~ .*AC.* ]]; then
#             for i in {9..119}; do
#                 IFS=: read -ra arr <<< "${fields[$i]}"
#                 if [[ ${arr[3]} -lt 20 ]]; then
#                     fields[$i]="./.:${fields[$i]#*:}"
#                 fi
#             done
#         elif [[ ${fields[8]} =~ .*RGQ.* ]]; then
#             for i in {9..119}; do
#                 IFS=: read -ra arr <<< "${fields[$i]}"
#                 if [[ ${arr[-1]} -lt 20 ]]; then
#                     fields[$i]="./.:${fields[$i]#*:}"
#                 fi
#             done
#         fi
#         printf '%s\t' "${fields[@]}"
#         echo
#     fi
# done > output_Default_allSites_myFilter.vcf
# 
# # zcat output_Default_allSites.vcf.gz | \
# # while read -r line; do
# #     if [[ $line =~ ^# ]]; then
# #         echo "$line"
# #     else
# #         IFS=$'\t' read -ra fields <<< "$line"
# #         if [[ ${fields[6]} =~ LowQual ]]; then
# #             for i in {9..119}; do
# #                 IFS=: read -ra arr <<< "${fields[$i]}"
# #                 if [[ ${arr[0]} != "0/0" ]]; then
# #                     fields[$i]="./.:${fields[$i]#*:}"
# #                 fi
# #             done
# #         fi
# #         printf '%s\t' "${fields[@]}"
# #         echo
# #     fi
# # done > output_Default_allSites_myFilter.vcf
# 
# zcat output_Default_allSites.vcf.gz | awk -v OFS='\t' '
#     /^#/ { print; next }
#     {
#         if ($7 ~ /LowQual/) {
#             for (i = 10; i <= 120; i++) {
#                 split($i, arr, ":")
#                 if (arr[1] != "0/0") {
#                     $i = "./.:" substr($i, index($i, ":") + 1)
#                 }
#             }
#         }
#         print $0
#     }
# ' > output_Default_allSites_myFilter.vcf
# 
# gzip -v output_Default_allSites_myFilter.vcf
# 
# 
# # SNP
# grep -P 'ScA8VGg_718_HRSCAF_1046\t1327\t' output_Default_allSites_myFilter.vcf
# zcat output_Default_allSites.vcf.gz | grep -P 'ScA8VGg_718_HRSCAF_1046\t1327\t'
# zcat ..//Yiguan/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_718.filtered.bi.vcf.gz | grep -P 'ScA8VGg_718\t1327\t'
# 
# # INDEL
# grep -P 'ScA8VGg_718_HRSCAF_1046\t1584\t' output_Default_allSites_myFilter.vcf
# zcat output_Default_allSites.vcf.gz | grep -P 'ScA8VGg_718_HRSCAF_1046\t1584\t'
# 
# # SNP no REF
# grep -P 'ScA8VGg_718_HRSCAF_1046\t7639\t' output_Default_allSites_myFilter.vcf
# zcat output_Default_allSites.vcf.gz | grep -P 'ScA8VGg_718_HRSCAF_1046\t7639\t'
# zcat ../
# 
# # LowQual
# grep -P 'ScA8VGg_718_HRSCAF_1046\t3017\t' output_Default_allSites_myFilter.vcf
# zcat output_Default_allSites.vcf.gz | grep -P 'ScA8VGg_718_HRSCAF_1046\t3017\t'
# 
# grep -P 'ScA8VGg_718_HRSCAF_1046\t4719\t' output_Default_allSites_myFilter.vcf
# zcat output_Default_allSites.vcf.gz | grep -P 'ScA8VGg_718_HRSCAF_1046\t4719\t'
# 
# 
# 
# # Subset to homoRef callset with SelectVariants
# gatk SelectVariants \
#     -V output_Default_allSites.vcf.gz \
#     -select-type NO_VARIATION \
#     -O homoRefs.vcf.gz
# 
# # Subset to SNPs-only callset with SelectVariants
# gatk SelectVariants \
#     -V output_Default_allSites.vcf.gz \
#     -select-type SNP \
#     -O snps.vcf.gz
# 
# # Subset to indels-only callset with SelectVariants
# gatk SelectVariants \
#     -V output_Default_allSites.vcf.gz \
#     -select-type INDEL \
#     -O indels.vcf.gz
# 
# # Merge SNPs and homoRef callsets
# gatk MergeVcfs \
#   I=homoRefs.vcf.gz \
#   I=snps.vcf.gz \
#   O=snps_homoRefs.vcf.gz
# 
# # Hard-filter SNPs on multiple expressions using VariantFiltration
# gatk VariantFiltration \
#     -V snps.vcf.gz \
#     -filter "QD < 2.0" --filter-name "QD2" \
#     -filter "QUAL < 30.0" --filter-name "QUAL30" \
#     -filter "SOR > 3.0" --filter-name "SOR3" \
#     -filter "FS > 60.0" --filter-name "FS60" \
#     -filter "MQ < 40.0" --filter-name "MQ40" \
#     -filter "MQRankSum < -12.5" --filter-name "MQRankSum-12.5" \
#     -filter "ReadPosRankSum < -8.0" --filter-name "ReadPosRankSum-8" \
#     -O snps_filtered.vcf.gz
# 
# gatk SelectVariants \
#     -V snps_filtered.vcf.gz \
#     --exclude-filtered true \
#     -O snps_PASS.vcf.gz
# 
# gatk SelectVariants \
#     -V snps_PASS.vcf.gz \
#     --restrict-alleles-to BIALLELIC \
#     -O snps_PASS_BIALLELIC.vcf.gz
# 
# gatk SelectVariants \
#     -V snps_PASS.vcf.gz \
#     --restrict-alleles-to MULTIALLELIC \
#     -O snps_PASS_MULTIALLELIC.vcf.gz
# 
# 
# # hard-filter indels on multiple expressions using VariantFiltration
# gatk VariantFiltration\
#  -V indels.vcf.gz\
#  -filter "QD < 2.0" --filter-name "QD2"\
#  -filter "QUAL < 30.0" --filter-name "QUAL30"\
#  -filter "FS > 200.0" --filter-name "FS200"\
#  -filter "ReadPosRankSum < -20.0" --filter-name "ReadPosRankSum-20"\
#  -O indels_filtered.vcf.gz
# 
# # Merge filtered SNPs and homoRef callsets
# gatk MergeVcfs\
#  I=homoRefs.vcf.gz\
#  I=snps_PASS_BIALLELIC.vcf.gz\
#  I=snps_PASS_MULTIALLELIC.vcf.gz\
#  O=snps_PASS_BIALLELIC_MULTIALLELIC_homoRefs.vcf.gz
# 
# # Merge filtered SNPs
# gatk MergeVcfs \
#   I=snps_PASS_MULTIALLELIC.vcf.gz \
#   I=snps_PASS_BIALLELIC.vcf.gz \
#   O=snps_PASS.vcf.gz

```

```{shell, vcftools get stats Dell 2}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya

VCF="GenotypeGVCFs_Default_allSites.vcf.gz"
VCF_top6="GenotypeGVCFs_Default_allSites_top6.vcf.gz"
VCF_NOTtop6="GenotypeGVCFs_Default_allSites_NOTtop6.vcf.gz"
OUT_PREFIX="GenotypeGVCFs_Default_allSites"
OUT_PREFIX_top6="GenotypeGVCFs_Default_allSites_top6"
OUT_PREFIX_NOTtop6="GenotypeGVCFs_Default_allSites_NOTtop6"
REF="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"

# allele frequency
vcftools --gzvcf $VCF --freq --out ${OUT_PREFIX}_alleleFreq
vcftools --gzvcf $VCF_top6 --freq --out ${OUT_PREFIX_top6}_alleleFreq
vcftools --gzvcf $VCF_NOTtop6 --freq --out ${OUT_PREFIX_NOTtop6}_alleleFreq

awk -F'\t' '{n = NF-1; if (n > max) { max = n; row = NR }} END { print "Row:", row, "Tabs:", max }' ${OUT_PREFIX_top6}_alleleFreq.frq
head ${OUT_PREFIX_top6}_alleleFreq.frq
perl -i -pe 's/.*/CHROM\tPOS\tN_ALLELES\tN_CHR\tALLELE1_FREQ\tALLELE2_FREQ\tALLELE3_FREQ\tALLELE4_FREQ\tALLELE5_FREQ\tALLELE6_FREQ\tALLELE7_FREQ/ if $. == 1' ${OUT_PREFIX_top6}_alleleFreq.frq
head ${OUT_PREFIX_top6}_alleleFreq.frq

awk -F'\t' '{n = NF-1; if (n > max) { max = n; row = NR }} END { print "Row:", row, "Tabs:", max }' ${OUT_PREFIX_NOTtop6}_alleleFreq.frq
head ${OUT_PREFIX_NOTtop6}_alleleFreq.frq
perl -i -pe 's/.*/CHROM\tPOS\tN_ALLELES\tN_CHR\tALLELE1_FREQ\tALLELE2_FREQ\tALLELE3_FREQ\tALLELE4_FREQ\tALLELE5_FREQ\tALLELE6_FREQ\tALLELE7_FREQ/ if $. == 1' ${OUT_PREFIX_NOTtop6}_alleleFreq.frq
head ${OUT_PREFIX_NOTtop6}_alleleFreq.frq

# allele frequency minimal information
vcftools --gzvcf $VCF --freq2 --out ${OUT_PREFIX}_alleleFreqMinInfo
vcftools --gzvcf $VCF_top6 --freq2 --out ${OUT_PREFIX_top6}_alleleFreqMinInfo
vcftools --gzvcf $VCF_NOTtop6 --freq2 --out ${OUT_PREFIX_NOTtop6}_alleleFreqMinInfo

awk -F'\t' '{n = NF-1; if (n > max) { max = n; row = NR }} END { print "Row:", row, "Tabs:", max }' ${OUT_PREFIX_top6}_alleleFreqMinInfo.frq
head ${OUT_PREFIX_top6}_alleleFreqMinInfo.frq
perl -i -pe 's/.*/CHROM\tPOS\tN_ALLELES\tN_CHR\tALLELE1_FREQ\tALLELE2_FREQ\tALLELE3_FREQ\tALLELE4_FREQ\tALLELE5_FREQ\tALLELE6_FREQ\tALLELE7_FREQ/ if $. == 1' ${OUT_PREFIX_top6}_alleleFreqMinInfo.frq
head ${OUT_PREFIX_top6}_alleleFreqMinInfo.frq

awk -F'\t' '{n = NF-1; if (n > max) { max = n; row = NR }} END { print "Row:", row, "Tabs:", max }' ${OUT_PREFIX_NOTtop6}_alleleFreqMinInfo.frq
head ${OUT_PREFIX_NOTtop6}_alleleFreqMinInfo.frq
perl -i -pe 's/.*/CHROM\tPOS\tN_ALLELES\tN_CHR\tALLELE1_FREQ\tALLELE2_FREQ\tALLELE3_FREQ\tALLELE4_FREQ\tALLELE5_FREQ\tALLELE6_FREQ\tALLELE7_FREQ/ if $. == 1' ${OUT_PREFIX_NOTtop6}_alleleFreqMinInfo.frq
head ${OUT_PREFIX_NOTtop6}_alleleFreqMinInfo.frq

# allele counts
vcftools --gzvcf $VCF --counts --out ${OUT_PREFIX}_alleleCounts
vcftools --gzvcf $VCF_top6 --counts --out ${OUT_PREFIX_top6}_alleleCounts
vcftools --gzvcf $VCF_NOTtop6 --counts --out ${OUT_PREFIX_NOTtop6}_alleleCounts

awk -F'\t' '{n = NF-1; if (n > max) { max = n; row = NR }} END { print "Row:", row, "Tabs:", max }' ${OUT_PREFIX_top6}_alleleCounts.frq.count
head ${OUT_PREFIX_top6}_alleleCounts.frq.count
perl -i -pe 's/.*/CHROM\tPOS\tN_ALLELES\tN_CHR\tALLELE1_FREQ\tALLELE2_FREQ\tALLELE3_FREQ\tALLELE4_FREQ\tALLELE5_FREQ\tALLELE6_FREQ\tALLELE7_FREQ/ if $. == 1' ${OUT_PREFIX_top6}_alleleCounts.frq.count
head ${OUT_PREFIX_top6}_alleleCounts.frq.count

awk -F'\t' '{n = NF-1; if (n > max) { max = n; row = NR }} END { print "Row:", row, "Tabs:", max }' ${OUT_PREFIX_NOTtop6}_alleleCounts.frq.count
head ${OUT_PREFIX_NOTtop6}_alleleCounts.frq.count
perl -i -pe 's/.*/CHROM\tPOS\tN_ALLELES\tN_CHR\tALLELE1_FREQ\tALLELE2_FREQ\tALLELE3_FREQ\tALLELE4_FREQ\tALLELE5_FREQ\tALLELE6_FREQ\tALLELE7_FREQ/ if $. == 1' ${OUT_PREFIX_NOTtop6}_alleleCounts.frq.count
head ${OUT_PREFIX_NOTtop6}_alleleCounts.frq.count

# allele counts minimal information
vcftools --gzvcf $VCF --counts2 --out ${OUT_PREFIX}_alleleCountsMinInfo
vcftools --gzvcf $VCF_top6 --counts2 --out ${OUT_PREFIX_top6}_alleleCountsMinInfo
vcftools --gzvcf $VCF_NOTtop6 --counts2 --out ${OUT_PREFIX_NOTtop6}_alleleCountsMinInfo

awk -F'\t' '{n = NF-1; if (n > max) { max = n; row = NR }} END { print "Row:", row, "Tabs:", max }' ${OUT_PREFIX_top6}_alleleCountsMinInfo.frq.count
head ${OUT_PREFIX_top6}_alleleCountsMinInfo.frq.count
perl -i -pe 's/.*/CHROM\tPOS\tN_ALLELES\tN_CHR\tALLELE1_FREQ\tALLELE2_FREQ\tALLELE3_FREQ\tALLELE4_FREQ\tALLELE5_FREQ\tALLELE6_FREQ\tALLELE7_FREQ/ if $. == 1' ${OUT_PREFIX_top6}_alleleCountsMinInfo.frq.count
head ${OUT_PREFIX_top6}_alleleCountsMinInfo.frq.count

awk -F'\t' '{n = NF-1; if (n > max) { max = n; row = NR }} END { print "Row:", row, "Tabs:", max }' ${OUT_PREFIX_NOTtop6}_alleleCountsMinInfo.frq.count
head ${OUT_PREFIX_NOTtop6}_alleleCountsMinInfo.frq.count
perl -i -pe 's/.*/CHROM\tPOS\tN_ALLELES\tN_CHR\tALLELE1_FREQ\tALLELE2_FREQ\tALLELE3_FREQ\tALLELE4_FREQ\tALLELE5_FREQ\tALLELE6_FREQ\tALLELE7_FREQ/ if $. == 1' ${OUT_PREFIX_NOTtop6}_alleleCountsMinInfo.frq.count
head ${OUT_PREFIX_NOTtop6}_alleleCountsMinInfo.frq.count

# mean depth per individual
vcftools --gzvcf $VCF --depth --out ${OUT_PREFIX}_depthMeanInd
vcftools --gzvcf $VCF_top6 --depth --out ${OUT_PREFIX_top6}_depthMeanInd
vcftools --gzvcf $VCF_NOTtop6 --depth --out ${OUT_PREFIX_NOTtop6}_depthMeanInd

# depth per site summed across all individuals
vcftools --gzvcf $VCF --site-depth --out ${OUT_PREFIX}_depthSumSite
vcftools --gzvcf $VCF_top6 --site-depth --out ${OUT_PREFIX_top6}_depthSumSite
vcftools --gzvcf $VCF_NOTtop6 --site-depth --out ${OUT_PREFIX_NOTtop6}_depthSumSite

# mean depth per site averaged across all individuals
vcftools --gzvcf $VCF --site-mean-depth --out ${OUT_PREFIX}_depthMeanSite
vcftools --gzvcf $VCF_top6 --site-mean-depth --out ${OUT_PREFIX_top6}_depthMeanSite
vcftools --gzvcf $VCF_NOTtop6 --site-mean-depth --out ${OUT_PREFIX_NOTtop6}_depthMeanSite

# depth for each genotype
vcftools --gzvcf $VCF --geno-depth --out ${OUT_PREFIX}_depthIndSite
vcftools --gzvcf $VCF_top6 --geno-depth --out ${OUT_PREFIX_top6}_depthIndSite
vcftools --gzvcf $VCF_NOTtop6 --geno-depth --out ${OUT_PREFIX_NOTtop6}_depthIndSite

# per-site SNP quality
vcftools --gzvcf $VCF --site-quality --out ${OUT_PREFIX}_qual
vcftools --gzvcf $VCF_top6 --site-quality --out ${OUT_PREFIX_top6}_qual
vcftools --gzvcf $VCF_NOTtop6 --site-quality --out ${OUT_PREFIX_NOTtop6}_qual

# missingness on a per-individual basis
vcftools --gzvcf $VCF --missing-indv --out ${OUT_PREFIX}_missingInd
vcftools --gzvcf $VCF_top6 --missing-indv --out ${OUT_PREFIX_top6}_missingInd
vcftools --gzvcf $VCF_NOTtop6 --missing-indv --out ${OUT_PREFIX_NOTtop6}_missingInd

# missingness on a per-site basis
vcftools --gzvcf $VCF --missing-site --out ${OUT_PREFIX}_missingSite
vcftools --gzvcf $VCF_top6 --missing-site --out ${OUT_PREFIX_top6}_missingSite
vcftools --gzvcf $VCF_NOTtop6 --missing-site --out ${OUT_PREFIX_NOTtop6}_missingSite

# heterozygosity  on  a  per-individual  basis
vcftools --gzvcf $VCF --het --out ${OUT_PREFIX}_heterozygosity
vcftools --gzvcf $VCF_top6 --het --out ${OUT_PREFIX_top6}_heterozygosity
vcftools --gzvcf $VCF_NOTtop6 --het --out ${OUT_PREFIX_NOTtop6}_heterozygosity

# Remove line170

gatk SelectVariants\
 -R ${REF}\
 -V $VCF_top6\
 -O ${OUT_PREFIX_top6}_NOline170.vcf.gz\
 --exclude-sample-name line170

```

```{r, vcftools analyse stats}

library(tidyverse)
library(skimr)
library(data.table)
library(parallel)
library(foreach)
library(doParallel)

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya")

# system("gzip -d output_Default_allSites.vcf.gz")

vcf <- fread("GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites.vcf.gz", skip=980)

missingInd <- read_tsv("GenotypeGVCFs_Default_allSites_top6_missingInd.imiss")
missingSite <- read_tsv("GenotypeGVCFs_Default_allSites_top6_missingSite.lmiss") %>% 
  rename(CHROM=CHR)
depthMeanInd <- read_tsv("GenotypeGVCFs_Default_allSites_top6_depthMeanInd.idepth")
depthSumSite <- read_tsv("GenotypeGVCFs_Default_allSites_top6_depthSumSite.ldepth")
depthMeanSite <- read_tsv("GenotypeGVCFs_Default_allSites_top6_depthMeanSite.ldepth.mean")
# depthIndSite <- read_tsv("GenotypeGVCFs_Default_allSites_top6_depthIndSite.gdepth")
qual <- read_tsv("GenotypeGVCFs_Default_allSites_top6_qual.lqual")

alleleFreq <- read_tsv(
  "GenotypeGVCFs_Default_allSites_top6_alleleFreq.frq", 
  col_types = list(col_character(), 
                   col_double(), 
                   col_double(), 
                   col_double(), 
                   col_character(), 
                   col_character(), 
                   col_character(), 
                   col_character(), 
                   col_character(), 
                   col_character()))

alleleFreqMinInfo <- read_tsv(
  "GenotypeGVCFs_Default_allSites_top6_alleleFreqMinInfo.frq", 
  col_types = list(col_character(), 
                   col_double(), 
                   col_double(), 
                   col_double(), 
                   col_double(), 
                   col_double(), 
                   col_double(), 
                   col_double(), 
                   col_double(), 
                   col_double()))

heterozygosity <- read_tsv("GenotypeGVCFs_Default_allSites_top6_heterozygosity.het")



# QUAL

qual %>% 
  filter(QUAL > -1) %>% 
  ggplot(aes(x=QUAL)) + 
  geom_density()

quantile(qual$QUAL[qual$QUAL > -1], probs = c(0.01, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 0.99))


# Missing

## Missing genotype per sample

missingInd %>% 
  ggplot(aes(x=INDV, y=F_MISS)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

## Missing per site

missingSite %>% 
  ggplot(aes(x=F_MISS)) + 
  geom_density()

for (i in c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.95)) {
  miss <- missingSite %>% 
    filter(F_MISS <= i) %>% 
    nrow()
  print(paste("<= ", i, ": ", miss/nrow(missingSite), sep=""))
  flush.console()
}

filter1 <- missingSite %>% 
  filter(F_MISS < 0.25)



# Depth

## Outlier samples n sites genotyped

depthMeanInd %>% arrange(N_SITES)
depthMeanInd %>% arrange(desc(N_SITES))

depthMeanInd %>% 
  ggplot(aes(x=INDV, y=N_SITES)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

## Outlier samples depth

depthMeanInd %>% arrange(MEAN_DEPTH)
depthMeanInd %>% arrange(desc(MEAN_DEPTH))

depthMeanInd %>% 
  ggplot(aes(x=INDV, y=MEAN_DEPTH)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

## Depth mean site

depthMeanSite %>% 
  ggplot(aes(x=MEAN_DEPTH)) + 
  geom_density() + 
  xlim(c(0,50))

quantile(depthMeanSite$MEAN_DEPTH, probs = c(0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99))

filter1 %>% 
  inner_join(depthSumSite) %>% 
  inner_join(depthMeanSite) %>% 
  mutate(MEAN_DEPTH_GENO = SUM_DEPTH/(111-(N_MISS)/2)) %>% 
  ggplot(aes(x=MEAN_DEPTH_GENO)) + 
  geom_density() + 
  xlim(c(0,50))

tempData <- filter1 %>% 
  inner_join(depthSumSite) %>% 
  inner_join(depthMeanSite) %>% 
  mutate(MEAN_DEPTH_GENO = SUM_DEPTH/(111-(N_MISS)/2))

quantile(tempData$MEAN_DEPTH_GENO, probs = c(0.01, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 0.99))

filter2 <- tempData %>% 
  filter(MEAN_DEPTH_GENO >= 10 & MEAN_DEPTH_GENO <= 30)

rm(tempData)

# ## Depth sample per site
# 
# depthIndSite %>% 
#   pivot_longer(line1:line97, names_to="line", values_to="depth") %>% 
#   ggplot(aes(x=depth, col=line)) + 
#   geom_density()



# QUAL

qual %>% 
  filter(QUAL > -1) %>% 
  ggplot(aes(x=QUAL)) + 
  geom_density()

quantile(qual$QUAL[qual$QUAL > -1], probs = c(0.01, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 0.99))

filter2 %>% 
  inner_join(qual) %>% 
  filter(QUAL > -1) %>% 
  ggplot(aes(x=QUAL)) + 
  geom_density()

tempData <- filter2 %>% 
  inner_join(qual)

quantile(tempData$QUAL[tempData$QUAL > -1], probs = c(0.01, 0.025, 0.05, 0.25, 0.5, 0.75, 0.95, 0.975, 0.99))
  

# Allele frequency



# Heterozygosity

heterozygosity %>% 
  ggplot(aes(x=INDV, y=F)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))




# remove unneeded info

# vcf_clean <- vcf %>% 
#   mutate_at(vars(line1:line97), ~str_replace(., ":.*", ""))

## Select columns to modify
columns_to_modify <- vcf %>% 
  head() %>% 
  select(line1:line97) %>% 
  names()

## Modify selected columns using data.table syntax
num_cores <- detectCores()
vcf[, (columns_to_modify) := mclapply(.SD, function(x) sub(":.*", "", x), mc.cores = num_cores), .SDcols = columns_to_modify]

rm(columns_to_modify, num_cores)


# recode low quality calls as monomorphic with missing data for the genotyping mask

vcf_HighQual <- vcf %>% 
  filter(FILTER!="LowQual")

vcf_LowQual <- vcf %>% 
  filter(FILTER=="LowQual")

vcf_LowQual %>% 
  pivot_longer(line1:line97, names_to="line", values_to="genotype") %>% 
  select(genotype) %>% 
  table()

vcf_LowQual_2 <- vcf_LowQual %>% 
  pivot_longer(line1:line97, names_to="line", values_to="genotype") %>% 
  mutate(genotype = ifelse(genotype != "0/0", "./.", genotype)) %>% 
  pivot_wider(names_from="line", values_from="genotype")

vcf_LowQual_2 %>% 
  pivot_longer(line1:line97, names_to="line", values_to="genotype") %>% 
  select(genotype) %>% 
  table()

vcf <- vcf_HighQual %>% 
  bind_rows(vcf_LowQual_2) %>% 
  arrange(POS)

rm(vcf_HighQual, vcf_LowQual, vcf_LowQual_2)



# recode biallelic no REF

vcf_multiallelic <- vcf %>% 
  filter(str_detect(ALT, ","))

vcf_other <- vcf %>% 
  anti_join(vcf_multiallelic)

## Split the data frame into 20 smaller data frames
split_data <- base::split(vcf_multiallelic, rep(1:40, each = nrow(vcf_multiallelic)/40))

recode <- function(split) {
  
  positions <- unique(split$POS)
  
  split2 <- split %>% 
    pivot_longer(line1:line97, names_to="line", values_to="genotype")
  
  for (i in 1:length(positions)) {
    
    genotypes <- split2 %>% 
      filter(POS==positions[i]) %>% 
      select(genotype) %>% 
      distinct()
    
    biallelic_noREF <- all(genotypes$genotype=="1/1" | 
                             genotypes$genotype=="1|1" | 
                             genotypes$genotype=="1/2" | 
                             genotypes$genotype=="1|2" | 
                             genotypes$genotype=="2/2" | 
                             genotypes$genotype=="2|2" | 
                             genotypes$genotype=="./.")
    
    if(biallelic_noREF==TRUE) { 
      
      tempData <- split2 %>% 
        filter(POS==positions[i]) %>% 
        mutate(genotype = str_replace_all(genotype, "1", "0")) %>% 
        mutate(genotype = str_replace_all(genotype, "2", "1"))
      
      alt1 <- str_split(tempData$ALT[1], pattern=",", simplify=TRUE)[1]
      alt2 <- str_split(tempData$ALT[1], pattern=",", simplify=TRUE)[2]
      
      tempData$REF = alt1
      tempData$ALT = alt2
      
      tempData <- tempData %>% 
        pivot_wider(names_from="line", values_from="genotype")
      
      if (i==1) { output <- tempData }
      if (i>=2) { output <- bind_rows(output, tempData) }
      
    }
    
    if(biallelic_noREF==FALSE) { 
      
      tempData <- split2 %>% 
        filter(POS==positions[i]) %>% 
        pivot_wider(names_from="line", values_from="genotype")
      
      if (i==1) { output <- tempData }
      if (i>=2) { output <- bind_rows(output, tempData) }
      
    }
  }
  return(output)
}

num_cores <- 40
recode_data <- mclapply(split_data, recode, mc.cores = num_cores)
modified_vcf <- do.call(bind_rows, recode_data)
modified_vcf <- modified_vcf %>% arrange(POS)

vcf <- vcf_other %>% 
  bind_rows(modified_vcf) %>% 
  arrange(POS)

rm(modified_vcf, num_cores, recode, recode_data, split_data, vcf_multiallelic, vcf_other)



# save

fwrite(vcf, file="temp.vcf", sep="\t", scipen=999)
system("zcat output_Default_allSites.vcf.gz | grep '^##' > header.txt")
system("cat header.txt temp.vcf > output_Default_allSites_myFilter.vcf")
system("gzip -v output_Default_allSites_myFilter.vcf")
system("rm -v temp.vcf header.txt")

```

```{python, filter Dell 2 filter_vcf.py}

import subprocess
import sys
import re

input_file = sys.argv[1]

with subprocess.Popen(['zcat', input_file], stdout=subprocess.PIPE, universal_newlines=True) as proc:
    for line in proc.stdout:
        line = line.rstrip('\n')
        if line.startswith('#'):
            print(line)
        else:
            fields = line.split('\t')
            if 'LowQual' in fields[6]:
                fields[3] = 'N'
            else:
                count = sum(field.count('./.') for field in fields[9:])
                F_MISS = count / 111
                if 'DP=' in fields[7] and count < 111:
                    match = re.search(r'DP=(\d+)', fields[7])
                    if match:
                        depth = int(match.group(1))/111
                        if depth < 5 or depth > 30:
                            fields[3] = 'N'
                            fields[6] = 'Depth'
                if F_MISS > 0.75:
                    fields[3] = 'N'
                    fields[6] = 'Missing75'
                if len(fields[3]) > 1 or len(fields[4]) > 1:
                    fields[3] = 'N'
                    fields[6] = 'Indel'
                if ',' in fields[4]:
                    fields[3] = 'N'
                    fields[6] = 'Multiallelic'
            print('\t'.join(fields))

```

```{shell, filter Dell 2}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya

vcfs="GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites GenotypeGVCFs_Default_allSites"

# for vcf in $vcfs; do
#     python filter_vcf.py ${vcf}.vcf.gz > ${vcf}_FILTERED.vcf &
# done

nohup python filter_vcf.py GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED.vcf &
nohup python filter_vcf.py GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED.vcf &
nohup python filter_vcf.py GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED.vcf &
nohup python filter_vcf.py GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED.vcf &
nohup python filter_vcf.py GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED.vcf &
nohup python filter_vcf.py GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED.vcf &
nohup python filter_vcf.py GenotypeGVCFs_Default_allSites.vcf.gz > GenotypeGVCFs_Default_allSites_FILTERED.vcf &

# for vcf in $vcfs; do
#     pigz -p 38 -v ${vcf}_FILTERED.vcf
# done

nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED.vcf
nohup pigz -p 38 -v GenotypeGVCFs_Default_allSites_FILTERED.vcf

# transfer to Bunya
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya
find ./ -type f -name 'GenotypeGVCFs_*FILTERED.vcf.gz' | sed 's/\.\///' > rsync_file_list.txt
rsync -ahPv --files-from=rsync_file_list.txt ./ uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/GenotypeGVCFs/

```

```{python, filter_vcf_and_recode_biallelicNoRef.py}

import subprocess
import sys
import re

input_file = sys.argv[1]

biallelicNoRef_genotypes = ["1/1", "1|1", "1/2", "1|2", "2/2", "2|2", "./."]

def extract_genotype(genotype):
    return genotype.split(':')[0]

def recode_genotype(genotype):
    genotype = re.sub(r'(^|[^0-9/])1/1([^0-9/]|$)', r'\g<1>0/0\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9|])1\|1([^0-9|]|$)', r'\g<1>0|0\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9/])1/2([^0-9/]|$)', r'\g<1>0/1\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9|])1\|2([^0-9|]|$)', r'\g<1>0|1\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9/])2/2([^0-9/]|$)', r'\g<1>1/1\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9|])2\|2([^0-9|]|$)', r'\g<1>1|1\g<2>', genotype)
    return genotype

with subprocess.Popen(['zcat', input_file], stdout=subprocess.PIPE, universal_newlines=True) as proc:
    for line in proc.stdout:
        line = line.rstrip('\n')
        if line.startswith('#'):
            print(line)
        else:
            fields = line.split('\t')
            if ',' in fields[4] and all(extract_genotype(genotype) in biallelicNoRef_genotypes for genotype in fields[9:]):
                alt1 = fields[4].split(',')[0]
                alt2 = fields[4].split(',')[1]
                fields[3] = alt1
                fields[4] = alt2
                fields[9:] = [recode_genotype(genotype) for genotype in fields[9:]]
            elif 'LowQual' in fields[6]:
                fields[3] = 'N'
            else:
                count = sum(field.count('./.') for field in fields[9:])
                F_MISS = count / 111
                if 'DP=' in fields[7] and count < 111:
                    match = re.search(r'DP=(\d+)', fields[7])
                    if match:
                        depth = int(match.group(1))/111
                        if depth < 5 or depth > 30:
                            fields[3] = 'N'
                            fields[6] = 'Depth'
                if F_MISS > 0.75:
                    fields[3] = 'N'
                    fields[6] = 'Missing75'
                if len(fields[3]) > 1 or len(fields[4]) > 1:
                    fields[3] = 'N'
                    fields[6] = 'Indel'
                if ',' in fields[4]:
                    fields[3] = 'N'
                    fields[6] = 'Multiallelic'
            print('\t'.join(fields))

```

```{shell, filter and recode biallelic no ref Dell 2}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya

vcfs="GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites GenotypeGVCFs_Default_allSites"

# for vcf in $vcfs; do
#     python filter_vcf.py ${vcf}.vcf.gz > ${vcf}_FILTERED.vcf &
# done

nohup python filter_vcf_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef.vcf &
nohup python filter_vcf_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef.vcf &
nohup python filter_vcf_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef.vcf &
nohup python filter_vcf_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef.vcf &
nohup python filter_vcf_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef.vcf &
nohup python filter_vcf_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef.vcf &
nohup python filter_vcf_and_recode_biallelicNoRef.py GenotypeGVCFs_Default_allSites.vcf.gz > GenotypeGVCFs_Default_allSites_FILTERED_recode_biallelicNoRef.vcf &
  
# check number of loci
  
# 542
zcat GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites.vcf.gz | awk '!/^#/{count++} END{print count}' # 21028048 != 21028053
awk '!/^#/{count++} END{print count}' GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef.vcf # 21028048 != 21028053

#594
zcat GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites.vcf.gz | awk '!/^#/{count++} END{print count}' # 30326872 != 30326872
awk '!/^#/{count++} END{print count}' GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef.vcf # 30326872 != 30326886

# 628
zcat GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites.vcf.gz | awk '!/^#/{count++} END{print count}' # 31265516 != 31265526
awk '!/^#/{count++} END{print count}' GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef.vcf # 31265516 != 31265526

# 718
zcat GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites.vcf.gz | awk '!/^#/{count++} END{print count}' # 8653123 == 8653123
awk '!/^#/{count++} END{print count}' GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef.vcf # 8653123 == 8653123

# 76
zcat GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites.vcf.gz | awk '!/^#/{count++} END{print count}' # 38731649 != 38731659
awk '!/^#/{count++} END{print count}' GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef.vcf # 38731649 != 38731659

# 785
zcat GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites.vcf.gz | awk '!/^#/{count++} END{print count}' # 28136194 != 28136203
awk '!/^#/{count++} END{print count}' GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef.vcf # 28136194 != 28136203

# genome
zcat GenotypeGVCFs_Default_allSites.vcf.gz | awk '!/^#/{count++} END{print count}' # 198010725 == 198341263
awk '!/^#/{count++} END{print count}' GenotypeGVCFs_Default_allSites_FILTERED_recode_biallelicNoRef.vcf # ? == 198341263

# conda deactivate
# conda activate pigz

nohup bash pigz.sh &

# nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef.vcf
# nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef.vcf
# nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef.vcf
# nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef.vcf
# nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef.vcf
# nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef.vcf
# nohup pigz -p 38 -v GenotypeGVCFs_Default_allSites_FILTERED_recode_biallelicNoRef.vcf

# transfer to Bunya
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya
find ./ -type f -name 'GenotypeGVCFs_*FILTERED.vcf.gz' | sed 's/\.\///' > rsync_file_list.txt
rsync -ahPv --files-from=rsync_file_list.txt ./ uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/GenotypeGVCFs/

```

```{python, format_and_recode_biallelicNoRef.py}

import subprocess
import sys
import re

input_file = sys.argv[1]

biallelic_genotypes = ["0/0", "0|0", "0/1", "0|1", "1/1", "1|1", "./."]
biallelicNoRef_genotypes = ["1/1", "1|1", "1/2", "1|2", "2/2", "2|2", "./."]
monomorphicRef_genotype = ["0/0", "0|0", "./."]
monomorphicNoRef_genotype = ["1/1", "1|1", "./."]

def extract_genotype(genotype):
    return genotype.split(':')[0]

def recode_genotype(genotype):
    genotype = re.sub(r'(^|[^0-9/])1/1([^0-9/]|$)', r'\g<1>0/0\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9|])1\|1([^0-9|]|$)', r'\g<1>0|0\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9/])1/2([^0-9/]|$)', r'\g<1>0/1\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9|])1\|2([^0-9|]|$)', r'\g<1>0|1\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9/])2/2([^0-9/]|$)', r'\g<1>1/1\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9|])2\|2([^0-9|]|$)', r'\g<1>1|1\g<2>', genotype)
    return genotype

with subprocess.Popen(['zcat', input_file], stdout=subprocess.PIPE, universal_newlines=True) as proc:
    for line in proc.stdout:
        line = line.rstrip('\n')
        if line.startswith('#'):
            print(line)
        else:
            fields = line.split('\t')
            count = sum(field.count('./.') for field in fields[9:])
            F_MISS = count / 111
            if all(extract_genotype(genotype) in biallelic_genotypes for genotype in fields[9:]):
                if F_MISS <= 0.25:
                    fields[6] = 'Biallelic75'
                if F_MISS <= 0.1:
                    fields[6] = 'Biallelic90'
                if F_MISS <= 0.05:
                    fields[6] = 'Biallelic95'
                if F_MISS == 0:
                    fields[6] = 'Biallelic100'
            if ',' in fields[4] and all(extract_genotype(genotype) in biallelicNoRef_genotypes for genotype in fields[9:]):
                alt1 = fields[4].split(',')[0]
                alt2 = fields[4].split(',')[1]
                fields[3] = alt1
                fields[4] = alt2
                fields[9:] = [recode_genotype(genotype) for genotype in fields[9:]]
                if F_MISS <= 0.25:
                    fields[6] = 'BiallelicNoRef75'
                if F_MISS <= 0.1:
                    fields[6] = 'BiallelicNoRef90'
                if F_MISS <= 0.05:
                    fields[6] = 'BiallelicNoRef95'
                if F_MISS == 0:
                    fields[6] = 'BiallelicNoRef100'
            if '.' in fields[4] and all(extract_genotype(genotype) in monomorphicRef_genotype for genotype in fields[9:]):
                fields[6] = 'MonomorphicRef'
            if all(extract_genotype(genotype) in monomorphicNoRef_genotype for genotype in fields[9:]):
                fields[6] = 'MonomorphicAlt'
            if 'DP=' in fields[7] and count < 111:
                match = re.search(r'DP=(\d+)', fields[7])
                if match:
                    depth = int(match.group(1))/111
                    if depth < 5 or depth > 30:
                        fields[6] = 'Depth'
            if F_MISS > 0.75:
                fields[6] = 'Missing75'
            if len(fields[3]) > 1 or len(fields[4]) > 1:
                fields[6] = 'Indel'
            if ',' in fields[4]:
                fields[6] = 'Multiallelic'
            if '*' in fields[3]:
                fields[6] = 'Complex'
            print('\t'.join(fields))

```

```{shell, filter for 100% genotyped biallelic SNPs}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya

vcfs="GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites GenotypeGVCFs_Default_allSites"

# for vcf in $vcfs; do
#     python filter_vcf.py ${vcf}.vcf.gz > ${vcf}_FILTERED.vcf &
# done

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

# nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_Default_allSites.vcf.gz > GenotypeGVCFs_Default_allSites_FILTERED_FORMATfilled_recode_biallelicNoRef.vcf &



# hard filter for biallelic at least 75% genotyped

# grep -E '^#|Biallelic100|Biallelic95|Biallelic90|Biallelic75|BiallelicNoRef100|BiallelicNoRef95|BiallelicNoRef90|BiallelicNoRef75' GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef.vcf

nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf &

nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf &

nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf &

# grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf

nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf &

nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf &



# conda deactivate
# conda activate pigz

nohup bash pigz.sh &

nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
# nohup pigz -p 38 -v GenotypeGVCFs_Default_allSites_FILTERED_recode_biallelicNoRef.vcf

```

```{shell, filter for 75% genotyped and homozygous alternate}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya



# hard filter for good calls and homozygous alternate

nohup zcat GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -E '^#|Biallelic100|Biallelic95|Biallelic90|Biallelic75|BiallelicNoRef100|BiallelicNoRef95|BiallelicNoRef90|BiallelicNoRef75|MonomorphicAlt' >\
 GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -E '^#|Biallelic100|Biallelic95|Biallelic90|Biallelic75|BiallelicNoRef100|BiallelicNoRef95|BiallelicNoRef90|BiallelicNoRef75|MonomorphicAlt' >\
 GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -E '^#|Biallelic100|Biallelic95|Biallelic90|Biallelic75|BiallelicNoRef100|BiallelicNoRef95|BiallelicNoRef90|BiallelicNoRef75|MonomorphicAlt' >\
 GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -E '^#|Biallelic100|Biallelic95|Biallelic90|Biallelic75|BiallelicNoRef100|BiallelicNoRef95|BiallelicNoRef90|BiallelicNoRef75|MonomorphicAlt' >\
 GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -E '^#|Biallelic100|Biallelic95|Biallelic90|Biallelic75|BiallelicNoRef100|BiallelicNoRef95|BiallelicNoRef90|BiallelicNoRef75|MonomorphicAlt' >\
 GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -E '^#|Biallelic100|Biallelic95|Biallelic90|Biallelic75|BiallelicNoRef100|BiallelicNoRef95|BiallelicNoRef90|BiallelicNoRef75|MonomorphicAlt' >\
 GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf &



# conda deactivate
# conda activate pigz

nohup bash pigz.sh &

nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
# nohup pigz -p 38 -v GenotypeGVCFs_Default_allSites_FILTERED_recode_biallelicNoRef.vcf

```

```{shell, filter for good calls and homozygous alternate}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya



# hard filter for good calls and homozygous alternate

nohup zcat GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -v -E 'LowQual|Depth|Missing75|Indel|Multiallelic|Complex|MonomorphicRef' >\
 GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -v -E 'LowQual|Depth|Missing75|Indel|Multiallelic|Complex|MonomorphicRef' >\
 GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -v -E 'LowQual|Depth|Missing75|Indel|Multiallelic|Complex|MonomorphicRef' >\
 GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -v -E 'LowQual|Depth|Missing75|Indel|Multiallelic|Complex|MonomorphicRef' >\
 GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -v -E 'LowQual|Depth|Missing75|Indel|Multiallelic|Complex|MonomorphicRef' >\
 GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf.gz |\
 grep -v -E 'LowQual|Depth|Missing75|Indel|Multiallelic|Complex|MonomorphicRef' >\
 GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf &



# conda deactivate
# conda activate pigz

nohup bash pigz.sh &

nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef_MonomorphicAlt.vcf
# nohup pigz -p 38 -v GenotypeGVCFs_Default_allSites_FILTERED_recode_biallelicNoRef.vcf

```

##### mask

```{r, create mask fasta Dell 2 OLD}

# library(tidyverse)
# library(data.table)
# library(parallel)
# library(foreach)
# library(doParallel)
# 
# setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller")
# 
# vcf <- fread("output_Default_allSites.vcf.gz", skip=980)
# 
# 
# 
# # remove unneeded info
# 
# ## Select columns to modify
# columns_to_modify <- vcf %>% 
#   head() %>% 
#   select(line1:line97) %>% 
#   names()
# 
# ## Modify selected columns using data.table syntax
# num_cores <- detectCores()
# vcf[, (columns_to_modify) := mclapply(.SD, function(x) sub(":.*", "", x), mc.cores = num_cores), .SDcols = columns_to_modify]
# 
# rm(columns_to_modify, num_cores)
# 
# 
# # recode low quality calls as monomorphic with missing data for the genotyping mask
# 
# vcf_HighQual <- vcf %>% 
#   filter(FILTER!="LowQual")
# 
# vcf_LowQual <- vcf %>% 
#   filter(FILTER=="LowQual")
# 
# vcf_LowQual %>% 
#   pivot_longer(line1:line97, names_to="line", values_to="genotype") %>% 
#   select(genotype) %>% 
#   table()
# 
# vcf_LowQual_2 <- vcf_LowQual %>% 
#   pivot_longer(line1:line97, names_to="line", values_to="genotype") %>% 
#   mutate(genotype = ifelse(genotype != "0/0", "./.", genotype)) %>% 
#   pivot_wider(names_from="line", values_from="genotype")
# 
# vcf_LowQual_2 %>% 
#   pivot_longer(line1:line97, names_to="line", values_to="genotype") %>% 
#   select(genotype) %>% 
#   table()
# 
# vcf <- vcf_HighQual %>% 
#   bind_rows(vcf_LowQual_2) %>% 
#   arrange(POS)
# 
# rm(vcf_HighQual, vcf_LowQual, vcf_LowQual_2)
# 
# 
# 
# # recode missing to 0
# 
# ## Select columns to modify
# columns_to_modify <- vcf %>% 
#   head() %>% 
#   select(line1:line97) %>% 
#   names()
# 
# ## Modify selected columns using data.table syntax
# num_cores <- detectCores()
# vcf[, (columns_to_modify) := mclapply(.SD, function(x) sub("./.", 0, x), mc.cores = num_cores), .SDcols = columns_to_modify]
# 
# ## Modify selected columns using data.table syntax
# num_cores <- detectCores()
# vcf[, (columns_to_modify) := mclapply(.SD, function(x) sub(".|.", 0, x), mc.cores = num_cores), .SDcols = columns_to_modify]
# 
# rm(columns_to_modify, num_cores)
# 
# 
# 
# # recode genotyped to 1
# 
# ## Select columns to modify
# columns_to_modify <- vcf %>% 
#   head() %>% 
#   select(line1:line97) %>% 
#   names()
# 
# ## Modify selected columns using data.table syntax
# num_cores <- detectCores()
# vcf[, (columns_to_modify) := mclapply(.SD, function(x) replace(x, x != "0", "1"), mc.cores = num_cores), .SDcols = columns_to_modify]
# 
# rm(columns_to_modify, num_cores)
# 
# 
# 
# # get genotyping rate
# 
# vcf %>%
#   head() %>% 
#   summarise(across(line1:line97, sum))
# 
# 
# 
# # write fasta file
# 
# # vcf_genosRate <- vcf %>% 
# #   rowwise() %>%
# #   mutate(genosRate = sum(c_across(line1:line97) != "./.")/111) %>% 
# #   as.data.frame()
# 
# # Set the number of cores to use
# num_cores <- 20  # Adjust the number of cores as per your system's capacity
# 
# # Define the function to calculate genosRate
# calc_genosRate <- function(x) {
#   1 - (rowSums(x) / length(x))
# }
# 
# ## Define columns to modify
# columns_to_modify <- vcf %>% 
#   head() %>% 
#   select(line1:line97) %>% 
#   names()
# 
# # Modify 'vcf' in-place using mclapply and data.table
# vcf[, genosRate := mclapply(.SD, calc_genosRate, mc.cores = num_cores), .SDcols = columns_to_modify]
# 
# # # Convert back to data.frame if needed
# # vcf <- as.data.frame(dt_vcf)
# 
# 
# 
# # Set the number of cores to use
# num_cores <- 20  # Adjust the number of cores as per your system's capacity
# 
# # Register parallel backend using 'doParallel'
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)
# 
# # Define the function to calculate genosRate
# calc_genosRate <- function(x) {
#   rowSums(x[,10:120] != "./.") / length(x)
# }
# 
# # Apply the function using foreach and modify the 'vcf' data.table in-place
# genosRate <- foreach(i = 1:nrow(vcf), .combine=c) %dopar% {
#   calc_genosRate(vcf[i,])
# }
# 
# # Stop the parallel backend
# stopCluster(cl)
# 
# # Print the resulting 'vcf' data.table with the genosRate column
# print(vcf)
# 
# 
# 
# 
# library(data.table)
# library(parallel)
# 
# # Set the number of cores to use
# num_cores <- 20  # Adjust the number of cores as per your system's capacity
# 
# # Split the data.table into chunks
# chunks <- split(vcf, f = ceiling(seq_len(nrow(vcf)) / (nrow(vcf) / num_cores)))
# 
# ## Define columns to modify
# columns_to_modify <- vcf %>% 
#   head() %>% 
#   select(line1:line97) %>% 
#   names()
# 
# # Define the function to calculate row sums
# calc_genosRate <- function(chunk) {
#   chunk[, genosRate := (1-(rowSums(.SD)/111)), .SDcols = columns_to_modify]
#   chunk
# }
# 
# # Enable parallel processing
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)
# 
# # Apply rowSums on each chunk in parallel
# results <- mclapply(chunks, calc_genosRate)
# 
# # Combine the results into a single data.table
# final_dt <- rbindlist(results)
# 
# # Stop parallel processing
# stopCluster(cl)
# 
# # Print the modified data.table
# print(final_dt)

```

```{shell, create mask fasta Dell 2}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP

# create mask bed

zcat /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_Default_allSites_FILTERED_recode_biallelicNoRef.vcf |\
 awk '!/^#/ && $4 ~ /N/ { print $1"\t"$2-1"\t"$2"\t"$4"\t"$7 }' >\
 GenotypeGVCFs_Default_allSites_FILTERED_MASK.bed


# create masked reference

REF="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
BED="GenotypeGVCFs_Default_allSites_FILTERED_MASK.bed"
REF_MASKED="drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta"

bedtools maskfasta -fi ${REF} -bed ${BED} -fo ${REF_MASKED}

# transfer to Bunya
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP
rsync -ahPv ./drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/

```

# Phase

https://mathgen.stats.ox.ac.uk/genetics_software/shapeit/shapeit.html#home

* Underflow error (https://www.jiscmail.ac.uk/cgi-bin/wa-jisc.exe)

From: Olivier Delaneau <[log in to unmask]>
Subject: Re: shapeit in assembler mode aborts "Underflow in sequencing"
In-Reply-To: <[log in to unmask]>
I'm getting close to solve this issue (raised by Aurelien). It would be
very useful if you could send me an example data set reproducing the error
you got in order for me to see if the fix I made also solve your problem.
It seems to be caused by some hets being covered by a very high number of
PIRs which causes an underflow (prob < 10^-300) for the less likely phase.

From: Ruoyun Hui <[log in to unmask]>
Subject: Re: SHAPEIT genotype calling from low coverage sequencing -
underflow error
Mime-Version: 1.0
Here's how I get around it in the end, in case anyone else runs into the same problem:
I partitioned the windows by the number of SNPs (I used 1500 SNPs per window) rather than by physical distance. After that the error rarely happens; when it still does, I enlarge the window on both ends by ~10kb at a step, until the error goes away.
To unsubscribe from the list visit this webpage https://www.jiscmail.ac.uk/cgi-bin/webadmin?SUBED1=OXSTATGEN&A=1

From: Ruoyun Hui <[log in to unmask]>
Subject: Re: shapeit in assembler mode aborts "Underflow in sequencing"
Mime-Version: 1.0
This problem persists in the current version (r904). I sometimes get an underflow error (what(): Underflow in sequencing / i=FOO b=162 sp=0.0000000000), sometimes not even a clear error message (terminate called after throwing an instance of 'myException'; terminate called recursively; /tmp/1521068340.7024222.7.shell: line 16: 25202 Aborted).
To unsubscribe from the list visit this webpage https://www.jiscmail.ac.uk/cgi-bin/webadmin?SUBED1=OXSTATGEN&A=1

```{shell, mapQ20}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/extractPIRs

mkdir -p bamfiles && cd bamfiles
mkdir -p mq20 && cd mq20

# create bam list

find /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/MarkDuplicates -type f -name '*.bam' > bampaths.txt

# Path to the bamlist.txt file
input_file="bampaths.txt"

# Read each line from the file and process it
while IFS= read -r line; do
    filename=$(basename "$line")  # Extract the filename from the path
    prefix="${filename%%_*}"      # Extract "lineX" using string manipulation
    echo -e "nohup samtools view -@ 38 -q 20 --bam --output $filename.MQ20.bam $line"      # Print "lineX", the original path, and chromosome with a tab separator
done < "$input_file" > samtools_mq20.sh


```

```{shell}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/extractPIRs

# create bam list

find /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/MarkDuplicates -type f -name '*.bam' > bampaths.txt

# Path to the bamlist.txt file
input_file="bampaths.txt"

# Read each line from the file and process it
while IFS= read -r line; do
    filename=$(basename "$line")  # Extract the filename from the path
    prefix="${filename%%_*}"      # Extract "lineX" using string manipulation
    echo -e "$prefix\t$line\tScA8VGg_542_HRSCAF_776"      # Print "lineX", the original path, and chromosome with a tab separator
done < "$input_file" > bamlist.txt

# sort -k1,1 bamlist.txt > sorted_bamlist.txt



# Extract phase informative reads

# ## >= 75% genotyped
# 
# nohup ~/shared/Programs/extractPIRs.v1.r68.x86_64/extractPIRs\
#  --bam bamlist.txt\
#  --vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef.vcf\
#  --out PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef.pir &

## 100% genotyped

# 542

while IFS= read -r line; do
    filename=$(basename "$line")  # Extract the filename from the path
    prefix="${filename%%_*}"      # Extract "lineX" using string manipulation
    echo -e "$prefix\t$line\tScA8VGg_542_HRSCAF_776"      # Print "lineX", the original path, and chromosome with a tab separator
done < "$input_file" > bamlist_542.txt

nohup ~/shared/Programs/extractPIRs.v1.r68.x86_64/extractPIRs\
 --bam bamlist_542.txt\
 --vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --out PIRsList_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 > extractPIRs_542.out 2> extractPIRs_542.err &

# 594

while IFS= read -r line; do
    filename=$(basename "$line")  # Extract the filename from the path
    prefix="${filename%%_*}"      # Extract "lineX" using string manipulation
    echo -e "$prefix\t$line\tScA8VGg_594_HRSCAF_845"      # Print "lineX", the original path, and chromosome with a tab separator
done < "$input_file" > bamlist_594.txt

nohup ~/shared/Programs/extractPIRs.v1.r68.x86_64/extractPIRs\
 --bam bamlist_594.txt\
 --vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --out PIRsList_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 > extractPIRs_594.out 2> extractPIRs_594.err &

# 628

while IFS= read -r line; do
    filename=$(basename "$line")  # Extract the filename from the path
    prefix="${filename%%_*}"      # Extract "lineX" using string manipulation
    echo -e "$prefix\t$line\tScA8VGg_628_HRSCAF_890"      # Print "lineX", the original path, and chromosome with a tab separator
done < "$input_file" > bamlist_628.txt

nohup ~/shared/Programs/extractPIRs.v1.r68.x86_64/extractPIRs\
 --bam bamlist_628.txt\
 --vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --out PIRsList_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 > extractPIRs_628.out 2> extractPIRs_628.err &

# 718

# VCF_718 = "GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf"
# 
# nohup ~/shared/Programs/extractPIRs.v1.r68.x86_64/extractPIRs\
#  --bam bamlist.txt\
#  --vcf ${VCF_PATH}${VCF_718}\
#  --out PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir &

# 76

while IFS= read -r line; do
    filename=$(basename "$line")  # Extract the filename from the path
    prefix="${filename%%_*}"      # Extract "lineX" using string manipulation
    echo -e "$prefix\t$line\tScA8VGg_76_HRSCAF_120"      # Print "lineX", the original path, and chromosome with a tab separator
done < "$input_file" > bamlist_76.txt

nohup ~/shared/Programs/extractPIRs.v1.r68.x86_64/extractPIRs\
 --bam bamlist_76.txt\
 --vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --out PIRsList_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 > extractPIRs_76.out 2> extractPIRs_76.err &

# 785

while IFS= read -r line; do
    filename=$(basename "$line")  # Extract the filename from the path
    prefix="${filename%%_*}"      # Extract "lineX" using string manipulation
    echo -e "$prefix\t$line\tScA8VGg_785_HRSCAF_1140"      # Print "lineX", the original path, and chromosome with a tab separator
done < "$input_file" > bamlist_785.txt

nohup ~/shared/Programs/extractPIRs.v1.r68.x86_64/extractPIRs\
 --bam bamlist_785.txt\
 --vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --out PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 > extractPIRs_785.out 2> extractPIRs_785.err &

nohup ~/shared/Programs/extractPIRs.v1.r68.x86_64/extractPIRs\
 --bam bamlist_785.txt\
 --vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --out PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_mq20_bq20.pir\
 --base-quality 20\
 --read-quality 20\
 > extractPIRs_785_mq20_bq20.out 2> extractPIRs_785_mq20_bq20.err &



# Phase the VCF using the extracted PIRs

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit

# ## >= 75% genotyped
# 
# # ERROR: 30061 SNPs with high rates of missing data (>10%).  These sites should be removed. You can disable this error with --force (at your own risk).
# 
# nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
#  -assemble\
#  --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef.vcf\
#  --input-pir ../extractPIRs/PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef.pir\
#  --output-max HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef.haps\
#  --output-graph HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef.graphs\
#  --output-log HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_75precentGenotyped_recode_biallelicNoRef.log\
#  --thread 38 &

## 100% genotyped

# 542

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -assemble\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --input-pir ../extractPIRs/PIRsList_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 --output-max HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-graph HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.graphs\
 --output-log HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-vcf HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf

# 594

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -assemble\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --input-pir ../extractPIRs/PIRsList_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 --output-max HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-graph HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.graphs\
 --output-log HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-vcf HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf

# 628

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -assemble\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --input-pir ../extractPIRs/PIRsList_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 --output-max HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-graph HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.graphs\
 --output-log HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-vcf HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf

# 718

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -assemble\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --input-pir ../extractPIRs/PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 --output-max HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-graph HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.graphs\
 --output-log HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-vcf HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf

# 76

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -assemble\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --input-pir ../extractPIRs/PIRsList_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 --output-max HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-graph HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.graphs\
 --output-log HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-vcf HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf

# 785

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -assemble\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --input-pir ../extractPIRs/PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.pir\
 --output-max HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-graph HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.graphs\
 --output-log HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.log\
 --thread 38 &

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -assemble\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --input-pir ../extractPIRs/PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_mq20_bq20.pir\
 --output-max HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_mq20_bq20.haps\
 --output-graph HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_mq20_bq20.graphs\
 --output-log HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_mq20_bq20.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.haps\
 --output-vcf HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf



# Phase no PIRs

# 542

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

# 594

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

# 628

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

# 718

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

# 76

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

# 785

nohup ~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

~/shared/Programs/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

```

# Synthetic inbreeding

```{python, synthetic_inbreed.py}

import subprocess
import sys
import random

input_file = sys.argv[1]
# input_file = 'toy.vcf'

# Set the seed for reproducibility
seed_value = 123
random.seed(seed_value)

# Generate a random list of 111 values (0 or 1)
random_list = [random.randint(0, 1) for _ in range(111)]

with subprocess.Popen(['cat', input_file], stdout=subprocess.PIPE, universal_newlines=True) as proc:
    for line in proc.stdout:
        line = line.rstrip('\n')
        if line.startswith('#'):
            print(line)
        else:
            fields = line.split('\t')
            for i in range(111):
                haplotype = fields[i+9].split("|")[random_list[i]]
                fields[i+9] = haplotype + "|" + haplotype
            print('\t'.join(fields))

```

```{shell}

module load anaconda3
source ~/conda-init
conda activate gatk

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit

# nohup python synthetic_inbreed.py\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf >\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_syntheticInbred.vcf &
# 
# nohup python synthetic_inbreed.py\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf >\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_syntheticInbred.vcf &
# 
# nohup python synthetic_inbreed.py\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf >\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_syntheticInbred.vcf &
# 
# nohup python synthetic_inbreed.py\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf >\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_syntheticInbred.vcf &
# 
# nohup python synthetic_inbreed.py\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf >\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_syntheticInbred.vcf &
# 
# nohup python synthetic_inbreed.py\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf >\
#  HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_syntheticInbred.vcf &



nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

```

# Recombination map

## conda

```{shell}

cd /scratch/project/chenoase/partialSHIC_DsGRP/

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=8\
 --mem=32G\
 --job-name=uqsalle3_TEST\
 --time=08:00:00\
 --partition=gpu_cuda\
 --account=a_chenoweth\
 --gres=gpu:l40:1\
 srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

module load anaconda3
source ~/conda-init

#cudnn/8.4.1.50-cuda-11.7.0
conda create -n tf2_gpu python=3.9
conda activate tf2_gpu
# pip install tensorflow==2.11.0 # compatible with cudnn/8.4.1.50-cuda-11.7.0
# pip install tensorflow==2.11.0[and-cuda]
pip install tensorflow[and-cuda]

cd /scratch/project/chenoase/partialSHIC_DsGRP/

git clone https://github.com/kr-colab/ReLERNN.git
cd ReLERNN
pip install .

# example run

# export PATH=$PATH:"/scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN/ReLERNN"

# module load cudnn
# module load cuda/11.7.0

cd /scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN/examples
bash ./example_pipeline.sh



# pip install tensorflow[and-cuda]

# CONDA_OVERRIDE_CUDA="11.2" conda create --name tf2-gpu "tensorflow==2.12.1=cuda112*" --channel conda-forge
# 
# conda install tensorrt 
# 
# conda install -c nvidia cuda-toolkit
# 
# export LD_LIBRARY_PATH="$CONDA_PREFIX/lib"
# 
# find /home/uqsalle3/.conda/envs/tf2-gpu -name libnvinfer.so* -print
# cd  /home/uqsalle3/.conda/envs/tf2-gpu/lib/python3.11/site-packages/tensorrt_libs
# ln -s libnvinfer.so.8 libnvinfer.so.7
# 
# find /home/uqsalle3/.conda/envs/tf2-gpu -name libnvinfer_plugin.so.* -print
# cd /home/uqsalle3/.conda/envs/tf2-gpu/lib/python3.11/site-packages/tensorrt_libs
# ln -s libnvinfer_plugin.so.8 libnvinfer_plugin.so.7
# 
# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:"/home/uqsalle3/.conda/envs/tf2-gpu/lib/python3.11/site-packages/tensorrt_libs/"
# 
# find /home/uqsalle3/.conda/envs/tf2-gpu -name libdevice.* -print
# 
# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:"/home/uqsalle3/.conda/envs/tf2-gpu/lib/python3.11/site-packages/jaxlib/cuda/nvvm/libdevice/"
# CUDA_DIR="/home/uqsalle3/.conda/envs/tf2-gpu/lib/python3.11/site-packages/jaxlib/cuda"
# export XLA_FLAGS=--xla_gpu_cuda_data_dir=/home/uqsalle3/.conda/envs/tf2-gpu/lib/python3.11/site-packages/jaxlib/cuda
# 
# find /home/uqsalle3/.conda/envs/tf2-gpu -name pts* -print
# 
# # pip install --upgrade pip
# # 
# # pip install tensorflow[and-cuda]
# 
# # pip install nvidia-cudnn-cu11 # already installed with above

```

```{shell, install and test example}

# salloc\
#  --nodes=1\
#  --ntasks-per-node=1\
#  --cpus-per-task=8\
#  --mem=32G\
#  --job-name=uqsalle3_TEST\
#  --time=01:00:00\
#  --partition=gpu_cuda\
#  --account=a_chenoweth\
#  --gres=gpu:l40:1\
#  srun\
#  --export=PATH,TERM,HOME,LANG\
#  --pty /bin/bash -l
# 
# module load anaconda3
# source ~/conda-init
# conda activate tf2-gpu
# 
# cd /scratch/project/chenoase/partialSHIC_DsGRP/
# 
# git clone https://github.com/kr-colab/ReLERNN.git
# cd ReLERNN
# pip install .
# 
# # example run
# 
# cd /scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN/examples
# bash ./example_pipeline.sh

```


```{shell, install and test example}

# salloc\
#  --nodes=1\
#  --ntasks-per-node=1\
#  --cpus-per-task=8\
#  --mem=32G\
#  --job-name=uqsalle3_TEST\
#  --time=01:00:00\
#  --partition=gpu_cuda\
#  --account=a_chenoweth\
#  --gres=gpu:l40:1\
#  srun\
#  --export=PATH,TERM,HOME,LANG\
#  --pty /bin/bash -l
# 
# module load anaconda3
# source ~/conda-init
# 
# conda create -n tensorflow2 python=3.9
# conda activate tensorflow2
# # conda install pip
# 
# conda create -n tf2_gpu_env python=3.9
# conda activate tf2_gpu_env
# conda install tensorflow-gpu -c anaconda
# conda install cudnn -c conda-forge 
# conda install cudatoolkit -c anaconda
# 
# module load cudnn
# module load cuda
# 
# pip install tensorflow==2.11.0
# 
# cd /scratch/project/chenoase/partialSHIC_DsGRP/
# 
# git clone https://github.com/kr-colab/ReLERNN.git
# cd ReLERNN
# pip install .
# 
# # example run
# 
# cd ~/ReLERNN/examples
# bash ./example_pipeline.sh

```

```{shell}

cd /scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN

mkdir -p DsGRP_partialSHIC_SNPs

cd DsGRP_partialSHIC_SNPs

# From Dell 2

rsync -ahPv /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/*_shortName.vcf uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN/DsGRP_partialSHIC_SNPs/

# Back to Bunya

cp /scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN/examples/example_pipeline.sh ./
cp example_pipeline.sh DsGRP_partialSHIC_SNPs_pipeline.sh

cp /scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN/examples/genome.bed ./

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=8\
 --mem=32G\
 --job-name=uqsalle3_TEST\
 --time=08:00:00\
 --partition=gpu_cuda\
 --account=a_chenoweth\
 --gres=gpu:l40:1 \
 srun \
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

cd /scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN/DsGRP_partialSHIC_SNPs

module load anaconda3
source ~/conda-init
conda activate tensorflow2
module load cudnn
# module load cuda

bash ./DsGRP_partialSHIC_SNPs_718_pipeline.sh

```

```{shell, template}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --job-name=DsGRP_partialSHIC_SNPs_718
#SBATCH --time=08:00:00
#SBATCH --partition=gpu_cuda
#SBATCH --gres=gpu:l40:1\
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN/DsGRP_partialSHIC_SNPs/slurm_DsGRP_partialSHIC_SNPs_718.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN/DsGRP_partialSHIC_SNPs/slurm_DsGRP_partialSHIC_SNPs_718.error

module load anaconda3
source ~/conda-init
conda activate tensorflow2
module load cudnn
# module load cuda

cd /scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN/DsGRP_partialSHIC_SNPs

srun bash ./DsGRP_partialSHIC_SNPs_718_pipeline.sh

```

```{shell}

rsync -ahPv uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/ReLERNN /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/

```


# ######################## Stairway Plot 2

* Run by Yiguan

## Intron region

Short intronic regions (<120bp) `short_intron_sorted.bed` were extracted based on `new_annotation_woFasta_major.sorted.gff`

```{bash}

cd ~/doveGenome/newAnno
head -6 ../scf1mbp.txt > chromSizes.txt
awk 'OFS="\t" {print $1, "0", $2}' chromSizes.txt | sort -k1,1 -k2,2n > chromSizes.bed

for ss in $(cut -f1 chromSizes.txt);do
    echo ${ss}
    awk -v a=${ss} '{if($1==a){print $0}}' new_annotation_woFasta.gff >> new_annotation_woFasta_major.gff
done


cat new_annotation_woFasta_major.gff | awk '$1 ~ /^#/ {print $0;next} {print $0 | "sort -k1,1 -k4,4n -k5,5n"}' > new_annotation_woFasta_major.sorted.gff
bedtools complement -i new_annotation_woFasta_major.sorted.gff -g chromSizes.txt > intergenic_sorted.bed


awk 'BEGIN{OFS="\t"}$1~/^#/ {print $0;next} {if ($3 == "exon") {print $1, $4-1, $5}}' new_annotation_woFasta_major.sorted.gff > exon_sorted.bed
bedtools complement -i <(cat exon_sorted.bed intergenic_sorted.bed | sort -k1,1 -k2,2n) -g chromSizes.txt > intron_sorted.bed

awk '{a=$3-$2+1;if(a<120){print $0}}' intron_sorted.bed > short_intron_sorted.bed

```

```{bash}

cd ~/myData/phd/p2/stairway

extractSNP(){
    t1=${1##*/}
    vid=${t1%%.*}_short_intron
    vcftools --vcf ${1} --bed short_intron_sorted.bed \
    --out ${vid} --recode --keep-INFO-all
    bgzip -c ${vid}.recode.vcf > ${vid}.recode.vcf.gz
    tabix -p vcf ${vid}.recode.vcf.gz
}

export -f extractSNP

find ../rawData/ -maxdepth 1 -name "*ScA8VGg_*.vcf" | parallel -j 7 extractSNP

```

### Estimate  SFS

#### VCF to SFS
    
```{r}

library(data.table)

vcfs <- list.files(pattern = 'ScA8VGg_.*_short_intron.recode.vcf.gz$')

st_dt <- data.table()

for(vcf in vcfs){
    iscf <- stringr::str_replace_all(vcf,'.recode.vcf.gz','')
    aa <- fread(vcf, skip=11L, header=F)
    aa <- aa[,c(1,2,10:119)]
    t1 <- apply(aa[,3:112],1, function(x) grepl("1/1",x))
    s1 <- apply(t1,2,sum)
    
    t0 <- apply(aa[,3:112],1, function(x) grepl("0/0",x))
    s0 <- apply(t0,2,sum)
    
    st <- data.table("ref"=s0, "alt"=s1)
    st[,hom:=ref + alt]
    # adjust heter site, avoid remove het
    st[,alt_adj:=round(alt/hom*110)]
    st[,scf:=iscf]
    st_dt <- rbind(st_dt, st)
}

st_dt[alt_adj>55, alt_adj:=110-alt_adj]

sfs <- table(st_dt[alt_adj!=0 & scf !='ScA8VGg_594_short_intron', alt_adj]) # exclude sexual chromosome
sfs_out <- paste0(as.integer(sfs),collapse = " ")
writeLines(sfs_out, "all_autosomal.sfs")

```

### Predict demography

```{bash}

cd ~/myData/phd/p2/stairway
git clone https://github.com/xiaoming-liu/stairway-plot-v2.git
# ...

cd stairway-plot-v2/stairway_plot_v2.1.1

# seq short intron total length
# 1:   ScA8VGg_76;HRSCAF=120 463119
# 2:  ScA8VGg_628;HRSCAF=890 309077
# 4: ScA8VGg_785;HRSCAF=1140 405202
# 5:  ScA8VGg_542;HRSCAF=776 250638
# 6: ScA8VGg_718;HRSCAF=1046  76740

java -cp stairway_plot_es Stairbuilder two-epoch.blueprint # do no change file name
bash two-epoch.blueprint.sh

```

### Assess 

* Run by Scott

* Yiguan's demography doesn't match Stairway Plot 2.

```{r}

library(tidyverse)
library(plotly)

setwd("C:\\Users\\scott\\OneDrive - The University of Queensland\\Documents\\Stephen_Chenoweth\\Yiguan\\partialSHIC\\stairway\\stairway-plot-v2\\stairway_plot_v2.1.1\\two-epoch")

sp2_summary <- read_tsv("dser_two-epoch_fold.final.summary")

sp2_summary %>% head(n=20)

```

```{r}

# Constants

N0 <- 1000000
gen_year <- 15

# expansionNe -en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2

expansionNe <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, # contant
                                 1000000*5.1, 1200000*5.1, # contraction
                                 1200000*5.1, 1200000*5.1, # constant
                                 1200000*5.1, 1600000*5.1, # contraction
                                 1600000*5.1, 1600000*5.1, # constant
                                 1600000*5.1, 1200000*5.1, # expansion
                                 1200000*5.1, 1200000*5.1, # constant
                                 700000*5.1, 700000*5.1, # expansion
                                 700000*5.1, 700000*5.1, # constant
                                 400000*5.1, 400000*5.1, # expansion
                                 400000*5.1, 400000*5.1, # constant
                                 240000*5.1, 240000*5.1, # expansion
                                 240000*5.1, 240000*5.1, # constant
                                 200000*5.1, 200000*5.1, # expansion
                                 200000*5.1, 200000*5.1), # constant
                          year = c(1, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (1*4*N0)/gen_year))

# longShallow -en 0.16 0 0.589 -en 0.216 0 1.47

longShallow <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 589000*5.1, 
                                 589000*5.1, 589000*5.1, 
                                 589000*5.1, 1470000*5.1, 
                                 1470000*5.1, 1470000*5.1), 
                          year = c(1, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (1*4*N0)/gen_year))

# shortSevere -en 0.33 0 0.033 -en 0.34 0 1.00

shortSevere <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 33000*5.1, 
                                 33000*5.1, 33000*5.1, 
                                 33000*5.1, 1000000*5.1, 
                                 1000000*5.1, 1000000*5.1), 
                          year = c(1, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (1*4*N0)/gen_year))

# discoal example -en 0.5 0 0.1 -en 1.2 0 0.8
# By way of example, the following command line specifies a bottleneck population history
# where at time 0.5 the population crashes to 10% of its initial size and then at time 1.2 it
# rebounds to 80% of its initial size

discoal_example <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                     1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.8*1000000*5.1), 
                              year = c(1, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (1.2*4*N0)/gen_year, 
                                       (1.2*4*N0)/gen_year, (1.2*4*N0)/gen_year))

# ms example -eN 0.2 .02
# For example, ms 10 30 -t 4.0 -eN 0.2 .02 specifies 
# that the population size was constant at size N0 from the present back to
# time 0.2  4N0, and farther back in time the population size was 0.02  4N0.
# The population size change was instantaneous and occurred at time 0.2  4N0
# generations before the present.

ms_example <- data.frame(Ne = c(N0*5.1, N0*5.1, 
                                N0*5.1, 0.02*N0*5.1, 
                                0.02*N0*5.1, 0.02*N0*5.1), 
                         year = c(1, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (1*4*N0)/gen_year))

p1 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_12.5%`)), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_87.5%`)), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=log10(year), y=log10(`Ne`), col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=log10(year), y=log10(`Ne`), col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=log10(year), y=log10(`Ne`), col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  xlim(c(log10(1), log10(400000)))

# Convert ggplot to a Plotly object
p_plotly1 <- ggplotly(p1)

# Display the interactive plot
p_plotly1

p2 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="Stairway2")) + 
  geom_line() + 
  geom_line(data=expansionNe, aes(x=log10(year), y=log10(`Ne`), col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=log10(year), y=log10(`Ne`), col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=log10(year), y=log10(`Ne`), col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  xlim(c(log10(1), log10(400000)))

# Convert ggplot to a Plotly object
p_plotly2 <- ggplotly(p2)

# Display the interactive plot
p_plotly2

p3 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_2.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_97.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_12.5%`), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_87.5%`), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=year, y=`Ne`, col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  xlim(c(0, 400000))

# Convert ggplot to a Plotly object
p_plotly3 <- ggplotly(p3)

# Display the interactive plot
p_plotly3

p4 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="Stairway2")) + 
  geom_line() + 
  geom_line(data=expansionNe, aes(x=year, y=`Ne`, col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  xlim(c(0, 400000))

# Convert ggplot to a Plotly object
p_plotly4 <- ggplotly(p4)

# Display the interactive plot
p_plotly4

```

#### Better match

```{r}

# Constants

N0 <- 1000000
gen_year <- 15

# expansionNe -en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2

expansionNe <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, # contant
                                 1000000*5.1, 1200000*5.1, # contraction
                                 1200000*5.1, 1200000*5.1, # constant
                                 1200000*5.1, 1600000*5.1, # contraction
                                 1600000*5.1, 1600000*5.1, # constant
                                 1600000*5.1, 1200000*5.1, # expansion
                                 1200000*5.1, 1200000*5.1, # constant
                                 700000*5.1, 700000*5.1, # expansion
                                 700000*5.1, 700000*5.1, # constant
                                 400000*5.1, 400000*5.1, # expansion
                                 400000*5.1, 400000*5.1, # constant
                                 240000*5.1, 240000*5.1, # expansion
                                 240000*5.1, 240000*5.1, # constant
                                 200000*5.1, 200000*5.1, # expansion
                                 200000*5.1, 200000*5.1), # constant
                          year = c(1, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (1*4*N0)/gen_year))

# longShallow -en 0.16 0 0.589 -en 0.216 0 1.47

longShallow <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 589000*5.1, 
                                 589000*5.1, 589000*5.1, 
                                 589000*5.1, 1470000*5.1, 
                                 1470000*5.1, 1470000*5.1), 
                          year = c(1, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (1*4*N0)/gen_year))

# shortSevere -en 0.33 0 0.033 -en 0.34 0 1.00

shortSevere <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 33000*5.1, 
                                 33000*5.1, 33000*5.1, 
                                 33000*5.1, 1000000*5.1, 
                                 1000000*5.1, 1000000*5.1), 
                          year = c(1, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (1*4*N0)/gen_year))

# discoal example -en 0.5 0 0.1 -en 1.2 0 0.8
# By way of example, the following command line specifies a bottleneck population history
# where at time 0.5 the population crashes to 10% of its initial size and then at time 1.2 it
# rebounds to 80% of its initial size

discoal_example <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                     1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.8*1000000*5.1), 
                              year = c(1, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (1.2*4*N0)/gen_year, 
                                       (1.2*4*N0)/gen_year, (1.2*4*N0)/gen_year))

# ms example -eN 0.2 .02
# For example, ms 10 30 -t 4.0 -eN 0.2 .02 specifies 
# that the population size was constant at size N0 from the present back to
# time 0.2  4N0, and farther back in time the population size was 0.02  4N0.
# The population size change was instantaneous and occurred at time 0.2  4N0
# generations before the present.

ms_example <- data.frame(Ne = c(N0*5.1, N0*5.1, 
                                N0*5.1, 0.02*N0*5.1, 
                                0.02*N0*5.1, 0.02*N0*5.1), 
                         year = c(1, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (1*4*N0)/gen_year))

# expansionNe -en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2

expansionNe2 <- data.frame(Ne = c(N0*5.1, N0*5.1, 
                                  N0*5.1, 1.43*N0*5.1, 
                                  1.43*N0*5.1, 1.43*N0*5.1, 
                                  1.43*N0*5.1, 1.57*N0*5.1, 
                                  1.57*N0*5.1, 1.57*N0*5.1, 
                                  1.57*N0*5.1, 1.67*N0*5.1, 
                                  1.67*N0*5.1, 1.67*N0*5.1, 
                                  1.67*N0*5.1, 1.39*N0*5.1, 
                                  1.39*N0*5.1, 1.39*N0*5.1, 
                                  1.39*N0*5.1, 0.67*N0*5.1, 
                                  0.67*N0*5.1, 0.67*N0*5.1, 
                                  0.67*N0*5.1, 0.33*N0*5.1, 
                                  0.33*N0*5.1, 0.33*N0*5.1, 
                                  0.33*N0*5.1, 0.22*N0*5.1, 
                                  0.22*N0*5.1, 0.22*N0*5.1), 
                           year = c(1, (0.00012*4*N0)/gen_year, 
                                    (0.00012*4*N0)/gen_year, (0.00012*4*N0)/gen_year, 
                                    (0.00012*4*N0)/gen_year, (0.0042*4*N0)/gen_year, 
                                    (0.0042*4*N0)/gen_year, (0.0042*4*N0)/gen_year, 
                                    (0.0042*4*N0)/gen_year, (0.010*4*N0)/gen_year, 
                                    (0.010*4*N0)/gen_year, (0.010*4*N0)/gen_year, 
                                    (0.010*4*N0)/gen_year, (0.022*4*N0)/gen_year, 
                                    (0.022*4*N0)/gen_year, (0.022*4*N0)/gen_year, 
                                    (0.022*4*N0)/gen_year, (0.026*4*N0)/gen_year, 
                                    (0.026*4*N0)/gen_year, (0.026*4*N0)/gen_year, 
                                    (0.026*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                    (0.11*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                    (0.11*4*N0)/gen_year, (0.49*4*N0)/gen_year, 
                                    (0.49*4*N0)/gen_year, (0.49*4*N0)/gen_year, 
                                    (0.49*4*N0)/gen_year, (1*4*N0)/gen_year))

p1 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_12.5%`)), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_87.5%`)), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=log10(year), y=log10(`Ne`), col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=log10(year), y=log10(`Ne`), col="longShallow")) +
  geom_line(data=shortSevere, aes(x=log10(year), y=log10(`Ne`), col="shortSevere")) +
  # geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  # geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  geom_line(data=expansionNe2, aes(x=log10(year), y=log10(`Ne`), col="expansionNe2")) + 
  xlim(c(log10(1), log10(400000)))

# Convert ggplot to a Plotly object
p_plotly1 <- ggplotly(p1)

# Display the interactive plot
p_plotly1

p3 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_2.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_97.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_12.5%`), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_87.5%`), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=year, y=`Ne`, col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) +
  geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) +
  # geom_line(data=discoal_example, aes(x=year, y=`Ne`, col="discoal_example")) + 
  # geom_line(data=ms_example, aes(x=year, y=`Ne`, col="ms_example")) + 
  geom_line(data=expansionNe2, aes(x=year, y=`Ne`, col="expansionNe2")) + 
  xlim(c(0, 400000))

# Convert ggplot to a Plotly object
p_plotly3 <- ggplotly(p3)

# Display the interactive plot
p_plotly3

```

## Neutral from constNe region

Short intronic regions (<120bp) `short_intron_sorted.bed` were extracted based on `new_annotation_woFasta_major.sorted.gff`

```{shell}

# cd ~/doveGenome/newAnno
# head -6 ../scf1mbp.txt > chromSizes.txt
# awk 'OFS="\t" {print $1, "0", $2}' chromSizes.txt | sort -k1,1 -k2,2n > chromSizes.bed
# 
# for ss in $(cut -f1 chromSizes.txt);do
#     echo ${ss}
#     awk -v a=${ss} '{if($1==a){print $0}}' new_annotation_woFasta.gff >> new_annotation_woFasta_major.gff
# done
# 
# 
# cat new_annotation_woFasta_major.gff | awk '$1 ~ /^#/ {print $0;next} {print $0 | "sort -k1,1 -k4,4n -k5,5n"}' > new_annotation_woFasta_major.sorted.gff
# bedtools complement -i new_annotation_woFasta_major.sorted.gff -g chromSizes.txt > intergenic_sorted.bed
# 
# 
# awk 'BEGIN{OFS="\t"}$1~/^#/ {print $0;next} {if ($3 == "exon") {print $1, $4-1, $5}}' new_annotation_woFasta_major.sorted.gff > exon_sorted.bed
# bedtools complement -i <(cat exon_sorted.bed intergenic_sorted.bed | sort -k1,1 -k2,2n) -g chromSizes.txt > intron_sorted.bed
# 
# awk '{a=$3-$2+1;if(a<120){print $0}}' intron_sorted.bed > short_intron_sorted.bed

```

```{bash}

cd ~/myData/phd/p2/stairway

extractSNP(){
    t1=${1##*/}
    vid=${t1%%.*}_short_intron
    vcftools --vcf ${1} --bed short_intron_sorted.bed \
    --out ${vid} --recode --keep-INFO-all
    bgzip -c ${vid}.recode.vcf > ${vid}.recode.vcf.gz
    tabix -p vcf ${vid}.recode.vcf.gz
}

export -f extractSNP

find ../rawData/ -maxdepth 1 -name "*ScA8VGg_*.vcf" | parallel -j 7 extractSNP

```

```{bash, install stairway_plot}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP

git clone https://github.com/xiaoming-liu/stairway-plot-v2.git

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2

unzip stairway_plot_v2.1.2.zip

```

```{bash, bed file for regions}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe

cat \
ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed \
ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed \
ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed \
ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed \
ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed \
ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed > \
temp1.pred.bed

grep -v 'track' temp1.pred.bed > temp2.pred.bed

grep 'Neutral' temp2.pred.bed | cut -f 1,2,3,4 > temp3.pred.bed

sed 's/chrScA8VGg_/ScA8VGg_/g' temp3.pred.bed > ScA8VGg_ALL_Neutral_constNe.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

rm temp1.pred.bed temp2.pred.bed temp3.pred.bed

```

```{bash, total loci}

ScA8VGg_ALL_Neutral_constNe.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

awk -F'\t' '{print $0, $3 - $2}' ScA8VGg_ALL_Neutral_constNe.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed | awk '{sum += $5} END {print "Sum:", sum}'

# 61,155,000

```


```{bash, filter vcfs}

bed_file="ScA8VGg_ALL_Neutral_constNe.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed"

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

```

### Estimate  SFS

#### VCF to SFS

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```{r}

library(data.table)

vcfs <- list.files(pattern = 'HaplotypeData_.*.vcf$')

# check 

st_dt <- data.table()

vcf <- vcfs[1]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[2]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[3]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[4]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[5]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[6]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_Neutral_constNe.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

st_dt[alt_adj>55, alt_adj:=110-alt_adj]

# sfs <- table(st_dt[alt_adj!=0 & scf !='ScA8VGg_594', alt_adj]) # exclude sexual chromosome
sfs <- st_dt %>% 
  filter(scf!="ScA8VGg_549") %>% 
  group_by(alt_adj) %>% 
  tally() %>% 
  filter(alt_adj!=0)

sfs_out <- paste0(as.integer(sfs$n),collapse = " ")
writeLines(sfs_out, "all_autosomal_Neutral_constNe.sfs")

```

### Predict demography

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2

cp two-epoch.blueprint two-epoch_Neutral_constNe.blueprint

# vim two-epoch_Neutral_constNe.blueprint

# #example blueprint file
# #input setting
# popid: Neutral_constNe # id of the population (no white space)
# nseq: 110 # number of sequences
# L: 61155000 # total number of observed nucleic sites, including polymorphic and monomorphic
# whether_folded: true # whethr the SFS is folded (true or false)
# SFS: 2234234 752294 381413 238466 165399 123033 96943 79224 65694 55754 48340 42535 37939 33923 30683 27993 25838 23996 22218 20654 19495 18417 17131 16818 15685 14981 14318 13784 13377 13021 12472 12093 11749 11387 11003 10746 10637 10587 10297 9854 10004 9777 9467 9431 9467 9314 9103 9085 9265 9005 8952 8782 8759 8725 4482 # snp frequency spectrum: number of singleton, number of doubleton, etc. (separated by white space)
# #smallest_size_of_SFS_bin_used_for_estimation: 1 # default is 1; to ignore singletons, uncomment this line and change this number to 2
# #largest_size_of_SFS_bin_used_for_estimation: 29 # default is nseq/2 for folded SFS
# pct_training: 0.67 # percentage of sites for training
# nrand: 27       54      81      108 # number of random break points for each try (separated by white space)
# project_dir: Neutral_constNe # project directory
# stairway_plot_dir: stairway_plot_es # directory to the stairway plot files
# ninput: 1000 # number of input files to be created for each estimation
# random_seed: 186457
# #output setting
# mu: 2.8e-9 # assumed mutation rate per site per generation
# year_per_generation: 0.067 # assumed generation time (in years)
# #plot setting
# plot_title: Neutral_constNe # title of the plot
# xrange: 0.1,400 # Time (1k year) range; format: xmin,xmax; "0,0" for default
# yrange: 0,0 # Ne (1k individual) range; format: xmin,xmax; "0,0" for default
# xspacing: 2 # X axis spacing
# yspacing: 2 # Y axis spacing
# fontsize: 12 # Font size

java -cp stairway_plot_es Stairbuilder two-epoch_Neutral_constNe.blueprint # do no change file name
# bash two-epoch_Neutral_constNe.blueprint.sh

# Parallel

grep 'java -Xmx1g -cp stairway_plot_es/:stairway_plot_es/swarmops.jar Stairway_fold_training_testing7 Neutral_constNe/input/Neutral_constNe' two-epoch_Neutral_constNe.blueprint.sh > Stairway_fold_training_testing7_Neutral_constNe.list

parallel -j 38 -a Stairway_fold_training_testing7_Neutral_constNe.list

grep 'mv -f' two-epoch_Neutral_constNe.blueprint.sh > Stairway_fold_mv_Neutral_constNe.list

bash Stairway_fold_mv_Neutral_constNe.list

java -Xmx1g -cp stairway_plot_es/ Stairpainter two-epoch_Neutral_constNe.blueprint

bash two-epoch_Neutral_constNe.blueprint.plot.sh

evince Neutral_constNe.final.summary.pdf

```

```{powershell}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2/Neutral_constNe/Neutral_constNe.final.summary ./

```


### Assess 

* Run by Scott

* Yiguan's demography doesn't match Stairway Plot 2.

```{r}

library(tidyverse)
library(plotly)

setwd("C:\\Users\\scott\\OneDrive - The University of Queensland\\Documents\\Stephen_Chenoweth\\Yiguan\\partialSHIC\\stairway\\stairway-plot-v2\\stairway_plot_v2.1.1\\two-epoch")

sp2_summary <- read_tsv("dser_two-epoch_fold.final.summary")

sp2_summary %>% head(n=20)

sp2_Neutral_constNe <- read_tsv("C:\\Users\\scott\\Dropbox\\PhD_Docu_Yiguan\\sweep_paper_draft\\MolEcol_Submission\\Neutral_constNe.final.summary")

sp2_Neutral_constNe %>% tail(n=20)

```

```{r}

p1 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="ShortIntron")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="mediumturquoise", alpha=0.25) + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="mediumturquoise", alpha=0.25) + 
  # geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_12.5%`)), col="mediumturquoise", alpha=0.5) + 
  # geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_87.5%`)), col="mediumturquoise", alpha=0.5) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=log10(year), y=log10(Ne_median), col="Neutral_constNe")) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="tomato", alpha=0.25) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="tomato", alpha=0.25) + 
  # geom_line(data=sp2_Neutral_constNe, aes(x=log10(year), y=log10(`Ne_12.5%`)), col="tomato", alpha=0.5) + 
  # geom_line(data=sp2_Neutral_constNe, aes(x=log10(year), y=log10(`Ne_87.5%`)), col="tomato", alpha=0.5) + 
  xlim(c(log10(1), log10(150000)))

# Convert ggplot to a Plotly object
p_plotly1 <- ggplotly(p1)

# Display the interactive plot
p_plotly1



p2 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="ShortIntron")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_2.5%`), col="mediumturquoise", alpha=0.25) + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_97.5%`), col="mediumturquoise", alpha=0.25) + 
  # geom_line(data=sp2_summary, aes(x=year, y=`Ne_12.5%`), col="mediumturquoise", alpha=0.5) + 
  # geom_line(data=sp2_summary, aes(x=year, y=`Ne_87.5%`), col="mediumturquoise", alpha=0.5) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=year, y=Ne_median, col="Neutral_constNe")) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=year, y=`Ne_2.5%`), col="tomato", alpha=0.25) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=year, y=`Ne_97.5%`), col="tomato", alpha=0.25) + 
  # geom_line(data=sp2_Neutral_constNe, aes(x=year, y=`Ne_12.5%`), col="tomato", alpha=0.5) + 
  # geom_line(data=sp2_Neutral_constNe, aes(x=year, y=`Ne_87.5%`), col="tomato", alpha=0.5) + 
  xlim(c(1, 150000))

# Convert ggplot to a Plotly object
p_plotly2 <- ggplotly(p2)

# Display the interactive plot
p_plotly2

```


```{r}

# Constants

N0 <- 1000000
gen_year <- 15

# expansionNe -en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2

expansionNe <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, # contant
                                 1000000*5.1, 1200000*5.1, # contraction
                                 1200000*5.1, 1200000*5.1, # constant
                                 1200000*5.1, 1600000*5.1, # contraction
                                 1600000*5.1, 1600000*5.1, # constant
                                 1600000*5.1, 1200000*5.1, # expansion
                                 1200000*5.1, 1200000*5.1, # constant
                                 700000*5.1, 700000*5.1, # expansion
                                 700000*5.1, 700000*5.1, # constant
                                 400000*5.1, 400000*5.1, # expansion
                                 400000*5.1, 400000*5.1, # constant
                                 240000*5.1, 240000*5.1, # expansion
                                 240000*5.1, 240000*5.1, # constant
                                 200000*5.1, 200000*5.1, # expansion
                                 200000*5.1, 200000*5.1), # constant
                          year = c(1, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (1*4*N0)/gen_year))

# longShallow -en 0.16 0 0.589 -en 0.216 0 1.47

longShallow <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 589000*5.1, 
                                 589000*5.1, 589000*5.1, 
                                 589000*5.1, 1470000*5.1, 
                                 1470000*5.1, 1470000*5.1), 
                          year = c(1, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (1*4*N0)/gen_year))

# shortSevere -en 0.33 0 0.033 -en 0.34 0 1.00

shortSevere <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 33000*5.1, 
                                 33000*5.1, 33000*5.1, 
                                 33000*5.1, 1000000*5.1, 
                                 1000000*5.1, 1000000*5.1), 
                          year = c(1, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (1*4*N0)/gen_year))

# discoal example -en 0.5 0 0.1 -en 1.2 0 0.8
# By way of example, the following command line specifies a bottleneck population history
# where at time 0.5 the population crashes to 10% of its initial size and then at time 1.2 it
# rebounds to 80% of its initial size

discoal_example <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                     1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.8*1000000*5.1), 
                              year = c(1, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (1.2*4*N0)/gen_year, 
                                       (1.2*4*N0)/gen_year, (1.2*4*N0)/gen_year))

# ms example -eN 0.2 .02
# For example, ms 10 30 -t 4.0 -eN 0.2 .02 specifies 
# that the population size was constant at size N0 from the present back to
# time 0.2  4N0, and farther back in time the population size was 0.02  4N0.
# The population size change was instantaneous and occurred at time 0.2  4N0
# generations before the present.

ms_example <- data.frame(Ne = c(N0*5.1, N0*5.1, 
                                N0*5.1, 0.02*N0*5.1, 
                                0.02*N0*5.1, 0.02*N0*5.1), 
                         year = c(1, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (1*4*N0)/gen_year))

p1 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_12.5%`)), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_87.5%`)), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=log10(year), y=log10(`Ne`), col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=log10(year), y=log10(`Ne`), col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=log10(year), y=log10(`Ne`), col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=log10(year), y=log10(Ne_median), col="sp2_Neutral_constNe")) + 
  xlim(c(log10(1), log10(400000)))

# Convert ggplot to a Plotly object
p_plotly1 <- ggplotly(p1)

# Display the interactive plot
p_plotly1

p2 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="Stairway2")) + 
  geom_line() + 
  geom_line(data=expansionNe, aes(x=log10(year), y=log10(`Ne`), col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=log10(year), y=log10(`Ne`), col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=log10(year), y=log10(`Ne`), col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  xlim(c(log10(1), log10(400000)))

# Convert ggplot to a Plotly object
p_plotly2 <- ggplotly(p2)

# Display the interactive plot
p_plotly2

p3 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_2.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_97.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_12.5%`), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_87.5%`), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=year, y=`Ne`, col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  xlim(c(0, 400000))

# Convert ggplot to a Plotly object
p_plotly3 <- ggplotly(p3)

# Display the interactive plot
p_plotly3

p4 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="Stairway2")) + 
  geom_line() + 
  geom_line(data=expansionNe, aes(x=year, y=`Ne`, col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  xlim(c(0, 400000))

# Convert ggplot to a Plotly object
p_plotly4 <- ggplotly(p4)

# Display the interactive plot
p_plotly4

```

#### Better match

```{r}

# Constants

N0 <- 1000000
gen_year <- 15

# expansionNe -en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2

expansionNe <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, # contant
                                 1000000*5.1, 1200000*5.1, # contraction
                                 1200000*5.1, 1200000*5.1, # constant
                                 1200000*5.1, 1600000*5.1, # contraction
                                 1600000*5.1, 1600000*5.1, # constant
                                 1600000*5.1, 1200000*5.1, # expansion
                                 1200000*5.1, 1200000*5.1, # constant
                                 700000*5.1, 700000*5.1, # expansion
                                 700000*5.1, 700000*5.1, # constant
                                 400000*5.1, 400000*5.1, # expansion
                                 400000*5.1, 400000*5.1, # constant
                                 240000*5.1, 240000*5.1, # expansion
                                 240000*5.1, 240000*5.1, # constant
                                 200000*5.1, 200000*5.1, # expansion
                                 200000*5.1, 200000*5.1), # constant
                          year = c(1, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (1*4*N0)/gen_year))

# longShallow -en 0.16 0 0.589 -en 0.216 0 1.47

longShallow <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 589000*5.1, 
                                 589000*5.1, 589000*5.1, 
                                 589000*5.1, 1470000*5.1, 
                                 1470000*5.1, 1470000*5.1), 
                          year = c(1, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (1*4*N0)/gen_year))

# shortSevere -en 0.33 0 0.033 -en 0.34 0 1.00

shortSevere <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 33000*5.1, 
                                 33000*5.1, 33000*5.1, 
                                 33000*5.1, 1000000*5.1, 
                                 1000000*5.1, 1000000*5.1), 
                          year = c(1, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (1*4*N0)/gen_year))

# discoal example -en 0.5 0 0.1 -en 1.2 0 0.8
# By way of example, the following command line specifies a bottleneck population history
# where at time 0.5 the population crashes to 10% of its initial size and then at time 1.2 it
# rebounds to 80% of its initial size

discoal_example <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                     1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.8*1000000*5.1), 
                              year = c(1, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (1.2*4*N0)/gen_year, 
                                       (1.2*4*N0)/gen_year, (1.2*4*N0)/gen_year))

# ms example -eN 0.2 .02
# For example, ms 10 30 -t 4.0 -eN 0.2 .02 specifies 
# that the population size was constant at size N0 from the present back to
# time 0.2  4N0, and farther back in time the population size was 0.02  4N0.
# The population size change was instantaneous and occurred at time 0.2  4N0
# generations before the present.

ms_example <- data.frame(Ne = c(N0*5.1, N0*5.1, 
                                N0*5.1, 0.02*N0*5.1, 
                                0.02*N0*5.1, 0.02*N0*5.1), 
                         year = c(1, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (1*4*N0)/gen_year))

# expansionNe -en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2

expansionNe2 <- data.frame(Ne = c(N0*5.1, N0*5.1, 
                                  N0*5.1, 1.43*N0*5.1, 
                                  1.43*N0*5.1, 1.43*N0*5.1, 
                                  1.43*N0*5.1, 1.57*N0*5.1, 
                                  1.57*N0*5.1, 1.57*N0*5.1, 
                                  1.57*N0*5.1, 1.67*N0*5.1, 
                                  1.67*N0*5.1, 1.67*N0*5.1, 
                                  1.67*N0*5.1, 1.39*N0*5.1, 
                                  1.39*N0*5.1, 1.39*N0*5.1, 
                                  1.39*N0*5.1, 0.67*N0*5.1, 
                                  0.67*N0*5.1, 0.67*N0*5.1, 
                                  0.67*N0*5.1, 0.33*N0*5.1, 
                                  0.33*N0*5.1, 0.33*N0*5.1, 
                                  0.33*N0*5.1, 0.22*N0*5.1, 
                                  0.22*N0*5.1, 0.22*N0*5.1), 
                           year = c(1, (0.00012*4*N0)/gen_year, 
                                    (0.00012*4*N0)/gen_year, (0.00012*4*N0)/gen_year, 
                                    (0.00012*4*N0)/gen_year, (0.0042*4*N0)/gen_year, 
                                    (0.0042*4*N0)/gen_year, (0.0042*4*N0)/gen_year, 
                                    (0.0042*4*N0)/gen_year, (0.010*4*N0)/gen_year, 
                                    (0.010*4*N0)/gen_year, (0.010*4*N0)/gen_year, 
                                    (0.010*4*N0)/gen_year, (0.022*4*N0)/gen_year, 
                                    (0.022*4*N0)/gen_year, (0.022*4*N0)/gen_year, 
                                    (0.022*4*N0)/gen_year, (0.026*4*N0)/gen_year, 
                                    (0.026*4*N0)/gen_year, (0.026*4*N0)/gen_year, 
                                    (0.026*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                    (0.11*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                    (0.11*4*N0)/gen_year, (0.49*4*N0)/gen_year, 
                                    (0.49*4*N0)/gen_year, (0.49*4*N0)/gen_year, 
                                    (0.49*4*N0)/gen_year, (1*4*N0)/gen_year))

p1 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_12.5%`)), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_87.5%`)), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=log10(year), y=log10(`Ne`), col="expansionNe")) + 
  # geom_line(data=longShallow, aes(x=log10(year), y=log10(`Ne`), col="longShallow")) + 
  # geom_line(data=shortSevere, aes(x=log10(year), y=log10(`Ne`), col="shortSevere")) + 
  # geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  # geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  geom_line(data=expansionNe2, aes(x=log10(year), y=log10(`Ne`), col="expansionNe2")) + 
  xlim(c(log10(1), log10(400000)))

# Convert ggplot to a Plotly object
p_plotly1 <- ggplotly(p1)

# Display the interactive plot
p_plotly1

p3 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_2.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_97.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_12.5%`), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_87.5%`), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=year, y=`Ne`, col="expansionNe")) + 
  # geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  # geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  # geom_line(data=discoal_example, aes(x=year, y=`Ne`, col="discoal_example")) + 
  # geom_line(data=ms_example, aes(x=year, y=`Ne`, col="ms_example")) + 
  geom_line(data=expansionNe2, aes(x=year, y=`Ne`, col="expansionNe2")) + 
  xlim(c(0, 400000))

# Convert ggplot to a Plotly object
p_plotly3 <- ggplotly(p3)

# Display the interactive plot
p_plotly3

```

## Intergenic region

```{bash, SNPdat to get intergenic}

## Install snpdat

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP

git clone https://github.com/agdoran/snpdat.git

cd snpdat

## Create input file

### A tab-delimited text file with chromosome_id, position, mutation (no header)

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/

vcf-concat HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf > HaplotypeData_GenotypeGVCFs_ALL_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpdat

cut -f 1,2,5 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ALL_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ALL_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

## Prepare genome fasta

cp /home/uqsalle3/shared/Reference_genome_2/dovetail_files/drosophila_06Jul2018_A8VGg.fasta ./
sed 's/\;HRSCAF\=.*//g' drosophila_06Jul2018_A8VGg.fasta > drosophila_06Jul2018_A8VGg_shortName.fasta
rm drosophila_06Jul2018_A8VGg.fasta

### Filter for chrom via VIM

## Prepare GTF

# cp /home/uqsalle3/shared/Reference_genome_2/annotation_transfer/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gtf ./
# sed 's/;HRSCAF=[0-9]\{1,\}\t/\t/' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName.gtf
# rm GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gtf

cp /home/uqsalle3/shared/Reference_genome_2/annotation_transfer/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gff ./
sed 's/;HRSCAF=[0-9]\{1,\}\t/\t/' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gff > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName.gff

~/shared/Programs/gffread-0.11.8.Linux_x86_64/gffread GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName.gff -T -o temp.gtf

grep 'CDS' temp.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf

grep -P '_542\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_542.gtf

grep -P '_594\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_594.gtf

grep -P '_628\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_628.gtf

grep -P '_718\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_718.gtf

grep -P '_76\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_76.gtf

grep -P '_785\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_785.gtf

## Run snpdat

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_542.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_542.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_542.log 2>&1 stderr_542.log &

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_594.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_594.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_594.log 2>&1 stderr_594.log &

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_628.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_628.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_628.log 2>&1 stderr_628.log &

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_718.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_718.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_718.log 2>&1 stderr_718.log &

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_76.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_76.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_76.log 2>&1 stderr_76.log &

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_785.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_785.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_785.log 2>&1 stderr_785.log &

```

```{r, predPeaks for constNe}

library(data.table)
library(dplyr)
library(stringr)
library(parallel)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe")

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

class2peaks <- function(dat){
  r <- rle(dat$State)
  rr <- data.frame("len" = r$lengths, "val" = r$values)
  rr$endWin <- cumsum(rr$len)
  rr$startWin <- lag(rr$endWin,1,0) + 1
  rr$start_pos <- dat[rr$startWin, 2]
  rr$end_pos <- dat[rr$endWin, 3]
  return(rr)
}

x <- preFiles[1]

main <- function(x) {
    aa <- fread(x, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    aa <- aa[,1:4]
    aa$t1 <- lag(aa$V2,1)
    aa$t2 <- aa$V2 - aa$t1
    # count gaps between wins
    aa$wingap <- aa$t2/5000
    aa$wingap[1] <- 1
    aa$win_seq <- 0
    win_s = 1
    # gap larger than 2*5000kbp were considered separately
    for(i in 1:nrow(aa)){
        if(aa[i,7]<=2){
            aa$win_seq[i] <- win_s
            }
        else{
            win_s <- win_s + 1
            aa$win_seq[i] <- win_s
        }
    }
    # for each segmentation, run class2peaks
    merge_return <- data.frame()
    for(j in unique(aa$win_seq)){
        sub_aa <- aa[aa$win_seq==j,]
        tmp <- class2peaks(sub_aa)
        tmp$scf <- str_split(x,"\\.")[[1]][1]
        tmp <- tmp[,c("scf","len","val","start_pos","end_pos")]
        tmp$win_seq <- j
        merge_return <- rbind(merge_return,tmp)
    }
    fwrite(merge_return, paste0(x,".peak"), col.names = T, row.names = F, sep = "\t", quote = F)
}

mclapply(preFiles, main, mc.cores = 6)



setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe")

for (scfid in c(542,594,628,718,76,785)) {
  peakFile <- paste0("ScA8VGg_",scfid,".train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed.peak")
  tempData <- fread(peakFile)
  if (scfid == 542) { predPeaks = tempData }
  else { predPeaks = rbind(predPeaks, tempData) }
}

predPeaks

fwrite(predPeaks, "predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv", col.names = T, row.names = F, sep = "\t", quote = F)

```

```{bash, intersect SNPdat with predPeaks}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpdat

module load bedtools

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.bed

# 542

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_542\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_542.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_542\t' > temp_SNPdat_542.txt

sed -i 's/SCA8VGG_542/ScA8VGg_542/g' temp_SNPdat_542.txt

bedtools intersect\
 -a temp_SNPdat_542.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_542.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 594

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_594\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_594.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_594\t' > temp_SNPdat_594.txt

sed -i 's/SCA8VGG_594/ScA8VGg_594/g' temp_SNPdat_594.txt

bedtools intersect\
 -a temp_SNPdat_594.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_594.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 628

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_628\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_628.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_628\t' > temp_SNPdat_628.txt

sed -i 's/SCA8VGG_628/ScA8VGg_628/g' temp_SNPdat_628.txt

bedtools intersect\
 -a temp_SNPdat_628.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_628.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 718

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_718\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_718.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_718\t' > temp_SNPdat_718.txt

sed -i 's/SCA8VGG_718/ScA8VGg_718/g' temp_SNPdat_718.txt

bedtools intersect\
 -a temp_SNPdat_718.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_718.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 76

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_76\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_76.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_76\t' > temp_SNPdat_76.txt

sed -i 's/SCA8VGG_76/ScA8VGg_76/g' temp_SNPdat_76.txt

bedtools intersect\
 -a temp_SNPdat_76.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_76.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 785

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_785\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_785.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_785\t' > temp_SNPdat_785.txt

sed -i 's/SCA8VGG_785/ScA8VGg_785/g' temp_SNPdat_785.txt

bedtools intersect\
 -a temp_SNPdat_785.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_785.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

```

```{shell, intersect SNPdat_predPeaks with recombination}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpdat

module load bedtools

# awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.bed
# 
# # 542
# 
# awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_542\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_542.bed
# 
# awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_542\t' > temp_SNPdat_542.txt
# 
# sed -i 's/SCA8VGG_542/ScA8VGg_542/g' temp_SNPdat_542.txt
# 
# bedtools intersect\
#  -a temp_SNPdat_542.txt\
#  -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_542.bed\
#  -wa\
#  -wb\
#  > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt
# 
# # 594
# 
# awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_594\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_594.bed
# 
# awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_594\t' > temp_SNPdat_594.txt
# 
# sed -i 's/SCA8VGG_594/ScA8VGg_594/g' temp_SNPdat_594.txt
# 
# bedtools intersect\
#  -a temp_SNPdat_594.txt\
#  -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_594.bed\
#  -wa\
#  -wb\
#  > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt
# 
# # 628
# 
# awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_628\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_628.bed
# 
# awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_628\t' > temp_SNPdat_628.txt
# 
# sed -i 's/SCA8VGG_628/ScA8VGg_628/g' temp_SNPdat_628.txt
# 
# bedtools intersect\
#  -a temp_SNPdat_628.txt\
#  -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_628.bed\
#  -wa\
#  -wb\
#  > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 718

# awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_718\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_718.bed

grep -v 'chrom' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/ReLERNN/DsGRP_partialSHIC_SNPs/DsGRP_partialSHIC_SNPs_scf718_output/HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.PREDICT.BSCORRECTED.txt | sed 's/b.*ScA8VGg/ScA8VGg/' | sed "s/718'/718/" > temp_recomb_718.txt

# awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_718\t' > temp_SNPdat_718.txt
# 
# sed -i 's/SCA8VGG_718/ScA8VGg_718/g' temp_SNPdat_718.txt

bedtools intersect\
 -a SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -b temp_recomb_718.txt\
 -wa\
 -wb\
 > SNPdat_predPeaks_recomb_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# # 76
# 
# awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_76\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_76.bed
# 
# awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_76\t' > temp_SNPdat_76.txt
# 
# sed -i 's/SCA8VGG_76/ScA8VGg_76/g' temp_SNPdat_76.txt
# 
# bedtools intersect\
#  -a temp_SNPdat_76.txt\
#  -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_76.bed\
#  -wa\
#  -wb\
#  > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt
# 
# # 785
# 
# awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_785\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_785.bed
# 
# awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_785\t' > temp_SNPdat_785.txt
# 
# sed -i 's/SCA8VGG_785/ScA8VGg_785/g' temp_SNPdat_785.txt
# 
# bedtools intersect\
#  -a temp_SNPdat_785.txt\
#  -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_785.bed\
#  -wa\
#  -wb\
#  > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

```

```{bash, eda}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpdat

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, eda}

library(tidyverse)

# Read SNPdat files

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpdat")

snpdat_files <- list.files(pattern = "^SNPdat_predPeaks_.*.txt$")

for (i in 1:length(snpdat_files)) {
  cat("Reading ", snpdat_files[i], "\n")
  tempData <- read_tsv(snpdat_files[i], col_names = FALSE)
  if(i==1) { snpdat <- tempData }
  if(i > 1) { snpdat <- rbind(snpdat, tempData) }
}

snpdat <- snpdat %>% mutate(X6 = as.numeric(X6))

# Read recombination

recomb <- read_tsv("SNPdat_predPeaks_recomb_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt", col_names = FALSE)

recomb %>% 
  select(X1, X2, X5, X6, X20, X26) %>% 
  distinct() %>% 
  ggplot(aes(x=X2, y=X26)) + 
  geom_line() + 
  xlab("scf718 position") + 
  ylab("Recombination rate")

recomb %>% 
  select(X1, X2, X5, X6, X20, X26) %>% 
  distinct() %>% 
  ggplot(aes(x=X26)) + 
  geom_histogram(bins=50) + 
  facet_wrap(~X20)

recomb %>% 
  select(X1, X2, X5, X6, X20, X26) %>% 
  distinct() %>% 
  ggplot(aes(x=X20, y=X26, col=X20)) + 
  geom_jitter(alpha=0.1, col="grey") + 
  geom_boxplot(fill = "transparent", alpha=0.75) + 
  ylab("Recombination Rate") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

recomb %>% 
  select(X1, X2, X5, X6, X20, X26) %>% 
  distinct() %>% 
  ggplot(aes(x=X20, y=X26, col=X20)) + 
  geom_jitter(alpha=0.1, col="grey") + 
  geom_violin(fill = "transparent", alpha=0.75) + 
  ylab("Recombination Rate") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

recomb %>% 
  select(X1, X2, X5, X6, X20, X26) %>% 
  distinct() %>% 
  group_by(X20) %>% 
  summarise(mean_recomb = mean(X26), 
            sd_recomb = sd(X26), 
            lci = mean_recomb - 1.96*(sd_recomb/sqrt(n())), 
            uci = mean_recomb + 1.96*(sd_recomb/sqrt(n()))) %>% 
  ggplot(aes(x=X20, y=mean_recomb)) + 
  geom_errorbar(aes(ymin = lci, ymax = uci), width=0.1)

recomb %>% 
  select(X1, X2, X5, X6, X20, X26) %>% 
  distinct() %>% 
  filter(X5=="Intergenic") %>% 
  filter(X6>=5000) %>% 
  filter(X26 >= 3e-09) %>% 
  group_by(X20) %>% 
  tally() %>% 
  ggplot(aes(x=X20, y=n)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))





snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  group_by(X20, X5) %>% 
  tally() %>% 
  ungroup() %>%
  pivot_wider(names_from=X5, values_from=n) %>% 
  mutate(Exonic_prop = Exonic/(Exonic+Intergenic+Intronic), 
         Intergenic_prop = Intergenic/(Exonic+Intergenic+Intronic), 
         Intronic_prop = Intronic/(Exonic+Intergenic+Intronic))

snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  filter(X5=="Intergenic") %>% 
  group_by(X20) %>% 
  summarise(mean_dist_CDS = mean(X6, na.rm=TRUE))

snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  filter(X5=="Intergenic") %>% 
  filter(X6>=5000) %>% 
  group_by(X20) %>% 
  tally()

snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  group_by(X20) %>% 
  tally() %>% 
  ggplot(aes(x=X20, y=n)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  filter(X5=="Exonic") %>% 
  group_by(X20) %>% 
  tally() %>% 
  ggplot(aes(x=X20, y=n)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  filter(X5=="Intronic") %>% 
  # filter(X6<=162) %>% 
  group_by(X20) %>%
  tally() %>%
  ggplot(aes(x=X20, y=n)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  filter(X5=="Intergenic") %>% 
  filter(X6>=5000) %>% 
  group_by(X20) %>% 
  tally() %>% 
  ggplot(aes(x=X20, y=n)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  filter(X5=="Intergenic") %>% 
  filter(X6>=10000) %>% 
  group_by(X20) %>% 
  tally() %>% 
  ggplot(aes(x=X20, y=n)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  filter(X5=="Intergenic") %>% 
  filter(X6>=20000) %>% 
  group_by(X20) %>% 
  tally() %>% 
  ggplot(aes(x=X20, y=n)) + 
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  group_by(X20, X5) %>%
  tally() %>% 
  ungroup() %>%
  pivot_wider(names_from=X5, values_from=n) %>% 
  mutate(Exonic_prop = Exonic/(Exonic+Intergenic+Intronic), 
         Intergenic_prop = Intergenic/(Exonic+Intergenic+Intronic), 
         Intronic_prop = Intronic/(Exonic+Intergenic+Intronic)) %>% 
  select(X20, Exonic_prop:Intronic_prop) %>% 
  pivot_longer(Exonic_prop:Intronic_prop, names_to="X5", values_to="n") %>% 
  ggplot(aes(x=X20, y=n, fill=X5)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



```{shell}

# cd ~/doveGenome/newAnno
# head -6 ../scf1mbp.txt > chromSizes.txt
# awk 'OFS="\t" {print $1, "0", $2}' chromSizes.txt | sort -k1,1 -k2,2n > chromSizes.bed
# 
# for ss in $(cut -f1 chromSizes.txt);do
#     echo ${ss}
#     awk -v a=${ss} '{if($1==a){print $0}}' new_annotation_woFasta.gff >> new_annotation_woFasta_major.gff
# done
# 
# 
# cat new_annotation_woFasta_major.gff | awk '$1 ~ /^#/ {print $0;next} {print $0 | "sort -k1,1 -k4,4n -k5,5n"}' > new_annotation_woFasta_major.sorted.gff
# bedtools complement -i new_annotation_woFasta_major.sorted.gff -g chromSizes.txt > intergenic_sorted.bed
# 
# 
# awk 'BEGIN{OFS="\t"}$1~/^#/ {print $0;next} {if ($3 == "exon") {print $1, $4-1, $5}}' new_annotation_woFasta_major.sorted.gff > exon_sorted.bed
# bedtools complement -i <(cat exon_sorted.bed intergenic_sorted.bed | sort -k1,1 -k2,2n) -g chromSizes.txt > intron_sorted.bed
# 
# awk '{a=$3-$2+1;if(a<120){print $0}}' intron_sorted.bed > short_intron_sorted.bed

```

```{shell}

# cd ~/myData/phd/p2/stairway
# 
# extractSNP(){
#     t1=${1##*/}
#     vid=${t1%%.*}_short_intron
#     vcftools --vcf ${1} --bed short_intron_sorted.bed \
#     --out ${vid} --recode --keep-INFO-all
#     bgzip -c ${vid}.recode.vcf > ${vid}.recode.vcf.gz
#     tabix -p vcf ${vid}.recode.vcf.gz
# }
# 
# export -f extractSNP
# 
# find ../rawData/ -maxdepth 1 -name "*ScA8VGg_*.vcf" | parallel -j 7 extractSNP

```

```{bash, install stairway_plot}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP

git clone https://github.com/xiaoming-liu/stairway-plot-v2.git

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2

unzip stairway_plot_v2.1.2.zip

```

```{r, bed file for regions}

library(tidyverse)
library(progress)
library(data.table)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpdat")

snpdat_files <- list.files(pattern = "^SNPdat_predPeaks_.*.txt$")

for (i in 1:length(snpdat_files)) {
  cat("Reading ", snpdat_files[i], "\n")
  tempData <- read_tsv(snpdat_files[i], col_names = FALSE)
  if(i==1) { snpdat <- tempData }
  if(i > 1) { snpdat <- rbind(snpdat, tempData) }
}

snpdat <- snpdat %>% mutate(X6 = as.numeric(X6))



# Intergenic 5kb

tempData <- snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  filter(X5=="Intergenic") %>% 
  filter(X6>=5000)

tempData <- tempData %>% 
  group_by(X1) %>% 
  mutate(lag_snp = X2 - lag(X2)) %>% 
  mutate(lag_snp = ifelse(is.na(lag_snp), 0, lag_snp)) %>% 
  ungroup() %>% 
  mutate(window = 0)

# # Create a progress bar
# pb <- progress_bar$new(
#   format = "[:bar] :percent ETA: :eta",
#   total = nrow(tempData)
# )
# 
# for (i in 1:nrow(tempData)) {
#   
#   pb$tick()  # Update the progress bar
#   
#   if(tempData$lag_snp[i] == 0) {
#     window <- 1
#     tempData$window[i] <- window
#   }
#   
#   else if(tempData$lag_snp[i] < 5000) {
#     tempData$window[i] <- window
#   }
#   
#   else if(tempData$lag_snp[i] >= 5000) {
#     window <- window + 1
#     tempData$window[i] <- window
#   }
#   
# }
# 
# # Close the progress bar
# pb$terminate()

# chatGPT based on my for loop, it's very fast but I'll need to run it by chromosome

windowData <- rbindlist(lapply(unique(tempData$X1), function(x) {
  temp <- tempData[tempData$X1 == x, ]
  temp$window <- 1 + cumsum(temp$lag_snp != 0 & temp$lag_snp >= 5000)
  temp
}))

bedFile <- windowData %>% 
  group_by(X1, window) %>% 
  summarise(start = min(X2), end = max(X2)) %>% 
  ungroup() %>% 
  select(X1, start, end)

write_tsv(bedFile, 
          file="ScA8VGg_ALL_intergenicSNPs.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed", 
          col_names = FALSE)



# Intergenic 10kb

tempData <- snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  filter(X5=="Intergenic") %>% 
  filter(X6>=10000)

tempData <- tempData %>% 
  group_by(X1) %>% 
  mutate(lag_snp = X2 - lag(X2)) %>% 
  mutate(lag_snp = ifelse(is.na(lag_snp), 0, lag_snp)) %>% 
  ungroup() %>% 
  mutate(window = 0)

# # Create a progress bar
# pb <- progress_bar$new(
#   format = "[:bar] :percent ETA: :eta",
#   total = nrow(tempData)
# )
# 
# for (i in 1:nrow(tempData)) {
#   
#   pb$tick()  # Update the progress bar
#   
#   if(tempData$lag_snp[i] == 0) {
#     window <- 1
#     tempData$window[i] <- window
#   }
#   
#   else if(tempData$lag_snp[i] < 5000) {
#     tempData$window[i] <- window
#   }
#   
#   else if(tempData$lag_snp[i] >= 5000) {
#     window <- window + 1
#     tempData$window[i] <- window
#   }
#   
# }
# 
# # Close the progress bar
# pb$terminate()

# chatGPT based on my for loop, it's very fast but I'll need to run it by chromosome

windowData <- rbindlist(lapply(unique(tempData$X1), function(x) {
  temp <- tempData[tempData$X1 == x, ]
  temp$window <- 1 + cumsum(temp$lag_snp != 0 & temp$lag_snp >= 5000)
  temp
}))

bedFile <- windowData %>% 
  group_by(X1, window) %>% 
  summarise(start = min(X2), end = max(X2)) %>% 
  ungroup() %>% 
  select(X1, start, end)

write_tsv(bedFile, 
          file="ScA8VGg_ALL_intergenicSNPs_10kb.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed", 
          col_names = FALSE)

```

```{bash, total loci}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpdat

# 5kb

awk -F'\t' '{print $0, $3 - $2}' ScA8VGg_ALL_intergenicSNPs.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed | awk '{sum += $4} END {print "Sum:", sum}'

# 27,461,057

# 10kb

awk -F'\t' '{print $0, $3 - $2}' ScA8VGg_ALL_intergenicSNPs_10kb.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed | awk '{sum += $4} END {print "Sum:", sum}'

# 17,095,935

```


```{bash, filter vcfs}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2

# 5kb

bed_file="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpdat/ScA8VGg_ALL_intergenicSNPs.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed"

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode



# 10kb

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2

bed_file="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpdat/ScA8VGg_ALL_intergenicSNPs_10kb.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed"

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

```

### Estimate  SFS

#### VCF to SFS

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```{r}

library(data.table)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2")

# 5kb

vcfs <- list.files(pattern = 'HaplotypeData_.*_intergenicSNPs.recode.vcf$')

st_dt <- data.table()

vcf <- vcfs[1]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[2]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[3]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[4]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[5]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[6]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

st_dt[alt_adj>55, alt_adj:=110-alt_adj]

library(tidyverse)

sfs <- st_dt %>% 
  filter(scf!="ScA8VGg_549") %>% 
  group_by(alt_adj) %>% 
  tally() %>% 
  filter(alt_adj!=0)

sfs_out <- paste0(as.integer(sfs$n),collapse = " ")
writeLines(sfs_out, "all_autosomal_intergenicSNPs.sfs")



# 10kb

vcfs <- list.files(pattern = 'HaplotypeData_.*_intergenicSNPs_10kb.recode.vcf$')

st_dt <- data.table()

vcf <- vcfs[1]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[2]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[3]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[4]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[5]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[6]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs_10kb.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

st_dt[alt_adj>55, alt_adj:=110-alt_adj]

library(tidyverse)

sfs <- st_dt %>% 
  filter(scf!="ScA8VGg_549") %>% 
  group_by(alt_adj) %>% 
  tally() %>% 
  filter(alt_adj!=0)

sfs_out <- paste0(as.integer(sfs$n),collapse = " ")
writeLines(sfs_out, "all_autosomal_intergenicSNPs_10kb.sfs")

```

### Predict demography

```{shell}

# 5kb

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2

cp two-epoch_Neutral_constNe.blueprint two-epoch_intergenicSNPs.blueprint

# vim two-epoch_intergenicSNPs.blueprint

# #example blueprint file
# #input setting
# popid: intergenicSNPs # id of the population (no white space)
# nseq: 110 # number of sequences
# L: 61155000 # total number of observed nucleic sites, including polymorphic and monomorphic
# whether_folded: true # whethr the SFS is folded (true or false)
# SFS: 2234234 752294 381413 238466 165399 123033 96943 79224 65694 55754 48340 42535 37939 33923 30683 27993 25838 23996 22218 20654 19495 18417 17131 16818 15685 14981 14318 13784 13377 13021 12472 12093 11749 11387 11003 10746 10637 10587 10297 9854 10004 9777 9467 9431 9467 9314 9103 9085 9265 9005 8952 8782 8759 8725 4482 # snp frequency spectrum: number of singleton, number of doubleton, etc. (separated by white space)
# #smallest_size_of_SFS_bin_used_for_estimation: 1 # default is 1; to ignore singletons, uncomment this line and change this number to 2
# #largest_size_of_SFS_bin_used_for_estimation: 29 # default is nseq/2 for folded SFS
# pct_training: 0.67 # percentage of sites for training
# nrand: 27       54      81      108 # number of random break points for each try (separated by white space)
# project_dir: intergenicSNPs # project directory
# stairway_plot_dir: stairway_plot_es # directory to the stairway plot files
# ninput: 1000 # number of input files to be created for each estimation
# random_seed: 186457
# #output setting
# mu: 2.8e-9 # assumed mutation rate per site per generation
# year_per_generation: 0.067 # assumed generation time (in years)
# #plot setting
# plot_title: intergenicSNPs # title of the plot
# xrange: 0.1,400 # Time (1k year) range; format: xmin,xmax; "0,0" for default
# yrange: 0,0 # Ne (1k individual) range; format: xmin,xmax; "0,0" for default
# xspacing: 2 # X axis spacing
# yspacing: 2 # Y axis spacing
# fontsize: 12 # Font size

java -cp stairway_plot_es Stairbuilder two-epoch_intergenicSNPs.blueprint # do no change file name
# bash two-epoch_intergenicSNPs.blueprint.sh

# Parallel

grep 'java -Xmx1g -cp stairway_plot_es/:stairway_plot_es/swarmops.jar Stairway_fold_training_testing7 intergenicSNPs/input/intergenicSNPs' two-epoch_intergenicSNPs.blueprint.sh > Stairway_fold_training_testing7_intergenicSNPs.list

parallel -j 38 -a Stairway_fold_training_testing7_intergenicSNPs.list

grep 'mv -f' two-epoch_intergenicSNPs.blueprint.sh > Stairway_fold_mv_intergenicSNPs.list

bash Stairway_fold_mv_intergenicSNPs.list

java -Xmx1g -cp stairway_plot_es/ Stairpainter two-epoch_intergenicSNPs.blueprint

bash two-epoch_intergenicSNPs.blueprint.plot.sh

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2/intergenicSNPs

evince intergenicSNPs.final.summary.pdf



# 5kb

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2

cp two-epoch_Neutral_constNe.blueprint two-epoch_intergenicSNPs_10kb.blueprint

# vim two-epoch_intergenicSNPs_10kb.blueprint

# #example blueprint file
# #input setting
# popid: intergenicSNPs_10kb # id of the population (no white space)
# nseq: 110 # number of sequences
# L: 17095935 # total number of observed nucleic sites, including polymorphic and monomorphic
# whether_folded: true # whethr the SFS is folded (true or false)
# SFS: 537323 178815 88965 55547 38520 29012 22855 18411 15196 12877 11072 9916 8895 7752 7073 6302 5755 5549 5099 4654 4381 4240 3884 3737 3499 3305 3206 3132 2991 2868 2862 2661 2596 2616 2599 2525 2347 2433 2282 2311 2289 2170 2166 2103 2055 2098 2108 2033 1997 1981 2022 1985 1885 1885 917 # snp frequency spectrum: number of singleton, number of doubleton, etc. (separated by white space)
# #smallest_size_of_SFS_bin_used_for_estimation: 1 # default is 1; to ignore singletons, uncomment this line and change this number to 2
# #largest_size_of_SFS_bin_used_for_estimation: 29 # default is nseq/2 for folded SFS
# pct_training: 0.67 # percentage of sites for training
# nrand: 27       54      81      108 # number of random break points for each try (separated by white space)
# project_dir: intergenicSNPs_10kb # project directory
# stairway_plot_dir: stairway_plot_es # directory to the stairway plot files
# ninput: 1000 # number of input files to be created for each estimation
# random_seed: 186457
# #output setting
# mu: 2.8e-9 # assumed mutation rate per site per generation
# year_per_generation: 0.067 # assumed generation time (in years)
# #plot setting
# plot_title: intergenicSNPs_10kb # title of the plot
# xrange: 0.1,400 # Time (1k year) range; format: xmin,xmax; "0,0" for default
# yrange: 0,0 # Ne (1k individual) range; format: xmin,xmax; "0,0" for default
# xspacing: 2 # X axis spacing
# yspacing: 2 # Y axis spacing
# fontsize: 12 # Font size

java -cp stairway_plot_es Stairbuilder two-epoch_intergenicSNPs_10kb.blueprint # do no change file name
# bash two-epoch_intergenicSNPs.blueprint.sh

# Parallel

grep 'java -Xmx1g -cp stairway_plot_es/:stairway_plot_es/swarmops.jar Stairway_fold_training_testing7 intergenicSNPs_10kb/input/intergenicSNPs_10kb' two-epoch_intergenicSNPs_10kb.blueprint.sh > Stairway_fold_training_testing7_intergenicSNPs_10kb.list

parallel -j 38 -a Stairway_fold_training_testing7_intergenicSNPs_10kb.list

grep 'mv -f' two-epoch_intergenicSNPs_10kb.blueprint.sh > Stairway_fold_mv_intergenicSNPs_10kb.list

bash Stairway_fold_mv_intergenicSNPs_10kb.list

java -Xmx1g -cp stairway_plot_es/ Stairpainter two-epoch_intergenicSNPs_10kb.blueprint

bash two-epoch_intergenicSNPs_10kb.blueprint.plot.sh

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2/intergenicSNPs_10kb

evince intergenicSNPs_10kb.final.summary.pdf

```

```{powershell}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2/intergenicSNPs/intergenicSNPs.final.summary* ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2/intergenicSNPs_10kb/intergenicSNPs_10kb.final.summary* ./

```


### Assess 

* Run by Scott

* Yiguan's demography doesn't match Stairway Plot 2.

```{r}

library(tidyverse)
library(plotly)

setwd("C:\\Users\\scott\\OneDrive - The University of Queensland\\Documents\\Stephen_Chenoweth\\Yiguan\\partialSHIC\\stairway\\stairway-plot-v2\\stairway_plot_v2.1.1\\two-epoch")

sp2_summary <- read_tsv("dser_two-epoch_fold.final.summary")
sp2_summary %>% head(n=20)

sp2_Neutral_constNe <- read_tsv("C:\\Users\\scott\\Dropbox\\PhD_Docu_Yiguan\\sweep_paper_draft\\MolEcol_Submission\\Neutral_constNe.final.summary")
sp2_Neutral_constNe %>% tail(n=20)

sp2_intergenicSNPs <- read_tsv("C:\\Users\\scott\\Dropbox\\PhD_Docu_Yiguan\\sweep_paper_draft\\MolEcol_Submission\\intergenicSNPs.final.summary")
sp2_intergenicSNPs %>% tail(n=20)

sp2_intergenicSNPs_10kb <- read_tsv("C:\\Users\\scott\\Dropbox\\PhD_Docu_Yiguan\\sweep_paper_draft\\MolEcol_Submission\\intergenicSNPs_10kb.final.summary")
sp2_intergenicSNPs_10kb %>% tail(n=20)

```

```{r}

# Constants

N0 <- 1000000
gen_year <- 15

# expansionNe -en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2

expansionNe <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, # contant
                                 1000000*5.1, 1200000*5.1, # contraction
                                 1200000*5.1, 1200000*5.1, # constant
                                 1200000*5.1, 1600000*5.1, # contraction
                                 1600000*5.1, 1600000*5.1, # constant
                                 1600000*5.1, 1200000*5.1, # expansion
                                 1200000*5.1, 1200000*5.1, # constant
                                 700000*5.1, 700000*5.1, # expansion
                                 700000*5.1, 700000*5.1, # constant
                                 400000*5.1, 400000*5.1, # expansion
                                 400000*5.1, 400000*5.1, # constant
                                 240000*5.1, 240000*5.1, # expansion
                                 240000*5.1, 240000*5.1, # constant
                                 200000*5.1, 200000*5.1, # expansion
                                 200000*5.1, 200000*5.1), # constant
                          year = c(1, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (1*4*N0)/gen_year))

# longShallow -en 0.16 0 0.589 -en 0.216 0 1.47

longShallow <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 589000*5.1, 
                                 589000*5.1, 589000*5.1, 
                                 589000*5.1, 1470000*5.1, 
                                 1470000*5.1, 1470000*5.1), 
                          year = c(1, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (1*4*N0)/gen_year))

# shortSevere -en 0.33 0 0.033 -en 0.34 0 1.00

shortSevere <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 33000*5.1, 
                                 33000*5.1, 33000*5.1, 
                                 33000*5.1, 1000000*5.1, 
                                 1000000*5.1, 1000000*5.1), 
                          year = c(1, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (1*4*N0)/gen_year))

# discoal example -en 0.5 0 0.1 -en 1.2 0 0.8
# By way of example, the following command line specifies a bottleneck population history
# where at time 0.5 the population crashes to 10% of its initial size and then at time 1.2 it
# rebounds to 80% of its initial size

discoal_example <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                     1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.8*1000000*5.1), 
                              year = c(1, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (1.2*4*N0)/gen_year, 
                                       (1.2*4*N0)/gen_year, (1.2*4*N0)/gen_year))

# ms example -eN 0.2 .02
# For example, ms 10 30 -t 4.0 -eN 0.2 .02 specifies 
# that the population size was constant at size N0 from the present back to
# time 0.2  4N0, and farther back in time the population size was 0.02  4N0.
# The population size change was instantaneous and occurred at time 0.2  4N0
# generations before the present.

ms_example <- data.frame(Ne = c(N0*5.1, N0*5.1, 
                                N0*5.1, 0.02*N0*5.1, 
                                0.02*N0*5.1, 0.02*N0*5.1), 
                         year = c(1, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (1*4*N0)/gen_year))

p1 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_12.5%`)), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_87.5%`)), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=log10(year), y=log10(`Ne`), col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=log10(year), y=log10(`Ne`), col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=log10(year), y=log10(`Ne`), col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=log10(year), y=log10(Ne_median), col="sp2_Neutral_constNe")) + 
  xlim(c(log10(1), log10(400000)))

# Convert ggplot to a Plotly object
p_plotly1 <- ggplotly(p1)

# Display the interactive plot
p_plotly1

p2 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="Stairway2")) + 
  geom_line() + 
  geom_line(data=expansionNe, aes(x=log10(year), y=log10(`Ne`), col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=log10(year), y=log10(`Ne`), col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=log10(year), y=log10(`Ne`), col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  xlim(c(log10(1), log10(400000)))

# Convert ggplot to a Plotly object
p_plotly2 <- ggplotly(p2)

# Display the interactive plot
p_plotly2

p3 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_2.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_97.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_12.5%`), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_87.5%`), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=year, y=`Ne`, col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  xlim(c(0, 400000))

# Convert ggplot to a Plotly object
p_plotly3 <- ggplotly(p3)

# Display the interactive plot
p_plotly3

p4 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="Stairway2")) + 
  geom_line() + 
  geom_line(data=expansionNe, aes(x=year, y=`Ne`, col="expansionNe")) + 
  geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  xlim(c(0, 400000))

# Convert ggplot to a Plotly object
p_plotly4 <- ggplotly(p4)

# Display the interactive plot
p_plotly4

```

#### Better match

```{r}

# Constants

N0 <- 1000000
gen_year <- 15

# expansionNe -en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2

expansionNe <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, # contant
                                 1000000*5.1, 1200000*5.1, # contraction
                                 1200000*5.1, 1200000*5.1, # constant
                                 1200000*5.1, 1600000*5.1, # contraction
                                 1600000*5.1, 1600000*5.1, # constant
                                 1600000*5.1, 1200000*5.1, # expansion
                                 1200000*5.1, 1200000*5.1, # constant
                                 700000*5.1, 700000*5.1, # expansion
                                 700000*5.1, 700000*5.1, # constant
                                 400000*5.1, 400000*5.1, # expansion
                                 400000*5.1, 400000*5.1, # constant
                                 240000*5.1, 240000*5.1, # expansion
                                 240000*5.1, 240000*5.1, # constant
                                 200000*5.1, 200000*5.1, # expansion
                                 200000*5.1, 200000*5.1), # constant
                          year = c(1, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00015*4*N0)/gen_year, 
                                   (0.00015*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.00075*4*N0)/gen_year, 
                                   (0.00075*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.0045*4*N0)/gen_year, 
                                   (0.0045*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.006*4*N0)/gen_year, 
                                   (0.006*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.03*4*N0)/gen_year, 
                                   (0.03*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                   (0.11*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (0.175*4*N0)/gen_year, 
                                   (0.175*4*N0)/gen_year, (1*4*N0)/gen_year))

# longShallow -en 0.16 0 0.589 -en 0.216 0 1.47

longShallow <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 589000*5.1, 
                                 589000*5.1, 589000*5.1, 
                                 589000*5.1, 1470000*5.1, 
                                 1470000*5.1, 1470000*5.1), 
                          year = c(1, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (1*4*N0)/gen_year))

# shortSevere -en 0.33 0 0.033 -en 0.34 0 1.00

shortSevere <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                 1000000*5.1, 33000*5.1, 
                                 33000*5.1, 33000*5.1, 
                                 33000*5.1, 1000000*5.1, 
                                 1000000*5.1, 1000000*5.1), 
                          year = c(1, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (1*4*N0)/gen_year))

# discoal example -en 0.5 0 0.1 -en 1.2 0 0.8
# By way of example, the following command line specifies a bottleneck population history
# where at time 0.5 the population crashes to 10% of its initial size and then at time 1.2 it
# rebounds to 80% of its initial size

discoal_example <- data.frame(Ne = c(1000000*5.1, 1000000*5.1, 
                                     1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.1*1000000*5.1, 
                                     0.1*1000000*5.1, 0.8*1000000*5.1), 
                              year = c(1, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (0.5*4*N0)/gen_year, 
                                       (0.5*4*N0)/gen_year, (1.2*4*N0)/gen_year, 
                                       (1.2*4*N0)/gen_year, (1.2*4*N0)/gen_year))

# ms example -eN 0.2 .02
# For example, ms 10 30 -t 4.0 -eN 0.2 .02 specifies 
# that the population size was constant at size N0 from the present back to
# time 0.2  4N0, and farther back in time the population size was 0.02  4N0.
# The population size change was instantaneous and occurred at time 0.2  4N0
# generations before the present.

ms_example <- data.frame(Ne = c(N0*5.1, N0*5.1, 
                                N0*5.1, 0.02*N0*5.1, 
                                0.02*N0*5.1, 0.02*N0*5.1), 
                         year = c(1, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (0.2*4*N0)/gen_year, 
                                  (0.2*4*N0)/gen_year, (1*4*N0)/gen_year))

# expansionNe -en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2

expansionNe2 <- data.frame(Ne = c(N0*5.1, N0*5.1, 
                                  N0*5.1, 1.43*N0*5.1, 
                                  1.43*N0*5.1, 1.43*N0*5.1, 
                                  1.43*N0*5.1, 1.57*N0*5.1, 
                                  1.57*N0*5.1, 1.57*N0*5.1, 
                                  1.57*N0*5.1, 1.67*N0*5.1, 
                                  1.67*N0*5.1, 1.67*N0*5.1, 
                                  1.67*N0*5.1, 1.39*N0*5.1, 
                                  1.39*N0*5.1, 1.39*N0*5.1, 
                                  1.39*N0*5.1, 0.67*N0*5.1, 
                                  0.67*N0*5.1, 0.67*N0*5.1, 
                                  0.67*N0*5.1, 0.33*N0*5.1, 
                                  0.33*N0*5.1, 0.33*N0*5.1, 
                                  0.33*N0*5.1, 0.22*N0*5.1, 
                                  0.22*N0*5.1, 0.22*N0*5.1), 
                           year = c(1, (0.00012*4*N0)/gen_year, 
                                    (0.00012*4*N0)/gen_year, (0.00012*4*N0)/gen_year, 
                                    (0.00012*4*N0)/gen_year, (0.0042*4*N0)/gen_year, 
                                    (0.0042*4*N0)/gen_year, (0.0042*4*N0)/gen_year, 
                                    (0.0042*4*N0)/gen_year, (0.010*4*N0)/gen_year, 
                                    (0.010*4*N0)/gen_year, (0.010*4*N0)/gen_year, 
                                    (0.010*4*N0)/gen_year, (0.022*4*N0)/gen_year, 
                                    (0.022*4*N0)/gen_year, (0.022*4*N0)/gen_year, 
                                    (0.022*4*N0)/gen_year, (0.026*4*N0)/gen_year, 
                                    (0.026*4*N0)/gen_year, (0.026*4*N0)/gen_year, 
                                    (0.026*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                    (0.11*4*N0)/gen_year, (0.11*4*N0)/gen_year, 
                                    (0.11*4*N0)/gen_year, (0.49*4*N0)/gen_year, 
                                    (0.49*4*N0)/gen_year, (0.49*4*N0)/gen_year, 
                                    (0.49*4*N0)/gen_year, (1*4*N0)/gen_year))

p1 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_12.5%`)), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_87.5%`)), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=log10(year), y=log10(`Ne`), col="expansionNe")) + 
  # geom_line(data=longShallow, aes(x=log10(year), y=log10(`Ne`), col="longShallow")) + 
  # geom_line(data=shortSevere, aes(x=log10(year), y=log10(`Ne`), col="shortSevere")) + 
  # geom_line(data=discoal_example, aes(x=log10(year), y=log10(`Ne`), col="discoal_example")) + 
  # geom_line(data=ms_example, aes(x=log10(year), y=log10(`Ne`), col="ms_example")) + 
  geom_line(data=expansionNe2, aes(x=log10(year), y=log10(`Ne`), col="expansionNe2")) + 
  xlim(c(log10(1), log10(400000)))

# Convert ggplot to a Plotly object
p_plotly1 <- ggplotly(p1)

# Display the interactive plot
p_plotly1

p3 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="Stairway2")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_2.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_97.5%`), col="lightgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_12.5%`), col="darkgrey") + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_87.5%`), col="darkgrey") + 
  geom_line(data=expansionNe, aes(x=year, y=`Ne`, col="expansionNe")) + 
  # geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  # geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  # geom_line(data=discoal_example, aes(x=year, y=`Ne`, col="discoal_example")) + 
  # geom_line(data=ms_example, aes(x=year, y=`Ne`, col="ms_example")) + 
  geom_line(data=expansionNe2, aes(x=year, y=`Ne`, col="expansionNe2")) + 
  xlim(c(0, 400000))

# Convert ggplot to a Plotly object
p_plotly3 <- ggplotly(p3)

# Display the interactive plot
p_plotly3

```

#### Compare StairwayPlot 2 demographies

```{r}

p1 <- sp2_summary %>% 
  ggplot(aes(x=log10(year), y=log10(Ne_median), col="ShortIntron")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="skyblue3", alpha=0.15) + 
  geom_line(data=sp2_summary, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="skyblue3", alpha=0.15) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=log10(year), y=log10(Ne_median), col="Neutral_constNe")) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="mediumturquoise", alpha=0.15) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="mediumturquoise", alpha=0.15) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=log10(year), y=log10(Ne_median), col="intergenic_SNPs")) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="tomato", alpha=0.15) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="tomato", alpha=0.15) + 
  geom_line(data=sp2_intergenicSNPs_10kb, aes(x=log10(year), y=log10(Ne_median), col="intergenic_SNPs_10kb")) + 
  geom_line(data=sp2_intergenicSNPs_10kb, aes(x=log10(year), y=log10(`Ne_2.5%`)), col="gold4", alpha=0.15) + 
  geom_line(data=sp2_intergenicSNPs_10kb, aes(x=log10(year), y=log10(`Ne_97.5%`)), col="gold4", alpha=0.15) + 
  geom_line(data=longShallow, aes(x=log10(year), y=log10(`Ne`), col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=log10(year), y=log10(`Ne`), col="shortSevere")) + 
  xlim(c(log10(1), log10(150000)))

# Convert ggplot to a Plotly object
p_plotly1 <- ggplotly(p1)

# Display the interactive plot
p_plotly1



p2 <- sp2_summary %>% 
  ggplot(aes(x=year, y=Ne_median, col="ShortIntron")) + 
  geom_line() + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_2.5%`), col="skyblue3", alpha=0.25) + 
  geom_line(data=sp2_summary, aes(x=year, y=`Ne_97.5%`), col="skyblue3", alpha=0.25) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=year, y=Ne_median, col="Neutral_constNe")) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=year, y=`Ne_2.5%`), col="mediumturquoise", alpha=0.25) + 
  geom_line(data=sp2_Neutral_constNe, aes(x=year, y=`Ne_97.5%`), col="mediumturquoise", alpha=0.25) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=Ne_median, col="intergenic_SNPs")) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_2.5%`), col="tomato", alpha=0.25) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_97.5%`), col="tomato", alpha=0.25) + 
  geom_line(data=sp2_intergenicSNPs_10kb, aes(x=year, y=Ne_median, col="intergenic_SNPs_10kb")) + 
  geom_line(data=sp2_intergenicSNPs_10kb, aes(x=year, y=`Ne_2.5%`), col="gold4", alpha=0.25) + 
  geom_line(data=sp2_intergenicSNPs_10kb, aes(x=year, y=`Ne_97.5%`), col="gold4", alpha=0.25) + 
  geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  xlim(c(1, 150000))

# Convert ggplot to a Plotly object
p_plotly2 <- ggplotly(p2)

# Display the interactive plot
p_plotly2

```

#### Simplify for discoal

```{r}

# Constants

N0 <- 3607692
gen_year <- 15

intergenicSNPs_simple <- data.frame(Ne = c(N0, 1.5*N0, 
                                  1.5*N0, 1.5*N0, 
                                  1.5*N0, 0.547*N0, 
                                  0.547*N0, 0.547*N0, 
                                  0.547*N0, 0.208*N0, 
                                  0.208*N0, 0.208*N0, 
                                  0.208*N0, 0.0805*N0, 
                                  0.0805*N0, 0.0805*N0), 
                           year = c(1, 1, 
                                    1, (0.007292*4*N0)/gen_year, 
                                    (0.007292*4*N0)/gen_year, (0.007292*4*N0)/gen_year, 
                                    (0.007292*4*N0)/gen_year, (0.02135*4*N0)/gen_year, 
                                    (0.02135*4*N0)/gen_year, (0.02135*4*N0)/gen_year, 
                                    (0.02135*4*N0)/gen_year, (0.05208*4*N0)/gen_year, 
                                    (0.05208*4*N0)/gen_year, (0.05208*4*N0)/gen_year, 
                                    (0.05208*4*N0)/gen_year, (0.09271*4*N0)/gen_year))

intergenicSNPs <- sp2_intergenicSNPs %>% 
  select(year, Ne_median) %>% 
  mutate(Ne_change = Ne_median - lag(Ne_median)) %>% 
  filter(Ne_change!=0) %>% 
  mutate(Ne_factor = Ne_median/N0) %>% 
  mutate(year_4N0 = (year*gen_year)/(4*N0))

# intergenicSNPs_frac <- sp2_intergenicSNPs %>%
#   select(year, Ne_median) %>%
#   mutate(Ne_change = Ne_median - lag(Ne_median, 10)) %>%
#   filter(Ne_change!=0) %>%
#   mutate(Ne_factor = Ne_median/N0) %>%
#   mutate(year_4N0 = (year*gen_year)/(4*N0)) %>%
#   filter(abs(Ne_change)>=10000) %>% 
#   mutate(Ne_factor=signif(Ne_factor, 3), 
#          year_4N0=signif(year_4N0, 3)) %>% 
#   select(-year, -Ne_median, -Ne_change) %>% 
#   distinct() %>% 
#   mutate(Ne2 = Ne_factor*N0) %>%
#   mutate(year2 = (year_4N0*4*N0)/gen_year)

intergenicSNPs_frac <- sp2_intergenicSNPs %>%
  select(year, Ne_median) %>%
  mutate(Ne_change10 = Ne_median - lag(Ne_median, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = Ne_median/N0) %>%
  mutate(year_4N0 = (year*gen_year)/(4*N0)) %>%
  filter(abs(Ne_change10)>=10000) %>% 
  mutate(Ne_change1 = Ne_median - lag(Ne_median, 1)) %>%
  filter(abs(Ne_change1) > 0) %>% 
  mutate(Ne2 = Ne_factor*N0) %>%
  mutate(year2 = (year_4N0*4*N0)/gen_year)

intergenicSNPs_frac_double <- intergenicSNPs_frac %>% 
  mutate(Ne_factor2 = 2*Ne_factor) %>% 
  mutate(Ne3 = Ne_factor2*N0)

intergenicSNPs_frac_half <- intergenicSNPs_frac %>% 
  mutate(Ne_factor2 = Ne_factor/2) %>% 
  mutate(Ne3 = Ne_factor2*N0)

intergenicSNPs_frac_lci <- sp2_intergenicSNPs %>%
  select(year, `Ne_2.5%`) %>%
  mutate(Ne_change10 = `Ne_2.5%` - lag(`Ne_2.5%`, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = `Ne_2.5%`/1041174) %>%
  mutate(year_4N0 = (year*gen_year)/(4*1041174)) %>%
  filter(abs(Ne_change10)>=25000) %>% 
  mutate(Ne_change1 = `Ne_2.5%` - lag(`Ne_2.5%`, 1)) %>%
  filter(abs(Ne_change1) > 0) %>% 
  mutate(Ne2 = Ne_factor*1041174) %>%
  mutate(year2 = (year_4N0*4*1041174)/gen_year)

intergenicSNPs_frac_uci <- sp2_intergenicSNPs %>%
  select(year, `Ne_97.5%`) %>%
  mutate(Ne_change10 = `Ne_97.5%` - lag(`Ne_97.5%`, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = `Ne_97.5%`/5182173) %>%
  mutate(year_4N0 = (year*gen_year)/(4*5182173)) %>%
  filter(abs(Ne_change10)>=50000) %>% 
  mutate(Ne_change1 = `Ne_97.5%` - lag(`Ne_97.5%`, 1)) %>%
  filter(abs(Ne_change1) > 0) %>% 
  mutate(Ne2 = Ne_factor*5182173) %>%
  mutate(year2 = (year_4N0*4*5182173)/gen_year)


# longShallow -en 0.16 0 0.589 -en 0.216 0 1.47

longShallow <- data.frame(Ne = c(N0, N0, 
                                 N0, 0.589*N0, 
                                 0.589*N0, 0.589*N0, 
                                 0.589*N0, 1.47*N0, 
                                 1.47*N0, 1.47*N0), 
                          year = c(1, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (0.999*4*N0)/gen_year))

# shortSevere -en 0.33 0 0.033 -en 0.34 0 1.00

shortSevere <- data.frame(Ne = c(N0, N0, 
                                 N0, 0.033*N0, 
                                 0.033*N0, 0.033*N0, 
                                 0.033*N0, N0, 
                                 N0, N0), 
                          year = c(1, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (0.999*4*N0)/gen_year))

# constNe

constNe <- data.frame(Ne = c(N0, N0), 
                          year = c(1, (0.999*4*N0)/gen_year))



p1 <- sp2_intergenicSNPs %>% 
  ggplot(aes(x=year, y=Ne_median, col="SP2_intergenic_SNPs")) + 
  geom_line() + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_2.5%`), col="mediumturquoise", alpha=0.25) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_97.5%`), col="mediumturquoise", alpha=0.25) + 
  geom_line(data=intergenicSNPs_simple, aes(x=year, y=`Ne`, col="Simplified_intergenic_SNPs"))
  xlim(c(1, 100000))

# Convert ggplot to a Plotly object
p_plotly1 <- ggplotly(p1)

# Display the interactive plot
p_plotly1



p2 <- sp2_intergenicSNPs %>% 
  ggplot(aes(x=year, y=Ne_median, col="SP2_intergenic_SNPs")) + 
  geom_line() + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_2.5%`), col="hotpink", alpha=0.5) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_97.5%`), col="hotpink", alpha=0.5) + 
  # geom_line(data=intergenicSNPs_simple, aes(x=year, y=`Ne`, col="Simplified_intergenic_SNPs")) + 
  # geom_line(data=intergenicSNPs, aes(x=year, y=Ne_median, col="intergenic_SNPs")) + 
  geom_line(data=intergenicSNPs_frac, aes(x=year2, y=Ne2, col="intergenicSNPs_frac")) + 
  geom_line(data=intergenicSNPs_frac_lci, aes(x=year2, y=Ne2, col="intergenicSNPs_frac_lci")) + 
  geom_line(data=intergenicSNPs_frac_uci, aes(x=year2, y=Ne2, col="intergenicSNPs_frac_uci")) + 
  # geom_line(data=intergenicSNPs_frac_double, aes(x=year2, y=Ne3, col="intergenicSNPs_frac_dbl")) + 
  # geom_line(data=intergenicSNPs_frac_half, aes(x=year2, y=Ne3, col="intergenicSNPs_frac_half")) +
  # geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  # geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  # geom_line(data=constNe, aes(x=year, y=`Ne`, col="constNe")) + 
  xlim(c(0, 100000))

# Convert ggplot to a Plotly object
p_plotly2 <- ggplotly(p2)

# Display the interactive plot
p_plotly2

```

#### ########## Figure for paper

```{r}

# Constants

N0 <- 3607692
gen_year <- 15

intergenicSNPs_simple <- data.frame(Ne = c(N0, 1.5*N0, 
                                  1.5*N0, 1.5*N0, 
                                  1.5*N0, 0.547*N0, 
                                  0.547*N0, 0.547*N0, 
                                  0.547*N0, 0.208*N0, 
                                  0.208*N0, 0.208*N0, 
                                  0.208*N0, 0.0805*N0, 
                                  0.0805*N0, 0.0805*N0), 
                           year = c(1, 1, 
                                    1, (0.007292*4*N0)/gen_year, 
                                    (0.007292*4*N0)/gen_year, (0.007292*4*N0)/gen_year, 
                                    (0.007292*4*N0)/gen_year, (0.02135*4*N0)/gen_year, 
                                    (0.02135*4*N0)/gen_year, (0.02135*4*N0)/gen_year, 
                                    (0.02135*4*N0)/gen_year, (0.05208*4*N0)/gen_year, 
                                    (0.05208*4*N0)/gen_year, (0.05208*4*N0)/gen_year, 
                                    (0.05208*4*N0)/gen_year, (0.09271*4*N0)/gen_year))

intergenicSNPs <- sp2_intergenicSNPs %>% 
  select(year, Ne_median) %>% 
  mutate(Ne_change = Ne_median - lag(Ne_median)) %>% 
  filter(Ne_change!=0) %>% 
  mutate(Ne_factor = Ne_median/N0) %>% 
  mutate(year_4N0 = (year*gen_year)/(4*N0))

# intergenicSNPs_frac <- sp2_intergenicSNPs %>%
#   select(year, Ne_median) %>%
#   mutate(Ne_change = Ne_median - lag(Ne_median, 10)) %>%
#   filter(Ne_change!=0) %>%
#   mutate(Ne_factor = Ne_median/N0) %>%
#   mutate(year_4N0 = (year*gen_year)/(4*N0)) %>%
#   filter(abs(Ne_change)>=10000) %>% 
#   mutate(Ne_factor=signif(Ne_factor, 3), 
#          year_4N0=signif(year_4N0, 3)) %>% 
#   select(-year, -Ne_median, -Ne_change) %>% 
#   distinct() %>% 
#   mutate(Ne2 = Ne_factor*N0) %>%
#   mutate(year2 = (year_4N0*4*N0)/gen_year)

intergenicSNPs_frac <- sp2_intergenicSNPs %>%
  select(year, Ne_median) %>%
  mutate(Ne_change10 = Ne_median - lag(Ne_median, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = Ne_median/N0) %>%
  mutate(year_4N0 = (year*gen_year)/(4*N0)) %>%
  filter(abs(Ne_change10)>=10000) %>% 
  mutate(Ne_change1 = Ne_median - lag(Ne_median, 1)) %>%
  filter(abs(Ne_change1) > 0) %>% 
  mutate(Ne2 = Ne_factor*N0) %>%
  mutate(year2 = (year_4N0*4*N0)/gen_year)

intergenicSNPs_frac_double <- intergenicSNPs_frac %>% 
  mutate(Ne_factor2 = 2*Ne_factor) %>% 
  mutate(Ne3 = Ne_factor2*N0)

intergenicSNPs_frac_half <- intergenicSNPs_frac %>% 
  mutate(Ne_factor2 = Ne_factor/2) %>% 
  mutate(Ne3 = Ne_factor2*N0)

intergenicSNPs_frac_lci <- sp2_intergenicSNPs %>%
  select(year, `Ne_2.5%`) %>%
  mutate(Ne_change10 = `Ne_2.5%` - lag(`Ne_2.5%`, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = `Ne_2.5%`/1041174) %>%
  mutate(year_4N0 = (year*gen_year)/(4*1041174)) %>%
  filter(abs(Ne_change10)>=25000) %>% 
  mutate(Ne_change1 = `Ne_2.5%` - lag(`Ne_2.5%`, 1)) %>%
  filter(abs(Ne_change1) > 0) %>% 
  mutate(Ne2 = Ne_factor*1041174) %>%
  mutate(year2 = (year_4N0*4*1041174)/gen_year)

intergenicSNPs_frac_uci <- sp2_intergenicSNPs %>%
  select(year, `Ne_97.5%`) %>%
  mutate(Ne_change10 = `Ne_97.5%` - lag(`Ne_97.5%`, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = `Ne_97.5%`/5182173) %>%
  mutate(year_4N0 = (year*gen_year)/(4*5182173)) %>%
  filter(abs(Ne_change10)>=50000) %>% 
  mutate(Ne_change1 = `Ne_97.5%` - lag(`Ne_97.5%`, 1)) %>%
  filter(abs(Ne_change1) > 0) %>% 
  mutate(Ne2 = Ne_factor*5182173) %>%
  mutate(year2 = (year_4N0*4*5182173)/gen_year)


# longShallow -en 0.16 0 0.589 -en 0.216 0 1.47

longShallow <- data.frame(Ne = c(N0, N0, 
                                 N0, 0.589*N0, 
                                 0.589*N0, 0.589*N0, 
                                 0.589*N0, 1.47*N0, 
                                 1.47*N0, 1.47*N0), 
                          year = c(1, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.16*4*N0)/gen_year, 
                                   (0.16*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (0.216*4*N0)/gen_year, 
                                   (0.216*4*N0)/gen_year, (0.999*4*N0)/gen_year))

# shortSevere -en 0.33 0 0.033 -en 0.34 0 1.00

shortSevere <- data.frame(Ne = c(N0, N0, 
                                 N0, 0.033*N0, 
                                 0.033*N0, 0.033*N0, 
                                 0.033*N0, N0, 
                                 N0, N0), 
                          year = c(1, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.33*4*N0)/gen_year, 
                                   (0.33*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (0.34*4*N0)/gen_year, 
                                   (0.34*4*N0)/gen_year, (0.999*4*N0)/gen_year))

# constNe

constNe <- data.frame(Ne = c(N0, N0), 
                          year = c(1, (0.999*4*N0)/gen_year))

```

```{r}

p2 <- sp2_intergenicSNPs %>% 
  ggplot(aes(x=year, y=Ne_median, col="SP2_intergenic_SNPs")) + 
  geom_line() + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_2.5%`), col="tomato", alpha=0.5) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_97.5%`), col="tomato", alpha=0.5) + 
  # geom_line(data=intergenicSNPs_simple, aes(x=year, y=`Ne`, col="Simplified_intergenic_SNPs")) + 
  # geom_line(data=intergenicSNPs, aes(x=year, y=Ne_median, col="intergenic_SNPs")) + 
  # geom_line(data=intergenicSNPs_frac, aes(x=year2, y=Ne2, col="intergenicSNPs_frac")) + 
  # geom_line(data=intergenicSNPs_frac_lci, aes(x=year2, y=Ne2, col="intergenicSNPs_frac_lci")) + 
  # geom_line(data=intergenicSNPs_frac_uci, aes(x=year2, y=Ne2, col="intergenicSNPs_frac_uci")) + 
  # geom_line(data=intergenicSNPs_frac_double, aes(x=year2, y=Ne3, col="intergenicSNPs_frac_dbl")) + 
  # geom_line(data=intergenicSNPs_frac_half, aes(x=year2, y=Ne3, col="intergenicSNPs_frac_half")) +
  # geom_line(data=longShallow, aes(x=year, y=`Ne`, col="longShallow")) + 
  # geom_line(data=shortSevere, aes(x=year, y=`Ne`, col="shortSevere")) + 
  geom_line(data=constNe, aes(x=year, y=`Ne`, col="constNe")) + 
  xlim(c(0, 100000))

# Convert ggplot to a Plotly object
p_plotly2 <- ggplotly(p2)

# Display the interactive plot
p_plotly2

```

```{r}

N0 <- 1000000
constNe <- sp2_intergenicSNPs
constNe <- constNe %>% 
  mutate(Ne = N0)

p3 <- sp2_intergenicSNPs %>% 
  ggplot(aes(x=year, y=Ne_median, col="StairwayPlot2 median")) + 
  geom_line() + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_2.5%`, col="StairwayPlot2 lower quantile")) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_97.5%`, col="StairwayPlot2 upper quantile")) + 
  geom_line(data=constNe, aes(x=year, y=`Ne`, col="Constant")) + 
  xlim(c(0, 100000)) + 
  xlab("Years since present") +
  ylab("Effective population size (Ne)") +
  theme_classic()

p3

setwd("C:\\Users\\scott\\Dropbox\\PhD_Docu_Yiguan\\sweep_paper_draft\\MolEcol_Submission")
ggsave("Demography_for_paper.png", width=8, height=4)
ggsave("Demography_for_paper.pdf", width=8, height=4)

```

```{r}

intergenicSNPs_frac_lci
intergenicSNPs_frac_uci

```


#### Save in -en format for discoal

```{r}

options(scipen = 999)

setwd("C:\\Users\\scott\\Dropbox\\PhD_Docu_Yiguan\\sweep_paper_draft\\MolEcol_Submission")

# en <- sp2_intergenicSNPs %>%
#   select(year, Ne_median) %>%
#   mutate(Ne_change = Ne_median - lag(Ne_median)) %>%
#   filter(Ne_change!=0) %>%
#   mutate(Ne_factor = Ne_median/N0) %>%
#   mutate(year_4N0 = (year*gen_year)/(4*N0)) %>%
#   mutate(Ne2 = Ne_factor*N0) %>%
#   mutate(year2 = (year_4N0*4*N0)/gen_year)

# en <- sp2_intergenicSNPs %>%
#   select(year, Ne_median) %>%
#   mutate(Ne_change = Ne_median - lag(Ne_median)) %>%
#   filter(Ne_change!=0) %>%
#   mutate(Ne_factor = Ne_median/N0) %>%
#   mutate(year_4N0 = (year*gen_year)/(4*N0)) %>%
#   mutate(Ne2 = Ne_factor*N0) %>%
#   mutate(year2 = (year_4N0*4*N0)/gen_year) %>%
#   sample_frac(size=0.01) %>%
#   arrange(year)

# en <- sp2_intergenicSNPs %>%
#   select(year, Ne_median) %>%
#   mutate(Ne_change = Ne_median - lag(Ne_median, 10)) %>%
#   filter(Ne_change!=0) %>%
#   mutate(Ne_factor = Ne_median/N0) %>%
#   mutate(year_4N0 = (year*gen_year)/(4*N0)) %>%
#   filter(abs(Ne_change)>=10000) %>% 
#   mutate(Ne_factor=signif(Ne_factor, 3), 
#          year_4N0=signif(year_4N0, 3)) %>% 
#   select(-year, -Ne_median, -Ne_change) %>% 
#   distinct() %>% 
#   mutate(Ne2 = Ne_factor*N0) %>%
#   mutate(year2 = (year_4N0*4*N0)/gen_year)

en <- sp2_intergenicSNPs %>%
  select(year, Ne_median) %>%
  mutate(Ne_change10 = Ne_median - lag(Ne_median, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = Ne_median/N0) %>%
  mutate(year_4N0 = (year*gen_year)/(4*N0)) %>%
  filter(abs(Ne_change10)>=10000) %>% 
  mutate(Ne_change1 = Ne_median - lag(Ne_median, 1)) %>%
  filter(abs(Ne_change1) > 0)

file_conn <- file("intergenicSNPs.final.en_for_discoal.txt", "w")

for (i in 1:nrow(en)) {
  cat("-en", en$year_4N0[i], "0", en$Ne_factor[i], "", file = file_conn)
}

close(file_conn)

```

```{r}
options(scipen = 999)

setwd("C:\\Users\\scott\\Dropbox\\PhD_Docu_Yiguan\\sweep_paper_draft\\MolEcol_Submission")

intergenicSNPs_frac_lci <- sp2_intergenicSNPs %>%
  select(year, `Ne_2.5%`) %>%
  mutate(Ne_change10 = `Ne_2.5%` - lag(`Ne_2.5%`, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = `Ne_2.5%`/1041174) %>%
  mutate(year_4N0 = (year*gen_year)/(4*1041174)) %>%
  filter(abs(Ne_change10)>=25000) %>% 
  mutate(Ne_change1 = `Ne_2.5%` - lag(`Ne_2.5%`, 1)) %>%
  filter(abs(Ne_change1) > 0) %>% 
  mutate(Ne2 = Ne_factor*1041174) %>%
  mutate(year2 = (year_4N0*4*1041174)/gen_year)

file_conn <- file("intergenicSNPs.final.en_for_discoal_lci.txt", "w")

for (i in 1:nrow(intergenicSNPs_frac_lci)) {
  cat("-en", intergenicSNPs_frac_lci$year_4N0[i], "0", intergenicSNPs_frac_lci$Ne_factor[i], "", file = file_conn)
}

close(file_conn)

getwd()

```

```{r}
options(scipen = 999)

setwd("C:\\Users\\scott\\Dropbox\\PhD_Docu_Yiguan\\sweep_paper_draft\\MolEcol_Submission")

intergenicSNPs_frac_uci <- sp2_intergenicSNPs %>%
  select(year, `Ne_97.5%`) %>%
  mutate(Ne_change10 = `Ne_97.5%` - lag(`Ne_97.5%`, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = `Ne_97.5%`/5182173) %>%
  mutate(year_4N0 = (year*gen_year)/(4*5182173)) %>%
  filter(abs(Ne_change10)>=50000) %>% 
  mutate(Ne_change1 = `Ne_97.5%` - lag(`Ne_97.5%`, 1)) %>%
  filter(abs(Ne_change1) > 0) %>% 
  mutate(Ne2 = Ne_factor*5182173) %>%
  mutate(year2 = (year_4N0*4*5182173)/gen_year)

file_conn <- file("intergenicSNPs.final.en_for_discoal_uci.txt", "w")

for (i in 1:nrow(intergenicSNPs_frac_uci)) {
  cat("-en", intergenicSNPs_frac_uci$year_4N0[i], "0", intergenicSNPs_frac_uci$Ne_factor[i], "", file = file_conn)
}

close(file_conn)

getwd()

```

# ######################## partialS/HIC

## conda

```{shell}

# Dell 2

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC
module load anaconda3
source ~/conda-init
conda create -n partialshic_py3
conda activate partialshic_py3
conda install pip
pip install gcc
#pip install diploshic
pip install numpy
python setup.py install
pip install h5py
pip install scikit-allel
pip install matplotlib
pip install pandas
#conda install -c conda-forge h5py
#conda install -c conda-forge scikit-allel
#pip install scikit-allel
#cd ~/shared/Scott_Allen/partialSHIC
pip install scikit-learn
pip install tensorflow # installs keras
pip install np_utils # NOT NEEDED



# Bunya

cd /home/uqsalle3/Programs/partialSHIC

module load anaconda3
source ~/conda-init
conda create -n partialshic_py3
conda activate partialshic_py3
conda install pip
#pip install diploshic
pip install numpy
module load gcc
python setup.py install # WARN: Could not locate executable armflang
pip install h5py scikit-allel matplotlib pandas scipy scikit-learn
module load intel-compilers
pip install tensorflow
# verify tensorflow CPU install
python3 -c "import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
pip install keras



module load anaconda3
source ~/conda-init
conda activate partialshic_py3

```

## Empirical feature vectors

### RepeatMasker

```{shell, RepeatMasker on Dell 1 NOT USED}

cd /data/shared/Reference_genome_2/RepeatMasker

mkdir -p /data/shared/Reference_genome_2/RepeatMasker/drosophila_06Jul2018_A8VGg_maskWithX_RUSH

RepeatMasker\
 -parallel 28\
 -qq\
 -species drosophila\
 -x\
 -gff\
 -dir "/data/shared/Reference_genome_2/RepeatMasker/drosophila_06Jul2018_A8VGg_maskWithX_RUSH"\
 /data/shared/Reference_genome_2/dovetail_files/drosophila_06Jul2018_A8VGg.fasta

RepeatMasker\
 -parallel 28\
 -qq\
 -species drosophila\
 -div 10\
 -x\
 -gff\
 -dir "/data/shared/Reference_genome_2/RepeatMasker/drosophila_06Jul2018_A8VGg_maskWithX_RUSH_DIV10"\
 /data/shared/Reference_genome_2/dovetail_files/drosophila_06Jul2018_A8VGg.fasta

RepeatMasker\
 -parallel 28\
 -qq\
 -species drosophila\
 -div 15\
 -x\
 -gff\
 -dir "/data/shared/Reference_genome_2/RepeatMasker/drosophila_06Jul2018_A8VGg_maskWithX_RUSH_DIV15"\
 /data/shared/Reference_genome_2/dovetail_files/drosophila_06Jul2018_A8VGg.fasta

```

### Convert vcf to h5 format

* Rename contigs to match top6.anc.fa and top6.mask.fa

```{shell, NOT USED}

# cd ~/myData/phd/p2/discoal_simu_partialSHIC/empiricalDat
# 
# cp ../../ihs/ScA8VGg_*.filtered.vcf.gz .
# sed -i 's/\(ScA8VGg_[0-9]*\);HRSCAF=[0-9]*/\1/g' top6.anc.fa
# sed -i 's/\(ScA8VGg_[0-9]*\);HRSCAF=[0-9]*/\1/g' top6.mask.fa
# 
# for ff in $(ls *filtered.vcf.gz);do
# echo ${ff}
# mv ${ff} ${ff/.vcf.gz/.longScfName.vcf.gz}
# zcat ${ff/.vcf.gz/.longScfName.vcf.gz} | sed 's/\(ScA8VGg_[0-9]*\);HRSCAF=[0-9]*/\1/g' | gzip >  ${ff}
# mv ${ff/.vcf.gz/.longScfName.vcf.gz}
# done
# 
# # zcat ScA8VGg_718.filtered.vcf.gz | sed 's/\(ScA8VGg_[0-9]*\);HRSCAF=[0-9]*/\1/g' >  ScA8VGg_718.filtered.vcf

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/

rsync -ahPv uqsalle3@bunya.rcc.uq.edu.au:/QRISdata/Q1013/sci-data05/Yiguan/partialSHIC/discoal_simu_partialSHIC/empiricalDat/top6* ./
  
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

zcat ../HaplotypeCaller/snps_PASS_BIALLELIC_homoRefs.vcf.gz |\
 sed 's/\(ScA8VGg_[0-9]*\)_HRSCAF_[0-9]*/\1/g' |\
 gzip > snps_PASS_BIALLELIC_homoRefs_shortScfName.vcf.gz

zcat ../HaplotypeCaller/snps_PASS.vcf.gz |\
 sed 's/\(ScA8VGg_[0-9]*\)_HRSCAF_[0-9]*/\1/g' |\
 gzip > snps_PASS_shortScfName.vcf.gz

zcat ../HaplotypeCaller/snps_PASS_BIALLELIC.vcf.gz |\
 sed 's/\(ScA8VGg_[0-9]*\)_HRSCAF_[0-9]*/\1/g' |\
 gzip > snps_PASS_BIALLELIC_shortScfName.vcf.gz

zcat ../HaplotypeCaller/snps_PASS_BIALLELIC_MULTIALLELIC_homoRefs.vcf.gz |\
 sed 's/\(ScA8VGg_[0-9]*\)_HRSCAF_[0-9]*/\1/g' |\
 gzip > snps_PASS_BIALLELIC_MULTIALLELIC_homoRefs_shortScfName.vcf.gz

zcat snps_PASS_shortScfName.vcf.gz |\
 sed 's/\|/\//g' |\
 gzip > snps_PASS_shortScfName_UNPHASED.vcf.gz

zcat snps_PASS_shortScfName_UNPHASED.vcf.gz |\
 grep -v -P '\*' |\
 gzip > snps_PASS_shortScfName_UNPHASED_noAstrix.vcf.gz

zcat snps_PASS_BIALLELIC_shortScfName.vcf.gz |\
 grep -v -P '\*' |\
 gzip > snps_PASS_BIALLELIC_shortScfName_noAstericks.vcf.gz

zcat snps_PASS_BIALLELIC_shortScfName_noAstericks.vcf.gz |\
 grep -v -P '\t\.\/\.' |\
 grep -v -P '\.\|\.' |\
 gzip > snps_PASS_BIALLELIC_shortScfName_noAstericks_noMissing.vcf.gz

```
    
### Empirical to FVs

```{shell, get vcf files}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

rsync -ahPv\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./
rsync -ahPv\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./
rsync -ahPv\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./
rsync -ahPv\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./
rsync -ahPv\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./
rsync -ahPv\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./

```

```{shell, short name}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/

sed 's/_HRSCAF_.*//' drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta > drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

nohup zcat GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit

sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_shortName.vcf



nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

```

```{shell, rerun failed}

# 594 failed at 17190001-17195000

nohup bcftools view\
 -i 'POS >= 17165001'\
 -O z -o GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef_shortName_SUBSET.vcf.gz\
 GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz &

# 76 failed at 35955001-35960000 

nohup bcftools view\
 -i 'POS >= 35930001'\
 -O z -o GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef_shortName_SUBSET.vcf.gz\
 GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz &

# 785 failed at 24200001-24205000



```

```{shell, sample_pops}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

zcat GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED.vcf.gz |\
 grep '#CHROM' |\
 cut -f 10-121 |\
 sed 's/\t/ dser\n/g' |\
 sed 's/line97/line97 dser/' > samples_pops.txt

```

#### Regular

```{python}

import gzip

def parse_msOut(msOut_filename):
    # Initialize lists to store variant information
    positions = []
    alleles = []
    samples = []
    # Read and parse the msOut.gz file
    with gzip.open(msOut_filename, 'rt') as f:
        for line in f:
            line = line.strip()
            if line.startswith('positions:'):
                positions = [int(pos) for pos in line.split()[1:]]
            elif line.startswith('segsites:'):
                num_segsites = int(line.split()[1])
            elif line.startswith('positions'):
                samples = line.split()[2:]
            # Assuming each line in the 'snp' section has the format: "snp [allele1] [allele2]"
            elif line.startswith('snp'):
                _, *alleles_data = line.split()
                alleles.append(alleles_data)
    return positions, alleles, samples

def write_vcf(output_filename, positions, alleles, samples):
    # VCF header
    vcf_header = '''\
##fileformat=VCFv4.2
##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">
#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\t{}\n'''.format('\t'.join(samples))

    # Write VCF data
    with open(output_filename, 'w') as f:
        f.write(vcf_header)
        for i, pos in enumerate(positions):
            ref, alt = alleles[i]
            gt_values = [f"{int(allele1)}/{int(allele2)}" for allele1, allele2 in alleles[i]]
            vcf_line = f"chr1\t{pos}\t.\t{ref}\t{alt}\t.\t.\t.\tGT\t{'\t'.join(gt_values)}\n"
            f.write(vcf_line)

if __name__ == "__main__":
    # Replace 'your_msOut.gz' with the actual filename of your msOut.gz file
    msOut_filename = 'your_msOut.gz'
    output_vcf_filename = 'output.vcf'

    positions, alleles, samples = parse_msOut(trainingDataFileName)
    write_vcf(output_vcf_filename, positions, alleles, samples)

```

```{python, regular empirical_convert_to_FVs_python3.py}
import time
#startTime=time.clock()
startTime = time.perf_counter() # SCOTT
import sys
# import h5py
import allel
from fvTools import *
import numpy as np
import math
import multiprocessing
import copy

'''usage eg:
pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
python2 empirical_convert_to_FVs.py ag1000g.phase1.ar3.haplotypes.2L.h5 2L 49364325 1 5000000 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.2L.fa samples_pops.txt AOM 2L.1.stats 2L.1.fvec
'''

# if not len(sys.argv) in [15,17]:
#   sys.exit("usage:\npython2 empirical_convert_to_FVs.py chrArmFileName chrArm chrLen [segmentStart segmentEnd] subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName sampleToPopFileName targetPop statFileName fvecFileName\n")
# if len(sys.argv)==17:
#   chrArmFileName, chrArm, chrLen, segmentStart, segmentEnd, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
# else:
#   chrArmFileName, chrArm, chrLen, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
#   segmentStart=None

# 718 Yiguan VCF and mask

# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_542.filtered.bi.h5'
chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_718.filtered.bi.vcf.gz'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/snps_PASS_BIALLELIC_homoRefs_shortScfName.vcf.gz'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz'
chrArm='ScA8VGg_718'
chrLen='8653123'
# segmentStart=1
segmentStart=None
# segmentEnd=5050000
segmentEnd=None
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt'
maskFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.mask.fa'
# maskFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta'
ancestralArmFaFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa'
sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/samples_pops.txt'
# sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt'
targetPop="dser"
statFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.1.yiguan.test.stats'
fvecFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.1.yiguan.test.fvec'

# 718 Scott VCF

# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_542.filtered.bi.h5'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_718.filtered.bi.vcf.gz'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/snps_PASS_BIALLELIC_homoRefs_shortScfName.vcf.gz'
chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz'
chrArm='ScA8VGg_718'
chrLen='8653123'
# segmentStart=1
segmentStart=None
# segmentEnd=5050000
segmentEnd=None
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt'
maskFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta'
ancestralArmFaFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa'
# sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/samples_pops.txt'
sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt'
targetPop="dser"
statFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.1.scott.test.stats'
fvecFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.1.scott.test.fvec'

# 542

# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_542.filtered.bi.h5'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_718.filtered.bi.vcf.gz'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/snps_PASS_BIALLELIC_homoRefs_shortScfName.vcf.gz'
chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz'
chrArm='ScA8VGg_542'
chrLen='21028053'
# segmentStart=1
segmentStart=None
# segmentEnd=5050000
segmentEnd=None
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt'
maskFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta'
ancestralArmFaFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa'
# sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/samples_pops.txt'
sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt'
targetPop="dser"
statFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542.1.scott.test.stats'
fvecFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542.1.scott.test.fvec'

# 594

# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_542.filtered.bi.h5'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_718.filtered.bi.vcf.gz'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/snps_PASS_BIALLELIC_homoRefs_shortScfName.vcf.gz'
chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz'
chrArm='ScA8VGg_594'
chrLen='30326886'
# segmentStart=1
segmentStart=None
# segmentEnd=5050000
segmentEnd=None
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt'
maskFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta'
ancestralArmFaFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa'
# sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/samples_pops.txt'
sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt'
targetPop="dser"
statFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594.1.scott.test.stats'
fvecFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594.1.scott.test.fvec'

# 594 rerun failed

# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_542.filtered.bi.h5'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_718.filtered.bi.vcf.gz'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/snps_PASS_BIALLELIC_homoRefs_shortScfName.vcf.gz'
chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef_shortName_SUBSET.vcf.gz'
chrArm='ScA8VGg_594'
chrLen='30326886'
segmentStart=17165001
# segmentStart=None
segmentEnd=30325000
# segmentEnd=None
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt'
maskFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta'
ancestralArmFaFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa'
# sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/samples_pops.txt'
# sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt'
sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt'
targetPop="dser"
statFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594.1.scott.test.stats'
fvecFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594.1.scott.test.fvec'

print("Reading data...") # SCOTT

# chrArmFile=h5py.File(chrArmFileName,"r")
chrArmFile=allel.read_vcf(chrArmFileName)
#genos=allel.GenotypeChunkedArray(chrArmFile[chrArm]["calldata"]["genotype"])
# genos = chrArmFile['calldata/GT'] # SCOTT HDF5
genos = allel.GenotypeArray(chrArmFile['calldata/GT']) # SCOTT VCF
# positions=allel.SortedIndex(chrArmFile["/%s/variants/POS" %(chrArm)][:])
# positions=chrArmFile['variants/POS'] # SCOTT HDF5
positions=allel.SortedIndex(chrArmFile['variants/POS'], copy=False) # SCOTT VCF
# refAlleles=chrArmFile[chrArm]['variants']['REF']
refAlleles=chrArmFile['variants/REF'] # SCOTT
# altAlleles=chrArmFile[chrArm]['variants']['ALT']
altAlleles=chrArmFile['variants/ALT'] # SCOTT
# samples=chrArmFile[chrArm]["samples"]
samples=chrArmFile['samples'] # SCOTT

chrLen=int(chrLen)
assert chrLen>0

print(chrArm + " is " + str(chrLen) + " basepairs, " + "Genotyped " + str(len(genos))) # SCOTT

if segmentStart!=None:
  segmentStart,segmentEnd = int(segmentStart),int(segmentEnd)
  assert segmentStart>0 and segmentEnd>=segmentStart
  snpIndicesToKeep=[x for x in range(len(positions)) if segmentStart<=positions[x]<=segmentEnd]
  genos=allel.GenotypeArray(genos.subset(sel0=snpIndicesToKeep))
  positions=[positions[x] for x in snpIndicesToKeep]
  refAlleles=[refAlleles[x] for x in snpIndicesToKeep]
  altAlleles=[altAlleles[x] for x in snpIndicesToKeep]


subWinSize,numSubWins,unmaskedFracCutoff,pMisPol = int(subWinSize),int(numSubWins),float(unmaskedFracCutoff),float(pMisPol)
assert subWinSize>0 and numSubWins>1

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")


if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")

standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none","false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*chrLen
  sys.stderr.write("Warning: a mask.fa file for the chr arm with all masked sites N'ed out is strongly recommended (pass in the reference to remove Ns at the very least)!\n")
else:
  unmasked=readMaskDataForScan(maskFileName,chrArm)
  assert len(unmasked)==chrLen

print(chrArm + " is " + str(chrLen) + " basepairs, " + "unmasked " + str(sum(unmasked)) + " basepairs") # SCOTT

print("Subset samples...")

def readSampleToPopFile(sampleToPopFileName):
  table={}
  with open(sampleToPopFileName) as sampleToPopFile:
    for line in sampleToPopFile:
      sample,pop = line.strip().split()
      table[sample]=pop
  return table

sampleToPop=readSampleToPopFile(sampleToPopFileName)

sampleIndicesToKeep=[x for x in range(len(samples)) if sampleToPop.get(samples[x],"popNotFound!")==targetPop]

genos=genos.subset(sel1=sampleIndicesToKeep)

# for i in range(len(genos)):
#   print(str(i) + " " + str(genos[i].count_called()) + " " + str(genos[i].count_het()))

print("Create alleleCounts and isBiallelic...")

alleleCounts=genos.count_alleles() # Count the number of calls of each allele per variant.

isBiallelic=alleleCounts.is_biallelic() # Find variants biallelic for the reference (0) and first alternate (1) allele.

sum(isBiallelic)

# remove indels and multiallelic
for i in range(len(isBiallelic)):
  if unmasked[positions[i]-1]==False:
    isBiallelic[i]=False

sum(isBiallelic)

# filter for biallelic SNPs, 100% genotyped, no heterozygotes

## mask < 100% genotyped or heterozygous

print("Masking not 100% genotyped and at least 1 heterozygote...")

genosRate100Mask_hetMask = []

# Define a worker function
def biallelic_mask(chunk):
    results = []
    for i in chunk:
        count_called = genos[i].count_called()
        genotypingRate = count_called/110
        count_het = genos[i].count_het()
        if genotypingRate < 1:
            out = False
        # elif count_het > 0:
        #     out = False
        else:
            out = True
        results.append(out)
    return results

# Split the indices into smaller chunks
chunk_size = 100000  # Adjust the chunk size as needed
indices = range(len(genos))
chunks = [indices[i:i + chunk_size] for i in range(0, len(indices), chunk_size)]

# Create a pool of worker processes
pool = multiprocessing.Pool()

# Use the pool.map() function to distribute the workload across multiple processes
results = pool.map(biallelic_mask, chunks)

# Flatten the results
genosRate100Mask_hetMask = [item for sublist in results for item in sublist]

# Close the pool to free up resources
pool.close()
pool.join()

sum(genosRate100Mask_hetMask)

## merge with unmasked

# for i in range(len(genosRate100Mask_hetMask)):
#   if unmasked[i]==False and genosRate100Mask_hetMask[i]==True:
#     # print(str(i) + " " + str(unmasked[i]) + " " + str(genosRate100Mask_hetMask[i]))
#     genosRate100Mask_hetMask[i]=False

genosRate100Mask_hetMask_2 = copy.copy(unmasked)

range(len(unmasked))
range(len(genosRate100Mask_hetMask))
range(len(positions))
range(len(genosRate100Mask_hetMask_2))
sum(unmasked)
sum(genosRate100Mask_hetMask_2)
sum(genosRate100Mask_hetMask)

for i in range(len(genosRate100Mask_hetMask)):
  if unmasked[positions[i]-1]==True and genosRate100Mask_hetMask[i]==False:
    genosRate100Mask_hetMask_2[positions[i]-1]=False

len(unmasked)
len(genosRate100Mask_hetMask_2)
sum(unmasked)
sum(genosRate100Mask_hetMask_2)

range(len(isBiallelic))
range(len(genosRate100Mask_hetMask_2))

## mask non biallelic
for i in range(len(isBiallelic)):
  if not isBiallelic[i]:
    genosRate100Mask_hetMask_2[positions[i]-1]=False

# ## mask non biallelic
# for i in range(len(isBiallelic)):
#   if genosRate100Mask_hetMask_2[positions[i]-1]==False:
#     isBiallelic[i]=False

sum(genosRate100Mask_hetMask_2)

#for i in range(len(isBiallelic)):
#  if not isBiallelic[i]:
#    unmasked[positions[i]-1]=False

# snpIndicesToKeep=[x for x in range(len(positions)) if unmasked[positions[x]-1]]
# genos=genos.subset(sel0=snpIndicesToKeep)
# haps=genos.to_haplotypes()
# alleleCounts=allel.AlleleCountsArray([alleleCounts[x] for x in snpIndicesToKeep])
# mapping=[mapping[x] for x in snpIndicesToKeep]
# alleleCounts=alleleCounts.map_alleles(mapping)
# positions=[positions[x] for x in snpIndicesToKeep]

snpIndicesToKeep=[x for x in range(len(positions)) if genosRate100Mask_hetMask_2[positions[x]-1]] # SCOTT
# snpIndicesToKeep=[x for x in range(len(positions)) if isBiallelic[x]] # SCOTT

# snpIndicesToKeep = []
# 
# for x in range(len(positions)):
#     print(str(x) + " " + str(positions[x]))
#     if genosRate100Mask_hetMask[positions[x]]:
#         snpIndicesToKeep.append(x)

genos_biallelic=genos.subset(sel0=snpIndicesToKeep) # SCOTT
haps=genos.to_haplotypes()
haps_biallelic=genos_biallelic.to_haplotypes() # SCOTT
# mapping_biallelic=[mapping[x] for x in snpIndicesToKeep] # SCOTT
alleleCounts_biallelic=genos_biallelic.count_alleles() # SCOTT
positions_biallelic=[positions[x] for x in snpIndicesToKeep] # SCOTT
refAlleles_biallelic=[refAlleles[x] for x in snpIndicesToKeep] # SCOTT
altAlleles_biallelic=[altAlleles[x] for x in snpIndicesToKeep] # SCOTT

unique_haps_biallelic = np.unique(haps_biallelic, axis=1)


##################################################################################################

# i=2657107 # SCOTT

# def my_polarizeSnps(unmasked, positions, refAlleles, altAlleles, ancArm):
#     assert len(unmasked) == len(ancArm)
#     assert len(positions) == len(refAlleles)
#     assert len(positions) == len(altAlleles)
#     isSnp = {}
#     for i in range(len(positions)):
#         isSnp[positions[i]] = i
#     mapping = []
#     for i in range(len(ancArm)):
#         if ancArm[i] in 'ACGT':
#             if i+1 in isSnp:
#                 # ref, alt = refAlleles[isSnp[i+1]], altAlleles[isSnp[i+1]]
#                 ref, alt = refAlleles[isSnp[i+1]], altAlleles[isSnp[i+1]][0] # SCOTT
#                 if ancArm[i] == ref:
#                     mapping.append([0, 1])  # no swap
#                 elif ancArm[i] == alt:
#                     mapping.append([1, 0])  # swap
#                 else:
#                     mapping.append([0, 1])  # no swap -- failed to polarize
#                     unmasked[i] = False
#         elif ancArm[i] == "N":
#             unmasked[i] = False
#             if i+1 in isSnp:
#                 mapping.append([0, 1])  # no swap -- failed to polarize
#         else:
#             sys.exit(
#                 "Found a character in ancestral chromosome "\
#                  "that is not 'A', 'C', 'G', 'T' or 'N' (all upper case)!\n")
#     assert len(mapping) == len(positions)
#     return mapping, unmasked

def my_polarizeSnps(unmasked, positions_biallelic, refAlleles_biallelic, altAlleles_biallelic, ancArm):
    assert len(unmasked) == len(ancArm)
    assert len(positions_biallelic) == len(refAlleles_biallelic)
    assert len(positions_biallelic) == len(altAlleles_biallelic)
    isSnp = {}
    for i in range(len(positions_biallelic)):
        isSnp[positions_biallelic[i]] = i
    mapping = []
    for i in range(len(ancArm)):
        if ancArm[i] in 'ACGT':
            if i+1 in isSnp:
                # ref, alt = refAlleles[isSnp[i+1]], altAlleles[isSnp[i+1]]
                ref, alt = refAlleles_biallelic[isSnp[i+1]], altAlleles_biallelic[isSnp[i+1]][0] # SCOTT
                if ancArm[i] == ref:
                    mapping.append([0, 1])  # no swap
                elif ancArm[i] == alt:
                    mapping.append([1, 0])  # swap
                else:
                    mapping.append([0, 1])  # no swap -- failed to polarize
                    unmasked[i] = False # mask SNP if cannot polarize
        elif ancArm[i] == "N":
            if i+1 in isSnp:
                mapping.append([0, 1])  # no swap -- failed to polarize
        else:
            sys.exit(
                "Found a character in ancestral chromosome "\
                 "that is not 'A', 'C', 'G', 'T' or 'N' (all upper case)!\n")
    assert len(mapping) == len(positions_biallelic)
    return mapping, unmasked

################################################################################################

# ancArm=readFaArm(ancestralArmFaFileName).upper()
ancArm=readFaArm(ancestralArmFaFileName, chrArm).upper() # SCOTT
# sys.stderr.write("polarizing SNPs\n")
print("polarizing SNPs\n") # SCOTT
# polTime=time.clock()
polTime=time.perf_counter() # SCOTT
# mapping,unmasked = polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm)
mapping,unmasked = my_polarizeSnps(unmasked,positions_biallelic,refAlleles_biallelic,altAlleles_biallelic,ancArm)
# sys.stderr.write("took %s seconds to polarize SNPs\n" %((time.clock()-polTime)))
print("took %s seconds to polarize SNPs" % (time.perf_counter() - polTime), file=sys.stderr) # SCOTT

print(chrArm + " is " + str(chrLen) + " basepairs, " + "unmasked " + str(sum(unmasked)) + " basepairs after polarization") # SCOTT

###########################################################################################################

# # mask non biallelic
# # only run if vcf is only SNPs
# for i in range(len(isBiallelic)):
#   if not isBiallelic[i]:
#     unmasked[positions[i]-1]=False

##########################################################################################################


statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))


statHeader="chrom start end".split()
header="chrom classifiedWinStart classifiedWinEnd bigWinRange".split()

for statName in statNames:
  statHeader.append(statName)
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName,i))


statHeader="\t".join(statHeader)
header="\t".join(header)

precomputedStats={}
def getSubWinBounds(subWinSize,positions):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  subWinBounds=[]
  for i in range(len(positions)):
    while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if (subWinStart,subWinEnd) not in subWinBounds:
      subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,positions)

# dafs=alleleCounts[:,1]/float(len(sampleIndicesToKeep)*2)
# ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
# nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
# nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
# sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))

dafs=alleleCounts_biallelic[:,1]/float(len(sampleIndicesToKeep)*2)
ihsVals=allel.stats.selection.ihs(haps_biallelic,positions_biallelic,use_threads=False,include_edges=False)
nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
# sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount)) # SCOTT

# if nonNanCount==0:
#   precomputedStats["iHS"]=[]
#   for subWinIndex in range(len(subWinBounds)):
#     precomputedStats["iHS"].append([])
# else:
#   ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
#   precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["iHS"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["iHS"].append([])
else:
  ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
  precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)


# nslVals=allel.stats.selection.nsl(haps,use_threads=False)
# nonNanCount=[x for x in np.isnan(nslVals)].count(False)
# sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))

nslVals=allel.stats.selection.nsl(haps_biallelic,use_threads=False)
nonNanCount=[x for x in np.isnan(nslVals)].count(False)
print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount)) # SCOTT

# if nonNanCount==0:
#   precomputedStats["nSL"]=[]
#   for subWinIndex in range(len(subWinBounds)):
#     precomputedStats["nSL"].append([])
# else:
#   nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
#   precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["nSL"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["nSL"].append([])
else:
  nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
  precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)


#mispolarization information not used since no inference on which specific SNPs may be mispolarized, and since each block contains a relatively small number of SNPs, especially with respect to the mispolarization rate, cannot effectively use the overall mispolarization rate

def SAFEstats(hapsInSubWin,mappingDerivedInSubWin,dafsInSubWin):
  haplotypes={}
  for i in range(len(hapsInSubWin[0])):
    haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
    haplotype="".join(str(x) for x in haplotype)
    if haplotype in haplotypes:
      haplotypes[haplotype].append(i)
    else:
      haplotypes[haplotype]=[i]
  HAF=[]
  HAFunique={}
  for i in haplotypes:
    HAFunique[i]=0
    for j in range(len(hapsInSubWin)):
      if hapsInSubWin[j][haplotypes[i][0]]==mappingDerivedInSubWin[j]:
        HAFunique[i]+=len([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j])) if hapsInSubWin[j][x]==mappingDerivedInSubWin[j]])
    for j in range(len(haplotypes[i])):
      HAF.append(HAFunique[i])
  phi=[]
  kappa=[]
  SAFE=[]
  for i in range(len(hapsInSubWin)):
    phi.append(0)
    kappa.append([])
    phiDenom=0
    for j in haplotypes:
      if int(list(j)[i])==mappingDerivedInSubWin[i]:
        phi[i]+=(HAFunique[j]*len(haplotypes[j]))
        if HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
          kappa[i].append(HAFunique[j])
      phiDenom+=(HAFunique[j]*len(haplotypes[j]))
    phi[i]/=float(phiDenom)
    kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
    if dafsInSubWin[i]==0 or dafsInSubWin[i]==1:
     SAFE.append(0.0)
    else:
     SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafsInSubWin[i]*(1-dafsInSubWin[i]))))
  statVals={}
  quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}
  for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
    if i=="SFS":
      windowStats=dafsInSubWin
    elif i=="HAFunique":
      # windowStats=[eval(i)[x] for x in eval(i)]
      windowStats = list(HAFunique.values()) # SCOTT
    else:
      windowStats=eval(i)
    statVals[i+"-Mean"]=np.mean(windowStats)
    statVals[i+"-Median"]=np.median(windowStats)
    if(len(np.unique(windowStats,return_counts=True)[1])==1):
      statVals[i+"-Mode"]=windowStats[0]
    else:
      if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
        statVals[i+"-Mode"]=scipy.stats.mstats.mode(windowStats)[0][0]
      else:
        mode=min(windowStats)
        for j in range(1,51):
          if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
            mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
        statVals[i+"-Mode"]=mode+((max(windowStats)-min(windowStats))/100)
    for j in quantiles:
      statVals[i+"-"+j]=np.percentile(windowStats,quantiles[j])
    statVals[i+"-Max"]=max(windowStats)
    statVals[i+"-Var"]=np.var(windowStats)
    statVals[i+"-SD"]=np.std(windowStats)
    statVals[i+"-Skew"]=scipy.stats.skew(windowStats)
    statVals[i+"-Kurt"]=scipy.stats.kurtosis(windowStats)
  return statVals

###########################################################################################

subWinIndex=-1
subWinStart=17190001
subWinEnd=subWinStart+subWinSize-1
unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
if len([x for x in positions if subWinStart<=x<=subWinEnd])==0:
  # sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
  print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
  goodSubWins.append(False)
else:
  subWinIndex+=1
  # sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
  print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))

if unmaskedFrac<unmaskedFracCutoff:
  goodSubWins.append(False)
else:
  goodSubWins.append(True)

hapsInSubWin=haps_biallelic.subset(sel0=snpIndicesInSubWins[subWinIndex])
mappingDerivedInSubWin=[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]]
dafsInSubWin=dafs[snpIndicesInSubWins[subWinIndex]]

genosInSubWin=genos.subset(sel0=snpIndicesInSubWins[subWinIndex])
genosInSubWin[:,47,:]

for i in range(len(positions)):
  if (positions[i]>=subWinStart and positions[i]<=subWinEnd):
    print(positions[i])

# def SAFEstats(hapsInSubWin,mappingDerivedInSubWin,dafsInSubWin):

haplotypes={}

for i in range(len(hapsInSubWin[0])):
  haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
  haplotype="".join(str(x) for x in haplotype)
  if haplotype in haplotypes:
    haplotypes[haplotype].append(i)
  else:
    haplotypes[haplotype]=[i]

HAF=[]
HAFunique={}

for i in haplotypes:
  HAFunique[i]=0
  for j in range(len(hapsInSubWin)):
    if hapsInSubWin[j][haplotypes[i][0]]==mappingDerivedInSubWin[j]:
      HAFunique[i]+=len([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j])) if hapsInSubWin[j][x]==mappingDerivedInSubWin[j]])
  for j in range(len(haplotypes[i])):
    HAF.append(HAFunique[i])

phi=[]
kappa=[]
SAFE=[]

for i in range(len(hapsInSubWin)):
  phi.append(0)
  kappa.append([])
  phiDenom=0
  for j in haplotypes:
    if int(list(j)[i])==mappingDerivedInSubWin[i]:
      phi[i]+=(HAFunique[j]*len(haplotypes[j]))
      if HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
        kappa[i].append(HAFunique[j])
    phiDenom+=(HAFunique[j]*len(haplotypes[j]))
  phi[i]/=float(phiDenom)
  kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
  if dafsInSubWin[i]==0 or dafsInSubWin[i]==1:
   SAFE.append(0.0)
  else:
   SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafsInSubWin[i]*(1-dafsInSubWin[i]))))

statVals={}
quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  if i=="SFS":
    windowStats=dafsInSubWin
  elif i=="HAFunique":
    windowStats=[eval(i)[x] for x in eval(i)]
  else:
    windowStats=eval(i)
  statVals[i+"-Mean"]=np.mean(windowStats)
  statVals[i+"-Median"]=np.median(windowStats)
  if(len(np.unique(windowStats,return_counts=True)[1])==1):
    statVals[i+"-Mode"]=windowStats[0]
  else:
    if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
      statVals[i+"-Mode"]=scipy.stats.mstats.mode(windowStats)[0][0]
    else:
      mode=min(windowStats)
      for j in range(1,51):
        if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
          mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
      statVals[i+"-Mode"]=mode+((max(windowStats)-min(windowStats))/100)

for j in quantiles:
  statVals[i+"-"+j]=np.percentile(windowStats,quantiles[j])


statVals[i+"-Max"]=max(windowStats)
statVals[i+"-Var"]=np.var(windowStats)
statVals[i+"-SD"]=np.std(windowStats)
statVals[i+"-Skew"]=scipy.stats.skew(windowStats)
statVals[i+"-Kurt"]=scipy.stats.kurtosis(windowStats)

###########################################################################################


if segmentStart==None:
  firstSubWinStart=1
  lastSubWinStart=(((chrLen-1)/subWinSize)*subWinSize)+1
else:
  firstSubWinStart=(((segmentStart-1)/subWinSize)*subWinSize)+1
  lastSubWinStart=(((segmentEnd-1)/subWinSize)*subWinSize)+1


goodSubWins=[]
for i in range(numSubWins):
  goodSubWins.append(False)


subWinIndex=-1


# def getSnpIndicesInSubWins(subWinSize,positions):
#   subWinStart=1
#   subWinEnd=subWinStart+subWinSize-1
#   snpIndicesInSubWins=None
#   for i in range(len(positions)):
#     if snpIndicesInSubWins and not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#       snpIndicesInSubWins.append([])
#     while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#       subWinStart+=subWinSize
#       subWinEnd+=subWinSize
#     if not snpIndicesInSubWins:
#       snpIndicesInSubWins=[[]]
#     snpIndicesInSubWins[-1].append(i)
#   return snpIndicesInSubWins
# 
# 
# snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions)

def getSnpIndicesInSubWins(subWinSize,positions_biallelic):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  snpIndicesInSubWins=None
  for i in range(len(positions_biallelic)):
    if snpIndicesInSubWins and not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      snpIndicesInSubWins.append([])
    while not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if not snpIndicesInSubWins:
      snpIndicesInSubWins=[[]]
    snpIndicesInSubWins[-1].append(i)
  return snpIndicesInSubWins

#snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions)
snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions_biallelic) # SCOTT

statVals={}
for statName in statNames:
  statVals[statName]=[]


if statFileName.lower() in ["none","false"]:
  statFileName=None
else:
  statFile=open(statFileName,"w")
  statFile.write(statHeader+"\n")


if fvecFileName.lower() in ["none","false","default"]:
  if segmentStart!=None:
    fvecFileName=targetPop+'.'+chrArm+'.'+segStart+'.fvec'
  else:
    fvecFileName=targetPop+'.'+chrArm+'.fvec'


fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")



# calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)

alleleCounts = copy.copy(alleleCounts_biallelic)
snpLocs = copy.copy(positions)

def calcAndAppendStatValForScan(alleleCounts, snpLocs, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats):
    if statName == "tajD":
        statVals[statName].append(allel.stats.diversity.tajima_d(
            alleleCounts, pos=snpLocs, start=subWinStart, stop=subWinEnd))
    elif statName == "pi":
        statVals[statName].append(allel.stats.diversity.sequence_diversity(
            snpLocs, alleleCounts, start=subWinStart, stop=subWinEnd, is_accessible=unmasked))  # NOQA
    elif statName == "thetaW":
        statVals[statName].append(allel.stats.diversity.watterson_theta(
            snpLocs, alleleCounts, start=subWinStart, stop=subWinEnd, is_accessible=unmasked))  # NOQA
    elif statName == "thetaH":
        statVals[statName].append(thetah(
            snpLocs, alleleCounts, start=subWinStart, stop=subWinEnd, is_accessible=unmasked))  # NOQA
    elif statName == "fayWuH":
        statVals[statName].append(
            statVals["thetaH"][subWinIndex]-statVals["pi"][subWinIndex])
    elif statName == "maxFDA":
        # AK: undefined variables
        statVals[statName].append(maxFDA(
            snpLocs, alleleCounts, start=subWinStart, stop=subWinEnd, is_accessible=unmasked))
    elif statName == "HapCount":
        statVals[statName].append(len(hapsInSubWin.distinct()))
    elif statName == "H1":
        h1, h12, h123, h21 = allel.stats.selection.garud_h(hapsInSubWin)
        statVals["H1"].append(h1)
        if "H12" in statVals:
            statVals["H12"].append(h12)
        if "H123" in statVals:
            statVals["H123"].append(h123)
        if "H2/H1" in statVals:
            statVals["H2/H1"].append(h21)
    elif statName == "ZnS":
        r2Matrix = shicstats.computeR2Matrix(hapsInSubWin)
        statVals["ZnS"].append(shicstats.ZnS(r2Matrix)[0])
        statVals["Omega"].append(shicstats.omega(r2Matrix)[0])
    elif statName == "RH":
        rMatrixFlat = allel.stats.ld.rogers_huff_r(
            hapsInSubWin.to_genotypes(ploidy=2).to_n_alt())
        rhAvg = rMatrixFlat.mean()
        statVals["RH"].append(rhAvg)
        r2Matrix = squareform(rMatrixFlat ** 2)
        statVals["Omega"].append(shicstats.omega(r2Matrix)[0])
    elif statName == "iHSMean":
        vals = [x for x in precomputedStats["iHS"][subWinIndex]
                if not (math.isnan(x) or math.isinf(x))]
        if len(vals) == 0:
            statVals["iHSMean"].append(0.0)
        else:
            statVals["iHSMean"].append(sum(vals)/float(len(vals)))
    elif statName == "nSLMean":
        vals = [x for x in precomputedStats["nSL"][subWinIndex]
                if not (math.isnan(x) or math.isnan(x))]
        if len(vals) == 0:
            statVals["nSLMean"].append(0.0)
        else:
            statVals["nSLMean"].append(sum(vals)/float(len(vals)))
    elif statName == "iHSMax":
        vals = [x for x in precomputedStats["iHS"][subWinIndex]
                if not (math.isnan(x) or math.isinf(x))]
        if len(vals) == 0:
            maxVal = 0.0
        else:
            maxVal = max(vals)
        statVals["iHSMax"].append(maxVal)
    elif statName == "nSLMax":
        vals = [x for x in precomputedStats["nSL"][subWinIndex]
                if not (math.isnan(x) or math.isnan(x))]
        if len(vals) == 0:
            maxVal = 0.0
        else:
            maxVal = max(vals)
        statVals["nSLMax"].append(maxVal)
    elif statName == "iHSOutFrac":
        statVals["iHSOutFrac"].append(getOutlierFrac(
            precomputedStats["iHS"][subWinIndex]))
    elif statName == "nSLOutFrac":
        statVals["nSLOutFrac"].append(getOutlierFrac(
            precomputedStats["nSL"][subWinIndex]))
    elif statName == "distVar":
        dists = shicstats.pairwiseDiffs(
            hapsInSubWin)/float(unmasked[subWinStart-1:subWinEnd].count(True))
        statVals["distVar"].append(np.var(dists, ddof=1))
        statVals["distSkew"].append(scipy.stats.skew(dists))
        statVals["distKurt"].append(scipy.stats.kurtosis(dists))
    elif statName in ["H12", "H123", "H2/H1",
                      "Omega", "distVar", "distSkew", "distKurt"]:
        assert len(statVals[statName]) == subWinIndex+1




subWinIndex=-1

#for subWinStart in range(firstSubWinStart,lastSubWinStart+1,subWinSize):
for subWinStart in range(int(firstSubWinStart),int(lastSubWinStart)+1,int(subWinSize)):
  subWinEnd=subWinStart+subWinSize-1
  unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
  # unmaskedFrac=genosRateMask25[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
  #if len([x for x in positions if subWinStart<=x<=subWinEnd])==0:
  if len([x for x in positions_biallelic if subWinStart<=x<=subWinEnd])==0:
    #sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    goodSubWins.append(False)
  else:
    subWinIndex+=1
    #sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    if unmaskedFrac<unmaskedFracCutoff:
      goodSubWins.append(False)
    else:
      goodSubWins.append(True)
      #hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
      hapsInSubWin=haps_biallelic.subset(sel0=snpIndicesInSubWins[subWinIndex])
      for statName in statNames:
        if statName!="H12" and statName!="H2/H1" and statName!="Omega" and statName!="distSkew" and statName!="distKurt" and statName[0:3]!="HAF" and statName[0:3]!="phi" and statName[0:3]!="kap" and statName[0:3]!="SFS" and statName[0:3]!="SAF":
          if statName=="fayWuH":
            #calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
            calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
          else:
            #calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
            calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
      #SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      for statName in SAFEVals:
        statVals[statName].append(SAFEVals[statName])
      if statFileName:
        statFile.write("\t".join([chrArm, str(subWinStart), str(subWinEnd)] + [str(statVals[statName][-1]) for statName in statNames]) + "\n")
  goodSubWins=goodSubWins[1:]
  if goodSubWins.count(True)==numSubWins:
    outVec=[]
    for statName in statNames:
      outVec+=normalizeFeatureVec(statVals[statName][-numSubWins:])
    midSubWinEnd=subWinEnd-(subWinSize*(numSubWins/2))
    midSubWinStart=midSubWinEnd-subWinSize+1
    fvecFile.write("\t".join([chrArm, str(midSubWinStart), str(midSubWinEnd), str((subWinEnd-(subWinSize*numSubWins)+1))+"-"+str(subWinEnd)] + [str(x) for x in outVec]) + "\n")

###########################################################################################

# subWinStart=110001
# subWinStart=120001
# subWinStart=5775001
# subWinStart=2610001
subWinStart=17190001

# for subWinStart in range(firstSubWinStart,int(lastSubWinStart+1),subWinSize):
  
subWinEnd=subWinStart+subWinSize-1
unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
  
if len([x for x in positions if subWinStart<=x<=subWinEnd])==0:
  # sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
  print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
  goodSubWins.append(False)
else:
  subWinIndex+=1
  # sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
  print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))

if unmaskedFrac<unmaskedFracCutoff:
  goodSubWins.append(False)
else:
  goodSubWins.append(True)
  hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
  for statName in statNames:
    if statName!="H12" and statName!="H2/H1" and statName!="Omega" and statName!="distSkew" and statName!="distKurt" and statName[0:3]!="HAF" and statName[0:3]!="phi" and statName[0:3]!="kap" and statName[0:3]!="SFS" and statName[0:3]!="SAF":
      if statName=="fayWuH":
        calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
      else:
        calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
  SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
  for statName in SAFEVals:
    statVals[statName].append(SAFEVals[statName])
  if statFileName:
    statFile.write("\t".join([chrArm, str(subWinStart), str(subWinEnd)] + [str(statVals[statName][-1]) for statName in statNames]) + "\n")

goodSubWins=goodSubWins[1:]

if goodSubWins.count(True)==numSubWins:
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][-numSubWins:])
  midSubWinEnd=subWinEnd-(subWinSize*(numSubWins/2))
  midSubWinStart=midSubWinEnd-subWinSize+1
  fvecFile.write("\t".join([chrArm, str(midSubWinStart), str(midSubWinEnd), str((subWinEnd-(subWinSize*numSubWins)+1))+"-"+str(subWinEnd)] + [str(x) for x in outVec]) + "\n")

####################################################################################


if statFileName:
  statFile.close()


fvecFile.close()
# sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### noMaskHet

```{python}

import time
#startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
#import h5py # NOT USED
import allel
from fvTools import *
import numpy as np
import math
import multiprocessing # USED FOR BIALLELIC 100% GENOTYPED and NOT HETEROZYGOUS MASK
import copy # USED FOR BIALLELIC 100% GENOTYPED and NOT HETEROZYGOUS MASK

'''usage eg:
pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
python2 empirical_convert_to_FVs.py ag1000g.phase1.ar3.haplotypes.2L.h5 2L 49364325 1 5000000 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.2L.fa samples_pops.txt AOM 2L.1.stats 2L.1.fvec
'''

if not len(sys.argv) in [15,17]:
  sys.exit("usage:\npython2 empirical_convert_to_FVs.py chrArmFileName chrArm chrLen [segmentStart segmentEnd] subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName sampleToPopFileName targetPop statFileName fvecFileName\n")

if len(sys.argv)==17:
  chrArmFileName, chrArm, chrLen, segmentStart, segmentEnd, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
else:
  chrArmFileName, chrArm, chrLen, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
  segmentStart=None

#chrArmFile=h5py.File(chrArmFileName,"r")
#genos=allel.GenotypeChunkedArray(chrArmFile[chrArm]["calldata"]["genotype"])
#positions=allel.SortedIndex(chrArmFile["/%s/variants/POS" %(chrArm)][:])
#refAlleles=chrArmFile[chrArm]['variants']['REF']
#altAlleles=chrArmFile[chrArm]['variants']['ALT']
#samples=chrArmFile[chrArm]["samples"]

# 718 Scott VCF

chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz'
chrArm='ScA8VGg_718'
chrLen='8653123'
# segmentStart=1
segmentStart=None
# segmentEnd=5050000
segmentEnd=None
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt'
maskFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta'
ancestralArmFaFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa'
sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt'
targetPop="dser"
statFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.1.scott.noMaskHet.110Lines.test.stats'
fvecFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.1.scott.noMaskHet.110Lines.test.fvec'

chrArmFile=allel.read_vcf(chrArmFileName) # change to scikit-allel
genos = allel.GenotypeArray(chrArmFile['calldata/GT']) # change to scikit-allel
positions=allel.SortedIndex(chrArmFile['variants/POS'], copy=False) # change to scikit-allel
refAlleles=chrArmFile['variants/REF'] # change to scikit-allel
altAlleles=chrArmFile['variants/ALT'] # change to scikit-allel
samples=chrArmFile['samples'] # change to scikit-allel

chrLen=int(chrLen)
assert chrLen>0

print(chrArm + " is " + str(chrLen) + " basepairs, " + "Genotyped " + str(len(genos))) # SCOTT

if segmentStart!=None:
  segmentStart,segmentEnd = int(segmentStart),int(segmentEnd)
  assert segmentStart>0 and segmentEnd>=segmentStart
  snpIndicesToKeep=[x for x in range(len(positions)) if segmentStart<=positions[x]<=segmentEnd]
  genos=allel.GenotypeArray(genos.subset(sel0=snpIndicesToKeep))
  positions=[positions[x] for x in snpIndicesToKeep]
  refAlleles=[refAlleles[x] for x in snpIndicesToKeep]
  altAlleles=[altAlleles[x] for x in snpIndicesToKeep]

subWinSize,numSubWins,unmaskedFracCutoff,pMisPol = int(subWinSize),int(numSubWins),float(unmaskedFracCutoff),float(pMisPol)

assert subWinSize>0 and numSubWins>1

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")

if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")

standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none","false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*chrLen
  sys.stderr.write("Warning: a mask.fa file for the chr arm with all masked sites N'ed out is strongly recommended (pass in the reference to remove Ns at the very least)!\n")
else:
  unmasked=readMaskDataForScan(maskFileName,chrArm)
  assert len(unmasked)==chrLen

print(chrArm + " is " + str(chrLen) + " basepairs, " + "unmasked " + str(sum(unmasked)) + " basepairs") # SCOTT

print("Subset samples...")

def readSampleToPopFile(sampleToPopFileName):
  table={}
  with open(sampleToPopFileName) as sampleToPopFile:
    for line in sampleToPopFile:
      sample,pop = line.strip().split()
      table[sample]=pop
  return table

sampleToPop=readSampleToPopFile(sampleToPopFileName)

sampleIndicesToKeep=[x for x in range(len(samples)) if sampleToPop.get(samples[x],"popNotFound!")==targetPop]

genos=genos.subset(sel1=sampleIndicesToKeep)

print("Create alleleCounts and isBiallelic...")

alleleCounts=genos.count_alleles() # Count the number of calls of each allele per variant.
isBiallelic=alleleCounts.is_biallelic() # Find variants biallelic for the reference (0) and first alternate (1) allele.

print("Total biallelic loci " + str(sum(isBiallelic)))

# filter for biallelic SNPs, 100% genotyped

## mask < 100% genotyped

print("Masking not 100% genotyped...")

genosRate100Mask_hetMask = []

# Define a worker function
def biallelic_mask(chunk):
    results = []
    for i in chunk:
        count_called = genos[i].count_called()
        genotypingRate = count_called/len(sampleToPop)
        count_het = genos[i].count_het()
        if genotypingRate < 1:
            out = False
        # elif count_het > 0:
        #     out = False
        else:
            out = True
        results.append(out)
    return results

# Split the indices into smaller chunks
chunk_size = 100000  # Adjust the chunk size as needed
indices = range(len(genos))
chunks = [indices[i:i + chunk_size] for i in range(0, len(indices), chunk_size)]

# Create a pool of worker processes
pool = multiprocessing.Pool()

# Use the pool.map() function to distribute the workload across multiple processes
results = pool.map(biallelic_mask, chunks)

# Flatten the results
genosRate100Mask_hetMask = [item for sublist in results for item in sublist]

# Close the pool to free up resources
pool.close()
pool.join()

# sum(genosRate100Mask_hetMask)

## merge with unmasked

print("Merging with unmasked to get rid of indels and multiallelic...")
genosRate100Mask_hetMask_2 = copy.copy(unmasked)
for i in range(len(genosRate100Mask_hetMask)):
  if unmasked[positions[i]-1]==True and genosRate100Mask_hetMask[i]==False:
    genosRate100Mask_hetMask_2[positions[i]-1]=False

## mask non biallelic
print("Masking non-biallelic")
for i in range(len(isBiallelic)):
  if not isBiallelic[i]:
    genosRate100Mask_hetMask_2[positions[i]-1]=False

print("Biallelic loci passing filters " + str(sum(genosRate100Mask_hetMask_2)))
# print("Biallelic loci passing filters " + str(sum(isBiallelic)))

snpIndicesToKeep=[x for x in range(len(positions)) if genosRate100Mask_hetMask_2[positions[x]-1]] # SCOTT
# snpIndicesToKeep=[x for x in range(len(positions)) if isBiallelic[x]] # SCOTT
genos_biallelic=genos.subset(sel0=snpIndicesToKeep) # SCOTT
haps=genos.to_haplotypes()
haps_biallelic=genos_biallelic.to_haplotypes() # SCOTT
# mapping_biallelic=[mapping[x] for x in snpIndicesToKeep] # SCOTT
alleleCounts_biallelic=genos_biallelic.count_alleles() # SCOTT
positions_biallelic=[positions[x] for x in snpIndicesToKeep] # SCOTT
refAlleles_biallelic=[refAlleles[x] for x in snpIndicesToKeep] # SCOTT
altAlleles_biallelic=[altAlleles[x] for x in snpIndicesToKeep] # SCOTT

##################################################################################################

# SCOTT fix problem only, still mask unpolarised

def my_polarizeSnps(unmasked, positions_biallelic, refAlleles_biallelic, altAlleles_biallelic, ancArm):
    assert len(unmasked) == len(ancArm)
    assert len(positions_biallelic) == len(refAlleles_biallelic)
    assert len(positions_biallelic) == len(altAlleles_biallelic)
    isSnp = {}
    for i in range(len(positions_biallelic)):
        isSnp[positions_biallelic[i]] = i
    mapping = []
    for i in range(len(ancArm)):
        if ancArm[i] in 'ACGT':
            if i+1 in isSnp:
                # ref, alt = refAlleles[isSnp[i+1]], altAlleles[isSnp[i+1]]
                ref, alt = refAlleles_biallelic[isSnp[i+1]], altAlleles_biallelic[isSnp[i+1]][0] # SCOTT
                if ancArm[i] == ref:
                    mapping.append([0, 1])  # no swap
                elif ancArm[i] == alt:
                    mapping.append([1, 0])  # swap
                else:
                    mapping.append([0, 1])  # no swap -- failed to polarize
                    unmasked[i] = False # mask SNP if cannot polarize
        elif ancArm[i] == "N":
            #unmasked[i] = False
            if i+1 in isSnp:
                mapping.append([0, 1])  # no swap -- failed to polarize
        else:
            sys.exit(
                "Found a character in ancestral chromosome "\
                 "that is not 'A', 'C', 'G', 'T' or 'N' (all upper case)!\n")
    assert len(mapping) == len(positions_biallelic)
    return mapping, unmasked

################################################################################################

#ancArm=readFaArm(ancestralArmFaFileName).upper()
#sys.stderr.write("polarizing SNPs\n")
#polTime=time.clock()
#mapping,unmasked = polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm)
#sys.stderr.write("took %s seconds to polarize SNPs\n" %((time.clock()-polTime)))

ancArm=readFaArm(ancestralArmFaFileName, chrArm).upper() # no change
print("polarizing SNPs\n") # changed to print but didn't need to
polTime=time.perf_counter() # Python 3
mapping,unmasked = my_polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm) # use my_polarizeSnps
print("took %s seconds to polarize SNPs" % (time.perf_counter() - polTime)) # Python 3

print(chrArm + " is " + str(chrLen) + " basepairs, " + "unmasked " + str(sum(unmasked)) + " basepairs after polarization") # SCOTT

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

statHeader="chrom start end".split()
header="chrom classifiedWinStart classifiedWinEnd bigWinRange".split()

for statName in statNames:
  statHeader.append(statName)
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName,i))

statHeader="\t".join(statHeader)
header="\t".join(header)

precomputedStats={}

def getSubWinBounds(subWinSize,positions):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  subWinBounds=[]
  for i in range(len(positions)):
    while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if (subWinStart,subWinEnd) not in subWinBounds:
      subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,positions)

#dafs=alleleCounts[:,1]/float(len(sampleIndicesToKeep)*2)
dafs=alleleCounts_biallelic[:,1]/float(len(sampleIndicesToKeep)*2) # SCOTT

#ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
#nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
#nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
#sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))

ihsVals=allel.stats.selection.ihs(haps_biallelic,positions_biallelic,use_threads=False,include_edges=False) # SCOTT
nonNanCount=[x for x in np.isnan(ihsVals)].count(False) # SCOTT
nonInfCount=[x for x in np.isinf(ihsVals)].count(False) # SCOTT
print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount)) # SCOTT

#if nonNanCount==0:
#  precomputedStats["iHS"]=[]
#  for subWinIndex in range(len(subWinBounds)):
#    precomputedStats["iHS"].append([])
#else:
#  ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
#  precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["iHS"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["iHS"].append([])
else:
  ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
  precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)

#nslVals=allel.stats.selection.nsl(haps,use_threads=False)
#nonNanCount=[x for x in np.isnan(nslVals)].count(False)
#sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))

nslVals=allel.stats.selection.nsl(haps_biallelic,use_threads=False) # SCOTT
nonNanCount=[x for x in np.isnan(nslVals)].count(False) # SCOTT
print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount)) # SCOTT

#if nonNanCount==0:
#  precomputedStats["nSL"]=[]
#  for subWinIndex in range(len(subWinBounds)):
#    precomputedStats["nSL"].append([])
#else:
#  nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
#  precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["nSL"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["nSL"].append([])
else:
  nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
  precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)

#mispolarization information not used since no inference on which specific SNPs may be mispolarized, and since each block contains a relatively small number of SNPs, especially with respect to the mispolarization rate, cannot effectively use the overall mispolarization rate

def SAFEstats(hapsInSubWin,mappingDerivedInSubWin,dafsInSubWin):
  haplotypes={}
  for i in range(len(hapsInSubWin[0])):
    haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
    haplotype="".join(str(x) for x in haplotype)
    if haplotype in haplotypes:
      haplotypes[haplotype].append(i)
    else:
      haplotypes[haplotype]=[i]
  HAF=[]
  HAFunique={}
  for i in haplotypes:
    HAFunique[i]=0
    for j in range(len(hapsInSubWin)):
      if hapsInSubWin[j][haplotypes[i][0]]==mappingDerivedInSubWin[j]:
        HAFunique[i]+=len([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j])) if hapsInSubWin[j][x]==mappingDerivedInSubWin[j]])
    for j in range(len(haplotypes[i])):
      HAF.append(HAFunique[i])
  phi=[]
  kappa=[]
  SAFE=[]
  for i in range(len(hapsInSubWin)):
    phi.append(0)
    kappa.append([])
    phiDenom=0
    for j in haplotypes:
      if int(list(j)[i])==mappingDerivedInSubWin[i]:
        phi[i]+=(HAFunique[j]*len(haplotypes[j]))
        if HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
          kappa[i].append(HAFunique[j])
      phiDenom+=(HAFunique[j]*len(haplotypes[j]))
    phi[i]/=float(phiDenom)
    kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
    if dafsInSubWin[i]==0 or dafsInSubWin[i]==1:
     SAFE.append(0.0)
    else:
     SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafsInSubWin[i]*(1-dafsInSubWin[i]))))
  statVals={}
  quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}
  for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
    if i=="SFS":
      windowStats=dafsInSubWin
    elif i=="HAFunique":
      #windowStats=[eval(i)[x] for x in eval(i)]
      windowStats = list(HAFunique.values()) # SCOTT
    else:
      windowStats=eval(i)
    statVals[i+"-Mean"]=np.mean(windowStats)
    statVals[i+"-Median"]=np.median(windowStats)
    if(len(np.unique(windowStats,return_counts=True)[1])==1):
      statVals[i+"-Mode"]=windowStats[0]
    else:
      if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
        statVals[i+"-Mode"]=scipy.stats.mstats.mode(windowStats)[0][0]
      else:
        mode=min(windowStats)
        for j in range(1,51):
          if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
            mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
        statVals[i+"-Mode"]=mode+((max(windowStats)-min(windowStats))/100)
    for j in quantiles:
      statVals[i+"-"+j]=np.percentile(windowStats,quantiles[j])
    statVals[i+"-Max"]=max(windowStats)
    statVals[i+"-Var"]=np.var(windowStats)
    statVals[i+"-SD"]=np.std(windowStats)
    statVals[i+"-Skew"]=scipy.stats.skew(windowStats)
    statVals[i+"-Kurt"]=scipy.stats.kurtosis(windowStats)
  return statVals

if segmentStart==None:
  firstSubWinStart=1
  lastSubWinStart=(((chrLen-1)/subWinSize)*subWinSize)+1
else:
  firstSubWinStart=(((segmentStart-1)/subWinSize)*subWinSize)+1
  lastSubWinStart=(((segmentEnd-1)/subWinSize)*subWinSize)+1

goodSubWins=[]

for i in range(numSubWins):
  goodSubWins.append(False)

subWinIndex=-1

#def getSnpIndicesInSubWins(subWinSize,positions):
#  subWinStart=1
#  subWinEnd=subWinStart+subWinSize-1
#  snpIndicesInSubWins=None
#  for i in range(len(positions)):
#    if snpIndicesInSubWins and not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#      snpIndicesInSubWins.append([])
#    while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#      subWinStart+=subWinSize
#      subWinEnd+=subWinSize
#    if not snpIndicesInSubWins:
#      snpIndicesInSubWins=[[]]
#    snpIndicesInSubWins[-1].append(i)
#  return snpIndicesInSubWins

def getSnpIndicesInSubWins(subWinSize,positions_biallelic):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  snpIndicesInSubWins=None
  for i in range(len(positions_biallelic)):
    if snpIndicesInSubWins and not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      snpIndicesInSubWins.append([])
    while not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if not snpIndicesInSubWins:
      snpIndicesInSubWins=[[]]
    snpIndicesInSubWins[-1].append(i)
  return snpIndicesInSubWins

#snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions)
snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions_biallelic) # SCOTT

statVals={}

for statName in statNames:
  statVals[statName]=[]

if statFileName.lower() in ["none","false"]:
  statFileName=None
else:
  statFile=open(statFileName,"w")
  statFile.write(statHeader+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  if segmentStart!=None:
    fvecFileName=targetPop+'.'+chrArm+'.'+segStart+'.fvec'
  else:
    fvecFileName=targetPop+'.'+chrArm+'.fvec'

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

#for subWinStart in range(firstSubWinStart,lastSubWinStart+1,subWinSize):
for subWinStart in range(int(firstSubWinStart),int(lastSubWinStart)+1,int(subWinSize)):
  subWinEnd=subWinStart+subWinSize-1
  unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
  #if len([x for x in positions if subWinStart<=x<=subWinEnd])==0:
  if len([x for x in positions_biallelic if subWinStart<=x<=subWinEnd])==0:
    #sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    goodSubWins.append(False)
  else:
    subWinIndex+=1
    #sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    if unmaskedFrac<unmaskedFracCutoff:
      goodSubWins.append(False)
    else:
      goodSubWins.append(True)
      #hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
      hapsInSubWin=haps_biallelic.subset(sel0=snpIndicesInSubWins[subWinIndex])
      for statName in statNames:
        if statName!="H12" and statName!="H2/H1" and statName!="Omega" and statName!="distSkew" and statName!="distKurt" and statName[0:3]!="HAF" and statName[0:3]!="phi" and statName[0:3]!="kap" and statName[0:3]!="SFS" and statName[0:3]!="SAF":
          if statName=="fayWuH":
            #calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
            calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
          else:
            #calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
            calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
      #SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      for statName in SAFEVals:
        statVals[statName].append(SAFEVals[statName])
      if statFileName:
        statFile.write("\t".join([chrArm, str(subWinStart), str(subWinEnd)] + [str(statVals[statName][-1]) for statName in statNames]) + "\n")
  goodSubWins=goodSubWins[1:]
  if goodSubWins.count(True)==numSubWins:
    outVec=[]
    for statName in statNames:
      outVec+=normalizeFeatureVec(statVals[statName][-numSubWins:])
    midSubWinEnd=subWinEnd-(subWinSize*(numSubWins/2))
    midSubWinStart=midSubWinEnd-subWinSize+1
    fvecFile.write("\t".join([chrArm, str(midSubWinStart), str(midSubWinEnd), str((subWinEnd-(subWinSize*numSubWins)+1))+"-"+str(subWinEnd)] + [str(x) for x in outVec]) + "\n")

if statFileName:
  statFile.close()
fvecFile.close()
#sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
print("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### Phased 100% genotyped

```{python}

import time
#startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
#import h5py # NOT USED
import allel
from fvTools import *
import numpy as np
import math
import multiprocessing # USED FOR BIALLELIC 100% GENOTYPED and NOT HETEROZYGOUS MASK
import copy # USED FOR BIALLELIC 100% GENOTYPED and NOT HETEROZYGOUS MASK

'''usage eg:
pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
python2 empirical_convert_to_FVs.py ag1000g.phase1.ar3.haplotypes.2L.h5 2L 49364325 1 5000000 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.2L.fa samples_pops.txt AOM 2L.1.stats 2L.1.fvec
'''

if not len(sys.argv) in [15,17]:
  sys.exit("usage:\npython2 empirical_convert_to_FVs.py chrArmFileName chrArm chrLen [segmentStart segmentEnd] subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName sampleToPopFileName targetPop statFileName fvecFileName\n")

if len(sys.argv)==17:
  chrArmFileName, chrArm, chrLen, segmentStart, segmentEnd, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
else:
  chrArmFileName, chrArm, chrLen, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
  segmentStart=None

#chrArmFile=h5py.File(chrArmFileName,"r")
#genos=allel.GenotypeChunkedArray(chrArmFile[chrArm]["calldata"]["genotype"])
#positions=allel.SortedIndex(chrArmFile["/%s/variants/POS" %(chrArm)][:])
#refAlleles=chrArmFile[chrArm]['variants']['REF']
#altAlleles=chrArmFile[chrArm]['variants']['ALT']
#samples=chrArmFile[chrArm]["samples"]

# 718 phased

chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_shortName.vcf'
chrArm='ScA8VGg_718'
chrLen='8653123'
# segmentStart=1
segmentStart=None
# segmentEnd=5050000
segmentEnd=None
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt'
maskFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta'
ancestralArmFaFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa'
sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt'
targetPop="dser"
statFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.1.scott.noMaskHet.110Lines.test.stats'
fvecFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.1.scott.noMaskHet.110Lines.test.fvec'

chrArmFile=allel.read_vcf(chrArmFileName) # change to scikit-allel
genos = allel.GenotypeArray(chrArmFile['calldata/GT']) # change to scikit-allel
positions=allel.SortedIndex(chrArmFile['variants/POS'], copy=False) # change to scikit-allel
refAlleles=chrArmFile['variants/REF'] # change to scikit-allel
altAlleles=chrArmFile['variants/ALT'] # change to scikit-allel
samples=chrArmFile['samples'] # change to scikit-allel

chrLen=int(chrLen)
assert chrLen>0

print(chrArm + " is " + str(chrLen) + " basepairs, " + "Genotyped " + str(len(genos))) # SCOTT

if segmentStart!=None:
  segmentStart,segmentEnd = int(segmentStart),int(segmentEnd)
  assert segmentStart>0 and segmentEnd>=segmentStart
  snpIndicesToKeep=[x for x in range(len(positions)) if segmentStart<=positions[x]<=segmentEnd]
  genos=allel.GenotypeArray(genos.subset(sel0=snpIndicesToKeep))
  positions=[positions[x] for x in snpIndicesToKeep]
  refAlleles=[refAlleles[x] for x in snpIndicesToKeep]
  altAlleles=[altAlleles[x] for x in snpIndicesToKeep]

subWinSize,numSubWins,unmaskedFracCutoff,pMisPol = int(subWinSize),int(numSubWins),float(unmaskedFracCutoff),float(pMisPol)

assert subWinSize>0 and numSubWins>1

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")

if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")

standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none","false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*chrLen
  sys.stderr.write("Warning: a mask.fa file for the chr arm with all masked sites N'ed out is strongly recommended (pass in the reference to remove Ns at the very least)!\n")
else:
  unmasked=readMaskDataForScan(maskFileName,chrArm)
  assert len(unmasked)==chrLen

print(chrArm + " is " + str(chrLen) + " basepairs, " + "unmasked " + str(sum(unmasked)) + " basepairs") # SCOTT

print("Subset samples...")

def readSampleToPopFile(sampleToPopFileName):
  table={}
  with open(sampleToPopFileName) as sampleToPopFile:
    for line in sampleToPopFile:
      sample,pop = line.strip().split()
      table[sample]=pop
  return table

sampleToPop=readSampleToPopFile(sampleToPopFileName)

sampleIndicesToKeep=[x for x in range(len(samples)) if sampleToPop.get(samples[x],"popNotFound!")==targetPop]

genos=genos.subset(sel1=sampleIndicesToKeep)

print("Create alleleCounts and isBiallelic...")

alleleCounts=genos.count_alleles() # Count the number of calls of each allele per variant.
isBiallelic=alleleCounts.is_biallelic() # Find variants biallelic for the reference (0) and first alternate (1) allele.

print("Total biallelic loci " + str(sum(isBiallelic)))

# # filter for biallelic SNPs, 100% genotyped
# 
# ## mask < 100% genotyped
# 
# print("Masking not 100% genotyped...")
# 
# genosRate100Mask_hetMask = []
# 
# # Define a worker function
# def biallelic_mask(chunk):
#     results = []
#     for i in chunk:
#         count_called = genos[i].count_called()
#         genotypingRate = count_called/len(sampleToPop)
#         count_het = genos[i].count_het()
#         if genotypingRate < 1:
#             out = False
#         # elif count_het > 0:
#         #     out = False
#         else:
#             out = True
#         results.append(out)
#     return results
# 
# # Split the indices into smaller chunks
# chunk_size = 100000  # Adjust the chunk size as needed
# indices = range(len(genos))
# chunks = [indices[i:i + chunk_size] for i in range(0, len(indices), chunk_size)]
# 
# # Create a pool of worker processes
# pool = multiprocessing.Pool()
# 
# # Use the pool.map() function to distribute the workload across multiple processes
# results = pool.map(biallelic_mask, chunks)
# 
# # Flatten the results
# genosRate100Mask_hetMask = [item for sublist in results for item in sublist]
# 
# # Close the pool to free up resources
# pool.close()
# pool.join()
# 
# # sum(genosRate100Mask_hetMask)
# 
# ## merge with unmasked
# 
# print("Merging with unmasked to get rid of indels and multiallelic...")
# genosRate100Mask_hetMask_2 = copy.copy(unmasked)
# for i in range(len(genosRate100Mask_hetMask)):
#   if unmasked[positions[i]-1]==True and genosRate100Mask_hetMask[i]==False:
#     genosRate100Mask_hetMask_2[positions[i]-1]=False
# 
# ## mask non biallelic
# print("Masking non-biallelic")
# for i in range(len(isBiallelic)):
#   if not isBiallelic[i]:
#     genosRate100Mask_hetMask_2[positions[i]-1]=False

# print("Biallelic loci passing filters " + str(sum(genosRate100Mask_hetMask_2)))
# print("Biallelic loci passing filters " + str(sum(isBiallelic)))

# snpIndicesToKeep=[x for x in range(len(positions)) if genosRate100Mask_hetMask_2[positions[x]-1]] # SCOTT
snpIndicesToKeep=[x for x in range(len(positions)) if isBiallelic[x]] # SCOTT
genos_biallelic=genos.subset(sel0=snpIndicesToKeep) # SCOTT
haps=genos.to_haplotypes()
haps_biallelic=genos_biallelic.to_haplotypes() # SCOTT
# mapping_biallelic=[mapping[x] for x in snpIndicesToKeep] # SCOTT
alleleCounts_biallelic=genos_biallelic.count_alleles() # SCOTT
positions_biallelic=[positions[x] for x in snpIndicesToKeep] # SCOTT
refAlleles_biallelic=[refAlleles[x] for x in snpIndicesToKeep] # SCOTT
altAlleles_biallelic=[altAlleles[x] for x in snpIndicesToKeep] # SCOTT

##################################################################################################

# SCOTT fix problem only, still mask unpolarised

def my_polarizeSnps(unmasked, positions_biallelic, refAlleles_biallelic, altAlleles_biallelic, ancArm):
    assert len(unmasked) == len(ancArm)
    assert len(positions_biallelic) == len(refAlleles_biallelic)
    assert len(positions_biallelic) == len(altAlleles_biallelic)
    isSnp = {}
    for i in range(len(positions_biallelic)):
        isSnp[positions_biallelic[i]] = i
    mapping = []
    for i in range(len(ancArm)):
        if ancArm[i] in 'ACGT':
            if i+1 in isSnp:
                # ref, alt = refAlleles[isSnp[i+1]], altAlleles[isSnp[i+1]]
                ref, alt = refAlleles_biallelic[isSnp[i+1]], altAlleles_biallelic[isSnp[i+1]][0] # SCOTT
                if ancArm[i] == ref:
                    mapping.append([0, 1])  # no swap
                elif ancArm[i] == alt:
                    mapping.append([1, 0])  # swap
                else:
                    mapping.append([0, 1])  # no swap -- failed to polarize
                    unmasked[i] = False # mask SNP if cannot polarize
        elif ancArm[i] == "N":
            #unmasked[i] = False
            if i+1 in isSnp:
                mapping.append([0, 1])  # no swap -- failed to polarize
        else:
            sys.exit(
                "Found a character in ancestral chromosome "\
                 "that is not 'A', 'C', 'G', 'T' or 'N' (all upper case)!\n")
    assert len(mapping) == len(positions_biallelic)
    return mapping, unmasked

################################################################################################

#ancArm=readFaArm(ancestralArmFaFileName).upper()
#sys.stderr.write("polarizing SNPs\n")
#polTime=time.clock()
#mapping,unmasked = polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm)
#sys.stderr.write("took %s seconds to polarize SNPs\n" %((time.clock()-polTime)))

ancArm=readFaArm(ancestralArmFaFileName, chrArm).upper() # no change
print("polarizing SNPs\n") # changed to print but didn't need to
polTime=time.perf_counter() # Python 3
mapping,unmasked = my_polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm) # use my_polarizeSnps
print("took %s seconds to polarize SNPs" % (time.perf_counter() - polTime)) # Python 3

print(chrArm + " is " + str(chrLen) + " basepairs, " + "unmasked " + str(sum(unmasked)) + " basepairs after polarization") # SCOTT

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

statHeader="chrom start end".split()
header="chrom classifiedWinStart classifiedWinEnd bigWinRange".split()

for statName in statNames:
  statHeader.append(statName)
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName,i))

statHeader="\t".join(statHeader)
header="\t".join(header)

precomputedStats={}

def getSubWinBounds(subWinSize,positions):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  subWinBounds=[]
  for i in range(len(positions)):
    while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if (subWinStart,subWinEnd) not in subWinBounds:
      subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,positions)

#dafs=alleleCounts[:,1]/float(len(sampleIndicesToKeep)*2)
dafs=alleleCounts_biallelic[:,1]/float(len(sampleIndicesToKeep)*2) # SCOTT

#ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
#nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
#nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
#sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))

ihsVals=allel.stats.selection.ihs(haps_biallelic,positions_biallelic,use_threads=False,include_edges=False) # SCOTT
nonNanCount=[x for x in np.isnan(ihsVals)].count(False) # SCOTT
nonInfCount=[x for x in np.isinf(ihsVals)].count(False) # SCOTT
print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount)) # SCOTT

#if nonNanCount==0:
#  precomputedStats["iHS"]=[]
#  for subWinIndex in range(len(subWinBounds)):
#    precomputedStats["iHS"].append([])
#else:
#  ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
#  precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["iHS"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["iHS"].append([])
else:
  ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
  precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)


#nslVals=allel.stats.selection.nsl(haps,use_threads=False)
#nonNanCount=[x for x in np.isnan(nslVals)].count(False)
#sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))

nslVals=allel.stats.selection.nsl(haps_biallelic,use_threads=False) # SCOTT
nonNanCount=[x for x in np.isnan(nslVals)].count(False) # SCOTT
print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount)) # SCOTT

#if nonNanCount==0:
#  precomputedStats["nSL"]=[]
#  for subWinIndex in range(len(subWinBounds)):
#    precomputedStats["nSL"].append([])
#else:
#  nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
#  precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["nSL"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["nSL"].append([])
else:
  nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
  precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)


#mispolarization information not used since no inference on which specific SNPs may be mispolarized, and since each block contains a relatively small number of SNPs, especially with respect to the mispolarization rate, cannot effectively use the overall mispolarization rate

def SAFEstats(hapsInSubWin,mappingDerivedInSubWin,dafsInSubWin):
  haplotypes={}
  for i in range(len(hapsInSubWin[0])):
    haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
    haplotype="".join(str(x) for x in haplotype)
    if haplotype in haplotypes:
      haplotypes[haplotype].append(i)
    else:
      haplotypes[haplotype]=[i]
  HAF=[]
  HAFunique={}
  for i in haplotypes:
    HAFunique[i]=0
    for j in range(len(hapsInSubWin)):
      if hapsInSubWin[j][haplotypes[i][0]]==mappingDerivedInSubWin[j]:
        HAFunique[i]+=len([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j])) if hapsInSubWin[j][x]==mappingDerivedInSubWin[j]])
    for j in range(len(haplotypes[i])):
      HAF.append(HAFunique[i])
  phi=[]
  kappa=[]
  SAFE=[]
  for i in range(len(hapsInSubWin)):
    phi.append(0)
    kappa.append([])
    phiDenom=0
    for j in haplotypes:
      if int(list(j)[i])==mappingDerivedInSubWin[i]:
        phi[i]+=(HAFunique[j]*len(haplotypes[j]))
        if HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
          kappa[i].append(HAFunique[j])
      phiDenom+=(HAFunique[j]*len(haplotypes[j]))
    phi[i]/=float(phiDenom)
    kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
    if dafsInSubWin[i]==0 or dafsInSubWin[i]==1:
     SAFE.append(0.0)
    else:
     SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafsInSubWin[i]*(1-dafsInSubWin[i]))))
  statVals={}
  quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}
  for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
    if i=="SFS":
      windowStats=dafsInSubWin
    elif i=="HAFunique":
      #windowStats=[eval(i)[x] for x in eval(i)]
      windowStats = list(HAFunique.values()) # SCOTT
    else:
      windowStats=eval(i)
    statVals[i+"-Mean"]=np.mean(windowStats)
    statVals[i+"-Median"]=np.median(windowStats)
    if(len(np.unique(windowStats,return_counts=True)[1])==1):
      statVals[i+"-Mode"]=windowStats[0]
    else:
      if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
        statVals[i+"-Mode"]=scipy.stats.mstats.mode(windowStats)[0][0]
      else:
        mode=min(windowStats)
        for j in range(1,51):
          if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
            mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
        statVals[i+"-Mode"]=mode+((max(windowStats)-min(windowStats))/100)
    for j in quantiles:
      statVals[i+"-"+j]=np.percentile(windowStats,quantiles[j])
    statVals[i+"-Max"]=max(windowStats)
    statVals[i+"-Var"]=np.var(windowStats)
    statVals[i+"-SD"]=np.std(windowStats)
    statVals[i+"-Skew"]=scipy.stats.skew(windowStats)
    statVals[i+"-Kurt"]=scipy.stats.kurtosis(windowStats)
  return statVals

if segmentStart==None:
  firstSubWinStart=1
  lastSubWinStart=(((chrLen-1)/subWinSize)*subWinSize)+1
else:
  firstSubWinStart=(((segmentStart-1)/subWinSize)*subWinSize)+1
  lastSubWinStart=(((segmentEnd-1)/subWinSize)*subWinSize)+1

goodSubWins=[]

for i in range(numSubWins):
  goodSubWins.append(False)

subWinIndex=-1

#def getSnpIndicesInSubWins(subWinSize,positions):
#  subWinStart=1
#  subWinEnd=subWinStart+subWinSize-1
#  snpIndicesInSubWins=None
#  for i in range(len(positions)):
#    if snpIndicesInSubWins and not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#      snpIndicesInSubWins.append([])
#    while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#      subWinStart+=subWinSize
#      subWinEnd+=subWinSize
#    if not snpIndicesInSubWins:
#      snpIndicesInSubWins=[[]]
#    snpIndicesInSubWins[-1].append(i)
#  return snpIndicesInSubWins

def getSnpIndicesInSubWins(subWinSize,positions_biallelic):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  snpIndicesInSubWins=None
  for i in range(len(positions_biallelic)):
    if snpIndicesInSubWins and not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      snpIndicesInSubWins.append([])
    while not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if not snpIndicesInSubWins:
      snpIndicesInSubWins=[[]]
    snpIndicesInSubWins[-1].append(i)
  return snpIndicesInSubWins

#snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions)
snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions_biallelic) # SCOTT

statVals={}

for statName in statNames:
  statVals[statName]=[]

if statFileName.lower() in ["none","false"]:
  statFileName=None
else:
  statFile=open(statFileName,"w")
  statFile.write(statHeader+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  if segmentStart!=None:
    fvecFileName=targetPop+'.'+chrArm+'.'+segStart+'.fvec'
  else:
    fvecFileName=targetPop+'.'+chrArm+'.fvec'

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

#for subWinStart in range(firstSubWinStart,lastSubWinStart+1,subWinSize):
for subWinStart in range(int(firstSubWinStart),int(lastSubWinStart)+1,int(subWinSize)):
  subWinEnd=subWinStart+subWinSize-1
  unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
  #if len([x for x in positions if subWinStart<=x<=subWinEnd])==0:
  if len([x for x in positions_biallelic if subWinStart<=x<=subWinEnd])==0:
    #sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    goodSubWins.append(False)
  else:
    subWinIndex+=1
    #sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    # print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    if unmaskedFrac<unmaskedFracCutoff:
      goodSubWins.append(False)
    else:
      goodSubWins.append(True)
      #hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
      hapsInSubWin=haps_biallelic.subset(sel0=snpIndicesInSubWins[subWinIndex])
      print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f; haplotypes - %d; unique haplotypes - %d\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac, hapsInSubWin.shape[1], np.unique(hapsInSubWin, axis=1).shape[1]))
      for statName in statNames:
        if statName!="H12" and statName!="H2/H1" and statName!="Omega" and statName!="distSkew" and statName!="distKurt" and statName[0:3]!="HAF" and statName[0:3]!="phi" and statName[0:3]!="kap" and statName[0:3]!="SFS" and statName[0:3]!="SAF":
          if statName=="fayWuH":
            #calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
            calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
          else:
            #calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
            calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
      #SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      for statName in SAFEVals:
        statVals[statName].append(SAFEVals[statName])
      if statFileName:
        statFile.write("\t".join([chrArm, str(subWinStart), str(subWinEnd)] + [str(statVals[statName][-1]) for statName in statNames]) + "\n")
  goodSubWins=goodSubWins[1:]
  if goodSubWins.count(True)==numSubWins:
    outVec=[]
    for statName in statNames:
      outVec+=normalizeFeatureVec(statVals[statName][-numSubWins:])
    midSubWinEnd=subWinEnd-(subWinSize*(numSubWins/2))
    midSubWinStart=midSubWinEnd-subWinSize+1
    fvecFile.write("\t".join([chrArm, str(midSubWinStart), str(midSubWinEnd), str((subWinEnd-(subWinSize*numSubWins)+1))+"-"+str(subWinEnd)] + [str(x) for x in outVec]) + "\n")

if statFileName:
  statFile.close()
fvecFile.close()
#sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
print("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### MAX windows

```{python, MAX windows}
import time
#startTime=time.clock()
startTime = time.perf_counter() # SCOTT
import sys
import h5py
import allel
from fvTools import *
import numpy as np
import pandas as pd
import math

'''usage eg:
pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
python2 empirical_convert_to_FVs.py ag1000g.phase1.ar3.haplotypes.2L.h5 2L 49364325 1 5000000 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.2L.fa samples_pops.txt AOM 2L.1.stats 2L.1.fvec
'''

# if not len(sys.argv) in [15,17]:
#   sys.exit("usage:\npython2 empirical_convert_to_FVs.py chrArmFileName chrArm chrLen [segmentStart segmentEnd] subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName sampleToPopFileName targetPop statFileName fvecFileName\n")
# if len(sys.argv)==17:
#   chrArmFileName, chrArm, chrLen, segmentStart, segmentEnd, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
# else:
#   chrArmFileName, chrArm, chrLen, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
#   segmentStart=None

### input variables

# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_542.filtered.bi.h5'
chrArmFileName_OLD='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/ScA8VGg_718.filtered.bi.vcf.gz'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/snps_PASS_BIALLELIC_homoRefs_shortScfName.vcf.gz'
# chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/snps_PASS_BIALLELIC_MULTIALLELIC_homoRefs_shortScfName.vcf.gz'
chrArmFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/output_Default_allSites_myFilter.vcf.gz'
chrArm='ScA8VGg_718'
chrLen='8653123'
segmentStart=1
# segmentStart=None
segmentEnd=8653123
# segmentEnd=None
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt'
maskFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/top6.mask.fa'
ancestralArmFaFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/top6.anc.fa'
# sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/samples_pops.txt'
sampleToPopFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt'
targetPop="dser"
statFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.1.scott.test.stats'
fvecFileName='/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.1.scott.test.fvec'

## import data

# chrArmFile=h5py.File(chrArmFileName,"r")
chrArmFile_OLD=allel.read_vcf(chrArmFileName_OLD)
chrArmFile=allel.read_vcf(chrArmFileName)
#genos=allel.GenotypeChunkedArray(chrArmFile[chrArm]["calldata"]["genotype"])
# genos = chrArmFile['calldata/GT'] # SCOTT HDF5
genos_OLD = allel.GenotypeArray(chrArmFile_OLD['calldata/GT']) # SCOTT VCF
genos = allel.GenotypeArray(chrArmFile['calldata/GT']) # SCOTT VCF
# positions=allel.SortedIndex(chrArmFile["/%s/variants/POS" %(chrArm)][:])
# positions=chrArmFile['variants/POS'] # SCOTT HDF5
positions_OLD=allel.SortedIndex(chrArmFile_OLD['variants/POS'], copy=False) # SCOTT VCF
positions=allel.SortedIndex(chrArmFile['variants/POS'], copy=False) # SCOTT VCF
# refAlleles=chrArmFile[chrArm]['variants']['REF']
refAlleles_OLD=chrArmFile_OLD['variants/REF'] # SCOTT
refAlleles=chrArmFile['variants/REF'] # SCOTT
# altAlleles=chrArmFile[chrArm]['variants']['ALT']
altAlleles_OLD=chrArmFile_OLD['variants/ALT'] # SCOTT
altAlleles=chrArmFile['variants/ALT'] # SCOTT
# samples=chrArmFile[chrArm]["samples"]
samples_OLD=chrArmFile_OLD['samples']
samples=chrArmFile['samples']

chrLen=int(chrLen)
assert chrLen>0

if segmentStart!=None:
  segmentStart,segmentEnd = int(segmentStart),int(segmentEnd)
  assert segmentStart>0 and segmentEnd>=segmentStart
  snpIndicesToKeep=[x for x in range(len(positions)) if segmentStart<=positions[x]<=segmentEnd]
  genos=allel.GenotypeArray(genos.subset(sel0=snpIndicesToKeep))
  positions=[positions[x] for x in snpIndicesToKeep]
  refAlleles=[refAlleles[x] for x in snpIndicesToKeep]
  altAlleles=[altAlleles[x] for x in snpIndicesToKeep]


subWinSize,numSubWins,unmaskedFracCutoff,pMisPol = int(subWinSize),int(numSubWins),float(unmaskedFracCutoff),float(pMisPol)
assert subWinSize>0 and numSubWins>1

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")


if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")


standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none","false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*chrLen
  sys.stderr.write("Warning: a mask.fa file for the chr arm with all masked sites N'ed out is strongly recommended (pass in the reference to remove Ns at the very least)!\n")
else:
  unmasked=readMaskDataForScan(maskFileName,chrArm)
  assert len(unmasked)==chrLen

# unmask all SCOTT
for i in range(len(unmasked)):
    unmasked[i] = True

len(genos)
len(positions)
len(refAlleles)
len(altAlleles)
len(unmasked)
sum(unmasked)

##################################################################################################

# This will polarise when it can and not mask when it can't (assumes REF is ancestral)

def my_polarizeSnps(unmasked, positions, refAlleles, altAlleles, ancArm):
    assert len(unmasked) == len(ancArm)
    assert len(positions) == len(refAlleles)
    assert len(positions) == len(altAlleles)
    isSnp = {}
    for i in range(len(positions)):
        isSnp[positions[i]] = i
    mapping = []
    for i in range(len(ancArm)):
        if ancArm[i] in 'ACGT':
            if i+1 in isSnp:
                # ref, alt = refAlleles[isSnp[i+1]], altAlleles[isSnp[i+1]]
                ref, alt = refAlleles[isSnp[i+1]], altAlleles[isSnp[i+1]][0] # SCOTT
                if ancArm[i] == ref:
                    mapping.append([0, 1])  # no swap
                elif ancArm[i] == alt:
                    mapping.append([1, 0])  # swap
                else:
                    mapping.append([0, 1])  # no swap -- failed to polarize
                    # unmasked[i] = False
        elif ancArm[i] == "N":
            # unmasked[i] = False
            if i+1 in isSnp:
                mapping.append([0, 1])  # no swap -- failed to polarize
        else:
            sys.exit(
                "Found a character in ancestral chromosome "\
                 "that is not 'A', 'C', 'G', 'T' or 'N' (all upper case)!\n")
    assert len(mapping) == len(positions)
    return mapping, unmasked

################################################################################################

# ancArm=readFaArm(ancestralArmFaFileName).upper()
ancArm=readFaArm(ancestralArmFaFileName, chrArm).upper() # SCOTT
# sys.stderr.write("polarizing SNPs\n")
print("polarizing SNPs\n") # SCOTT
# polTime=time.clock()
polTime=time.perf_counter() # SCOTT
# mapping,unmasked = polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm)
mapping,unmasked = my_polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm)
# sys.stderr.write("took %s seconds to polarize SNPs\n" %((time.clock()-polTime)))
print("took %s seconds to polarize SNPs" % (time.process_time() - polTime), file=sys.stderr) # SCOTT


def readSampleToPopFile(sampleToPopFileName):
  table={}
  with open(sampleToPopFileName) as sampleToPopFile:
    for line in sampleToPopFile:
      sample,pop = line.strip().split()
      table[sample]=pop
  return table


sampleToPop=readSampleToPopFile(sampleToPopFileName)

sampleIndicesToKeep=[x for x in range(len(samples)) if sampleToPop.get(samples[x],"popNotFound!")==targetPop]

genos=genos.subset(sel1=sampleIndicesToKeep)
alleleCounts=genos.count_alleles() # Count the number of calls of each allele per variant.
isBiallelic=alleleCounts.is_biallelic() # Find variants biallelic for the reference (0) and first alternate (1) allele.

alleleCounts_OLD=genos_OLD.count_alleles() # Count the number of calls of each allele per variant.
isBiallelic_OLD=alleleCounts_OLD.is_biallelic() # Find variants biallelic for the reference (0) and first alternate (1) allele.

# # Genotyping rate per position
# genosRate = []
# 
# # for i in range(len(genos)):
# #   temp1 = genos[i].count_called()
# #   temp2 = temp1/111
# #   genoRate.append(temp2)
# 
# # parallel
# import multiprocessing
# 
# genosRate = []
# 
# # Define a worker function
# def calculate_genotype_rate(i):
#     temp1 = genos[i].count_called()
#     return temp1 / 111
# 
# # run worker in parallel
# if __name__ == '__main__':
#     # Create a pool of worker processes
#     pool = multiprocessing.Pool()
#     # Use the pool.map() function to distribute the workload across multiple processes
#     genosRate = pool.map(calculate_genotype_rate, range(len(genos)))
#     # Close the pool to free up resources
#     pool.close()
#     pool.join()
# 
# # plot distribution
# import matplotlib.pyplot as plt
# 
# # Plot the density distribution of genoRate
# plt.hist(genosRate, bins=30, density=True, alpha=0.5)
# plt.xlabel('Genotyping Rate per Position (called/111 lines)')
# plt.ylabel('Density')
# plt.title('Density Distribution of Genotyping Rate per Position')
# plt.show()

# genotyping rate mask

import multiprocessing

unmasked = readMaskDataForScan(maskFileName, chrArm)

for i in range(len(unmasked)):
    unmasked[i] = True

genosRateMask25 = []

# Define a worker function
def genotype_rate_mask(chunk):
    results = []
    for i in chunk:
        temp1 = genos[i].count_called()
        temp2 = temp1/111 >= 0.25
        results.append(temp2)
    return results

# Split the indices into smaller chunks
chunk_size = 100000  # Adjust the chunk size as needed
indices = range(len(unmasked))
chunks = [indices[i:i + chunk_size] for i in range(0, len(indices), chunk_size)]

# Create a pool of worker processes
pool = multiprocessing.Pool()

# Use the pool.map() function to distribute the workload across multiple processes
results = pool.map(genotype_rate_mask, chunks)

# Flatten the results
genosRateMask25 = [item for sublist in results for item in sublist]

# Close the pool to free up resources
pool.close()
pool.join()

1-(sum(genosRateMask25)/len(genos))

1% genotyped = 0.03761872321005022 masked
5% genotyped = 0.044541975300459336 masked
10% genotyped = 0.049460271575644366 masked
25% genotyped = 0.05857855970791559 masked
50% genotyped = 0.07887607432018506 masked
75% genotyped = 0.10119866531961408 masked
90% genotyped = 0.13361586561541017 masked
95% genotyped = 0.17024264462903815 masked
100% genotyped = 0.49395341306445306 masked

###########################################################################################################

# # mask non biallelic
# # only run if vcf is only SNPs
# for i in range(len(isBiallelic)):
#   if not isBiallelic[i]:
#     unmasked[positions[i]-1]=False

##########################################################################################################

##########################################################################################################

# filter for biallelic SNPs 100% genotyped

## mask < 100% genotyped

unmasked = readMaskDataForScan(maskFileName, chrArm)

for i in range(len(unmasked)):
    unmasked[i] = True

genosRateMask100 = []

# Define a worker function
def genotype_rate_mask(chunk):
    results = []
    for i in chunk:
        temp1 = genos[i].count_called()
        temp2 = temp1/111 >= 1
        # unmasked[i] = temp2
        results.append(temp2)
    return results

# Split the indices into smaller chunks
chunk_size = 100000  # Adjust the chunk size as needed
indices = range(len(unmasked))
chunks = [indices[i:i + chunk_size] for i in range(0, len(indices), chunk_size)]

# Create a pool of worker processes
pool = multiprocessing.Pool()

# Use the pool.map() function to distribute the workload across multiple processes
results = pool.map(genotype_rate_mask, chunks)

# Flatten the results
genosRateMask100 = [item for sublist in results for item in sublist]

# Close the pool to free up resources
pool.close()
pool.join()

sum(genosRateMask100)

## mask non biallelic
for i in range(len(isBiallelic)):
  if not isBiallelic[i]:
    genosRateMask100[i]=False

sum(genosRateMask100)

# alleleCounts=genos.count_alleles() # Count the number of calls of each allele per variant.
# isBiallelic=alleleCounts.is_biallelic() # Find variants biallelic for the reference (0) and first alternate (1) allele.

# snpIndicesToKeep=[x for x in range(len(positions)) if unmasked[positions[x]-1]]
snpIndicesToKeep=[x for x in range(len(positions)) if genosRateMask100[positions[x]-1]] # SCOTT
# genos=genos.subset(sel0=snpIndicesToKeep)
genos_biallelic=genos.subset(sel0=snpIndicesToKeep) # SCOTT
haps=genos.to_haplotypes()
haps_biallelic=genos_biallelic.to_haplotypes() # SCOTT
# mapping=[mapping[x] for x in snpIndicesToKeep]
mapping_biallelic=[mapping[x] for x in snpIndicesToKeep] # SCOTT
# alleleCounts=alleleCounts.map_alleles(mapping)
alleleCounts_biallelic=genos_biallelic.count_alleles() # SCOTT
# positions=[positions[x] for x in snpIndicesToKeep]
positions_biallelic=[positions[x] for x in snpIndicesToKeep] # SCOTT

# recode biallelic no REF
genos_biallelic_recode = genos_biallelic
for row in genos_biallelic_recode:
    if not np.any(row == 0):
        row[row == 1] = 0
        row[row == 2] = 1

# Identify rows with heterozygous genotypes (0/1)
het_mask = np.logical_and(genos_biallelic_recode[:, :, 0] == 0, genos_biallelic_recode[:, :, 1] == 1)

# Exclude rows with heterozygous genotypes
genos_biallelic_filtered = genos_biallelic_recode[~np.any(het_mask, axis=1)]

print(genos_biallelic_recode)
print(genos_biallelic_filtered)

haps_biallelic=genos_biallelic_filtered() # SCOTT
alleleCounts_biallelic=genos_biallelic_filtered() # SCOTT
positions_biallelic=[positions[x] for x in snpIndicesToKeep] # SCOTT


import pandas as pd

df = pd.DataFrame(alleleCounts_biallelic, columns=['REF', 'ALT1', 'ALT2'])


##########################################################################################################

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))


statHeader="chrom start end".split()
header="chrom classifiedWinStart classifiedWinEnd bigWinRange".split()

for statName in statNames:
  statHeader.append(statName)
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName,i))


statHeader="\t".join(statHeader)
header="\t".join(header)

precomputedStats={}
def getSubWinBounds(subWinSize,positions):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  subWinBounds=[]
  for i in range(len(positions)):
    while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if (subWinStart,subWinEnd) not in subWinBounds:
      subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds


subWinBounds=getSubWinBounds(subWinSize,positions)



# dafs=alleleCounts[:,1]/float(len(sampleIndicesToKeep)*2)
dafs=alleleCounts_biallelic[:,1]/float(len(sampleIndicesToKeep)*2) # SCOTT
# ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
ihsVals=allel.stats.selection.ihs(haps_biallelic,positions_biallelic,use_threads=False,include_edges=False)
nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
# sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount)) # SCOTT

# if nonNanCount==0:
#   precomputedStats["iHS"]=[]
#   for subWinIndex in range(len(subWinBounds)):
#     precomputedStats["iHS"].append([])
# else:
#   ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
#   precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["iHS"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["iHS"].append([])
else:
  ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
  precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)


# nslVals=allel.stats.selection.nsl(haps,use_threads=False)
nslVals=allel.stats.selection.nsl(haps_biallelic,use_threads=False) # SCOTT
nonNanCount=[x for x in np.isnan(nslVals)].count(False)
# sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount)) # SCOTT

# if nonNanCount==0:
#   precomputedStats["nSL"]=[]
#   for subWinIndex in range(len(subWinBounds)):
#     precomputedStats["nSL"].append([])
# else:
#   nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
#   precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["nSL"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["nSL"].append([])
else:
  nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
  precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)


#mispolarization information not used since no inference on which specific SNPs may be mispolarized, and since each block contains a relatively small number of SNPs, especially with respect to the mispolarization rate, cannot effectively use the overall mispolarization rate

def SAFEstats(hapsInSubWin,mappingDerivedInSubWin,dafsInSubWin):
  haplotypes={}
  for i in range(len(hapsInSubWin[0])):
    haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
    haplotype="".join(str(x) for x in haplotype)
    if haplotype in haplotypes:
      haplotypes[haplotype].append(i)
    else:
      haplotypes[haplotype]=[i]
  HAF=[]
  HAFunique={}
  for i in haplotypes:
    HAFunique[i]=0
    for j in range(len(hapsInSubWin)):
      if hapsInSubWin[j][haplotypes[i][0]]==mappingDerivedInSubWin[j]:
        HAFunique[i]+=len([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j])) if hapsInSubWin[j][x]==mappingDerivedInSubWin[j]])
    for j in range(len(haplotypes[i])):
      HAF.append(HAFunique[i])
  phi=[]
  kappa=[]
  SAFE=[]
  for i in range(len(hapsInSubWin)):
    phi.append(0)
    kappa.append([])
    phiDenom=0
    for j in haplotypes:
      if int(list(j)[i])==mappingDerivedInSubWin[i]:
        phi[i]+=(HAFunique[j]*len(haplotypes[j]))
        if HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
          kappa[i].append(HAFunique[j])
      phiDenom+=(HAFunique[j]*len(haplotypes[j]))
    phi[i]/=float(phiDenom)
    kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
    if dafsInSubWin[i]==0 or dafsInSubWin[i]==1:
     SAFE.append(0.0)
    else:
     SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafsInSubWin[i]*(1-dafsInSubWin[i]))))
  statVals={}
  quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}
  for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
    if i=="SFS":
      windowStats=dafsInSubWin
    elif i=="HAFunique":
      #windowStats=[eval(i)[x] for x in eval(i)]
      windowStats = list(HAFunique.values()) # SCOTT
    else:
      windowStats=eval(i)
    statVals[i+"-Mean"]=np.mean(windowStats)
    statVals[i+"-Median"]=np.median(windowStats)
    if(len(np.unique(windowStats,return_counts=True)[1])==1):
      statVals[i+"-Mode"]=windowStats[0]
    else:
      if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
        statVals[i+"-Mode"]=scipy.stats.mstats.mode(windowStats)[0][0]
      else:
        mode=min(windowStats)
        for j in range(1,51):
          if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
            mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
        statVals[i+"-Mode"]=mode+((max(windowStats)-min(windowStats))/100)
    for j in quantiles:
      statVals[i+"-"+j]=np.percentile(windowStats,quantiles[j])
    statVals[i+"-Max"]=max(windowStats)
    statVals[i+"-Var"]=np.var(windowStats)
    statVals[i+"-SD"]=np.std(windowStats)
    statVals[i+"-Skew"]=scipy.stats.skew(windowStats)
    statVals[i+"-Kurt"]=scipy.stats.kurtosis(windowStats)
  return statVals

###########################################################################################

subWinIndex=-1
subWinStart=1
subWinEnd=subWinStart+subWinSize-1
unmaskedFrac=genosRateMask25[subWinStart-1:subWinEnd].count(True)/float(subWinSize)

# if len([x for x in positions if subWinStart<=x<=subWinEnd])==0:
#   print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
#   goodSubWins.append(False)
# else:
#   subWinIndex+=1
#   print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))

if len([x for x in positions_biallelic if subWinStart<=x<=subWinEnd])==0:
  print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
  goodSubWins.append(False)
else:
  subWinIndex+=1
  print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))

if unmaskedFrac<unmaskedFracCutoff:
  goodSubWins.append(False)
else:
  goodSubWins.append(True)

hapsInSubWin=haps_biallelic.subset(sel0=snpIndicesInSubWins[subWinIndex])
mappingDerivedInSubWin=[mapping_biallelic[x][1] for x in snpIndicesInSubWins[subWinIndex]]
dafsInSubWin=dafs[snpIndicesInSubWins[subWinIndex]]

genosInSubWin=genos_biallelic.subset(sel0=snpIndicesInSubWins[subWinIndex])
# genosInSubWin[:,47,:]

# for i in range(len(positions)):
#   if (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#     print(positions[i])

# def SAFEstats(hapsInSubWin,mappingDerivedInSubWin,dafsInSubWin):

haplotypes={}

for i in range(len(hapsInSubWin[0])):
  haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
  haplotype="".join(str(x) for x in haplotype)
  if haplotype in haplotypes:
    haplotypes[haplotype].append(i)
  else:
    haplotypes[haplotype]=[i]

HAF=[]
HAFunique={}

for i in haplotypes:
  HAFunique[i]=0
  for j in range(len(hapsInSubWin)):
    if hapsInSubWin[j][haplotypes[i][0]]==mappingDerivedInSubWin[j]:
      HAFunique[i]+=len([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j])) if hapsInSubWin[j][x]==mappingDerivedInSubWin[j]])
  for j in range(len(haplotypes[i])):
    HAF.append(HAFunique[i])

phi=[]
kappa=[]
SAFE=[]

for i in range(len(hapsInSubWin)):
  phi.append(0)
  kappa.append([])
  phiDenom=0
  for j in haplotypes:
    if int(list(j)[i])==mappingDerivedInSubWin[i]:
      phi[i]+=(HAFunique[j]*len(haplotypes[j]))
      if HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
        kappa[i].append(HAFunique[j])
    phiDenom+=(HAFunique[j]*len(haplotypes[j]))
  phi[i]/=float(phiDenom)
  kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
  if dafsInSubWin[i]==0 or dafsInSubWin[i]==1:
   SAFE.append(0.0)
  else:
   SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafsInSubWin[i]*(1-dafsInSubWin[i]))))

statVals={}
quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  if i=="SFS":
    windowStats=dafsInSubWin
  elif i=="HAFunique":
    #windowStats=[eval(i)[x] for x in eval(i)]
    windowStats = list(HAFunique.values()) # SCOTT
  else:
    windowStats=eval(i)
  statVals[i+"-Mean"]=np.mean(windowStats)
  statVals[i+"-Median"]=np.median(windowStats)
  if(len(np.unique(windowStats,return_counts=True)[1])==1):
    statVals[i+"-Mode"]=windowStats[0]
  else:
    if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
      statVals[i+"-Mode"]=scipy.stats.mstats.mode(windowStats)[0][0]
    else:
      mode=min(windowStats)
      for j in range(1,51):
        if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
          mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
      statVals[i+"-Mode"]=mode+((max(windowStats)-min(windowStats))/100)

for j in quantiles:
  statVals[i+"-"+j]=np.percentile(windowStats,quantiles[j])


statVals[i+"-Max"]=max(windowStats)
statVals[i+"-Var"]=np.var(windowStats)
statVals[i+"-SD"]=np.std(windowStats)
statVals[i+"-Skew"]=scipy.stats.skew(windowStats)
statVals[i+"-Kurt"]=scipy.stats.kurtosis(windowStats)

###########################################################################################


if segmentStart==None:
  firstSubWinStart=1
  lastSubWinStart=(((chrLen-1)/subWinSize)*subWinSize)+1
else:
  firstSubWinStart=(((segmentStart-1)/subWinSize)*subWinSize)+1
  lastSubWinStart=(((segmentEnd-1)/subWinSize)*subWinSize)+1


goodSubWins=[]
for i in range(numSubWins):
  goodSubWins.append(False)


subWinIndex=-1


# def getSnpIndicesInSubWins(subWinSize,positions):
#   subWinStart=1
#   subWinEnd=subWinStart+subWinSize-1
#   snpIndicesInSubWins=None
#   for i in range(len(positions)):
#     if snpIndicesInSubWins and not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#       snpIndicesInSubWins.append([])
#     while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#       subWinStart+=subWinSize
#       subWinEnd+=subWinSize
#     if not snpIndicesInSubWins:
#       snpIndicesInSubWins=[[]]
#     snpIndicesInSubWins[-1].append(i)
#   return snpIndicesInSubWins

def getSnpIndicesInSubWins(subWinSize,positions_biallelic):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  snpIndicesInSubWins=None
  for i in range(len(positions_biallelic)):
    if snpIndicesInSubWins and not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      snpIndicesInSubWins.append([])
    while not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if not snpIndicesInSubWins:
      snpIndicesInSubWins=[[]]
    snpIndicesInSubWins[-1].append(i)
  return snpIndicesInSubWins


# snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions)
snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions_biallelic) # SCOTT

statVals={}
for statName in statNames:
  statVals[statName]=[]


if statFileName.lower() in ["none","false"]:
  statFileName=None
else:
  statFile=open(statFileName,"w")
  statFile.write(statHeader+"\n")


if fvecFileName.lower() in ["none","false","default"]:
  if segmentStart!=None:
    fvecFileName=targetPop+'.'+chrArm+'.'+segStart+'.fvec'
  else:
    fvecFileName=targetPop+'.'+chrArm+'.fvec'


fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

for subWinStart in range(firstSubWinStart,int(lastSubWinStart+1),subWinSize):
  subWinEnd=subWinStart+subWinSize-1
  unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
  if len([x for x in positions if subWinStart<=x<=subWinEnd])==0:
    # sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    goodSubWins.append(False)
  else:
    subWinIndex+=1
    # sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    if unmaskedFrac<unmaskedFracCutoff:
      goodSubWins.append(False)
    else:
      goodSubWins.append(True)
      hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
      for statName in statNames:
        if statName!="H12" and statName!="H2/H1" and statName!="Omega" and statName!="distSkew" and statName!="distKurt" and statName[0:3]!="HAF" and statName[0:3]!="phi" and statName[0:3]!="kap" and statName[0:3]!="SFS" and statName[0:3]!="SAF":
          if statName=="fayWuH":
            calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
          else:
            calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
      SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      for statName in SAFEVals:
        statVals[statName].append(SAFEVals[statName])
      if statFileName:
        statFile.write("\t".join([chrArm, str(subWinStart), str(subWinEnd)] + [str(statVals[statName][-1]) for statName in statNames]) + "\n")
  goodSubWins=goodSubWins[1:]
  if goodSubWins.count(True)==numSubWins:
    outVec=[]
    for statName in statNames:
      outVec+=normalizeFeatureVec(statVals[statName][-numSubWins:])
    midSubWinEnd=subWinEnd-(subWinSize*(numSubWins/2))
    midSubWinStart=midSubWinEnd-subWinSize+1
    fvecFile.write("\t".join([chrArm, str(midSubWinStart), str(midSubWinEnd), str((subWinEnd-(subWinSize*numSubWins)+1))+"-"+str(subWinEnd)] + [str(x) for x in outVec]) + "\n")

###########################################################################################

subWinIndex=-1

subWinStart=1
# subWinStart=110001
# subWinStart=120001
# subWinStart=5775001
# subWinStart=2610001

# for subWinStart in range(firstSubWinStart,int(lastSubWinStart+1),subWinSize):
  
subWinEnd=subWinStart+subWinSize-1
# unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
unmaskedFrac=genosRateMask25[subWinStart-1:subWinEnd].count(True)/float(subWinSize) # SCOTT

# if len([x for x in positions if subWinStart<=x<=subWinEnd])==0:
if len([x for x in positions_biallelic if subWinStart<=x<=subWinEnd])==0:
  # sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
  print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
  goodSubWins.append(False)
else:
  subWinIndex+=1
  # sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
  print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))

if unmaskedFrac<unmaskedFracCutoff:
  goodSubWins.append(False)
else:
  goodSubWins.append(True)
  # hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
  hapsInSubWin=haps_biallelic.subset(sel0=snpIndicesInSubWins[subWinIndex]) # SCOTT
  for statName in statNames:
    if statName!="H12" and statName!="H2/H1" and statName!="Omega" and statName!="distSkew" and statName!="distKurt" and statName[0:3]!="HAF" and statName[0:3]!="phi" and statName[0:3]!="kap" and statName[0:3]!="SFS" and statName[0:3]!="SAF":
      if statName=="fayWuH":
        # calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
        calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, genosRateMask25, precomputedStats)
      else:
        # calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, genosRateMask25, precomputedStats)
  # SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
  SAFEVals=SAFEstats(hapsInSubWin,[mapping_biallelic[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
  for statName in SAFEVals:
    statVals[statName].append(SAFEVals[statName])
  if statFileName:
    statFile.write("\t".join([chrArm, str(subWinStart), str(subWinEnd)] + [str(statVals[statName][-1]) for statName in statNames]) + "\n")

goodSubWins=goodSubWins[1:]

if goodSubWins.count(True)==numSubWins:
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][-numSubWins:])
  midSubWinEnd=subWinEnd-(subWinSize*(numSubWins/2))
  midSubWinStart=midSubWinEnd-subWinSize+1
  fvecFile.write("\t".join([chrArm, str(midSubWinStart), str(midSubWinEnd), str((subWinEnd-(subWinSize*numSubWins)+1))+"-"+str(subWinEnd)] + [str(x) for x in outVec]) + "\n")

####################################################################################


if statFileName:
  statFile.close()


fvecFile.close()
# sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### MAX windows python

```{python}

import time
#startTime=time.clock()
startTime = time.perf_counter() # SCOTT
import sys
import h5py
import allel
from fvTools import *
import numpy as np
import math

'''usage eg:
pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
python2 empirical_convert_to_FVs.py ag1000g.phase1.ar3.haplotypes.2L.h5 2L 49364325 1 5000000 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.2L.fa samples_pops.txt AOM 2L.1.stats 2L.1.fvec
'''

if not len(sys.argv) in [15,17]:
  sys.exit("usage:\npython2 empirical_convert_to_FVs.py chrArmFileName chrArm chrLen [segmentStart segmentEnd] subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName sampleToPopFileName targetPop statFileName fvecFileName\n")
if len(sys.argv)==17:
  chrArmFileName, chrArm, chrLen, segmentStart, segmentEnd, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
else:
  chrArmFileName, chrArm, chrLen, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
  segmentStart=None

#chrArmFile=h5py.File(chrArmFileName,"r")
#genos=allel.GenotypeChunkedArray(chrArmFile[chrArm]["calldata"]["genotype"])
#positions=allel.SortedIndex(chrArmFile["/%s/variants/POS" %(chrArm)][:])
#refAlleles=chrArmFile[chrArm]['variants']['REF']
#altAlleles=chrArmFile[chrArm]['variants']['ALT']
#samples=chrArmFile[chrArm]["samples"]
#chrLen=int(chrLen)
#assert chrLen>0

chrArmFile=allel.read_vcf(chrArmFileName)
genos = allel.GenotypeArray(chrArmFile['calldata/GT']) # SCOTT VCF
positions=allel.SortedIndex(chrArmFile['variants/POS'], copy=False) # SCOTT VCF
refAlleles=chrArmFile['variants/REF'] # SCOTT
altAlleles=chrArmFile['variants/ALT'] # SCOTT
samples=chrArmFile['samples'] # SCOTT
chrLen=int(chrLen)
assert chrLen>0

if segmentStart!=None:
  segmentStart,segmentEnd = int(segmentStart),int(segmentEnd)
  assert segmentStart>0 and segmentEnd>=segmentStart
  snpIndicesToKeep=[x for x in range(len(positions)) if segmentStart<=positions[x]<=segmentEnd]
  genos=allel.GenotypeArray(genos.subset(sel0=snpIndicesToKeep))
  positions=[positions[x] for x in snpIndicesToKeep]
  refAlleles=[refAlleles[x] for x in snpIndicesToKeep]
  altAlleles=[altAlleles[x] for x in snpIndicesToKeep]

subWinSize,numSubWins,unmaskedFracCutoff,pMisPol = int(subWinSize),int(numSubWins),float(unmaskedFracCutoff),float(pMisPol)

assert subWinSize>0 and numSubWins>1

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")

if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")

standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none","false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*chrLen
  sys.stderr.write("Warning: a mask.fa file for the chr arm with all masked sites N'ed out is strongly recommended (pass in the reference to remove Ns at the very least)!\n")
else:
  unmasked=readMaskDataForScan(maskFileName,chrArm)
  assert len(unmasked)==chrLen

# unmask all SCOTT
for i in range(len(unmasked)):
    unmasked[i] = True

##################################################################################################

# This will polarise when it can and not mask when it can't (assumes REF is ancestral)

def my_polarizeSnps(unmasked, positions, refAlleles, altAlleles, ancArm):
    assert len(unmasked) == len(ancArm)
    assert len(positions) == len(refAlleles)
    assert len(positions) == len(altAlleles)
    isSnp = {}
    for i in range(len(positions)):
        isSnp[positions[i]] = i
    mapping = []
    for i in range(len(ancArm)):
        if ancArm[i] in 'ACGT':
            if i+1 in isSnp:
                # ref, alt = refAlleles[isSnp[i+1]], altAlleles[isSnp[i+1]]
                ref, alt = refAlleles[isSnp[i+1]], altAlleles[isSnp[i+1]][0] # SCOTT
                if ancArm[i] == ref:
                    mapping.append([0, 1])  # no swap
                elif ancArm[i] == alt:
                    mapping.append([1, 0])  # swap
                else:
                    mapping.append([0, 1])  # no swap -- failed to polarize
                    # unmasked[i] = False
        elif ancArm[i] == "N":
            # unmasked[i] = False
            if i+1 in isSnp:
                mapping.append([0, 1])  # no swap -- failed to polarize
        else:
            sys.exit(
                "Found a character in ancestral chromosome "\
                 "that is not 'A', 'C', 'G', 'T' or 'N' (all upper case)!\n")
    assert len(mapping) == len(positions)
    return mapping, unmasked

################################################################################################

#ancArm=readFaArm(ancestralArmFaFileName).upper()
#sys.stderr.write("polarizing SNPs\n")
#polTime=time.clock()
#mapping,unmasked = polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm)
#sys.stderr.write("took %s seconds to polarize SNPs\n" %((time.clock()-polTime)))

ancArm=readFaArm(ancestralArmFaFileName, chrArm).upper() # SCOTT
print("polarizing SNPs\n") # SCOTT
polTime=time.perf_counter() # SCOTT
mapping,unmasked = my_polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm) # SCOTT
print("took %s seconds to polarize SNPs" % (time.process_time() - polTime)) # SCOTT

def readSampleToPopFile(sampleToPopFileName):
  table={}
  with open(sampleToPopFileName) as sampleToPopFile:
    for line in sampleToPopFile:
      sample,pop = line.strip().split()
      table[sample]=pop
  return table

sampleToPop=readSampleToPopFile(sampleToPopFileName)
sampleIndicesToKeep=[x for x in range(len(samples)) if sampleToPop.get(samples[x],"popNotFound!")==targetPop]

genos=genos.subset(sel1=sampleIndicesToKeep)
alleleCounts=genos.count_alleles()
isBiallelic=alleleCounts.is_biallelic()

# genotyping rate mask

import multiprocessing

genosRateMask25 = []

# Define a worker function
def genotype_rate_mask(chunk):
    results = []
    for i in chunk:
        temp1 = genos[i].count_called()
        temp2 = temp1/111 >= 0.25
        results.append(temp2)
    return results

# Split the indices into smaller chunks
chunk_size = 100000  # Adjust the chunk size as needed
indices = range(len(genos))
chunks = [indices[i:i + chunk_size] for i in range(0, len(indices), chunk_size)]

# Create a pool of worker processes
pool = multiprocessing.Pool()

# Use the pool.map() function to distribute the workload across multiple processes
results = pool.map(genotype_rate_mask, chunks)

# Flatten the results
genosRateMask25 = [item for sublist in results for item in sublist]

# Close the pool to free up resources
pool.close()
pool.join()

# filter for biallelic SNPs 100% genotyped

## mask < 100% genotyped

genosRateMask100 = []

# Define a worker function
def genotype_rate_mask(chunk):
    results = []
    for i in chunk:
        temp1 = genos[i].count_called()
        temp2 = temp1/111 >= 1
        results.append(temp2)
    return results

# Split the indices into smaller chunks
chunk_size = 100000  # Adjust the chunk size as needed
indices = range(len(genos))
chunks = [indices[i:i + chunk_size] for i in range(0, len(indices), chunk_size)]

# Create a pool of worker processes
pool = multiprocessing.Pool()

# Use the pool.map() function to distribute the workload across multiple processes
results = pool.map(genotype_rate_mask, chunks)

# Flatten the results
genosRateMask100 = [item for sublist in results for item in sublist]

# Close the pool to free up resources
pool.close()
pool.join()

## mask non biallelic
for i in range(len(isBiallelic)):
  if not isBiallelic[i]:
    genosRateMask100[i]=False

#for i in range(len(isBiallelic)):
#  if not isBiallelic[i]:
#    unmasked[positions[i]-1]=False

#snpIndicesToKeep=[x for x in range(len(positions)) if unmasked[positions[x]-1]]
#genos=genos.subset(sel0=snpIndicesToKeep)
#haps=genos.to_haplotypes()
#alleleCounts=allel.AlleleCountsArray([alleleCounts[x] for x in snpIndicesToKeep])
#mapping=[mapping[x] for x in snpIndicesToKeep]
#alleleCounts=alleleCounts.map_alleles(mapping)
#positions=[positions[x] for x in snpIndicesToKeep]

snpIndicesToKeep=[x for x in range(len(positions)) if genosRateMask100[positions[x]-1]] # SCOTT
genos_biallelic=genos.subset(sel0=snpIndicesToKeep) # SCOTT
haps=genos.to_haplotypes()
haps_biallelic=genos_biallelic.to_haplotypes() # SCOTT
mapping_biallelic=[mapping[x] for x in snpIndicesToKeep] # SCOTT
alleleCounts_biallelic=genos_biallelic.count_alleles() # SCOTT
positions_biallelic=[positions[x] for x in snpIndicesToKeep] # SCOTT

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

statHeader="chrom start end".split()

header="chrom classifiedWinStart classifiedWinEnd bigWinRange".split()

for statName in statNames:
  statHeader.append(statName)
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName,i))

statHeader="\t".join(statHeader)

header="\t".join(header)

precomputedStats={}

def getSubWinBounds(subWinSize,positions):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  subWinBounds=[]
  for i in range(len(positions)):
    while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if (subWinStart,subWinEnd) not in subWinBounds:
      subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,positions)

#dafs=alleleCounts[:,1]/float(len(sampleIndicesToKeep)*2)
#ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
#nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
#nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
#sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))

dafs=alleleCounts_biallelic[:,1]/float(len(sampleIndicesToKeep)*2) # SCOTT
ihsVals=allel.stats.selection.ihs(haps_biallelic,positions_biallelic,use_threads=False,include_edges=False)
nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount)) # SCOTT

#if nonNanCount==0:
#  precomputedStats["iHS"]=[]
#  for subWinIndex in range(len(subWinBounds)):
#    precomputedStats["iHS"].append([])
#else:
#  ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
#  precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["iHS"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["iHS"].append([])
else:
  ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
  precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)

#nslVals=allel.stats.selection.nsl(haps,use_threads=False)
#nonNanCount=[x for x in np.isnan(nslVals)].count(False)
#sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))

nslVals=allel.stats.selection.nsl(haps_biallelic,use_threads=False) # SCOTT
nonNanCount=[x for x in np.isnan(nslVals)].count(False)
print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount)) # SCOTT

#if nonNanCount==0:
#  precomputedStats["nSL"]=[]
#  for subWinIndex in range(len(subWinBounds)):
#    precomputedStats["nSL"].append([])
#else:
#  nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
#  precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["nSL"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["nSL"].append([])
else:
  nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
  precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)

#mispolarization information not used since no inference on which specific SNPs may be mispolarized, and since each block contains a relatively small number of SNPs, especially with respect to the mispolarization rate, cannot effectively use the overall mispolarization rate
def SAFEstats(hapsInSubWin,mappingDerivedInSubWin,dafsInSubWin):
  haplotypes={}
  for i in range(len(hapsInSubWin[0])):
    haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
    haplotype="".join(str(x) for x in haplotype)
    if haplotype in haplotypes:
      haplotypes[haplotype].append(i)
    else:
      haplotypes[haplotype]=[i]
  HAF=[]
  HAFunique={}
  for i in haplotypes:
    HAFunique[i]=0
    for j in range(len(hapsInSubWin)):
      if hapsInSubWin[j][haplotypes[i][0]]==mappingDerivedInSubWin[j]:
        HAFunique[i]+=len([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j])) if hapsInSubWin[j][x]==mappingDerivedInSubWin[j]])
    for j in range(len(haplotypes[i])):
      HAF.append(HAFunique[i])
  phi=[]
  kappa=[]
  SAFE=[]
  for i in range(len(hapsInSubWin)):
    phi.append(0)
    kappa.append([])
    phiDenom=0
    for j in haplotypes:
      if int(list(j)[i])==mappingDerivedInSubWin[i]:
        phi[i]+=(HAFunique[j]*len(haplotypes[j]))
        if HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
          kappa[i].append(HAFunique[j])
      phiDenom+=(HAFunique[j]*len(haplotypes[j]))
    phi[i]/=float(phiDenom)
    kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
    if dafsInSubWin[i]==0 or dafsInSubWin[i]==1:
     SAFE.append(0.0)
    else:
     SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafsInSubWin[i]*(1-dafsInSubWin[i]))))
  statVals={}
  quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}
  for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
    if i=="SFS":
      windowStats=dafsInSubWin
    elif i=="HAFunique":
      #windowStats=[eval(i)[x] for x in eval(i)]
      windowStats = list(HAFunique.values()) # SCOTT
    else:
      windowStats=eval(i)
    statVals[i+"-Mean"]=np.mean(windowStats)
    statVals[i+"-Median"]=np.median(windowStats)
    if(len(np.unique(windowStats,return_counts=True)[1])==1):
      statVals[i+"-Mode"]=windowStats[0]
    else:
      if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
        statVals[i+"-Mode"]=scipy.stats.mstats.mode(windowStats)[0][0]
      else:
        mode=min(windowStats)
        for j in range(1,51):
          if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
            mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
        statVals[i+"-Mode"]=mode+((max(windowStats)-min(windowStats))/100)
    for j in quantiles:
      statVals[i+"-"+j]=np.percentile(windowStats,quantiles[j])
    statVals[i+"-Max"]=max(windowStats)
    statVals[i+"-Var"]=np.var(windowStats)
    statVals[i+"-SD"]=np.std(windowStats)
    statVals[i+"-Skew"]=scipy.stats.skew(windowStats)
    statVals[i+"-Kurt"]=scipy.stats.kurtosis(windowStats)
  return statVals

if segmentStart==None:
  firstSubWinStart=1
  lastSubWinStart=(((chrLen-1)/subWinSize)*subWinSize)+1
else:
  firstSubWinStart=(((segmentStart-1)/subWinSize)*subWinSize)+1
  lastSubWinStart=(((segmentEnd-1)/subWinSize)*subWinSize)+1

goodSubWins=[]

for i in range(numSubWins):
  goodSubWins.append(False)

subWinIndex=-1

#def getSnpIndicesInSubWins(subWinSize,positions):
#  subWinStart=1
#  subWinEnd=subWinStart+subWinSize-1
#  snpIndicesInSubWins=None
#  for i in range(len(positions)):
#    if snpIndicesInSubWins and not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#      snpIndicesInSubWins.append([])
#    while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#      subWinStart+=subWinSize
#      subWinEnd+=subWinSize
#    if not snpIndicesInSubWins:
#      snpIndicesInSubWins=[[]]
#    snpIndicesInSubWins[-1].append(i)
#  return snpIndicesInSubWins

def getSnpIndicesInSubWins(subWinSize,positions_biallelic):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  snpIndicesInSubWins=None
  for i in range(len(positions_biallelic)):
    if snpIndicesInSubWins and not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      snpIndicesInSubWins.append([])
    while not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if not snpIndicesInSubWins:
      snpIndicesInSubWins=[[]]
    snpIndicesInSubWins[-1].append(i)
  return snpIndicesInSubWins

#snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions)
snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions_biallelic) # SCOTT

statVals={}

for statName in statNames:
  statVals[statName]=[]
if statFileName.lower() in ["none","false"]:
  statFileName=None
else:
  statFile=open(statFileName,"w")
  statFile.write(statHeader+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  if segmentStart!=None:
    fvecFileName=targetPop+'.'+chrArm+'.'+segStart+'.fvec'
  else:
    fvecFileName=targetPop+'.'+chrArm+'.fvec'

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

#for subWinStart in range(firstSubWinStart,lastSubWinStart+1,subWinSize):
for subWinStart in range(int(firstSubWinStart),int(lastSubWinStart)+1,int(subWinSize)):
  subWinEnd=subWinStart+subWinSize-1
  unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
  unmaskedFrac=genosRateMask25[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
  #if len([x for x in positions if subWinStart<=x<=subWinEnd])==0:
  if len([x for x in positions_biallelic if subWinStart<=x<=subWinEnd])==0:
    #sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    goodSubWins.append(False)
  else:
    subWinIndex+=1
    #sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    if unmaskedFrac<unmaskedFracCutoff:
      goodSubWins.append(False)
    else:
      goodSubWins.append(True)
      #hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
      hapsInSubWin=haps_biallelic.subset(sel0=snpIndicesInSubWins[subWinIndex])
      for statName in statNames:
        if statName!="H12" and statName!="H2/H1" and statName!="Omega" and statName!="distSkew" and statName!="distKurt" and statName[0:3]!="HAF" and statName[0:3]!="phi" and statName[0:3]!="kap" and statName[0:3]!="SFS" and statName[0:3]!="SAF":
          if statName=="fayWuH":
            #calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
            calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
          else:
            #calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
            calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
      #SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      SAFEVals=SAFEstats(hapsInSubWin,[mapping_biallelic[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      for statName in SAFEVals:
        statVals[statName].append(SAFEVals[statName])
      if statFileName:
        statFile.write("\t".join([chrArm, str(subWinStart), str(subWinEnd)] + [str(statVals[statName][-1]) for statName in statNames]) + "\n")
  goodSubWins=goodSubWins[1:]
  if goodSubWins.count(True)==numSubWins:
    outVec=[]
    for statName in statNames:
      outVec+=normalizeFeatureVec(statVals[statName][-numSubWins:])
    midSubWinEnd=subWinEnd-(subWinSize*(numSubWins/2))
    midSubWinStart=midSubWinEnd-subWinSize+1
    fvecFile.write("\t".join([chrArm, str(midSubWinStart), str(midSubWinEnd), str((subWinEnd-(subWinSize*numSubWins)+1))+"-"+str(subWinEnd)] + [str(x) for x in outVec]) + "\n")

if statFileName:
  statFile.close()

fvecFile.close()
#sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### RUN

```{shell}

# Regular

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC/

## scaffold 542

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_542\
 21028053\
 1\
 21025000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542.regular.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542.regular.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542.regular.log &

## scaffold 594

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_594\
 30326886\
 1\
 30325000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594.regular.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594.regular.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594.regular.log &

## scaffold 594 rerun subset

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef_shortName_SUBSET.vcf.gz\
 ScA8VGg_594\
 30326886\
 17165001\
 30325000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594.regular.rerun_subset.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594.regular.rerun_subset.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594.regular.rerun_subset.log &

## scaffold 628

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_628\
 31265526\
 1\
 31265000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_628.regular.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_628.regular.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_628.regular.log &

## scaffold 718

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.log &

## scaffold 76

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_76\
 38731659\
 1\
 38730000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_76.regular.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_76.regular.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_76.regular.log &

## scaffold 785

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_785\
 28136203\
 1\
 28135000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_785.regular.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_785.regular.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_785.regular.log &



# MaskHets

## scaffold 718 110 lines

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.MaskHets.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.MaskHets.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.MaskHets.log &

## scaffold 718 55 lines

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_55.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_55Lines.MaskHets.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_55Lines.MaskHets.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_55Lines.MaskHets.log &


  
# noMaskHets

## scaffold 718 110 lines

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_noMaskHets.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.noMaskHets.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.noMaskHets.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.noMaskHets.log &

## scaffold 718 55 lines

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_noMaskHets.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_55.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_55Lines.noMaskHets.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_55Lines.noMaskHets.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_55Lines.noMaskHets.log &

## scaffold 718 110 lines old mask

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_noMaskHets.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.mask.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.noMaskHets.top6mask.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.noMaskHets.top6mask.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.noMaskHets.top6mask.log &

## scaffold 718 55 lines old mask

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_noMaskHets.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf.gz\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.mask.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_55.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_55Lines.noMaskHets.top6mask.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_55Lines.noMaskHets.top6mask.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_55Lines.noMaskHets.top6mask.log &



# phased

## scaffold 718 110 lines

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_shortName.vcf\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.log &



# phased / synthetic inbred

## scaffold 718 110 lines

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_shortName_syntheticInbred.vcf\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.inbred.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.inbred.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.inbred.log &



# phased / no PIRs / synthetic inbred / 110 lines

## scaffold 542

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_542\
 21028053\
 1\
 21025000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542_110Lines.phased.noPIRs.inbred.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542_110Lines.phased.noPIRs.inbred.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542_110Lines.phased.noPIRs.inbred.log &

## scaffold 594

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_594\
 30326886\
 1\
 30325000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594_110Lines.phased.noPIRs.inbred.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594_110Lines.phased.noPIRs.inbred.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594_110Lines.phased.noPIRs.inbred.log &

## scaffold 628

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_628\
 31265526\
 1\
 31265000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_628_110Lines.phased.noPIRs.inbred.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_628_110Lines.phased.noPIRs.inbred.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_628_110Lines.phased.noPIRs.inbred.log &

## scaffold 718

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.noPIRs.inbred.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.noPIRs.inbred.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.noPIRs.inbred.log &

## scaffold 76

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_76\
 38731659\
 1\
 38730000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_76_110Lines.phased.noPIRs.inbred.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_76_110Lines.phased.noPIRs.inbred.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_76_110Lines.phased.noPIRs.inbred.log &

## scaffold 785

nohup python /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_785\
 28136203\
 1\
 28135000\
 5000\
 11\
 0.25\
 0.003\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/top6.anc.fa\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_785_110Lines.phased.noPIRs.inbred.stats\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec >\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_785_110Lines.phased.noPIRs.inbred.log &


```

#### Merge FVs

```{bash}
for ff in ScA8VGg_76 ScA8VGg_785 ScA8VGg_628 ScA8VGg_594 ScA8VGg_542 ScA8VGg_718;do
echo ${ff}
outfile=${ff}.all.fvec
[ -f "${outfile}" ] && rm ${outfile}
head -1 ${ff}.1.fvec > ${outfile}
ls -ltrh ${ff}.[0-9]*.fvec | awk '{print $9}' | xargs -I {} tail -n +2 {} >> ${outfile}
done
```

#### stats

```{shell log stats}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

# total sub-windows
# grep 'window' ScA8VGg_718.yiguan.log | wc -l
# grep 'window' ScA8VGg_718.scott.log | wc -l
# grep 'window' ScA8VGg_718_noMaskAnc.yiguan.log | wc -l
# grep 'window' ScA8VGg_718_noMaskAnc.scott.log | wc -l
grep 'window' ScA8VGg_718.regular.log | wc -l

# FAIL SNPs=0 | masked > 75%
# grep 'window' ScA8VGg_718.yiguan.log | awk -F ' ' '$10 == "0;" || $16 < 0.25' | wc -l
# grep 'window' ScA8VGg_718.scott.log | awk -F ' ' '$10 == "0;" || $16 < 0.25' | wc -l
# grep 'window' ScA8VGg_718_noMaskAnc.yiguan.log | awk -F ' ' '$10 == "0;" || $16 < 0.25' | wc -l
# grep 'window' ScA8VGg_718_noMaskAnc.scott.log | awk -F ' ' '$10 == "0;" || $16 < 0.25' | wc -l
grep 'window' ScA8VGg_718.regular.log | awk -F ' ' '$10 == "0;" || $16 < 0.25' | wc -l

# FAIL SNPs=0 & masked > 75%
# grep 'window' ScA8VGg_718.yiguan.log | awk -F ' ' '$10 == "0;" && $16 < 0.25' | wc -l
# grep 'window' ScA8VGg_718.scott.log | awk -F ' ' '$10 == "0;" && $16 < 0.25' | wc -l
# grep 'window' ScA8VGg_718_noMaskAnc.yiguan.log | awk -F ' ' '$10 == "0;" && $16 < 0.25' | wc -l
# grep 'window' ScA8VGg_718_noMaskAnc.scott.log | awk -F ' ' '$10 == "0;" && $16 < 0.25' | wc -l
grep 'window' ScA8VGg_718.regular.log | awk -F ' ' '$10 == "0;" && $16 < 0.25' | wc -l

# FAIL SNPs=0 (masked < 75%)
# grep 'window' ScA8VGg_718.yiguan.log | awk -F ' ' '$10 == "0;" && $16 > 0.25' | wc -l
# grep 'window' ScA8VGg_718.scott.log | awk -F ' ' '$10 == "0;" && $16 > 0.25' | wc -l
# grep 'window' ScA8VGg_718_noMaskAnc.yiguan.log | awk -F ' ' '$10 == "0;" && $16 > 0.25' | wc -l
# grep 'window' ScA8VGg_718_noMaskAnc.scott.log | awk -F ' ' '$10 == "0;" && $16 > 0.25' | wc -l
grep 'window' ScA8VGg_718.regular.log | awk -F ' ' '$10 == "0;" && $16 > 0.25' | wc -l

# FAIL masked > 75% (SNPs>0)
# grep 'window' ScA8VGg_718.yiguan.log | awk -F ' ' '$10 != "0;" && $16 < 0.25' | wc -l
# grep 'window' ScA8VGg_718.scott.log | awk -F ' ' '$10 != "0;" && $16 < 0.25' | wc -l
# grep 'window' ScA8VGg_718_noMaskAnc.yiguan.log | awk -F ' ' '$10 != "0;" && $16 < 0.25' | wc -l
# grep 'window' ScA8VGg_718_noMaskAnc.scott.log | awk -F ' ' '$10 != "0;" && $16 < 0.25' | wc -l
grep 'window' ScA8VGg_718.regular.log | awk -F ' ' '$10 != "0;" && $16 < 0.25' | wc -l

# Genome scanned
module load bedtools

# cut -f 1,4 ScA8VGg_718.yiguan.fvec | grep -v 'chrom' | sed 's/-/\t/' > ScA8VGg_718.yiguan.fvec.bed
# bedtools merge -i ScA8VGg_718.yiguan.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += $6} END {print sum}'
# bedtools merge -i ScA8VGg_718.yiguan.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += $6*5000} END {print sum}'
# bedtools merge -i ScA8VGg_718.yiguan.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += ($6*5000)/8653123} END {print sum}'
# 
# cut -f 1,4 ScA8VGg_718.scott.fvec | grep -v 'chrom' | sed 's/-/\t/' > ScA8VGg_718.scott.fvec.bed
# bedtools merge -i ScA8VGg_718.scott.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += $6} END {print sum}'
# bedtools merge -i ScA8VGg_718.scott.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += $6*5000} END {print sum}'
# bedtools merge -i ScA8VGg_718.scott.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += ($6*5000)/8653123} END {print sum}'
# 
# cut -f 1,4 ScA8VGg_718_noMaskAnc.yiguan.fvec | grep -v 'chrom' | sed 's/-/\t/' > ScA8VGg_718_noMaskAnc.yiguan.fvec.bed
# bedtools merge -i ScA8VGg_718_noMaskAnc.yiguan.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += $6} END {print sum}'
# bedtools merge -i ScA8VGg_718_noMaskAnc.yiguan.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += $6*5000} END {print sum}'
# bedtools merge -i ScA8VGg_718_noMaskAnc.yiguan.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += ($6*5000)/8653123} END {print sum}'
# 
# cut -f 1,4 ScA8VGg_718_noMaskAnc.scott.fvec | grep -v 'chrom' | sed 's/-/\t/' > ScA8VGg_718_noMaskAnc.scott.fvec.bed
# bedtools merge -i ScA8VGg_718_noMaskAnc.scott.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += $6} END {print sum}'
# bedtools merge -i ScA8VGg_718_noMaskAnc.scott.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += $6*5000} END {print sum}'
# bedtools merge -i ScA8VGg_718_noMaskAnc.scott.fvec.bed |\
#  awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
#  awk '{sum += ($6*5000)/8653123} END {print sum}'

cut -f 1,4 ScA8VGg_542_110Lines.phased.noPIRs.inbred.log | grep -v 'chrom' | sed 's/-/\t/' > ScA8VGg_718.regular.fvec.bed
bedtools merge -i ScA8VGg_718.regular.fvec.bed |\
 awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
 awk '{sum += $6} END {print sum}'
bedtools merge -i ScA8VGg_718.regular.fvec.bed |\
 awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
 awk '{sum += $6*5000} END {print sum}'
bedtools merge -i ScA8VGg_718.regular.fvec.bed |\
 awk '{print $1, $2, $3, $3-$2+1, ($3-$2+1)/5000, (($3-$2+1)/5000)-11+1}' |\
 awk '{sum += ($6*5000)/8653123} END {print sum}'

```

## Training - calcStatsAndDafForEachSnp

```{python, calcStatsAndDafForEachSnpSingleMsFile_Python3}

import sys
import allel
import random
import numpy as np
from msTools import *
from fvTools import *
import time

'''usage example
python calcStatsAndDafForEachSnpSingleMsFile.py /san/data/dan/simulations/discoal_multipopStuff/spatialSVMSims/trainingSets/equilibNeut.msout.gz 110000 iHS nSL
'''

trainingDataFileName, totalPhysLen = sys.argv[1:3]
totalPhysLen = int(totalPhysLen)

hapArraysIn, positionArrays = msOutToHaplotypeArrayIn(trainingDataFileName, totalPhysLen)
numInstances = len(hapArraysIn)

header = "daf\tiHS\tnSL"
#print header # Python 2
print(header) # Python 3

dafs = []
#start = time.clock() # Python 2
start = time.perf_counter() # Python 3
numInstancesDone = 0

numSnpsDone = 0
for instanceIndex in range(numInstances):
    if instanceIndex % 10 == 0:
        sys.stderr.write("done wtih %d of %d instances (%d SNPs)\n" %(instanceIndex, numInstances, numSnpsDone))
    haps = allel.HaplotypeArray(hapArraysIn[instanceIndex], dtype='i1')
    genos = haps.to_genotypes(ploidy=2)
    ac = genos.count_alleles()
    sampleSizes = [sum(x) for x in ac]
    assert len(set(sampleSizes)) == 1
    dafs = ac[:,1]/float(sampleSizes[0])
    ihsVals = allel.stats.selection.ihs(haps, positionArrays[instanceIndex], use_threads=True, include_edges=False)
    nslVals = allel.stats.selection.nsl(haps, use_threads=True)
    assert len(dafs) == len(ihsVals) == len(nslVals)
    numSnpsDone += len(dafs)
    for i in range(len(dafs)):
        #print "%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i]) # Python 2
        print ("%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i])) #Python 3
    if numSnpsDone > 5000000:
        break

```

```{shell, test Dell2}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/consNe

rsync -ahPv /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/constNe/*.msOut.gz ./

msOut_files=$(find ./ -type f -name '*.msOut.gz' | sort | sed 's/\.\///')

# python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/constNe/spHard_0.msOut.gz 55000 > spHard_0.msOut.gz_stats.txt

gen_stats() {
    python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${1} 55000 > ${1/.msOut.gz/}_stats.txt
}
export -f gen_stats
ls *msOut.gz | parallel -j 8 gen_stats

# spHard_0 2GB RMA 2 minutes
time python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py spHard_0.msOut.gz 55000 > spHard_0_test_stats.txt
pgrep -f calcStatsAndDafForEachSnpSingleMsFile_Python3.py
pidstat -r -u -h -p 164914

# spNeut 22GB RAM 10 minutes (stops after 5M SNPs)
time python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py spNeut.msOut.gz 55000 > spNeut_test_stats.txt
pgrep -f calcStatsAndDafForEachSnpSingleMsFile_Python3.py
pidstat -r -u -h -p 171341

# spHard_5 10GB RAM 7 minutes
time python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py spHard_5.msOut.gz 55000 > spHard_5_test_stats.txt
pgrep -f calcStatsAndDafForEachSnpSingleMsFile_Python3.py
pidstat -r -u -h -p 179283

```

```{shell, bunya test}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=2G
#SBATCH --job-name=calcStatsAndDafForEachSnpSingleMsFile_Python3_expansionNe_spHard_0
#SBATCH --time=00:05:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_expansionNe_spHard_0.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_expansionNe_spHard_0.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe

msOut="spHard_0.msOut.gz"

python /home/uqsalle3/Programs/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, bunya template}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=2G
#SBATCH --job-name=calcStatsAndDafForEachSnpSingleMsFile_Python3_expansionNe_[SAMPLE]
#SBATCH --time=00:05:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_expansionNe_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_expansionNe_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe

msOut="[SAMPLE]"

python /home/uqsalle3/Programs/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe

find ./ -type f -name '*.msOut.gz' | sed 's/\.\///' | sed 's/\.msOut\.gz//' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_calcStatsAndDafForEachSnpSingleMsFile_Python3.txt calcStatsAndDafForEachSnpSingleMsFile_Python3_${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" calcStatsAndDafForEachSnpSingleMsFile_Python3_${p}.slurm.sh
done < msOut_files.txt

chmod 755 ./*.slurm.sh

# vim RAM to 24GB and time to 15 minutes for neut 
# vim RAM to 12GB and time to 10 minutes for central sub-window (5)


# Find all files starting with 'calcStatsAndDafForEachSnpSingleMsFile_Python3_'
find ./ -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*' -print0 |
while IFS= read -r -d '' file; do
    # Perform the in-place replacement using sed
    sed -i 's/longShallow/shortSevere/g' "$file"
done

# Find all files starting with 'calcStatsAndDafForEachSnpSingleMsFile_Python3_'
find ./ -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*' -print0 |
while IFS= read -r -d '' file; do
    # Perform the in-place replacement using sed
    sed -i 's/_self_newDat_self_newDat_/_self_newDat_/g' "$file"
done

# Find all files starting with 'calcStatsAndDafForEachSnpSingleMsFile_Python3_'
find ./ -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*' -print0 |
while IFS= read -r -d '' file; do
    # Perform the in-place replacement using sed
    sed -i 's/time\=00\:05\:00/time\=00\:15\:00/g' "$file"
done

# check stats files
tail -n 2 ./slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_*.error

```

## Training - training_convert_to_FVs

```{python, training_convert_to_FVs_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
from msTools import *
from fvTools import *
import random
import allel
import numpy as np
import math

# '''usage eg:
# pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
# python2 training_convert_to_FVs.py spNeut.msOut.gz 2L,2R,3L,3R 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.fa ./ spNeut.msOut.fvec
# '''
# 
# if len(sys.argv)!=12:
#   sys.exit("usage:\npython2 training_convert_to_FVs.py trainingDataFileName chrArmsForMasking subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName statDir fvecFileName\n")
# else:
#   trainingDataFileName, chrArmsForMasking, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, statDir, fvecFileName = sys.argv[1:]

trainingDataFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5.msOut.gz"
chrArmsForMasking="ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140"
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_stats.txt"
maskFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta"
# ancestralArmFaFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/top6.anc.fa"
ancestralArmFaFileName="none"
statDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/"
fvecFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_test.fvec"

subWinSize,numSubWins = int(subWinSize),int(numSubWins)
assert subWinSize>0 and numSubWins>1

totalPhysLen=(subWinSize*numSubWins)

hapArraysIn,positionArrays=msOutToHaplotypeArrayIn(trainingDataFileName,totalPhysLen)

chrArmsForMasking=chrArmsForMasking.split(",")

unmaskedFracCutoff,pMisPol=float(unmaskedFracCutoff),float(pMisPol)

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")


if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")


standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none", "false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*totalPhysLen
  sys.stderr.write("Warning: not doing any masking! (i.e. mask.fa file for the chr arm with all masked sites N'ed out, or at least the reference with Ns, is not provided)\n")
  maskFileName=False
else:
  if ancestralArmFaFileName.lower() in ["none", "false"]:
    maskData=readMaskDataForTraining(maskFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  else:
    maskData=readMaskAndAncDataForTraining(maskFileName,ancestralArmFaFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  if len(maskData)<len(hapArraysIn):
    sys.stderr.write("Warning: not enough windows from masked data (needed %d; got %d); will draw with replacement!!\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=True
  else:
    sys.stderr.write("enough windows from masked data (needed %d; got %d); will draw without replacement.\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=False


# for i in range(len(maskData)):
#   print(str(i) + " " + str(len(maskData[i])) + " " + str(sum(maskData[i])))

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

header=[]

for statName in statNames:
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName, i))

header="\t".join(header)

statVals={}

for statName in statNames:
  statVals[statName]=[]

def getSubWinBounds(subWinSize,totalPhysLen):
  subWinStart=1-subWinSize
  subWinEnd=0
  subWinBounds=[]
  for i in range(0,(numSubWins-1)):
    subWinStart+=subWinSize
    subWinEnd+=subWinSize
    subWinBounds.append((subWinStart,subWinEnd))
  subWinStart+=subWinSize
  subWinEnd=totalPhysLen
  subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,totalPhysLen)

def getSnpIndicesInSubWins(subWinBounds,positions):
  snpIndicesInSubWins=[]
  for subWinIndex in range(len(subWinBounds)):
    snpIndicesInSubWins.append([])
  subWinIndex=0
  for i in range(len(positions)):
    while not (positions[i]>=subWinBounds[subWinIndex][0] and positions[i]<=subWinBounds[subWinIndex][1]):
      subWinIndex+=1
    snpIndicesInSubWins[subWinIndex].append(i)
  return snpIndicesInSubWins

quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

if statDir.lower() in ["none","false","default"]:
  statDir='./'

statFiles=[]

for subWinIndex in range(numSubWins):
  statFileName='/'.join(("%s/%s.subWin%d.stats" %(statDir, trainingDataFileName.split("/")[-1].replace(".msOut.gz",""), subWinIndex)).split('//'))
  statFiles.append(open(statFileName,"w"))
  statFiles[-1].write("\t".join(statNames)+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  fvecFileName="%s.fvec" %trainingDataFileName.split("/")[-1].replace(".msOut.gz","")

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

# for testing for loop below only
instanceIndex=0
statName='pi'

for instanceIndex in range(len(hapArraysIn)):
  print("Starting index " + str(instanceIndex))
  for statName in statNames:
    statVals[statName].append([])
  if maskFileName:
    if drawWithReplacement:
      unmasked=random.choice(maskData)
    else:
      unmasked=maskData[instanceIndex]
    assert len(unmasked)==totalPhysLen
    print(str(sum(unmasked)) + " Loci unmasked")
  snpIndicesToKeep=[x for x in range(len(positionArrays[instanceIndex])) if unmasked[positionArrays[instanceIndex][x]-1]]
  if len(snpIndicesToKeep)==0:
    for subWinIndex in range(numSubWins):
      for statName in statNames:
        print("No SNPs in BIGwindow, appendStatValsForMonomorphic")
        appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  else:
    haps=allel.HaplotypeArray(hapArraysIn[instanceIndex],dtype='i1').subset(sel0=snpIndicesToKeep)
    if pMisPol>0:
      misPolarizeCorrectionIndex=np.random.binomial(1,pMisPol,len(haps))
      for i in range(len(misPolarizeCorrectionIndex)):
        if misPolarizeCorrectionIndex[i]==1:
          for j in range(len(haps[i])):
            if haps[i][j]==0:
              haps[i][j]=1
            else:
              haps[i][j]=0
    genos=haps.to_genotypes(ploidy=2)
    alleleCounts=genos.count_alleles()
    positions=[positionArrays[instanceIndex][x] for x in snpIndicesToKeep]
    precomputedStats={}
    sampleSizes=[sum(x) for x in alleleCounts]
    assert len(set(sampleSizes))==1
    dafs=alleleCounts[:,1]/float(sampleSizes[0])
    ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
    nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
    nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
    # sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    if nonNanCount==0:
      precomputedStats["iHS"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["iHS"].append([])
    else:
      ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
      precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)
    nslVals=allel.stats.selection.nsl(haps,use_threads=False)
    nonNanCount=[x for x in np.isnan(nslVals)].count(False)
    # sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    if nonNanCount==0:
      precomputedStats["nSL"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["nSL"].append([])
    else:
      nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
      precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)
    snpIndicesInSubWins=getSnpIndicesInSubWins(subWinBounds,positions)
    for subWinIndex in range(numSubWins):
      subWinStart,subWinEnd = subWinBounds[subWinIndex]
      unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
      assert unmaskedFrac>=unmaskedFracCutoff
      if len(snpIndicesInSubWins[subWinIndex])==0:
        print("No SNPS in " + str(subWinIndex) + ", appendStatValsForMonomorphic")
        for statName in statNames:
          appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      else:
        print(str(snpIndicesInSubWins[subWinIndex])) + " SNPs in " + str(subWinIndex) + ", calcAndAppendStatVal")
        hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
        for statName in [x for x in statNames if x[0:3]!="HAF" and x[0:3]!="phi" and x[0:3]!="kap" and x[0:3]!="SFS" and x[0:3]!="SAF"]:
          calcAndAppendStatVal(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, instanceIndex, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        haplotypes={}
        for i in range(len(hapsInSubWin[0])):
          haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
          haplotype="".join(str(x) for x in haplotype)
          if haplotype in haplotypes:
            haplotypes[haplotype].append(i)
          else:
            haplotypes[haplotype]=[i]
        HAF=[]
        HAFunique={}
        for i in haplotypes:
          HAFunique[i]=0
          for j in range(len(hapsInSubWin)):
            if hapsInSubWin[j][haplotypes[i][0]]==1:
              HAFunique[i]+=sum([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j]))])
          for j in range(len(haplotypes[i])):
            HAF.append(HAFunique[i])
        phi=[]
        kappa=[]
        SAFE=[]
        for i in range(len(hapsInSubWin)):
          phi.append(0)
          kappa.append([])
          phiDenom=0
          for j in haplotypes:
            phi[i]+=(int(list(j)[i])*HAFunique[j]*len(haplotypes[j]))
            if int(list(j)[i])==1 and HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
              kappa[i].append(HAFunique[j])
            phiDenom+=(HAFunique[j]*len(haplotypes[j]))
          phi[i]/=float(phiDenom)
          kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
          if dafs[snpIndicesInSubWins[subWinIndex]][i]==0 or dafs[snpIndicesInSubWins[subWinIndex]][i]==1:
           SAFE.append(0.0)
          else:
           SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafs[snpIndicesInSubWins[subWinIndex]][i]*(1-dafs[snpIndicesInSubWins[subWinIndex]][i]))))
        for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
          if i=="SFS":
            windowStats=dafs[snpIndicesInSubWins[subWinIndex]]
          elif i=="HAFunique":
            # windowStats=[eval(i)[x] for x in eval(i)]
            windowStats = list(HAFunique.values()) # SCOTT
          else:
            windowStats=eval(i)
          statVals[i+"-Mean"][instanceIndex].append(np.mean(windowStats))
          statVals[i+"-Median"][instanceIndex].append(np.median(windowStats))
          if(len(np.unique(windowStats,return_counts=True)[1])==1):
            statVals[i+"-Mode"][instanceIndex].append(windowStats[0])
          else:
            if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
              statVals[i+"-Mode"][instanceIndex].append(scipy.stats.mstats.mode(windowStats)[0][0])
            else:
              mode=min(windowStats)
              for j in range(1,51):
                if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
                  mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
              statVals[i+"-Mode"][instanceIndex].append(mode+((max(windowStats)-min(windowStats))/100))
          for j in quantiles:
            statVals[i+"-"+j][instanceIndex].append(np.percentile(windowStats,quantiles[j]))
          statVals[i+"-Max"][instanceIndex].append(max(windowStats))
          statVals[i+"-Var"][instanceIndex].append(np.var(windowStats))
          statVals[i+"-SD"][instanceIndex].append(np.std(windowStats))
          statVals[i+"-Skew"][instanceIndex].append(scipy.stats.skew(windowStats))
          statVals[i+"-Kurt"][instanceIndex].append(scipy.stats.kurtosis(windowStats))
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][instanceIndex])
  fvecFile.write("\t".join([str(x) for x in outVec])+"\n")

for subWinIndex in range(numSubWins):
  statFiles[subWinIndex].close()
fvecFile.close()
# sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, test Dell2}

# trainingDataFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0.msOut.gz"
# chrArmsForMasking="ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140"
# subWinSize=5000
# numSubWins=11
# unmaskedFracCutoff=0.25
# pMisPol=0.003
# partialStatAndDafFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0_stats.txt"
# maskFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta"
# ancestralArmFaFileName="none"
# statDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/"
# fvecFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0_test.fvec"
# 
# 
# python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py \
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0.msOut.gz \
#  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140 \
#  5000 \
#  11 \
#  0.25 \
#  0.003 \
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0_stats.txt \
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta \
#  none \
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/ \
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0_test.fvec

gen_fvec() {
  python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py\
  ${1}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  ${1/.msOut.gz/}_stats.txt\
  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/\
  ${1/.msOut.gz/}.fvec
}
export -f gen_fvec
ls *msOut.gz | sort -V | parallel -j 38 gen_fvec

nohup python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py\
  spPartialHard_5.msOut.gz\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spPartialHard_5_stats.txt\
  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/\
  spPartialHard_5.fvec > nohup_training_convert_to_FVs_Python3_spPartialHard_5.out &

nohup python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py\
  spNeut.msOut.gz\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spNeut_stats.txt\
  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/\
  sspNeut.fvec > nohup_training_convert_to_FVs_Python3_spNeut.out &

spPartialSoft_5

nohup python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py\
  spPartialSoft_5.msOut.gz\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spPartialSoft_5_stats.txt\
  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/\
  spPartialSoft_5.fvec > nohup_training_convert_to_FVs_Python3_spPartialSoft_5.out &

# transfer to Bunya
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya
find ./ -type f -name 'cohort.*' | sed 's/\.\///' > rsync_file_list.txt
rsync -ahPv --files-from=rsync_file_list.txt ./ uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/HaplotypeCaller/

```

```{shell, bunya template}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=3G
#SBATCH --job-name=training_convert_to_FVs_Python3_expansionNe_[SAMPLE]
#SBATCH --time=00:05:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe/slurm_training_convert_to_FVs_Python3_expansionNe_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe/slurm_training_convert_to_FVs_Python3_expansionNe_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe

msOut="[SAMPLE].msOut.gz"

srun python /home/uqsalle3/Programs/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  ${msOut/.msOut.gz/}_stats.txt\
  /scratch/project/chenoase/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell, create scripts Bunya}

# constNe/test_self_newDat

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe/test_self_newDat

find ./ -maxdepth 1 -type f -name '*.msOut.gz' | sed 's/\.\///' | sed 's/\.msOut\.gz//' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_training_convert_to_FVs_Python3.txt training_convert_to_FVs_Python3_${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" training_convert_to_FVs_Python3_${p}.slurm.sh
done < msOut_files.txt

chmod 755 ./*.slurm.sh

# vim RAM to 32GB and time to 48 hours for neut 
# vim RAM to 24GB and time to 36 hours for central sub-window (5)

# constNe

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe
cp ../constNe/test_self_newDat/template_training_convert_to_FVs_Python3.txt ./
# vim template_training_convert_to_FVs_Python3.txt

find ./ -maxdepth 1 -type f -name '*.msOut.gz' | sed 's/\.\///' | sed 's/\.msOut\.gz//' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_training_convert_to_FVs_Python3.txt training_convert_to_FVs_Python3_${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" training_convert_to_FVs_Python3_${p}.slurm.sh
done < msOut_files.txt

chmod 755 ./*.slurm.sh

# vim RAM to 32GB and time to 48 hours for neut 
# vim RAM to 24GB and time to 36 hours for central sub-window (5)

# expansionNe

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe
cp ../constNe/training_convert_to_FVs_Python3_sp*.slurm.sh ./
# vim template_training_convert_to_FVs_Python3.txt

find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*' -print0 |
while IFS= read -r -d '' file; do
    # Perform the in-place replacement using sed
    sed -i 's/constNe/expansionNe/g' "$file"
done

chmod 755 ./*.slurm.sh

# vim RAM to 32GB and time to 48 hours for neut 
# vim RAM to 24GB and time to 36 hours for central sub-window (5)

# expansionNe/test_self_newDat

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe/test_self_newDat
cp ../../constNe/test_self_newDat/training_convert_to_FVs_Python3_sp*.slurm.sh ./
# vim template_training_convert_to_FVs_Python3.txt

find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*' -print0 |
while IFS= read -r -d '' file; do
    # Perform the in-place replacement using sed
    sed -i 's/constNe/expansionNe/g' "$file"
done

chmod 755 ./*.slurm.sh

# vim RAM to 32GB and time to 48 hours for neut 
# vim RAM to 24GB and time to 36 hours for central sub-window (5)

# longShallow

cd /scratch/project/chenoase/partialSHIC_DsGRP/longShallow
cp ../constNe/training_convert_to_FVs_Python3_sp*.slurm.sh ./
# vim template_training_convert_to_FVs_Python3.txt

find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*' -print0 |
while IFS= read -r -d '' file; do
    # Perform the in-place replacement using sed
    sed -i 's/constNe/longShallow/g' "$file"
done

chmod 755 ./*.slurm.sh

# vim RAM to 32GB and time to 48 hours for neut 
# vim RAM to 24GB and time to 36 hours for central sub-window (5)

# longShallow/test_self_newDat

cd /scratch/project/chenoase/partialSHIC_DsGRP/longShallow/test_self_newDat
cp ../../constNe/test_self_newDat/training_convert_to_FVs_Python3_sp*.slurm.sh ./
# vim template_training_convert_to_FVs_Python3.txt

find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*' -print0 |
while IFS= read -r -d '' file; do
    # Perform the in-place replacement using sed
    sed -i 's/constNe/longShallow/g' "$file"
done

chmod 755 ./*.slurm.sh

# vim RAM to 32GB and time to 48 hours for neut 
# vim RAM to 24GB and time to 36 hours for central sub-window (5)

# shortSevere

cd /scratch/project/chenoase/partialSHIC_DsGRP/shortSevere
cp ../constNe/training_convert_to_FVs_Python3_sp*.slurm.sh ./
# vim template_training_convert_to_FVs_Python3.txt

find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*' -print0 |
while IFS= read -r -d '' file; do
    # Perform the in-place replacement using sed
    sed -i 's/constNe/shortSevere/g' "$file"
done

chmod 755 ./*.slurm.sh

# vim RAM to 32GB and time to 48 hours for neut 
# vim RAM to 24GB and time to 36 hours for central sub-window (5)

# shortSevere/test_self_newDat

cd /scratch/project/chenoase/partialSHIC_DsGRP/shortSevere/test_self_newDat
cp ../../constNe/test_self_newDat/training_convert_to_FVs_Python3_sp*.slurm.sh ./
# vim template_training_convert_to_FVs_Python3.txt

find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*' -print0 |
while IFS= read -r -d '' file; do
    # Perform the in-place replacement using sed
    sed -i 's/constNe/shortSevere/g' "$file"
done

chmod 755 ./*.slurm.sh

# vim RAM to 32GB and time to 48 hours for neut 
# vim RAM to 24GB and time to 36 hours for central sub-window (5)


```

```{shell, failed rerun}

# constNe
# constNe/test_self_newDat
cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe/test_self_newDat
# cp ../../shortSevere/test_self_newDat/training_convert_to_FVs_Python3_sp*.slurm.sh ./
# Find all files starting with 'calcStatsAndDafForEachSnpSingleMsFile_Python3_'
find ./ -type f -name 'training_convert_to_FVs_Python3_*' -print0 |
while IFS= read -r -d '' file; do
    # Perform the in-place replacement using sed
    sed -i 's/expansionNe/constNe/g' "$file"
done


# expansionNe
# expansionNe/test_self_newDat

# longShallow
spPartialSoft_10 walltime (RUNNING)
spPartialSoft_4 walltime (RUNNING)
# longShallow/test_self_newDat
spPartialSoft_10.fvec walltime (RUNNING)

# shortSevere


```

```{shell transfer to Dell 2}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP

# constNe

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/constNe\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/

# There should be 45 fvec files
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/*.fvec | wc -l
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/test_self_newDat/*.fvec | wc -l

# expansionNe

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/expansionNe\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/

# There should be 45 fvec files
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/*.fvec | wc -l
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/test_self_newDat/*.fvec | wc -l

# longShallow

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/longShallow\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/

# There should be 45 fvec files
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/*.fvec | wc -l
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/test_self_newDat/*.fvec | wc -l

```

```{r, assess feature vectors}

# module load anaconda3
# source ~/conda-init
# conda activate r_env

library(tidyverse)
library(data.table)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe")

spHard_5 <- read_tsv("spHard_5.fvec")

spHard_5 %>% 
  mutate(sim=c(1:nrow(spHard_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

spSoft_5 <- read_tsv("spSoft_5.fvec")

spSoft_5 %>% 
  mutate(sim=c(1:nrow(spSoft_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

```

## Training - training_sample_FVs

```{python, training_sample_FVs.py}

# import sys,os,random
import sys # SCOTT
import os # SCOTT
import random # SCOTT

'''usage eg:
python2 training_sample_FVs.py spNeut.fvec spHard_ spSoft_ spPartialHard_ spPartialSoft_ 5 0,1,2,3,4,6,7,8,9,10 ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec
'''

if len(sys.argv)!=10:
  sys.exit("usage:\npython2 training_sample_FVs.py neutTrainingFileName hardTrainingFilesPrefix softTrainingFilesPrefix partialHardTrainingFilesPrefix partialSoftTrainingFilesPrefix sweepTrainingWindow linkedTrainingWindows sampledFVsDir sampledFVsFiles\n")
else:
  neutTrainingFileName, hardTrainingFilesPrefix, softTrainingFilesPrefix, partialHardTrainingFilesPrefix, partialSoftTrainingFilesPrefix, sweepTrainingWindow, linkedTrainingWindows, sampledFVsDir, sampledFVsFiles = sys.argv[1:]

neutTrainingFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spNeut.fvec"
hardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_"
softTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spSoft_"
partialHardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialHard_"
partialSoftTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialSoft_"
sweepTrainingWindow=5
linkedTrainingWindows=0,1,2,3,4,6,7,8,9,10
sampledFVsDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe"
sampledFVsFiles="neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec"

sweepFilePaths,linkedFilePaths = {},{}
for trainingFilePrefix in [hardTrainingFilesPrefix,softTrainingFilesPrefix,partialHardTrainingFilesPrefix,partialSoftTrainingFilesPrefix]:
  trainingSetDir="/".join(trainingFilePrefix.split("/")[:-1])
  trainingFilePrefixDirless=trainingFilePrefix.split("/")[-1]
  sweepFilePaths[trainingFilePrefix]=[]
  linkedFilePaths[trainingFilePrefix]=[]
  for fileName in os.listdir(trainingSetDir):
    if fileName.startswith(trainingFilePrefixDirless):
      winNum=int(fileName.split("_")[1].split(".")[0])
      if winNum==int(sweepTrainingWindow):
        sweepFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)
      elif winNum in [int(x) for x in linkedTrainingWindows.split(",")]:
        linkedFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)

def getExamplesFromFVFile(simFileName):
  try:
    simFile=open(simFileName)
    lines=[line.strip() for line in simFile.readlines()]
    header=lines[0]
    examples=lines[1:]
    simFile.close()
    return header,examples
  except Exception:
    return "",[]

def getExamplesFromFVFileLs(simFileLs):
  examples=[]
  keptHeader=""
  for filePath in simFileLs:
    header,currExamples=getExamplesFromFVFile(filePath)
    if header:
      keptHeader=header
    examples+=currExamples
  return keptHeader,examples

header,neutExamples=getExamplesFromFVFile(neutTrainingFileName)
hardHeader,hardExamples=getExamplesFromFVFileLs(sweepFilePaths[hardTrainingFilesPrefix])
linkedHardHeader,linkedHardExamples=getExamplesFromFVFileLs(linkedFilePaths[hardTrainingFilesPrefix])
softHeader,softExamples=getExamplesFromFVFileLs(sweepFilePaths[softTrainingFilesPrefix])
linkedSoftHeader,linkedSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[softTrainingFilesPrefix])
partialHardHeader,partialHardExamples=getExamplesFromFVFileLs(sweepFilePaths[partialHardTrainingFilesPrefix])
linkedPartialHardHeader,linkedPartialHardExamples=getExamplesFromFVFileLs(linkedFilePaths[partialHardTrainingFilesPrefix])
partialSoftHeader,partialSoftExamples=getExamplesFromFVFileLs(sweepFilePaths[partialSoftTrainingFilesPrefix])
linkedPartialSoftHeader,linkedPartialSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[partialSoftTrainingFilesPrefix])

def getMinButNonZeroExamples(lsLs):
  counts=[]
  for ls in lsLs:
    if len(ls)>0:
      counts.append(len(ls))
  if not counts:
    raise Exception
  return min(counts)

trainingSetLs=[hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
numExamplesToKeep=getMinButNonZeroExamples(trainingSetLs)
for i in range(len(trainingSetLs)):
  random.shuffle(trainingSetLs[i])
  trainingSetLs[i]=trainingSetLs[i][:numExamplesToKeep]
hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples=trainingSetLs

if sampledFVsDir.lower() in ["none","false","default"]:
  sampledFVsDir='./'
if sampledFVsFiles.lower() in ["none","false","default"]:
  sampledFVsFiles=["neut.fvec","hard.fvec","linkedHard.fvec","soft.fvec","linkedSoft.fvec","partialHard.fvec","linkedPartialHard.fvec","partialSoft.fvec","linkedPartialSoft.fvec"]
else:
  sampledFVsFiles=sampledFVsFiles.split(",")
  assert len(sampledFVsFiles)==9
outExamples=[neutExamples,hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
for i in range(len(sampledFVsFiles)):
  if outExamples[i]:
    outFile=open('/'.join((sampledFVsDir+'/'+sampledFVsFiles[i]).split('//')),"w")
    outFile.write("classLabel\t%s\n" %(hardHeader))
    for example in outExamples[i]:
      outFile.write("%s\t%s\n" %(sampledFVsFiles[i].replace(".fvec",""),example))
    outFile.close()

```

```{shell, test Dell 2}

# rm *stats
# mkdir FVs
# mv *fvec FVs

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

# expansionNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

# longShallow

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

# shortSevere

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

```

## Run training

```{python, training_deep_learning_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import numpy as np
np.random.seed(123)
from sklearn.model_selection import train_test_split
# from keras.utils import np_utils
from keras.utils import to_categorical # SCOTT
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
from keras import optimizers
from keras.callbacks import EarlyStopping,ModelCheckpoint

'''usage eg:
python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
'''

if len(sys.argv)!=9:
  sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
else:
  fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]

fvecDir='./'
fvecFiles='neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec'
numSubWins='11'
numSumStatsPerSubWin='92'
validationSize='0.1'
weightsFileName='constNe.hdf5'
jsonFileName='constNe.json'
npyFileName='constNe.npy'

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

if fvecFiles.lower() in ["none","false","default"]:
  fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
else:
  fvecFiles=fvecFiles.split(",")
  assert len(fvecFiles)==9

numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

if validationSize.lower() in ["none","false","default"]:
  validationSize=0.1
else:
  validationSize=float(validationSize)

sweeps=[]

for i in range(0,9):
 sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
 if i>0:
  assert len(sweeps[0])==len(sweeps[i])

sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))

sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)

models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))

sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=42)

# models=np_utils.to_categorical(models,9)
models=to_categorical(models,9) # SCOTT

# models_val=np_utils.to_categorical(models_val,9)
models_val=to_categorical(models_val,9) # SCOTT

netlayers=Sequential()
netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
netlayers.add(Dropout(0.25))
netlayers.add(Flatten())
netlayers.add(Dense(512, activation='relu'))
netlayers.add(Dropout(0.5))
netlayers.add(Dense(128, activation='relu'))
netlayers.add(Dropout(0.5))
netlayers.add(Dense(9, activation='softmax'))
netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')

netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)

netlayers_json=netlayers.to_json()

with open(jsonFileName,"w") as json_file:
  json_file.write(netlayers_json)

netlayers.save(npyFileName)

sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))

```

```{shell, test Dell 2}

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_50epoch.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 temp_constNe.hdf5\
 temp_constNe.json\
 temp_constNe.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# expansionNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 expansionNe.hdf5\
 expansionNe.json\
 expansionNe.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# longShallow

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 longShallow.hdf5\
 longShallow.json\
 longShallow.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# shortSevere

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 shortSevere.hdf5\
 shortSevere.json\
 shortSevere.npy 

# time on 40 cores ~12 minutes with 5GB RAM

```

# ######################## PartialSHIC - expansionNe intergenic

## Generate training data

* Bunya

```{shell, install discoal}

cd /scratch/project/chenoase/partialSHIC_DsGRP

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=1\
 --mem=2G\
 --job-name=uqsalle3_TEST\
 --time=00:30:00\
 --partition=general\
 --account=a_chenoweth srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

git clone https://github.com/kr-colab/discoal.git

cd /scratch/project/chenoase/partialSHIC_DsGRP/discoal

make discoal

```

* Dell 2 port from R to bash

```{shell}

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

# setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat')

DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal' 
TIMES = 2000

Ne = 3607692
# Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000002771855
# s_high = 0.01
s_high = 0.002771855
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.002771855 # 0.002771855*4*3607692 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
# bottleneck <- "-en 0.007292 0 1.5 -en 0.02135 0 0.547 -en 0.05208 0 0.208 -en 0.09271 0 0.0805"

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
# bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
cat("bottleneck: ", bottleneck, "\n")
cat("sweep_pos: ", sweep_pos, "\n")

```

* Back to Bunya

```{shell, bunya example}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name=Neut_sim
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/slurm_discoal_Neutral.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/slurm_discoal_Neutral.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/

DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell, linked}

sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_0.slurm.sh
sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_1.slurm.sh
sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_2.slurm.sh
sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_3.slurm.sh
sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_4.slurm.sh
sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_6.slurm.sh
sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_7.slurm.sh
sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_8.slurm.sh
sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_9.slurm.sh
sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_10.slurm.sh

sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_0.slurm.sh
sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_1.slurm.sh
sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_2.slurm.sh
sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_3.slurm.sh
sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_4.slurm.sh
sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_6.slurm.sh
sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_7.slurm.sh
sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_8.slurm.sh
sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_9.slurm.sh
sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_10.slurm.sh

sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_0.slurm.sh
sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_1.slurm.sh
sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_2.slurm.sh
sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_3.slurm.sh
sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_4.slurm.sh
sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_6.slurm.sh
sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_7.slurm.sh
sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_8.slurm.sh
sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_9.slurm.sh
sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_10.slurm.sh

sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_0.slurm.sh
sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_1.slurm.sh
sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_2.slurm.sh
sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_3.slurm.sh
sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_4.slurm.sh
sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_6.slurm.sh
sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_7.slurm.sh
sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_8.slurm.sh
sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_9.slurm.sh
sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_10.slurm.sh

```

```{shell, submit}

find ./ -maxdepth 1 -type f -name 'discoal*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 1
done < list_discoal_files.txt



ls ./spNeut_*.msOut.gz | wc -l
ls ./spHard_5_*.msOut.gz | wc -l
ls ./spSoft_5_*.msOut.gz | wc -l
ls ./spHardPartial_5_*.msOut.gz | wc -l
ls ./spSoftPartial_5_*.msOut.gz | wc -l



ls ./spHard_0_*.msOut.gz | wc -l
ls ./spHard_1_*.msOut.gz | wc -l
ls ./spHard_2_*.msOut.gz | wc -l
ls ./spHard_3_*.msOut.gz | wc -l
ls ./spHard_4_*.msOut.gz | wc -l
ls ./spHard_6_*.msOut.gz | wc -l
ls ./spHard_7_*.msOut.gz | wc -l
ls ./spHard_8_*.msOut.gz | wc -l
ls ./spHard_9_*.msOut.gz | wc -l
ls ./spHard_10_*.msOut.gz | wc -l



ls ./spSoft_0_*.msOut.gz | wc -l
ls ./spSoft_1_*.msOut.gz | wc -l
ls ./spSoft_2_*.msOut.gz | wc -l
ls ./spSoft_3_*.msOut.gz | wc -l
ls ./spSoft_4_*.msOut.gz | wc -l
ls ./spSoft_6_*.msOut.gz | wc -l
ls ./spSoft_7_*.msOut.gz | wc -l
ls ./spSoft_8_*.msOut.gz | wc -l
ls ./spSoft_9_*.msOut.gz | wc -l
ls ./spSoft_10_*.msOut.gz | wc -l



ls ./spHardPartial_0_*.msOut.gz | wc -l
ls ./spHardPartial_1_*.msOut.gz | wc -l
ls ./spHardPartial_2_*.msOut.gz | wc -l
ls ./spHardPartial_3_*.msOut.gz | wc -l
ls ./spHardPartial_4_*.msOut.gz | wc -l
ls ./spHardPartial_6_*.msOut.gz | wc -l
ls ./spHardPartial_7_*.msOut.gz | wc -l
ls ./spHardPartial_8_*.msOut.gz | wc -l
ls ./spHardPartial_9_*.msOut.gz | wc -l
ls ./spHardPartial_10_*.msOut.gz | wc -l



ls ./spSoftPartial_0_*.msOut.gz | wc -l
ls ./spSoftPartial_1_*.msOut.gz | wc -l
ls ./spSoftPartial_2_*.msOut.gz | wc -l
ls ./spSoftPartial_3_*.msOut.gz | wc -l
ls ./spSoftPartial_4_*.msOut.gz | wc -l
ls ./spSoftPartial_6_*.msOut.gz | wc -l
ls ./spSoftPartial_7_*.msOut.gz | wc -l
ls ./spSoftPartial_8_*.msOut.gz | wc -l
ls ./spSoftPartial_9_*.msOut.gz | wc -l
ls ./spSoftPartial_10_*.msOut.gz | wc -l

```

```{shell}

# check and fix empty simulation

PREFIX="spSoftPartial_10"

ls ./${PREFIX}_*.msOut.gz

gzip -d ./${PREFIX}_*.msOut.gz

grep 'segsites' ./${PREFIX}_COMBINED.msOut | wc -l

rm ./${PREFIX}_COMBINED.msOut

for file in ./${PREFIX}_*.msOut; do
  count=$(grep -c 'segsites' "$file")
  echo "$file: $count"
done | grep ': 0'

gzip ./${PREFIX}_*.msOut



DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

# TIMES=982
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

# For Neutral
for TIME in $(seq 981 $TIMES); do
  $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

# For Hard_5
# ./spHard_5_1258.msOut: 0
# ./spHard_5_1259.msOut: 0
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_1258.msOut.gz
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_1259.msOut.gz

# For Soft_5
# ./spSoft_5_772.msOut: 0
# ./spSoft_5_773.msOut: 0
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_772.msOut.gz
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_773.msOut.gz

# For HardPartial_5
# ./spHardPartial_5_1326.msOut: 0
# ./spHardPartial_5_1327.msOut: 0
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_1326.msOut.gz
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_1327.msOut.gz

# For SoftPartial_5
# ./spSoftPartial_5_665.msOut: 0
# ./spSoftPartial_5_666.msOut: 0
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_665.msOut.gz
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_666.msOut.gz

```

```{shell, combine simulations}

# Function to process and combine sweep files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail ./${PREFIX}_*.msOut -n +3 > temp1.msOut
    perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
    awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/2000/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spNeut spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
    process_and_combine "$PREFIX"
done



# Function to process and combine linked files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail -n +3 ./${PREFIX}_*.msOut > temp1.msOut
    grep -v "$PREFIX" temp1.msOut > temp2.msOut
    awk 'BEGIN { RS="\n\n"; ORS="\n" } { print }' temp2.msOut > temp3.msOut
    
    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/400/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    process_and_combine "$PREFIX"
done

# # mini for testing
# 
# gzip -d spHard_5_1.msOut.gz
# gzip -d spHard_5_2.msOut.gz
# gzip -d spHard_5_3.msOut.gz
# 
# # Concatenate files and remove headers
# tail spHard_5_*.msOut -n +3 > temp1.msOut
# perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
# awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut
# 
# # Extract the first two lines from ${PREFIX}_1.msOut
# head -n 2 "spHard_5_1.msOut" > header.txt
# 
# # Edit the header to change the third space-delimited element from 1 to 2000
# sed -i '1s/\<1\>/3/' header.txt
# 
# # Concatenate the edited header with temp3.msOut
# cat header.txt temp3.msOut > spHard_5_COMBINED_MINI.msOut
# 
# # Move and compress the combined file
# gzip spHard_5_COMBINED_MINI.msOut
# 
# # Compress files
# find ./ -maxdepth 1 -type f -name "spHard_5_*.msOut" -exec gzip {} \;
# 
# # Clean up temporary files
# rm temp1.msOut temp2.msOut

```

## Generate test data

* Bunya

```{shell, install discoal}

cd /scratch/project/chenoase/partialSHIC_DsGRP

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=1\
 --mem=2G\
 --job-name=uqsalle3_TEST\
 --time=00:30:00\
 --partition=general\
 --account=a_chenoweth srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

git clone https://github.com/kr-colab/discoal.git

cd /scratch/project/chenoase/partialSHIC_DsGRP/discoal

make discoal

```

* Dell 2 port from R to bash

```{shell}

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

# setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat')

DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal' 
TIMES = 2000

Ne = 3607692
# Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000002771855
# s_high = 0.01
s_high = 0.002771855
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.002771855 # 0.002771855*4*3607692 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
# bottleneck <- "-en 0.007292 0 1.5 -en 0.02135 0 0.547 -en 0.05208 0 0.208 -en 0.09271 0 0.0805"

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
# bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
cat("bottleneck: ", bottleneck, "\n")
cat("sweep_pos: ", sweep_pos, "\n")

```
* Back to Bunya

```{shell, bunya example}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name=Neut_sim
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/test_self_newDat/expansionNe_intergenic/slurm_discoal_Neutral.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/test_self_newDat/eexpansionNe_intergenic/slurm_discoal_Neutral.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/

DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell, linked}

sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_0.slurm.sh
sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_1.slurm.sh
sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_2.slurm.sh
sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_3.slurm.sh
sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_4.slurm.sh
sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_6.slurm.sh
sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_7.slurm.sh
sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_8.slurm.sh
sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_9.slurm.sh
sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_10.slurm.sh

sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_0.slurm.sh
sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_1.slurm.sh
sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_2.slurm.sh
sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_3.slurm.sh
sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_4.slurm.sh
sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_6.slurm.sh
sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_7.slurm.sh
sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_8.slurm.sh
sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_9.slurm.sh
sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_10.slurm.sh

sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_0.slurm.sh
sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_1.slurm.sh
sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_2.slurm.sh
sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_3.slurm.sh
sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_4.slurm.sh
sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_6.slurm.sh
sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_7.slurm.sh
sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_8.slurm.sh
sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_9.slurm.sh
sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_10.slurm.sh

sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_0.slurm.sh
sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_1.slurm.sh
sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_2.slurm.sh
sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_3.slurm.sh
sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_4.slurm.sh
sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_6.slurm.sh
sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_7.slurm.sh
sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_8.slurm.sh
sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_9.slurm.sh
sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_10.slurm.sh

```

```{shell, submit}

find ./ -maxdepth 1 -type f -name 'discoal*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 1
done < list_discoal_files.txt



ls ./spNeut_*.msOut.gz | wc -l
ls ./spHard_5_*.msOut.gz | wc -l
ls ./spSoft_5_*.msOut.gz | wc -l
ls ./spHardPartial_5_*.msOut.gz | wc -l
ls ./spSoftPartial_5_*.msOut.gz | wc -l



ls ./spHard_0_*.msOut.gz | wc -l
ls ./spHard_1_*.msOut.gz | wc -l
ls ./spHard_2_*.msOut.gz | wc -l
ls ./spHard_3_*.msOut.gz | wc -l
ls ./spHard_4_*.msOut.gz | wc -l
ls ./spHard_6_*.msOut.gz | wc -l
ls ./spHard_7_*.msOut.gz | wc -l
ls ./spHard_8_*.msOut.gz | wc -l
ls ./spHard_9_*.msOut.gz | wc -l
ls ./spHard_10_*.msOut.gz | wc -l



ls ./spSoft_0_*.msOut.gz | wc -l
ls ./spSoft_1_*.msOut.gz | wc -l
ls ./spSoft_2_*.msOut.gz | wc -l
ls ./spSoft_3_*.msOut.gz | wc -l
ls ./spSoft_4_*.msOut.gz | wc -l
ls ./spSoft_6_*.msOut.gz | wc -l
ls ./spSoft_7_*.msOut.gz | wc -l
ls ./spSoft_8_*.msOut.gz | wc -l
ls ./spSoft_9_*.msOut.gz | wc -l
ls ./spSoft_10_*.msOut.gz | wc -l



ls ./spHardPartial_0_*.msOut.gz | wc -l
ls ./spHardPartial_1_*.msOut.gz | wc -l
ls ./spHardPartial_2_*.msOut.gz | wc -l
ls ./spHardPartial_3_*.msOut.gz | wc -l
ls ./spHardPartial_4_*.msOut.gz | wc -l
ls ./spHardPartial_6_*.msOut.gz | wc -l
ls ./spHardPartial_7_*.msOut.gz | wc -l
ls ./spHardPartial_8_*.msOut.gz | wc -l
ls ./spHardPartial_9_*.msOut.gz | wc -l
ls ./spHardPartial_10_*.msOut.gz | wc -l



ls ./spSoftPartial_0_*.msOut.gz | wc -l
ls ./spSoftPartial_1_*.msOut.gz | wc -l
ls ./spSoftPartial_2_*.msOut.gz | wc -l
ls ./spSoftPartial_3_*.msOut.gz | wc -l
ls ./spSoftPartial_4_*.msOut.gz | wc -l
ls ./spSoftPartial_6_*.msOut.gz | wc -l
ls ./spSoftPartial_7_*.msOut.gz | wc -l
ls ./spSoftPartial_8_*.msOut.gz | wc -l
ls ./spSoftPartial_9_*.msOut.gz | wc -l
ls ./spSoftPartial_10_*.msOut.gz | wc -l

```

```{shell, combine simulations}

# Function to process and combine sweep files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail ./${PREFIX}_*.msOut -n +3 > temp1.msOut
    perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
    awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/2000/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

# for PREFIX in spNeut spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
#     process_and_combine "$PREFIX"
# done

# for PREFIX in spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
#     process_and_combine "$PREFIX"
# done

for PREFIX in spHard_5; do
    process_and_combine "$PREFIX"
done



# Function to process and combine linked files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail -n +3 ./${PREFIX}_*.msOut > temp1.msOut
    grep -v "$PREFIX" temp1.msOut > temp2.msOut
    awk 'BEGIN { RS="\n\n"; ORS="\n" } { print }' temp2.msOut > temp3.msOut
    
    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/400/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    process_and_combine "$PREFIX"
done

# mini for testing

gzip -d spHard_5_1.msOut.gz
gzip -d spHard_5_2.msOut.gz
gzip -d spHard_5_3.msOut.gz

# Concatenate files and remove headers
tail spHard_5_*.msOut -n +3 > temp1.msOut
perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

# Extract the first two lines from ${PREFIX}_1.msOut
head -n 2 "spHard_5_1.msOut" > header.txt

# Edit the header to change the third space-delimited element from 1 to 2000
sed -i '1s/\<1\>/3/' header.txt

# Concatenate the edited header with temp3.msOut
cat header.txt temp3.msOut > spHard_5_COMBINED_MINI.msOut

# Move and compress the combined file
gzip spHard_5_COMBINED_MINI.msOut

# Compress files
find ./ -maxdepth 1 -type f -name "spHard_5_*.msOut" -exec gzip {} \;

# Clean up temporary files
rm temp1.msOut temp2.msOut

```

```{shell}

# check and fix empty Neutral simulation

PREFIX="spSoftPartial_5"

ls ./${PREFIX}_*.msOut.gz

gzip -d ./${PREFIX}_*.msOut.gz

grep 'segsites' ./${PREFIX}_COMBINED.msOut | wc -l

rm ./${PREFIX}_COMBINED.msOut

for file in ./${PREFIX}_*.msOut; do
  count=$(grep -c 'segsites' "$file")
  echo "$file: $count"
done | grep ': 0'



DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

# TIMES=982
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

# For Neutral
for TIME in $(seq 981 $TIMES); do
  $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

# For Hard_5
# ./spHard_5_1258.msOut: 0
# ./spHard_5_1259.msOut: 0
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_1258.msOut.gz
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_1259.msOut.gz

# For Soft_5
# ./spSoft_5_772.msOut: 0
# ./spSoft_5_773.msOut: 0
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_772.msOut.gz
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_773.msOut.gz

# For HardPartial_5
# ./spHardPartial_5_1326.msOut: 0
# ./spHardPartial_5_1327.msOut: 0
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_1326.msOut.gz
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_1327.msOut.gz

# For SoftPartial_5
# ./spSoftPartial_5_665.msOut: 0
# ./spSoftPartial_5_666.msOut: 0
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_665.msOut.gz
$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_666.msOut.gz

```


## Run training

```{python, training_deep_learning_python3.py}

# import time
# # startTime=time.clock() # Python 2
# startTime = time.perf_counter() # Python 3
# import sys
# import numpy as np
# np.random.seed(123)
# from sklearn.model_selection import train_test_split
# # from keras.utils import np_utils
# from keras.utils import to_categorical # SCOTT
# from keras.models import Sequential
# from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
# from keras import optimizers
# from keras.callbacks import EarlyStopping,ModelCheckpoint
# 
# '''usage eg:
# python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
# '''
# 
# if len(sys.argv)!=9:
#   sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
# else:
#   fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]
# 
# fvecDir='./'
# fvecFiles='neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec'
# numSubWins='11'
# numSumStatsPerSubWin='92'
# validationSize='0.1'
# weightsFileName='constNe.hdf5'
# jsonFileName='constNe.json'
# npyFileName='constNe.npy'
# 
# if fvecDir.lower() in ["none","false","default"]:
#   fvecDir='./'
# 
# if fvecFiles.lower() in ["none","false","default"]:
#   fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
# else:
#   fvecFiles=fvecFiles.split(",")
#   assert len(fvecFiles)==9
# 
# numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)
# 
# if validationSize.lower() in ["none","false","default"]:
#   validationSize=0.1
# else:
#   validationSize=float(validationSize)
# 
# sweeps=[]
# 
# for i in range(0,9):
#  sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
#  if i>0:
#   assert len(sweeps[0])==len(sweeps[i])
# 
# sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))
# 
# sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)
# 
# models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))
# 
# sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=42)
# 
# # models=np_utils.to_categorical(models,9)
# models=to_categorical(models,9) # SCOTT
# 
# # models_val=np_utils.to_categorical(models_val,9)
# models_val=to_categorical(models_val,9) # SCOTT
# 
# netlayers=Sequential()
# netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
# netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
# netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
# netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
# netlayers.add(Dropout(0.25))
# netlayers.add(Flatten())
# netlayers.add(Dense(512, activation='relu'))
# netlayers.add(Dropout(0.5))
# netlayers.add(Dense(128, activation='relu'))
# netlayers.add(Dropout(0.5))
# netlayers.add(Dense(9, activation='softmax'))
# netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
# 
# checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')
# 
# netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)
# 
# netlayers_json=netlayers.to_json()
# 
# with open(jsonFileName,"w") as json_file:
#   json_file.write(netlayers_json)
# 
# netlayers.save(npyFileName)
# 
# sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))

```

### Training - calcStatsAndDafForEachSnp

```{python, calcStatsAndDafForEachSnpSingleMsFile_Python3}

import sys
import allel
import random
import numpy as np
from msTools import *
from fvTools import *
import time

'''usage example
python calcStatsAndDafForEachSnpSingleMsFile.py /san/data/dan/simulations/discoal_multipopStuff/spatialSVMSims/trainingSets/equilibNeut.msout.gz 110000 iHS nSL
'''

trainingDataFileName, totalPhysLen = sys.argv[1:3]
totalPhysLen = int(totalPhysLen)

hapArraysIn, positionArrays = msOutToHaplotypeArrayIn(trainingDataFileName, totalPhysLen)
numInstances = len(hapArraysIn)

header = "daf\tiHS\tnSL"
#print header # Python 2
print(header) # Python 3

dafs = []
#start = time.clock() # Python 2
start = time.perf_counter() # Python 3
numInstancesDone = 0

numSnpsDone = 0
for instanceIndex in range(numInstances):
    if instanceIndex % 10 == 0:
        sys.stderr.write("done wtih %d of %d instances (%d SNPs)\n" %(instanceIndex, numInstances, numSnpsDone))
    haps = allel.HaplotypeArray(hapArraysIn[instanceIndex], dtype='i1')
    genos = haps.to_genotypes(ploidy=2)
    ac = genos.count_alleles()
    sampleSizes = [sum(x) for x in ac]
    assert len(set(sampleSizes)) == 1
    dafs = ac[:,1]/float(sampleSizes[0])
    ihsVals = allel.stats.selection.ihs(haps, positionArrays[instanceIndex], use_threads=True, include_edges=False)
    nslVals = allel.stats.selection.nsl(haps, use_threads=True)
    assert len(dafs) == len(ihsVals) == len(nslVals)
    numSnpsDone += len(dafs)
    for i in range(len(dafs)):
        #print "%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i]) # Python 2
        print ("%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i])) #Python 3
    if numSnpsDone > 5000000:
        break

```

```{shell, test Dell2}

# module load anaconda3
# source ~/conda-init
# conda activate partialshic_py3
# 
# # cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/consNe
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
# 
# # rsync -ahPv /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/constNe/*.msOut.gz ./
# 
# msOut_files=$(find ./ -type f -name '*.msOut.gz' | sort | sed 's/\.\///')
# 
# # python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/constNe/spHard_0.msOut.gz 55000 > spHard_0.msOut.gz_stats.txt
# 
# gen_stats() {
#     python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${1} 55000 > ${1/.msOut.gz/}_stats.txt
# }
# export -f gen_stats
# ls *msOut.gz | parallel -j 8 gen_stats
# 
# python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py spPartialSoft_5.msOut.gz 55000 > spPartialSoft_5_stats.txt
# 
# # # spHard_0 6GB RMA 3 minutes
# # time python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py spHard_0.msOut.gz 55000 > spHard_0_test_stats.txt
# # # pgrep -f calcStatsAndDafForEachSnpSingleMsFile_Python3.py
# # # pidstat -r -u -h -p 214015
# # 
# # # spNeut 22GB RAM 10 minutes (stops after 5M SNPs)
# # time python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py spNeut.msOut.gz 55000 > spNeut_test_stats.txt
# # # pgrep -f calcStatsAndDafForEachSnpSingleMsFile_Python3.py
# # # pidstat -r -u -h -p 171341
# # 
# # # spHard_5 10GB RAM 7 minutes
# # time python ~/shared/Scott_Allen/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py spHard_5.msOut.gz 55000 > spHard_5_test_stats.txt
# # # pgrep -f calcStatsAndDafForEachSnpSingleMsFile_Python3.py
# # # pidstat -r -u -h -p 179283

```

```{shell, bunya test}

# #!/bin/bash --login
# #SBATCH --nodes=1
# #SBATCH --ntasks-per-node=1
# #SBATCH --cpus-per-task=1
# #SBATCH --mem=2G
# #SBATCH --job-name=H_1_1
# #SBATCH --time=00:05:00
# #SBATCH --partition=general
# #SBATCH --account=a_chenoweth
# #SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_expansionNe_intergenic_spHard_1_1.output
# #SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_expansionNe_spHard_1_1.error
# 
module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic

msOut="spNeut_COMBINED.msOut"

python /home/uqsalle3/Programs/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut/}_stats.txt

```

```{shell, bunya test parallel}

# #!/bin/bash --login
# #SBATCH --nodes=1
# #SBATCH --ntasks-per-node=1
# #SBATCH --cpus-per-task=1
# #SBATCH --mem=2G
# #SBATCH --job-name=H_1_1
# #SBATCH --time=00:05:00
# #SBATCH --partition=general
# #SBATCH --account=a_chenoweth
# #SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_expansionNe_intergenic_spHard_1_1.output
# #SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_expansionNe_spHard_1_1.error
# 
# module load anaconda3
# source ~/conda-init
# conda activate partialshic_py3
# 
# cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic
# 
# msOut="spHard_0_1.msOut.gz"
# 
# python /home/uqsalle3/Programs/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, bunya template}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=8G
#SBATCH --job-name=[SAMPLE]
#SBATCH --time=00:30:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/

msOut="[SAMPLE]"

srun python /home/uqsalle3/Programs/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/

# linked (*_msOut.gz)

find ./ -maxdepth 1 -type f -name '*_COMBINED.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_calcStatsAndDafForEachSnpSingleMsFile_Python3.txt calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
done < msOut_files.txt

find ./ -maxdepth 1 -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*.slurm.sh' -print0 | xargs -0 chmod 755

ls ./calcStatsAndDafForEachSnpSingleMsFile_Python3_*.slurm.sh

# # vim RAM to 24GB and time to 15 minutes for neut
# # vim RAM to 12GB and time to 10 minutes for central sub-window (5)
# 
# # Find all files starting with 'calcStatsAndDafForEachSnpSingleMsFile_Python3_'
# find ./ -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*' -print0 |
# while IFS= read -r -d '' file; do
#     # Perform the in-place replacement using sed
#     sed -i 's/longShallow/shortSevere/g' "$file"
# done
# 
# # Find all files starting with 'calcStatsAndDafForEachSnpSingleMsFile_Python3_'
# find ./ -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*' -print0 |
# while IFS= read -r -d '' file; do
#     # Perform the in-place replacement using sed
#     sed -i 's/_self_newDat_self_newDat_/_self_newDat_/g' "$file"
# done
# 
# # Find all files starting with 'calcStatsAndDafForEachSnpSingleMsFile_Python3_'
# find ./ -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*' -print0 |
# while IFS= read -r -d '' file; do
#     # Perform the in-place replacement using sed
#     sed -i 's/time\=00\:05\:00/time\=00\:15\:00/g' "$file"
# done
# 
# # check stats files
# tail -n 2 ./slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_*.error

```

```{shell, submit scripts Bunya}

while read p; do
  echo "calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh submitted"
  sbatch calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
  sleep 0.25
done < msOut_files.txt

```

```{shell}

# module load anaconda3
# source ~/conda-init
# conda activate partialshic_py3
# 
# cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic
# 
# msOut="spHard_5_COMBINED.msOut.gz"
# 
# python /home/uqsalle3/Programs/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

### Training - training_convert_to_FVs

```{python, training_convert_to_FVs_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
from msTools import *
from fvTools import *
import random
import allel
import numpy as np
import math

# '''usage eg:
# pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
# python2 training_convert_to_FVs.py spNeut.msOut.gz 2L,2R,3L,3R 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.fa ./ spNeut.msOut.fvec
# '''
# 
# if len(sys.argv)!=12:
#   sys.exit("usage:\npython2 training_convert_to_FVs.py trainingDataFileName chrArmsForMasking subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName statDir fvecFileName\n")
# else:
#   trainingDataFileName, chrArmsForMasking, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, statDir, fvecFileName = sys.argv[1:]

trainingDataFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5.msOut.gz"
chrArmsForMasking="ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140"
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_stats.txt"
maskFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta"
# ancestralArmFaFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/top6.anc.fa"
ancestralArmFaFileName="none"
statDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/"
fvecFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_test.fvec"

subWinSize,numSubWins = int(subWinSize),int(numSubWins)
assert subWinSize>0 and numSubWins>1

totalPhysLen=(subWinSize*numSubWins)

hapArraysIn,positionArrays=msOutToHaplotypeArrayIn(trainingDataFileName,totalPhysLen)

chrArmsForMasking=chrArmsForMasking.split(",")

unmaskedFracCutoff,pMisPol=float(unmaskedFracCutoff),float(pMisPol)

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")


if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")


standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none", "false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*totalPhysLen
  sys.stderr.write("Warning: not doing any masking! (i.e. mask.fa file for the chr arm with all masked sites N'ed out, or at least the reference with Ns, is not provided)\n")
  maskFileName=False
else:
  if ancestralArmFaFileName.lower() in ["none", "false"]:
    maskData=readMaskDataForTraining(maskFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  else:
    maskData=readMaskAndAncDataForTraining(maskFileName,ancestralArmFaFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  if len(maskData)<len(hapArraysIn):
    sys.stderr.write("Warning: not enough windows from masked data (needed %d; got %d); will draw with replacement!!\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=True
  else:
    sys.stderr.write("enough windows from masked data (needed %d; got %d); will draw without replacement.\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=False


# for i in range(len(maskData)):
#   print(str(i) + " " + str(len(maskData[i])) + " " + str(sum(maskData[i])))

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

header=[]

for statName in statNames:
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName, i))

header="\t".join(header)

statVals={}

for statName in statNames:
  statVals[statName]=[]

def getSubWinBounds(subWinSize,totalPhysLen):
  subWinStart=1-subWinSize
  subWinEnd=0
  subWinBounds=[]
  for i in range(0,(numSubWins-1)):
    subWinStart+=subWinSize
    subWinEnd+=subWinSize
    subWinBounds.append((subWinStart,subWinEnd))
  subWinStart+=subWinSize
  subWinEnd=totalPhysLen
  subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,totalPhysLen)

def getSnpIndicesInSubWins(subWinBounds,positions):
  snpIndicesInSubWins=[]
  for subWinIndex in range(len(subWinBounds)):
    snpIndicesInSubWins.append([])
  subWinIndex=0
  for i in range(len(positions)):
    while not (positions[i]>=subWinBounds[subWinIndex][0] and positions[i]<=subWinBounds[subWinIndex][1]):
      subWinIndex+=1
    snpIndicesInSubWins[subWinIndex].append(i)
  return snpIndicesInSubWins

quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

if statDir.lower() in ["none","false","default"]:
  statDir='./'

statFiles=[]

for subWinIndex in range(numSubWins):
  statFileName='/'.join(("%s/%s.subWin%d.stats" %(statDir, trainingDataFileName.split("/")[-1].replace(".msOut.gz",""), subWinIndex)).split('//'))
  statFiles.append(open(statFileName,"w"))
  statFiles[-1].write("\t".join(statNames)+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  fvecFileName="%s.fvec" %trainingDataFileName.split("/")[-1].replace(".msOut.gz","")

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

# for testing for loop below only
instanceIndex=0
statName='pi'

for instanceIndex in range(len(hapArraysIn)):
  print("Starting index " + str(instanceIndex))
  for statName in statNames:
    statVals[statName].append([])
  if maskFileName:
    if drawWithReplacement:
      unmasked=random.choice(maskData)
    else:
      unmasked=maskData[instanceIndex]
    assert len(unmasked)==totalPhysLen
    print(str(sum(unmasked)) + " Loci unmasked")
  snpIndicesToKeep=[x for x in range(len(positionArrays[instanceIndex])) if unmasked[positionArrays[instanceIndex][x]-1]]
  if len(snpIndicesToKeep)==0:
    for subWinIndex in range(numSubWins):
      for statName in statNames:
        print("No SNPs in BIGwindow, appendStatValsForMonomorphic")
        appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  else:
    haps=allel.HaplotypeArray(hapArraysIn[instanceIndex],dtype='i1').subset(sel0=snpIndicesToKeep)
    if pMisPol>0:
      misPolarizeCorrectionIndex=np.random.binomial(1,pMisPol,len(haps))
      for i in range(len(misPolarizeCorrectionIndex)):
        if misPolarizeCorrectionIndex[i]==1:
          for j in range(len(haps[i])):
            if haps[i][j]==0:
              haps[i][j]=1
            else:
              haps[i][j]=0
    genos=haps.to_genotypes(ploidy=2)
    alleleCounts=genos.count_alleles()
    positions=[positionArrays[instanceIndex][x] for x in snpIndicesToKeep]
    precomputedStats={}
    sampleSizes=[sum(x) for x in alleleCounts]
    assert len(set(sampleSizes))==1
    dafs=alleleCounts[:,1]/float(sampleSizes[0])
    ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
    nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
    nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
    # sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    if nonNanCount==0:
      precomputedStats["iHS"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["iHS"].append([])
    else:
      ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
      precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)
    nslVals=allel.stats.selection.nsl(haps,use_threads=False)
    nonNanCount=[x for x in np.isnan(nslVals)].count(False)
    # sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    if nonNanCount==0:
      precomputedStats["nSL"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["nSL"].append([])
    else:
      nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
      precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)
    snpIndicesInSubWins=getSnpIndicesInSubWins(subWinBounds,positions)
    for subWinIndex in range(numSubWins):
      subWinStart,subWinEnd = subWinBounds[subWinIndex]
      unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
      assert unmaskedFrac>=unmaskedFracCutoff
      if len(snpIndicesInSubWins[subWinIndex])==0:
        print("No SNPS in " + str(subWinIndex) + ", appendStatValsForMonomorphic")
        for statName in statNames:
          appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      else:
        print(str(snpIndicesInSubWins[subWinIndex])) + " SNPs in " + str(subWinIndex) + ", calcAndAppendStatVal")
        hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
        for statName in [x for x in statNames if x[0:3]!="HAF" and x[0:3]!="phi" and x[0:3]!="kap" and x[0:3]!="SFS" and x[0:3]!="SAF"]:
          calcAndAppendStatVal(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, instanceIndex, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        haplotypes={}
        for i in range(len(hapsInSubWin[0])):
          haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
          haplotype="".join(str(x) for x in haplotype)
          if haplotype in haplotypes:
            haplotypes[haplotype].append(i)
          else:
            haplotypes[haplotype]=[i]
        HAF=[]
        HAFunique={}
        for i in haplotypes:
          HAFunique[i]=0
          for j in range(len(hapsInSubWin)):
            if hapsInSubWin[j][haplotypes[i][0]]==1:
              HAFunique[i]+=sum([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j]))])
          for j in range(len(haplotypes[i])):
            HAF.append(HAFunique[i])
        phi=[]
        kappa=[]
        SAFE=[]
        for i in range(len(hapsInSubWin)):
          phi.append(0)
          kappa.append([])
          phiDenom=0
          for j in haplotypes:
            phi[i]+=(int(list(j)[i])*HAFunique[j]*len(haplotypes[j]))
            if int(list(j)[i])==1 and HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
              kappa[i].append(HAFunique[j])
            phiDenom+=(HAFunique[j]*len(haplotypes[j]))
          phi[i]/=float(phiDenom)
          kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
          if dafs[snpIndicesInSubWins[subWinIndex]][i]==0 or dafs[snpIndicesInSubWins[subWinIndex]][i]==1:
           SAFE.append(0.0)
          else:
           SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafs[snpIndicesInSubWins[subWinIndex]][i]*(1-dafs[snpIndicesInSubWins[subWinIndex]][i]))))
        for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
          if i=="SFS":
            windowStats=dafs[snpIndicesInSubWins[subWinIndex]]
          elif i=="HAFunique":
            # windowStats=[eval(i)[x] for x in eval(i)]
            windowStats = list(HAFunique.values()) # SCOTT
          else:
            windowStats=eval(i)
          statVals[i+"-Mean"][instanceIndex].append(np.mean(windowStats))
          statVals[i+"-Median"][instanceIndex].append(np.median(windowStats))
          if(len(np.unique(windowStats,return_counts=True)[1])==1):
            statVals[i+"-Mode"][instanceIndex].append(windowStats[0])
          else:
            if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
              statVals[i+"-Mode"][instanceIndex].append(scipy.stats.mstats.mode(windowStats)[0][0])
            else:
              mode=min(windowStats)
              for j in range(1,51):
                if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
                  mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
              statVals[i+"-Mode"][instanceIndex].append(mode+((max(windowStats)-min(windowStats))/100))
          for j in quantiles:
            statVals[i+"-"+j][instanceIndex].append(np.percentile(windowStats,quantiles[j]))
          statVals[i+"-Max"][instanceIndex].append(max(windowStats))
          statVals[i+"-Var"][instanceIndex].append(np.var(windowStats))
          statVals[i+"-SD"][instanceIndex].append(np.std(windowStats))
          statVals[i+"-Skew"][instanceIndex].append(scipy.stats.skew(windowStats))
          statVals[i+"-Kurt"][instanceIndex].append(scipy.stats.kurtosis(windowStats))
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][instanceIndex])
  fvecFile.write("\t".join([str(x) for x in outVec])+"\n")

for subWinIndex in range(numSubWins):
  statFiles[subWinIndex].close()
fvecFile.close()
# sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, test Dell2}

# # trainingDataFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0.msOut.gz"
# # chrArmsForMasking="ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140"
# # subWinSize=5000
# # numSubWins=11
# # unmaskedFracCutoff=0.25
# # pMisPol=0.003
# # partialStatAndDafFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0_stats.txt"
# # maskFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta"
# # ancestralArmFaFileName="none"
# # statDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/"
# # fvecFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0_test.fvec"
# # 
# # 
# # python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py \
# #  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0.msOut.gz \
# #  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140 \
# #  5000 \
# #  11 \
# #  0.25 \
# #  0.003 \
# #  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0_stats.txt \
# #  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta \
# #  none \
# #  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/ \
# #  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_0_test.fvec
# 
# module load anaconda3
# source ~/conda-init
# conda activate partialshic_py3
# 
# # cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/consNe
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
# 
# # linked
# 
# gen_fvec() {
#   python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py\
#   ${1}\
#   ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
#   5000\
#   11\
#   0.25\
#   0.003\
#   ${1/.msOut.gz/}_stats.txt\
#   /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
#   none\
#   /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/\
#   ${1/.msOut.gz/}.fvec
# }
# export -f gen_fvec
# ls *[0-4]*msOut.gz *[6-9]*msOut.gz *10*msOut.gz | sort -V | parallel -j 20 gen_fvec
# 
# 
# 
# gen_fvec() {
#   python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py\
#   ${1}\
#   ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
#   5000\
#   11\
#   0.25\
#   0.003\
#   ${1/.msOut.gz/}_stats.txt\
#   /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
#   none\
#   /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/\
#   ${1/.msOut.gz/}.fvec
# }
# export -f gen_fvec
# ls *msOut.gz | sort -V | parallel -j 20 gen_fvec
# 
# # nohup python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py\
# #   spPartialHard_5.msOut.gz\
# #   ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
# #   5000\
# #   11\
# #   0.25\
# #   0.003\
# #   spPartialHard_5_stats.txt\
# #   /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
# #   none\
# #   /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/\
# #   spPartialHard_5.fvec > nohup_training_convert_to_FVs_Python3_spPartialHard_5.out &
# # 
# # nohup python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py\
# #   spNeut.msOut.gz\
# #   ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
# #   5000\
# #   11\
# #   0.25\
# #   0.003\
# #   spNeut_stats.txt\
# #   /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
# #   none\
# #   /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/\
# #   sspNeut.fvec > nohup_training_convert_to_FVs_Python3_spNeut.out &
# # 
# # spPartialSoft_5
# # 
# # nohup python ~/shared/Scott_Allen/partialSHIC/training_convert_to_FVs_Python3.py\
# #   spPartialSoft_5.msOut.gz\
# #   ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
# #   5000\
# #   11\
# #   0.25\
# #   0.003\
# #   spPartialSoft_5_stats.txt\
# #   /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
# #   none\
# #   /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/\
# #   spPartialSoft_5.fvec > nohup_training_convert_to_FVs_Python3_spPartialSoft_5.out &
# # 
# # # transfer to Bunya
# # cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya
# # find ./ -type f -name 'cohort.*' | sed 's/\.\///' > rsync_file_list.txt
# # rsync -ahPv --files-from=rsync_file_list.txt ./ uqsalle3@bunya.rcc.uq.edu.au:/scratch/project_mnt/S0032/partialSHIC_DsGRP/HaplotypeCaller/

```

```{shell, bunya test}

# #!/bin/bash --login
# #SBATCH --nodes=1
# #SBATCH --ntasks-per-node=1
# #SBATCH --cpus-per-task=1
# #SBATCH --mem=8G
# #SBATCH --job-name=spHard_0_1.msOut.gz
# #SBATCH --time=00:15:00
# #SBATCH --partition=general
# #SBATCH --account=a_chenoweth
# #SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_training_convert_to_FVs_Python3_spHard_0_1_msOut.gz.output
# #SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_training_convert_to_FVs_Python3_spHard_0_1_msOut.gz.error
# 
# module load anaconda3
# source ~/conda-init
# conda activate partialshic_py3
# 
# cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic
# 
# msOut="spHard_5_1.msOut.gz"
# 
# srun python /home/uqsalle3/Programs/partialSHIC/training_convert_to_FVs_Python3.py\
#   ${msOut}\
#   ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
#   5000\
#   11\
#   0.25\
#   0.003\
#   spHard_5_COMBINED_stats.txt\
#   /scratch/project/chenoase/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
#   none\
#   /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/\
#   ${msOut/.msOut.gz/}_TEST.fvec
# 
# 
# 
# cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe
# 
# msOut="spHard_0.msOut.gz"
# 
# python /home/uqsalle3/Programs/partialSHIC/training_convert_to_FVs_Python3.py\
#   ${msOut}\
#   ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
#   5000\
#   11\
#   0.25\
#   0.003\
#   ${msOut/.msOut.gz/}_stats.txt\
#   /scratch/project/chenoase/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
#   none\
#   /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe/\
#   ${msOut/.msOut.gz/}_test.fvec

```

* Separate and combined likely differ to due random masking!!!

```{shell, mini test}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/

msOut="spHard_5_COMBINED_MINI.msOut.gz"

python /home/uqsalle3/Programs/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spHard_5_COMBINED_stats.txt\
  /scratch/project/chenoase/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/\
  ${msOut/.msOut.gz/}.fvec

msOut="spHard_5_1.msOut.gz"

python /home/uqsalle3/Programs/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spHard_5_COMBINED_stats.txt\
  /scratch/project/chenoase/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell, bunya template Neutral}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=8G
#SBATCH --job-name=[SAMPLE]
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_training_convert_to_FVs_Python3_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_training_convert_to_FVs_Python3_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/

msOut="[SAMPLE]"

srun python /home/uqsalle3/Programs/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spNeut_COMBINED_stats.txt\
  /scratch/project/chenoase/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell}

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spHard_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_10.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

find ./ -type f -name '*.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt



# Neutral

grep 'Neut' msOut_files.txt > temp.txt
grep -v 'COMBINED' temp.txt > msOut_files_Neutral.txt
rm temp.txt

while read p; do
  echo "$p"
  cp template_training_convert_to_FVs_Python3_Neutral.txt training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
done < msOut_files_Neutral.txt



# Function to process files based on the specified prefix
process_files() {
    local prefix="$1"
    local template="template_training_convert_to_FVs_Python3_${prefix}.txt"
    local output_file="msOut_files_${prefix}.txt"

    # Create a list of files based on the prefix
    grep "${prefix}_" msOut_files.txt > temp.txt
    grep -v 'COMBINED' temp.txt > "${output_file}"
    rm temp.txt

    # Process each file in the list
    while read -r p; do
        echo "$p"
        cp "${template}" "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh"
        replace_CMD="s/\[SAMPLE\]/${p}/g"
        perl -pi -e "$replace_CMD" "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh"
    done < "${output_file}"
}

# Loop through different prefixes
for i in {0..10}; do
    process_files "Hard_${i}"
done

for i in {0..10}; do
    process_files "Soft_${i}"
done

for i in {0..10}; do
    process_files "HardPartial_${i}"
done

for i in {0..10}; do
    process_files "SoftPartial_${i}"
done



# # Function to process files based on the specified prefix
# process_files() {
#     local output_file="temp.txt"
#     
#     # Create a list of files based on the prefix
#     find ./ -maxdepth 1 -type f -name 'training_*_COMBINED*' > "${output_file}"
#     
#     # Process each file in the list
#     while read -r p; do
#         echo "$p"
#         sed -i 's/mem\=8G/mem\=64G/' ${p}
#         sed -i 's/time\=00\:15\:00/time\=24\:00\:00/' ${p}
#     done < "${output_file}"
# }
# 
# # Loop through different prefixes
# process_files



find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*.slurm.sh' -print0 | xargs -0 chmod 755

```

```{shell, submit scripts Bunya}

grep -v 'COMBINED' msOut_files.txt > msOut_files_SINGLE.txt

while read p; do
  echo "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh submitted"
  sbatch training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
  sleep 0.05
done < msOut_files_SINGLE.txt

ls ./spNeut_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_5_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spHard_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spSoft_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spHardPartial_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spSoftPartial_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_10_*.fvec | grep -v 'COMBINED' | wc -l

## rerun failed

grep 'error' ./slurm_training_convert_to_FVs_Python3_sp*.error | perl -ne 's/\.\/slurm_/sbatch\ /; s/\.msOut\.gz.*/\.slurm\.sh/; print $_;' | uniq > rerun_failed_training_convert_to_FVs_Python3.sh

bash rerun_failed_training_convert_to_FVs_Python3.sh

```

```{shell, failed rerun}

# # constNe
# # constNe/test_self_newDat
# cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe/test_self_newDat
# # cp ../../shortSevere/test_self_newDat/training_convert_to_FVs_Python3_sp*.slurm.sh ./
# # Find all files starting with 'calcStatsAndDafForEachSnpSingleMsFile_Python3_'
# find ./ -type f -name 'training_convert_to_FVs_Python3_*' -print0 |
# while IFS= read -r -d '' file; do
#     # Perform the in-place replacement using sed
#     sed -i 's/expansionNe/constNe/g' "$file"
# done
# 
# 
# # expansionNe
# # expansionNe/test_self_newDat
# 
# # longShallow
# spPartialSoft_10 walltime (RUNNING)
# spPartialSoft_4 walltime (RUNNING)
# # longShallow/test_self_newDat
# spPartialSoft_10.fvec walltime (RUNNING)
# 
# # shortSevere


```

```{shell, combined fvec}

process_files() {
  local PREFIX="$1"
  find ./ -maxdepth 1 -type f | grep -E "${PREFIX}_[0-9]+\.fvec" > temp.txt
  
  while read -r p; do
    wc ${p}
  done < temp.txt
  
  while read -r p; do
    awk 'NR == 2' ${p}
  done < temp.txt > temp.fvec

  # Extract the first line from ${PREFIX}_1.msOut
  head -n 1 "${PREFIX}_1.fvec" > header.txt

  # Concatenate the edited header with temp.fvec
  cat header.txt temp.fvec > "${PREFIX}_SINGLE_COMBINED.fvec"
}

rm -v ./spNeut_SINGLE_COMBINED.fvec
for PREFIX in spNeut; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spHard_*_SINGLE_COMBINED.fvec
for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_5 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spSoft_*_SINGLE_COMBINED.fvec
for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_5 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spHardPartial_*_SINGLE_COMBINED.fvec
for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_5 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spSoftPartial_*_SINGLE_COMBINED.fvec
for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_5 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done



find ./ -maxdepth 1 -type f -name '*_SINGLE_COMBINED.fvec' -exec wc {} \;

```

```{shell, transfer to Dell 2}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

# expansionNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/*_SINGLE_COMBINED.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/*_SINGLE_COMBINED.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/

# There should be 45 fvec files
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/*.fvec | wc -l
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/*.fvec | wc -l

```

```{r, assess feature vectors}

# module load anaconda3
# source ~/conda-init
# conda activate r_env

library(tidyverse)
library(data.table)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic")

spHard_5 <- read_tsv("spHard_5_SINGLE_COMBINED.fvec")

spHard_5 %>% 
  mutate(sim=c(1:nrow(spHard_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

spSoft_5 <- read_tsv("spSoft_5_SINGLE_COMBINED.fvec")

spSoft_5 %>% 
  mutate(sim=c(1:nrow(spSoft_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

```

### Training - training_sample_FVs

```{python, training_sample_FVs.py}

# import sys,os,random
import sys # SCOTT
import os # SCOTT
import random # SCOTT

'''usage eg:
python2 training_sample_FVs.py spNeut.fvec spHard_ spSoft_ spPartialHard_ spPartialSoft_ 5 0,1,2,3,4,6,7,8,9,10 ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec
'''

if len(sys.argv)!=10:
  sys.exit("usage:\npython2 training_sample_FVs.py neutTrainingFileName hardTrainingFilesPrefix softTrainingFilesPrefix partialHardTrainingFilesPrefix partialSoftTrainingFilesPrefix sweepTrainingWindow linkedTrainingWindows sampledFVsDir sampledFVsFiles\n")
else:
  neutTrainingFileName, hardTrainingFilesPrefix, softTrainingFilesPrefix, partialHardTrainingFilesPrefix, partialSoftTrainingFilesPrefix, sweepTrainingWindow, linkedTrainingWindows, sampledFVsDir, sampledFVsFiles = sys.argv[1:]

neutTrainingFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spNeut.fvec"
hardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_"
softTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spSoft_"
partialHardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialHard_"
partialSoftTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialSoft_"
sweepTrainingWindow=5
linkedTrainingWindows=0,1,2,3,4,6,7,8,9,10
sampledFVsDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe"
sampledFVsFiles="neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec"

sweepFilePaths,linkedFilePaths = {},{}
for trainingFilePrefix in [hardTrainingFilesPrefix,softTrainingFilesPrefix,partialHardTrainingFilesPrefix,partialSoftTrainingFilesPrefix]:
  trainingSetDir="/".join(trainingFilePrefix.split("/")[:-1])
  trainingFilePrefixDirless=trainingFilePrefix.split("/")[-1]
  sweepFilePaths[trainingFilePrefix]=[]
  linkedFilePaths[trainingFilePrefix]=[]
  for fileName in os.listdir(trainingSetDir):
    if fileName.startswith(trainingFilePrefixDirless):
      winNum=int(fileName.split("_")[1].split(".")[0])
      if winNum==int(sweepTrainingWindow):
        sweepFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)
      elif winNum in [int(x) for x in linkedTrainingWindows.split(",")]:
        linkedFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)

def getExamplesFromFVFile(simFileName):
  try:
    simFile=open(simFileName)
    lines=[line.strip() for line in simFile.readlines()]
    header=lines[0]
    examples=lines[1:]
    simFile.close()
    return header,examples
  except Exception:
    return "",[]

def getExamplesFromFVFileLs(simFileLs):
  examples=[]
  keptHeader=""
  for filePath in simFileLs:
    header,currExamples=getExamplesFromFVFile(filePath)
    if header:
      keptHeader=header
    examples+=currExamples
  return keptHeader,examples

header,neutExamples=getExamplesFromFVFile(neutTrainingFileName)
hardHeader,hardExamples=getExamplesFromFVFileLs(sweepFilePaths[hardTrainingFilesPrefix])
linkedHardHeader,linkedHardExamples=getExamplesFromFVFileLs(linkedFilePaths[hardTrainingFilesPrefix])
softHeader,softExamples=getExamplesFromFVFileLs(sweepFilePaths[softTrainingFilesPrefix])
linkedSoftHeader,linkedSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[softTrainingFilesPrefix])
partialHardHeader,partialHardExamples=getExamplesFromFVFileLs(sweepFilePaths[partialHardTrainingFilesPrefix])
linkedPartialHardHeader,linkedPartialHardExamples=getExamplesFromFVFileLs(linkedFilePaths[partialHardTrainingFilesPrefix])
partialSoftHeader,partialSoftExamples=getExamplesFromFVFileLs(sweepFilePaths[partialSoftTrainingFilesPrefix])
linkedPartialSoftHeader,linkedPartialSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[partialSoftTrainingFilesPrefix])

def getMinButNonZeroExamples(lsLs):
  counts=[]
  for ls in lsLs:
    if len(ls)>0:
      counts.append(len(ls))
  if not counts:
    raise Exception
  return min(counts)

trainingSetLs=[hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
numExamplesToKeep=getMinButNonZeroExamples(trainingSetLs)
for i in range(len(trainingSetLs)):
  random.shuffle(trainingSetLs[i])
  trainingSetLs[i]=trainingSetLs[i][:numExamplesToKeep]
hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples=trainingSetLs

if sampledFVsDir.lower() in ["none","false","default"]:
  sampledFVsDir='./'
if sampledFVsFiles.lower() in ["none","false","default"]:
  sampledFVsFiles=["neut.fvec","hard.fvec","linkedHard.fvec","soft.fvec","linkedSoft.fvec","partialHard.fvec","linkedPartialHard.fvec","partialSoft.fvec","linkedPartialSoft.fvec"]
else:
  sampledFVsFiles=sampledFVsFiles.split(",")
  assert len(sampledFVsFiles)==9
outExamples=[neutExamples,hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
for i in range(len(sampledFVsFiles)):
  if outExamples[i]:
    outFile=open('/'.join((sampledFVsDir+'/'+sampledFVsFiles[i]).split('//')),"w")
    outFile.write("classLabel\t%s\n" %(hardHeader))
    for example in outExamples[i]:
      outFile.write("%s\t%s\n" %(sampledFVsFiles[i].replace(".fvec",""),example))
    outFile.close()

```

```{shell, test Dell 2}

# rm *stats
# mkdir FVs
# mv *fvec FVs

# expansionNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs

for file in *_SINGLE_COMBINED.fvec; do
  mv "$file" "${file/_SINGLE_COMBINED.fvec/.fvec}"
done

for file in spHardPartial_*.fvec; do
  mv "$file" "${file/spHardPartial_/spPartialHard_}"
done

for file in spSoftPartial_*.fvec; do
  mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
done

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

```

### Training

```{shell, test Dell 2}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

# expansionNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
mkdir -p FVs && cd ./FVs

# for i in 1 2 3 4 5 6 7 8 9 10; do
  time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    expansionNe.hdf5\
    expansionNe.json\
    expansionNe.npy > training_deep_learning_python3.log 2>&1
# done

tensorboard --logdir logs

# time on 40 cores ~12 minutes with 5GB RAM

```

#### Training with early stopping

```{shell, test Dell 2}

# module load anaconda3
# source ~/conda-init
# conda activate partialshic_py3
# 
# # expansionNe
# 
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
# mkdir -p FVs && cd ./FVs
# mkdir -p earlyStopping && cd ./earlyStopping
# rm ./*
# cp ../*.fvec
# 
# time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_test.hdf5\
#     expansionNe_earlyStopping_test.json\
#     expansionNe_earlyStopping_test.npy
# 
# for i in 1 2 3 4 5 6 7 8 9 10; do
#   time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_${i}.hdf5\
#     expansionNe_earlyStopping_${i}.json\
#     expansionNe_earlyStopping_${i}.npy > training_deep_learning_python3_earlyStopping_${i}.log 2>&1
# done
# 
# # time on 40 cores ~12 minutes with 5GB RAM

```

#### Training with early stopping and 10-fold CV

```{shell, test Dell 2}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

# expansionNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
rm ./*
cp ../*.fvec ./

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping_kFold.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    expansionNe_earlyStopping_fold.hdf5\
    expansionNe_earlyStopping_fold.json\
    expansionNe_earlyStopping_fold.npy

tensorboard --logdir logs

# for i in 1 2 3 4 5 6 7 8 9 10; do
#   time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_${i}.hdf5\
#     expansionNe_earlyStopping_${i}.json\
#     expansionNe_earlyStopping_${i}.npy > training_deep_learning_python3_earlyStopping_${i}.log 2>&1
# done

# time on 40 cores ~12 minutes with 5GB RAM

```

### Run testing

#### test on self

```{shell}

# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
# mkdir -p test_self && cd test_self
# rm ./*
# cp ../FVs/*.fvec ./
# # cp ../../expansionNe/FVs/*.fvec ./
# rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
# 
# for file in spHardPartial_*.fvec; do
#   mv "$file" "${file/spHardPartial_/spPartialHard_}"
# done
# 
# for file in spSoftPartial_*.fvec; do
#   mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
# done
# 
# python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
#  ../FVs/expansionNe.npy\
#  ./\
#  11\
#  92\
#  ./\
#  expansionNe_accuracy\
#  expansionNe_confusion_matrix.pdf
# 
# evince expansionNe_confusion_matrix.pdf

```

#### test on self-new data

```{shell}

# fold_${i}_constNe_earlyStopping_fold.npy

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp ../../*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  for file in *_SINGLE_COMBINED.fvec; do
    mv "$file" "${file/_SINGLE_COMBINED.fvec/.fvec}"
  done
  for file in spHardPartial_*.fvec; do
    mv "$file" "${file/spHardPartial_/spPartialHard_}"
  done
  for file in spSoftPartial_*.fvec; do
    mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
  done
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

```{bash}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/FVs/train_1/expansionNe_confusion_matrix_1.pdf ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/FVs/train_2/expansionNe_confusion_matrix_2.pdf ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/FVs/train_3/expansionNe_confusion_matrix_3.pdf ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/FVs/train_4/expansionNe_confusion_matrix_4.pdf ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/FVs/train_5/expansionNe_confusion_matrix_5.pdf ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/FVs/train_6/expansionNe_confusion_matrix_6.pdf ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/FVs/train_7/expansionNe_confusion_matrix_7.pdf ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/FVs/train_8/expansionNe_confusion_matrix_8.pdf ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/FVs/train_9/expansionNe_confusion_matrix_9.pdf ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/FVs/train_10/expansionNe_confusion_matrix_10.pdf ./

```

#### test on constNe_intergenic

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/
  mkdir -p test_constNe_intergenic && cd test_constNe_intergenic
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on constNe

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/
  mkdir -p test_constNe && cd test_constNe
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic_lci

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/
  mkdir -p test_expansionNe_intergenic_lci && cd test_expansionNe_intergenic_lci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic_uci

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/
  mkdir -p test_expansionNe_intergenic_uci && cd test_expansionNe_intergenic_uci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on empirical

```{python}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell, OLD}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

mkdir -p pred_expansionNe_intergenic && cd pred_expansionNe_intergenic

# ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_542_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_542_${i}.log 2>&1
done

# made predictions for 2475 total instances
# predicted 560 neutral regions (0.226263 of all classified regions)
# predicted 81 Hard sweep regions (0.032727 of all classified regions)
# predicted 292 Hard-linked sweep regions (0.117980 of all classified regions)
# predicted 136 Soft sweep regions (0.054949 of all classified regions)
# predicted 1190 Soft-linked sweep regions (0.480808 of all classified regions)
# predicted 8 HardPartial sweep regions (0.003232 of all classified regions)
# predicted 24 HardPartial-linked sweep regions (0.009697 of all classified regions)
# predicted 33 SoftPartial sweep regions (0.013333 of all classified regions)
# predicted 151 SoftPartial-linked sweep regions (0.061010 of all classified regions)

# made predictions for 2475 total instances
# predicted 821 neutral regions (0.331717 of all classified regions)
# predicted 43 Hard sweep regions (0.017374 of all classified regions)
# predicted 403 Hard-linked sweep regions (0.162828 of all classified regions)
# predicted 99 Soft sweep regions (0.040000 of all classified regions)
# predicted 866 Soft-linked sweep regions (0.349899 of all classified regions)
# predicted 9 HardPartial sweep regions (0.003636 of all classified regions)
# predicted 52 HardPartial-linked sweep regions (0.021010 of all classified regions)
# predicted 5 SoftPartial sweep regions (0.002020 of all classified regions)
# predicted 177 SoftPartial-linked sweep regions (0.071515 of all classified regions)

# ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_594_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_594_${i}.log 2>&1
done

# made predictions for 3117 total instances
# predicted 1121 neutral regions (0.359641 of all classified regions)
# predicted 31 Hard sweep regions (0.009945 of all classified regions)
# predicted 112 Hard-linked sweep regions (0.035932 of all classified regions)
# predicted 260 Soft sweep regions (0.083414 of all classified regions)
# predicted 1375 Soft-linked sweep regions (0.441129 of all classified regions)
# predicted 15 HardPartial sweep regions (0.004812 of all classified regions)
# predicted 29 HardPartial-linked sweep regions (0.009304 of all classified regions)
# predicted 33 SoftPartial sweep regions (0.010587 of all classified regions)
# predicted 141 SoftPartial-linked sweep regions (0.045236 of all classified regions)

made predictions for 3117 total instances
predicted 1474 neutral regions (0.472891 of all classified regions)
predicted 19 Hard sweep regions (0.006096 of all classified regions)
predicted 148 Hard-linked sweep regions (0.047482 of all classified regions)
predicted 169 Soft sweep regions (0.054219 of all classified regions)
predicted 1121 Soft-linked sweep regions (0.359641 of all classified regions)
predicted 13 HardPartial sweep regions (0.004171 of all classified regions)
predicted 47 HardPartial-linked sweep regions (0.015079 of all classified regions)
predicted 3 SoftPartial sweep regions (0.000962 of all classified regions)
predicted 123 SoftPartial-linked sweep regions (0.039461 of all classified regions)

# ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_628_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_628_${i}.log 2>&1
done

# made predictions for 4618 total instances
# predicted 1276 neutral regions (0.276310 of all classified regions)
# predicted 53 Hard sweep regions (0.011477 of all classified regions)
# predicted 277 Hard-linked sweep regions (0.059983 of all classified regions)
# predicted 328 Soft sweep regions (0.071026 of all classified regions)
# predicted 2287 Soft-linked sweep regions (0.495236 of all classified regions)
# predicted 10 HardPartial sweep regions (0.002165 of all classified regions)
# predicted 36 HardPartial-linked sweep regions (0.007796 of all classified regions)
# predicted 72 SoftPartial sweep regions (0.015591 of all classified regions)
# predicted 279 SoftPartial-linked sweep regions (0.060416 of all classified regions)

made predictions for 4618 total instances
predicted 1926 neutral regions (0.417064 of all classified regions)
predicted 44 Hard sweep regions (0.009528 of all classified regions)
predicted 388 Hard-linked sweep regions (0.084019 of all classified regions)
predicted 237 Soft sweep regions (0.051321 of all classified regions)
predicted 1704 Soft-linked sweep regions (0.368991 of all classified regions)
predicted 21 HardPartial sweep regions (0.004547 of all classified regions)
predicted 56 HardPartial-linked sweep regions (0.012126 of all classified regions)
predicted 10 SoftPartial sweep regions (0.002165 of all classified regions)
predicted 232 SoftPartial-linked sweep regions (0.050238 of all classified regions)

# ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_718_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_718_${i}.log 2>&1
done

# made predictions for 1375 total instances
# predicted 220 neutral regions (0.160000 of all classified regions)
# predicted 29 Hard sweep regions (0.021091 of all classified regions)
# predicted 97 Hard-linked sweep regions (0.070545 of all classified regions)
# predicted 116 Soft sweep regions (0.084364 of all classified regions)
# predicted 704 Soft-linked sweep regions (0.512000 of all classified regions)
# predicted 1 HardPartial sweep regions (0.000727 of all classified regions)
# predicted 24 HardPartial-linked sweep regions (0.017455 of all classified regions)
# predicted 29 SoftPartial sweep regions (0.021091 of all classified regions)
# predicted 155 SoftPartial-linked sweep regions (0.112727 of all classified regions)

made predictions for 1375 total instances
predicted 288 neutral regions (0.209455 of all classified regions)
predicted 20 Hard sweep regions (0.014545 of all classified regions)
predicted 173 Hard-linked sweep regions (0.125818 of all classified regions)
predicted 88 Soft sweep regions (0.064000 of all classified regions)
predicted 573 Soft-linked sweep regions (0.416727 of all classified regions)
predicted 12 HardPartial sweep regions (0.008727 of all classified regions)
predicted 54 HardPartial-linked sweep regions (0.039273 of all classified regions)
predicted 8 SoftPartial sweep regions (0.005818 of all classified regions)
predicted 159 SoftPartial-linked sweep regions (0.115636 of all classified regions)

# ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_76_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_76_${i}.log 2>&1
done

# made predictions for 6080 total instances
# predicted 1648 neutral regions (0.271053 of all classified regions)
# predicted 57 Hard sweep regions (0.009375 of all classified regions)
# predicted 219 Hard-linked sweep regions (0.036020 of all classified regions)
# predicted 474 Soft sweep regions (0.077961 of all classified regions)
# predicted 3178 Soft-linked sweep regions (0.522697 of all classified regions)
# predicted 3 HardPartial sweep regions (0.000493 of all classified regions)
# predicted 49 HardPartial-linked sweep regions (0.008059 of all classified regions)
# predicted 88 SoftPartial sweep regions (0.014474 of all classified regions)
# predicted 364 SoftPartial-linked sweep regions (0.059868 of all classified regions)

made predictions for 6080 total instances
predicted 2419 neutral regions (0.397862 of all classified regions)
predicted 57 Hard sweep regions (0.009375 of all classified regions)
predicted 368 Hard-linked sweep regions (0.060526 of all classified regions)
predicted 332 Soft sweep regions (0.054605 of all classified regions)
predicted 2420 Soft-linked sweep regions (0.398026 of all classified regions)
predicted 19 HardPartial sweep regions (0.003125 of all classified regions)
predicted 78 HardPartial-linked sweep regions (0.012829 of all classified regions)
predicted 14 SoftPartial sweep regions (0.002303 of all classified regions)
predicted 373 SoftPartial-linked sweep regions (0.061349 of all classified regions)

# ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_785_${i}.log 2>&1
done

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe2.npy\
 ../ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred2.pred.bed

# made predictions for 3836 total instances
# predicted 1085 neutral regions (0.282847 of all classified regions)
# predicted 33 Hard sweep regions (0.008603 of all classified regions)
# predicted 145 Hard-linked sweep regions (0.037800 of all classified regions)
# predicted 315 Soft sweep regions (0.082117 of all classified regions)
# predicted 1903 Soft-linked sweep regions (0.496090 of all classified regions)
# predicted 9 HardPartial sweep regions (0.002346 of all classified regions)
# predicted 33 HardPartial-linked sweep regions (0.008603 of all classified regions)
# predicted 61 SoftPartial sweep regions (0.015902 of all classified regions)
# predicted 252 SoftPartial-linked sweep regions (0.065693 of all classified regions)

made predictions for 3836 total instances
predicted 1521 neutral regions (0.396507 of all classified regions)
predicted 19 Hard sweep regions (0.004953 of all classified regions)
predicted 229 Hard-linked sweep regions (0.059698 of all classified regions)
predicted 206 Soft sweep regions (0.053702 of all classified regions)
predicted 1523 Soft-linked sweep regions (0.397028 of all classified regions)
predicted 14 HardPartial sweep regions (0.003650 of all classified regions)
predicted 73 HardPartial-linked sweep regions (0.019030 of all classified regions)
predicted 11 SoftPartial sweep regions (0.002868 of all classified regions)
predicted 240 SoftPartial-linked sweep regions (0.062565 of all classified regions)

```

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/
mkdir -p pred_empirical && cd pred_empirical
rsync -ahPv /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/*_110Lines.phased.noPIRs.inbred.fvec ./

for CONTIG in 542 594 628 718 76 785; do
  for i in 1 2 3 4 5 6 7 8 9 10; do
    python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
      /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
      ./ScA8VGg_${CONTIG}_110Lines.phased.noPIRs.inbred.fvec\
      11\
      92\
      ScA8VGg_${CONTIG}.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_fold_${i}.pred.bed >\
      empirical_deep_learning_classify_ScA8VGg_${CONTIG}_fold_${i}.log 2>&1
  done
done

```

#### summary prediction

```{bash}

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical

```

```{r}

library(tidyverse)
library(data.table)
library(RColorBrewer)
# library(ggplot2)
# library(dplyr)
# library(tidyr)
library(cowplot)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

# ensemble <- prob_pred %>% 
#   group_by(contig, start, end) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob)

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})



# a1 <- pred %>% 
#   extract('V4','State','(.*)_ScA8VGg.*') %>% 
#   mutate(V1=gsub('chr','', V1))


# a1_df <- data.table(table(a1$State))
a1_df <- data.table(table(ensemble$pred_ensemble))
# hard_num <- pred_df[V1=="hard",N]
# soft_num <- pred_df[V1=="soft",N]
# hard_pct <- hard_num/(hard_num + soft_num)
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

aplot[V1 %in% c('HardPartial','HardPartial-linked'),lbl2:=lbl]


PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
    geom_bar(width=1, stat="identity", color="white") +
    coord_polar("y", start=0) +
    geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
                  y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), label=lbl2), 
              color="black", size=3) +
    scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                               my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                               "grey60")) +
    theme_void() + theme(legend.position = "none")



a1 <- data.table(ensemble)

a1[,chr:="xxxx"][contig=="chrScA8VGg_76",chr:="3R"][contig=="chrScA8VGg_718",chr:="2L"][contig=="chrScA8VGg_542",chr:="2L"][contig=="chrScA8VGg_594",chr:="X"][contig=="chrScA8VGg_628",chr:="3L"][contig=="chrScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$pred_ensemble)) -> a3
a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))

# ggsave("diploSHIC_proprotion_figure1.pdf",PP, width = 8, height = 4, 
#        dpi = 300, units = "in")


# chisquare test
library(rcompanion)

bc <- table(a2$chr, a2$pred_ensemble)

b2 <- cbind(rowSums(bc[,c(1,3)]), rowSums(bc[,c(2,4:9)]))
# 
# b1 <- cbind(bc,other = rowSums(bc[,2:9]))
# b2 <- b1[,c(1,10)]
chisq.test(b2)
b2 <- matrix(c(29, 43, 59, 46, 56, 3600, 3876, 4249, 5688, 4857), ncol=2)
colnames(b2) <- c('hard','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")
apply(b2,1,prop.table)

# c1 <- cbind(bc,other = rowSums(bc[,c(1:5,7:9)]))
# c2 <- c1[,c(6,10)
c2 <- cbind(rowSums(bc[,c(6,8)]), rowSums(bc[,c(1:5,7,9)]))
chisq.test(c2)

c2 <- matrix(c(381, 452, 503, 579, 615, 3248, 3467, 3805, 5155, 4298), ncol=2)
colnames(c2) <- c('soft','others')
rownames(c2) <- c('2L','2R','3L','3R','X')
pairwiseNominalIndependence(c2, fisher = F, gtest = F, chisq = T, method = "fdr")
apply(c2,1,prop.table)

ph <- cbind(bc[,3], rowSums(bc[,c(1:2,4:9)]))
apply(ph,1,prop.table)

```

```
                  V1     N         pct
1:               Hard   202 0.008976581 202
2:        Hard-linked  1319 0.058614407 1327
3:        HardPartial    31 0.001377594 22
4: HardPartial-linked    67 0.002977381
5:            Neutral  3192 0.141847754 3195
6:               Soft  2181 0.096920411 2183
7:        Soft-linked 14685 0.652579656 14694
8:        SoftPartial   349 0.015509043 360
9: SoftPartial-linked   477 0.021197174

```

#### prob pred

```{shell}

# module load anaconda3
# source ~/conda-init
# conda activate r_env
# 
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe

```

```{r}

# library(tidyverse)
# library(data.table)
# 
# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")
# 
# beds <- list.files(pattern = 'ScA8VGg_.*.csv$')
# 
# aa <- data.table()
# 
# for(ff in beds){
#     tmp <- fread(ff, header = TRUE, sep = ",")
#     aa <- rbind(aa, tmp)
# }
# 
# aa %>% 
#   mutate(window = c(1:nrow(aa))) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   filter(pred=="Neutral") %>% 
#   ggplot(aes(x=prob, fill=class)) + 
#   geom_density(alpha=0.5) + 
#   facet_wrap(~class, scale="free_y")
# 
# aa %>% 
#   mutate(window = c(1:nrow(aa))) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   filter(pred=="Soft-linked") %>% 
#   ggplot(aes(x=prob, fill=class)) + 
#   geom_density(alpha=0.5) + 
#   facet_wrap(~class, scale="free_y")
# 
# # Create a new column "prob" with the highest probability
# aa$prob <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   sorted_probs[1]
# })
# 
# # Create a new column "pred2" with the second highest probability class
# aa$pred2 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   names(sorted_probs)[2]
# })
# 
# # Create a new column "prob2" with the second highest probability
# aa$prob2 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   sorted_probs[2]
# })
# 
# # Create a new column "pred3" with the third highest probability class
# aa$pred3 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   names(sorted_probs)[3]
# })
# 
# # Create a new column "prob3" with the third highest probability
# aa$prob3 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   sorted_probs[3]
# })

```


# ######################## PartialSHIC - constNe intergenic

```
lapply(str_split(names(aa),'_'), function(s) s[1]) %>% unique() %>% unlist
[1] "pi"                 "thetaW"             "tajD"               "thetaH"
[5] "fayWuH"             "HapCount"           "H1"                 "H12"
[9] "H2/H1"              "ZnS"                "Omega"              "iHSMean"
[13] "iHSMax"             "iHSOutFrac"         "nSLMean"            "nSLMax"
[17] "nSLOutFrac"         "distVar"            "distSkew"           "distKurt"
[21] "HAF-Mean"           "HAF-Median"         "HAF-Mode"           "HAF-Lower95%"
[25] "HAF-Lower50%"       "HAF-Upper50%"       "HAF-Upper95%"       "HAF-Max"
[29] "HAF-Var"            "HAF-SD"             "HAF-Skew"           "HAF-Kurt"
[33] "HAFunique-Mean"     "HAFunique-Median"   "HAFunique-Mode"     "HAFunique-Lower95%"
[37] "HAFunique-Lower50%" "HAFunique-Upper50%" "HAFunique-Upper95%" "HAFunique-Max"
[41] "HAFunique-Var"      "HAFunique-SD"       "HAFunique-Skew"     "HAFunique-Kurt"
[45] "phi-Mean"           "phi-Median"         "phi-Mode"           "phi-Lower95%"
[49] "phi-Lower50%"       "phi-Upper50%"       "phi-Upper95%"       "phi-Max"
[53] "phi-Var"            "phi-SD"             "phi-Skew"           "phi-Kurt"
[57] "kappa-Mean"         "kappa-Median"       "kappa-Mode"         "kappa-Lower95%"
[61] "kappa-Lower50%"     "kappa-Upper50%"     "kappa-Upper95%"     "kappa-Max"
[65] "kappa-Var"          "kappa-SD"           "kappa-Skew"         "kappa-Kurt"
[69] "SFS-Mean"           "SFS-Median"         "SFS-Mode"           "SFS-Lower95%"
[73] "SFS-Lower50%"       "SFS-Upper50%"       "SFS-Upper95%"       "SFS-Max"
[77] "SFS-Var"            "SFS-SD"             "SFS-Skew"           "SFS-Kurt"
[81] "SAFE-Mean"          "SAFE-Median"        "SAFE-Mode"          "SAFE-Lower95%"
[85] "SAFE-Lower50%"      "SAFE-Upper50%"      "SAFE-Upper95%"      "SAFE-Max"
[89] "SAFE-Var"           "SAFE-SD"            "SAFE-Skew"          "SAFE-Kurt"
```

```
From Yiguan:

Source code change log:
    
    - training_sample_FVs.py:line67, fixed to 2000

    - fix bugs in empirical_convert_to_FVs.py:24-28 in parsing VCF file format. ancestral fasta file must split by chromosome
```

## Generate training data

```{bash, OLD}
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP
conda activate r_env
```

```{r, DISCOAL, OLD}
setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/discoal/constNe')
options(scipen = 999)
library("parallel")
library("stringr")

# DISCOAL = '~/discoal/discoal' 
DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal'
TIMES = 2000

Ne = 1*10^6
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3

# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len

# selection coefficient s: 0.0001-0.01
# strength of selection: alpha = 2*Ne*s
s_low = 0.00001
s_high = 0.01
Pa_low <- 2*Ne*s_low
Pa_high <- 2*Ne*s_high

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
Pu_high <- 0.01

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2

# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

# For NEUTRAL
# sample size = 220
# numReplicates = TIMES = 10
# nSites = len = 55000
# -Pt = low high (prior on theta)
# -Pre mean upperBound (prior on rho -- exponentially distributed but truncated at an upper bound)
# runNeutral <- str_glue("{DISCOAL} 220 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} | gzip > spNeut.msOut.gz &")
# system.time(runNeutral)
runNeutral <- function(i){
    outname <- str_glue("spNeut_{i-1}.msOut.gz")
    sm_cmd <- str_glue("{DISCOAL} 220 {TIMES/16} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} | gzip > {outname}")
    cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:16, runNeutral, mc.cores = 16)

# For complete HARD
# sample size = 110
# numReplicates = TIMES = 10
# nSites = len = 55000
# -Pt = low high (prior on theta)
# -Pre mean upperBound (prior on rho -- exponentially distributed but truncated at an upper bound)
# -ws tau (sweep happend tau generations ago- stochastic sweep)
# -Pa low high (prior on alpha)
# -Pu low high (prior on tau; sweep models only; still must use "-ws tau" and "tau" will be ignored)
# -x sweepSite (0-1)
runHard <- function(i){
    outname <- str_glue("spHard_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -x {sweep_pos[i]} | gzip > {outname}")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -x {sweep_pos[i]} | gzip > {outname}")
    }
    
    cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runHard, mc.cores = 11)


# For complete SOFT
# sample size = 110
# numReplicates = TIMES = 10
# nSites = len = 55000
# -Pt = low high (prior on theta)
# -Pre mean upperBound (prior on rho -- exponentially distributed but truncated at an upper bound)
# -ws tau (sweep happend tau generations ago- stochastic sweep)
# -Pa low high (prior on alpha)
# -Pu low high (prior on tau; sweep models only; still must use "-ws tau" and "tau" will be ignored)
# -Pf low high (prior on F0; sweep models only)
# -x sweepSite (0-1)
runSoft <- function(i){
    outname <- str_glue("spSoft_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0  -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname}")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0  -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname}")
    }
    # cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runSoft, mc.cores = 11)


# for partial HARD
runPHard <- function(i){
    outname <- str_glue("spPartialHard_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -x {sweep_pos[i]} | gzip > {outname}")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -x {sweep_pos[i]} | gzip > {outname}")
    }
    
    # cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runPHard, mc.cores = 11)


# for partial SOFT
runPSoft <- function(i){
    outname <- str_glue("spPartialSoft_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname}")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname}")
    }
    
    #cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runPSoft, mc.cores = 11)

```

* Bunya

```{shell, install discoal}

cd /scratch/project/chenoase/partialSHIC_DsGRP

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=1\
 --mem=2G\
 --job-name=uqsalle3_TEST\
 --time=00:30:00\
 --partition=general\
 --account=a_chenoweth srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

git clone https://github.com/kr-colab/discoal.git

cd /scratch/project/chenoase/partialSHIC_DsGRP/discoal

make discoal

```

* Dell 2 port from R to bash

```{shell}

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

# setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat')

DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal' 
TIMES = 2000

Ne = 3607692
# Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000002771855
# s_high = 0.01
s_high = 0.002771855
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.002771855 # 0.002771855*4*3607692 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
# bottleneck <- "-en 0.007292 0 1.5 -en 0.02135 0 0.547 -en 0.05208 0 0.208 -en 0.09271 0 0.0805"

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
# bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
cat("bottleneck: ", bottleneck, "\n")
cat("sweep_pos: ", sweep_pos, "\n")

```

* Back to Bunya

```{shell, bunya example}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name=Neut_sim
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/slurm_discoal_Neutral.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/slurm_discoal_Neutral.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/

DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper | gzip > spNeut_${TIME}.msOut.gz
done

$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper | gzip > spNeut_1.msOut.gz

```

```{shell, linked}

# sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_0.slurm.sh
# sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_1.slurm.sh
# sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_2.slurm.sh
# sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_3.slurm.sh
# sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_4.slurm.sh
# sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_6.slurm.sh
# sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_7.slurm.sh
# sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_8.slurm.sh
# sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_9.slurm.sh
# sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_10.slurm.sh
# 
# sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_0.slurm.sh
# sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_1.slurm.sh
# sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_2.slurm.sh
# sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_3.slurm.sh
# sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_4.slurm.sh
# sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_6.slurm.sh
# sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_7.slurm.sh
# sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_8.slurm.sh
# sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_9.slurm.sh
# sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_10.slurm.sh
# 
# sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_0.slurm.sh
# sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_1.slurm.sh
# sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_2.slurm.sh
# sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_3.slurm.sh
# sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_4.slurm.sh
# sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_6.slurm.sh
# sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_7.slurm.sh
# sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_8.slurm.sh
# sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_9.slurm.sh
# sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_10.slurm.sh
# 
# sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_0.slurm.sh
# sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_1.slurm.sh
# sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_2.slurm.sh
# sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_3.slurm.sh
# sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_4.slurm.sh
# sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_6.slurm.sh
# sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_7.slurm.sh
# sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_8.slurm.sh
# sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_9.slurm.sh
# sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_10.slurm.sh



sed 's/\[5\]/\[0\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_0.txt
sed 's/\[5\]/\[1\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_1.txt
sed 's/\[5\]/\[2\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_2.txt
sed 's/\[5\]/\[3\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_3.txt
sed 's/\[5\]/\[4\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_4.txt
sed 's/\[5\]/\[6\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_6.txt
sed 's/\[5\]/\[7\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_7.txt
sed 's/\[5\]/\[8\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_8.txt
sed 's/\[5\]/\[9\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_9.txt
sed 's/\[5\]/\[10\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_10.txt

sed 's/\[5\]/\[0\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_0.txt
sed 's/\[5\]/\[1\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_1.txt
sed 's/\[5\]/\[2\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_2.txt
sed 's/\[5\]/\[3\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_3.txt
sed 's/\[5\]/\[4\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_4.txt
sed 's/\[5\]/\[6\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_6.txt
sed 's/\[5\]/\[7\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_7.txt
sed 's/\[5\]/\[8\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_8.txt
sed 's/\[5\]/\[9\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_9.txt
sed 's/\[5\]/\[10\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_10.txt

sed 's/\[5\]/\[0\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_0.txt
sed 's/\[5\]/\[1\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_1.txt
sed 's/\[5\]/\[2\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_2.txt
sed 's/\[5\]/\[3\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_3.txt
sed 's/\[5\]/\[4\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_4.txt
sed 's/\[5\]/\[6\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_6.txt
sed 's/\[5\]/\[7\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_7.txt
sed 's/\[5\]/\[8\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_8.txt
sed 's/\[5\]/\[9\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_9.txt
sed 's/\[5\]/\[10\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_10.txt

sed 's/\[5\]/\[0\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_0.txt
sed 's/\[5\]/\[1\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_1.txt
sed 's/\[5\]/\[2\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_2.txt
sed 's/\[5\]/\[3\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_3.txt
sed 's/\[5\]/\[4\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_4.txt
sed 's/\[5\]/\[6\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_6.txt
sed 's/\[5\]/\[7\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_7.txt
sed 's/\[5\]/\[8\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_8.txt
sed 's/\[5\]/\[9\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_9.txt
sed 's/\[5\]/\[10\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_10.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/

## Neutral

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Neutral.txt discoal_Neutral_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Neutral_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Hard_5.txt discoal_Hard_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Hard_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Soft_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Soft_5.txt discoal_Soft_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Soft_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## HardPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_HardPartial_5.txt discoal_HardPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_HardPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## SoftPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_SoftPartial_5.txt discoal_SoftPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_SoftPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"

  for i in {1..400}; do
    echo "$i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## Soft-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## HardPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## SoftPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

```

```{shell, submit}

## Neutral

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Neutral.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral.txt

## Hard_5

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Hard_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5.txt

## Soft_5

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Soft_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5.txt

## HardPartial_5

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_HardPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5.txt

## SoftPartial_5

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_SoftPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5.txt

## Hard-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## Soft-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## HardPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## SoftPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done



ls ./spNeut_*.msOut.gz | wc -l
ls ./spHard_5_*.msOut.gz | wc -l
ls ./spSoft_5_*.msOut.gz | wc -l
ls ./spHardPartial_5_*.msOut.gz | wc -l
ls ./spSoftPartial_5_*.msOut.gz | wc -l



ls ./spHard_0_*.msOut.gz | wc -l
ls ./spHard_1_*.msOut.gz | wc -l
ls ./spHard_2_*.msOut.gz | wc -l
ls ./spHard_3_*.msOut.gz | wc -l
ls ./spHard_4_*.msOut.gz | wc -l
ls ./spHard_6_*.msOut.gz | wc -l
ls ./spHard_7_*.msOut.gz | wc -l
ls ./spHard_8_*.msOut.gz | wc -l
ls ./spHard_9_*.msOut.gz | wc -l
ls ./spHard_10_*.msOut.gz | wc -l



ls ./spSoft_0_*.msOut.gz | wc -l
ls ./spSoft_1_*.msOut.gz | wc -l
ls ./spSoft_2_*.msOut.gz | wc -l
ls ./spSoft_3_*.msOut.gz | wc -l
ls ./spSoft_4_*.msOut.gz | wc -l
ls ./spSoft_6_*.msOut.gz | wc -l
ls ./spSoft_7_*.msOut.gz | wc -l
ls ./spSoft_8_*.msOut.gz | wc -l
ls ./spSoft_9_*.msOut.gz | wc -l
ls ./spSoft_10_*.msOut.gz | wc -l



ls ./spHardPartial_0_*.msOut.gz | wc -l
ls ./spHardPartial_1_*.msOut.gz | wc -l
ls ./spHardPartial_2_*.msOut.gz | wc -l
ls ./spHardPartial_3_*.msOut.gz | wc -l
ls ./spHardPartial_4_*.msOut.gz | wc -l
ls ./spHardPartial_6_*.msOut.gz | wc -l
ls ./spHardPartial_7_*.msOut.gz | wc -l
ls ./spHardPartial_8_*.msOut.gz | wc -l
ls ./spHardPartial_9_*.msOut.gz | wc -l
ls ./spHardPartial_10_*.msOut.gz | wc -l



ls ./spSoftPartial_0_*.msOut.gz | wc -l
ls ./spSoftPartial_1_*.msOut.gz | wc -l
ls ./spSoftPartial_2_*.msOut.gz | wc -l
ls ./spSoftPartial_3_*.msOut.gz | wc -l
ls ./spSoftPartial_4_*.msOut.gz | wc -l
ls ./spSoftPartial_6_*.msOut.gz | wc -l
ls ./spSoftPartial_7_*.msOut.gz | wc -l
ls ./spSoftPartial_8_*.msOut.gz | wc -l
ls ./spSoftPartial_9_*.msOut.gz | wc -l
ls ./spSoftPartial_10_*.msOut.gz | wc -l

```

```{shell, check and fix empty simulation}

## Neutral

grep 'Out Of Memory' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt
grep ' MAXMUTS' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt
cat list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt > list_discoal_files_Neutral_RERUN_10.txt
wc -l list_discoal_files_Neutral_RERUN_10.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral_RERUN_10.txt

## Hard_5

grep 'Out Of Memory' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Hard_5_RERUN_OutOfMemory.txt list_discoal_files_Hard_5_RERUN_MAXMUTS.txt > list_discoal_files_Hard_5_RERUN.txt
wc -l list_discoal_files_Hard_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5_RERUN.txt

## Soft_5

rm list_discoal_files_Soft_5_RERUN_9.txt
grep 'Out Of Memory' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_OutOfMemory_9.txt
grep ' MAXMUTS' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_MAXMUTS_9.txt
cat list_discoal_files_Soft_5_RERUN_OutOfMemory_9.txt list_discoal_files_Soft_5_RERUN_MAXMUTS_9.txt > list_discoal_files_Soft_5_RERUN_9.txt
wc -l list_discoal_files_Soft_5_RERUN_9.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5_RERUN_9.txt

## HardPartial_5

rm list_discoal_files_HardPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_HardPartial_5_RERUN.txt
wc -l list_discoal_files_HardPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5_RERUN.txt

## SoftPartial_5

rm list_discoal_files_SoftPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_SoftPartial_5_RERUN.txt
wc -l list_discoal_files_SoftPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5_RERUN.txt

## check all

rm list_discoal_files_RERUN.txt
grep 'Out Of Memory' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' >  list_discoal_files_RERUN_MAXMUTS.txt
cat list_discoal_files_RERUN_OutOfMemory.txt list_discoal_files_RERUN_MAXMUTS.txt > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt



find ./ -maxdepth 1 -type f -name '*.msOut.gz' -size -21c



```

```{shell, combine simulations}

# Function to process and combine sweep files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail ./${PREFIX}_*.msOut -n +3 > temp1.msOut
    perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
    awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/2000/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

# for PREFIX in spNeut spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
#     process_and_combine "$PREFIX"
# done

for PREFIX in spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
    process_and_combine "$PREFIX"
done



# Function to process and combine linked files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail -n +3 ./${PREFIX}_*.msOut > temp1.msOut
    grep -v "$PREFIX" temp1.msOut > temp2.msOut
    awk 'BEGIN { RS="\n\n"; ORS="\n" } { print }' temp2.msOut > temp3.msOut
    
    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/400/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

# DONE
for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    process_and_combine "$PREFIX"
done

# DONE
for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    process_and_combine "$PREFIX"
done

# DONE
for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    process_and_combine "$PREFIX"
done

# RUNNING
for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    process_and_combine "$PREFIX"
done

# # mini for testing
# 
# gzip -d spHard_5_1.msOut.gz
# gzip -d spHard_5_2.msOut.gz
# gzip -d spHard_5_3.msOut.gz
# 
# # Concatenate files and remove headers
# tail spHard_5_*.msOut -n +3 > temp1.msOut
# perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
# awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut
# 
# # Extract the first two lines from ${PREFIX}_1.msOut
# head -n 2 "spHard_5_1.msOut" > header.txt
# 
# # Edit the header to change the third space-delimited element from 1 to 2000
# sed -i '1s/\<1\>/3/' header.txt
# 
# # Concatenate the edited header with temp3.msOut
# cat header.txt temp3.msOut > spHard_5_COMBINED_MINI.msOut
# 
# # Move and compress the combined file
# gzip spHard_5_COMBINED_MINI.msOut
# 
# # Compress files
# find ./ -maxdepth 1 -type f -name "spHard_5_*.msOut" -exec gzip {} \;
# 
# # Clean up temporary files
# rm temp1.msOut temp2.msOut

```

## Generate test data

* Bunya

```{shell, install discoal}

cd /scratch/project/chenoase/partialSHIC_DsGRP

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=1\
 --mem=2G\
 --job-name=uqsalle3_TEST\
 --time=00:30:00\
 --partition=general\
 --account=a_chenoweth srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

git clone https://github.com/kr-colab/discoal.git

cd /scratch/project/chenoase/partialSHIC_DsGRP/discoal

make discoal

```

* Dell 2 port from R to bash

```{shell}

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

# setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat')

DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal' 
TIMES = 2000

Ne = 3607692
# Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000002771855
# s_high = 0.01
s_high = 0.002771855
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.002771855 # 0.002771855*4*3607692 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
# bottleneck <- "-en 0.007292 0 1.5 -en 0.02135 0 0.547 -en 0.05208 0 0.208 -en 0.09271 0 0.0805"

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
# bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
cat("bottleneck: ", bottleneck, "\n")
cat("sweep_pos: ", sweep_pos, "\n")

```

* Back to Bunya

```{shell, bunya example}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name=Neut_sim
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat/slurm_discoal_Neutral.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat/slurm_discoal_Neutral.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat

DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper | gzip > spNeut_${TIME}.msOut.gz
done

$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper | gzip > spNeut_1.msOut.gz

```

```{shell, linked}

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_10.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat

## Neutral (DONE)

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Neutral.txt discoal_Neutral_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Neutral_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard_5 (DONE)

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Hard_5.txt discoal_Hard_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Hard_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Soft_5 (DONE)

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Soft_5.txt discoal_Soft_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Soft_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## HardPartial_5 (DONE)

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_HardPartial_5.txt discoal_HardPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_HardPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## SoftPartial_5 (DONE)

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_SoftPartial_5.txt discoal_SoftPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_SoftPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## Soft-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## HardPartial-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## SoftPartial-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

```

```{shell, submit}

## Neutral (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Neutral.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral.txt

## Hard_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Hard_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5.txt

## Soft_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Soft_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5.txt

## HardPartial_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_HardPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5.txt

## SoftPartial_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_SoftPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5.txt

## Hard-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## Soft-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## HardPartial-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## SoftPartial-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done



ls ./spNeut_*.msOut.gz | wc -l
ls ./spHard_5_*.msOut.gz | wc -l
ls ./spSoft_5_*.msOut.gz | wc -l
ls ./spHardPartial_5_*.msOut.gz | wc -l
ls ./spSoftPartial_5_*.msOut.gz | wc -l



ls ./spHard_0_*.msOut.gz | wc -l
ls ./spHard_1_*.msOut.gz | wc -l
ls ./spHard_2_*.msOut.gz | wc -l
ls ./spHard_3_*.msOut.gz | wc -l
ls ./spHard_4_*.msOut.gz | wc -l
ls ./spHard_6_*.msOut.gz | wc -l
ls ./spHard_7_*.msOut.gz | wc -l
ls ./spHard_8_*.msOut.gz | wc -l
ls ./spHard_9_*.msOut.gz | wc -l
ls ./spHard_10_*.msOut.gz | wc -l



ls ./spSoft_0_*.msOut.gz | wc -l
ls ./spSoft_1_*.msOut.gz | wc -l
ls ./spSoft_2_*.msOut.gz | wc -l
ls ./spSoft_3_*.msOut.gz | wc -l
ls ./spSoft_4_*.msOut.gz | wc -l
ls ./spSoft_6_*.msOut.gz | wc -l
ls ./spSoft_7_*.msOut.gz | wc -l
ls ./spSoft_8_*.msOut.gz | wc -l
ls ./spSoft_9_*.msOut.gz | wc -l
ls ./spSoft_10_*.msOut.gz | wc -l



ls ./spHardPartial_0_*.msOut.gz | wc -l
ls ./spHardPartial_1_*.msOut.gz | wc -l
ls ./spHardPartial_2_*.msOut.gz | wc -l
ls ./spHardPartial_3_*.msOut.gz | wc -l
ls ./spHardPartial_4_*.msOut.gz | wc -l
ls ./spHardPartial_6_*.msOut.gz | wc -l
ls ./spHardPartial_7_*.msOut.gz | wc -l
ls ./spHardPartial_8_*.msOut.gz | wc -l
ls ./spHardPartial_9_*.msOut.gz | wc -l
ls ./spHardPartial_10_*.msOut.gz | wc -l



ls ./spSoftPartial_0_*.msOut.gz | wc -l
ls ./spSoftPartial_1_*.msOut.gz | wc -l
ls ./spSoftPartial_2_*.msOut.gz | wc -l
ls ./spSoftPartial_3_*.msOut.gz | wc -l
ls ./spSoftPartial_4_*.msOut.gz | wc -l
ls ./spSoftPartial_6_*.msOut.gz | wc -l
ls ./spSoftPartial_7_*.msOut.gz | wc -l
ls ./spSoftPartial_8_*.msOut.gz | wc -l
ls ./spSoftPartial_9_*.msOut.gz | wc -l
ls ./spSoftPartial_10_*.msOut.gz | wc -l

```

```{shell, check and fix empty simulation}

## Neutral (DONE)

rm list_discoal_files_Neutral_RERUN_10.txt
grep 'Out Of Memory' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt
grep ' MAXMUTS' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt
cat list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt > list_discoal_files_Neutral_RERUN_10.txt
wc -l list_discoal_files_Neutral_RERUN_10.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral_RERUN_10.txt

## Hard_5 (DONE)

rm list_discoal_files_Hard_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Hard_5_RERUN_OutOfMemory.txt list_discoal_files_Hard_5_RERUN_MAXMUTS.txt > list_discoal_files_Hard_5_RERUN.txt
wc -l list_discoal_files_Hard_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5_RERUN.txt

## Soft_5 (DONE)

rm list_discoal_files_Soft_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Soft_5_RERUN_OutOfMemory.txt list_discoal_files_Soft_5_RERUN_MAXMUTS.txt > list_discoal_files_Soft_5_RERUN.txt
wc -l list_discoal_files_Soft_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5_RERUN.txt

## HardPartial_5 (DONE)

rm list_discoal_files_HardPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_HardPartial_5_RERUN.txt
wc -l list_discoal_files_HardPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5_RERUN.txt

## SoftPartial_5 (DONE)

rm list_discoal_files_SoftPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_SoftPartial_5_RERUN.txt
wc -l list_discoal_files_SoftPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5_RERUN.txt

## check all

rm list_discoal_files_RERUN.txt
grep 'Out Of Memory' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' >  list_discoal_files_RERUN_MAXMUTS.txt
cat list_discoal_files_RERUN_OutOfMemory.txt list_discoal_files_RERUN_MAXMUTS.txt > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt


rm list_discoal_files_RERUN.txt
find ./ -maxdepth 1 -type f -name '*.msOut.gz' -size -21c | sed 's/\.\/sp/discoal_/; s/msOut\.gz/slurm\.sh/; s/Neut/Neutral/;' > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt

```

```{shell, combine simulations}

# Function to process and combine sweep files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail ./${PREFIX}_*.msOut -n +3 > temp1.msOut
    perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
    awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/2000/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spHard_5; do
    process_and_combine "$PREFIX"
done

for PREFIX in spNeut spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
    process_and_combine "$PREFIX"
done



# Function to process and combine linked files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail -n +3 ./${PREFIX}_*.msOut > temp1.msOut
    grep -v "$PREFIX" temp1.msOut > temp2.msOut
    awk 'BEGIN { RS="\n\n"; ORS="\n" } { print }' temp2.msOut > temp3.msOut
    
    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/400/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    process_and_combine "$PREFIX"
done

# # mini for testing
# 
# gzip -d spHard_5_1.msOut.gz
# gzip -d spHard_5_2.msOut.gz
# gzip -d spHard_5_3.msOut.gz
# 
# # Concatenate files and remove headers
# tail spHard_5_*.msOut -n +3 > temp1.msOut
# perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
# awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut
# 
# # Extract the first two lines from ${PREFIX}_1.msOut
# head -n 2 "spHard_5_1.msOut" > header.txt
# 
# # Edit the header to change the third space-delimited element from 1 to 2000
# sed -i '1s/\<1\>/3/' header.txt
# 
# # Concatenate the edited header with temp3.msOut
# cat header.txt temp3.msOut > spHard_5_COMBINED_MINI.msOut
# 
# # Move and compress the combined file
# gzip spHard_5_COMBINED_MINI.msOut
# 
# # Compress files
# find ./ -maxdepth 1 -type f -name "spHard_5_*.msOut" -exec gzip {} \;
# 
# # Clean up temporary files
# rm temp1.msOut temp2.msOut

```

## Run training

### Training - calcStatsAndDafForEachSnp

```{python, calcStatsAndDafForEachSnpSingleMsFile_Python3}

import sys
import allel
import random
import numpy as np
from msTools import *
from fvTools import *
import time

'''usage example
python calcStatsAndDafForEachSnpSingleMsFile.py /san/data/dan/simulations/discoal_multipopStuff/spatialSVMSims/trainingSets/equilibNeut.msout.gz 110000 iHS nSL
'''

trainingDataFileName, totalPhysLen = sys.argv[1:3]
totalPhysLen = int(totalPhysLen)

hapArraysIn, positionArrays = msOutToHaplotypeArrayIn(trainingDataFileName, totalPhysLen)
numInstances = len(hapArraysIn)

header = "daf\tiHS\tnSL"
#print header # Python 2
print(header) # Python 3

dafs = []
#start = time.clock() # Python 2
start = time.perf_counter() # Python 3
numInstancesDone = 0

numSnpsDone = 0
for instanceIndex in range(numInstances):
    if instanceIndex % 10 == 0:
        sys.stderr.write("done wtih %d of %d instances (%d SNPs)\n" %(instanceIndex, numInstances, numSnpsDone))
    haps = allel.HaplotypeArray(hapArraysIn[instanceIndex], dtype='i1')
    genos = haps.to_genotypes(ploidy=2)
    ac = genos.count_alleles()
    sampleSizes = [sum(x) for x in ac]
    assert len(set(sampleSizes)) == 1
    dafs = ac[:,1]/float(sampleSizes[0])
    ihsVals = allel.stats.selection.ihs(haps, positionArrays[instanceIndex], use_threads=True, include_edges=False)
    nslVals = allel.stats.selection.nsl(haps, use_threads=True)
    assert len(dafs) == len(ihsVals) == len(nslVals)
    numSnpsDone += len(dafs)
    for i in range(len(dafs)):
        #print "%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i]) # Python 2
        print ("%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i])) #Python 3
    if numSnpsDone > 5000000:
        break

```

```{shell, bunya template}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=32G
#SBATCH --job-name=[SAMPLE]
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/contsNe_intergenic/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/

msOut="[SAMPLE]"

srun python /home/uqsalle3/Programs/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/

find ./ -maxdepth 1 -type f -name '*_COMBINED.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_calcStatsAndDafForEachSnpSingleMsFile_Python3.txt calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
done < msOut_files.txt

find ./ -maxdepth 1 -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*.slurm.sh' -print0 | xargs -0 chmod 755

ls ./calcStatsAndDafForEachSnpSingleMsFile_Python3_*.slurm.sh

```

```{shell, submit scripts Bunya}

while read p; do
  echo "calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh submitted"
  sbatch calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
  sleep 0.25
done < msOut_files.txt

```

### Training - training_convert_to_FVs

```{python, training_convert_to_FVs_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
from msTools import *
from fvTools import *
import random
import allel
import numpy as np
import math

# '''usage eg:
# pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
# python2 training_convert_to_FVs.py spNeut.msOut.gz 2L,2R,3L,3R 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.fa ./ spNeut.msOut.fvec
# '''
# 
# if len(sys.argv)!=12:
#   sys.exit("usage:\npython2 training_convert_to_FVs.py trainingDataFileName chrArmsForMasking subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName statDir fvecFileName\n")
# else:
#   trainingDataFileName, chrArmsForMasking, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, statDir, fvecFileName = sys.argv[1:]

trainingDataFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5.msOut.gz"
chrArmsForMasking="ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140"
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_stats.txt"
maskFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta"
# ancestralArmFaFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/top6.anc.fa"
ancestralArmFaFileName="none"
statDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/"
fvecFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_test.fvec"

subWinSize,numSubWins = int(subWinSize),int(numSubWins)
assert subWinSize>0 and numSubWins>1

totalPhysLen=(subWinSize*numSubWins)

hapArraysIn,positionArrays=msOutToHaplotypeArrayIn(trainingDataFileName,totalPhysLen)

chrArmsForMasking=chrArmsForMasking.split(",")

unmaskedFracCutoff,pMisPol=float(unmaskedFracCutoff),float(pMisPol)

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")


if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")


standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none", "false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*totalPhysLen
  sys.stderr.write("Warning: not doing any masking! (i.e. mask.fa file for the chr arm with all masked sites N'ed out, or at least the reference with Ns, is not provided)\n")
  maskFileName=False
else:
  if ancestralArmFaFileName.lower() in ["none", "false"]:
    maskData=readMaskDataForTraining(maskFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  else:
    maskData=readMaskAndAncDataForTraining(maskFileName,ancestralArmFaFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  if len(maskData)<len(hapArraysIn):
    sys.stderr.write("Warning: not enough windows from masked data (needed %d; got %d); will draw with replacement!!\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=True
  else:
    sys.stderr.write("enough windows from masked data (needed %d; got %d); will draw without replacement.\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=False


# for i in range(len(maskData)):
#   print(str(i) + " " + str(len(maskData[i])) + " " + str(sum(maskData[i])))

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

header=[]

for statName in statNames:
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName, i))

header="\t".join(header)

statVals={}

for statName in statNames:
  statVals[statName]=[]

def getSubWinBounds(subWinSize,totalPhysLen):
  subWinStart=1-subWinSize
  subWinEnd=0
  subWinBounds=[]
  for i in range(0,(numSubWins-1)):
    subWinStart+=subWinSize
    subWinEnd+=subWinSize
    subWinBounds.append((subWinStart,subWinEnd))
  subWinStart+=subWinSize
  subWinEnd=totalPhysLen
  subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,totalPhysLen)

def getSnpIndicesInSubWins(subWinBounds,positions):
  snpIndicesInSubWins=[]
  for subWinIndex in range(len(subWinBounds)):
    snpIndicesInSubWins.append([])
  subWinIndex=0
  for i in range(len(positions)):
    while not (positions[i]>=subWinBounds[subWinIndex][0] and positions[i]<=subWinBounds[subWinIndex][1]):
      subWinIndex+=1
    snpIndicesInSubWins[subWinIndex].append(i)
  return snpIndicesInSubWins

quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

if statDir.lower() in ["none","false","default"]:
  statDir='./'

statFiles=[]

for subWinIndex in range(numSubWins):
  statFileName='/'.join(("%s/%s.subWin%d.stats" %(statDir, trainingDataFileName.split("/")[-1].replace(".msOut.gz",""), subWinIndex)).split('//'))
  statFiles.append(open(statFileName,"w"))
  statFiles[-1].write("\t".join(statNames)+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  fvecFileName="%s.fvec" %trainingDataFileName.split("/")[-1].replace(".msOut.gz","")

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

# for testing for loop below only
instanceIndex=0
statName='pi'

for instanceIndex in range(len(hapArraysIn)):
  print("Starting index " + str(instanceIndex))
  for statName in statNames:
    statVals[statName].append([])
  if maskFileName:
    if drawWithReplacement:
      unmasked=random.choice(maskData)
    else:
      unmasked=maskData[instanceIndex]
    assert len(unmasked)==totalPhysLen
    print(str(sum(unmasked)) + " Loci unmasked")
  snpIndicesToKeep=[x for x in range(len(positionArrays[instanceIndex])) if unmasked[positionArrays[instanceIndex][x]-1]]
  if len(snpIndicesToKeep)==0:
    for subWinIndex in range(numSubWins):
      for statName in statNames:
        print("No SNPs in BIGwindow, appendStatValsForMonomorphic")
        appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  else:
    haps=allel.HaplotypeArray(hapArraysIn[instanceIndex],dtype='i1').subset(sel0=snpIndicesToKeep)
    if pMisPol>0:
      misPolarizeCorrectionIndex=np.random.binomial(1,pMisPol,len(haps))
      for i in range(len(misPolarizeCorrectionIndex)):
        if misPolarizeCorrectionIndex[i]==1:
          for j in range(len(haps[i])):
            if haps[i][j]==0:
              haps[i][j]=1
            else:
              haps[i][j]=0
    genos=haps.to_genotypes(ploidy=2)
    alleleCounts=genos.count_alleles()
    positions=[positionArrays[instanceIndex][x] for x in snpIndicesToKeep]
    precomputedStats={}
    sampleSizes=[sum(x) for x in alleleCounts]
    assert len(set(sampleSizes))==1
    dafs=alleleCounts[:,1]/float(sampleSizes[0])
    ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
    nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
    nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
    # sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    if nonNanCount==0:
      precomputedStats["iHS"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["iHS"].append([])
    else:
      ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
      precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)
    nslVals=allel.stats.selection.nsl(haps,use_threads=False)
    nonNanCount=[x for x in np.isnan(nslVals)].count(False)
    # sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    if nonNanCount==0:
      precomputedStats["nSL"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["nSL"].append([])
    else:
      nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
      precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)
    snpIndicesInSubWins=getSnpIndicesInSubWins(subWinBounds,positions)
    for subWinIndex in range(numSubWins):
      subWinStart,subWinEnd = subWinBounds[subWinIndex]
      unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
      assert unmaskedFrac>=unmaskedFracCutoff
      if len(snpIndicesInSubWins[subWinIndex])==0:
        print("No SNPS in " + str(subWinIndex) + ", appendStatValsForMonomorphic")
        for statName in statNames:
          appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      else:
        print(str(snpIndicesInSubWins[subWinIndex])) + " SNPs in " + str(subWinIndex) + ", calcAndAppendStatVal")
        hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
        for statName in [x for x in statNames if x[0:3]!="HAF" and x[0:3]!="phi" and x[0:3]!="kap" and x[0:3]!="SFS" and x[0:3]!="SAF"]:
          calcAndAppendStatVal(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, instanceIndex, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        haplotypes={}
        for i in range(len(hapsInSubWin[0])):
          haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
          haplotype="".join(str(x) for x in haplotype)
          if haplotype in haplotypes:
            haplotypes[haplotype].append(i)
          else:
            haplotypes[haplotype]=[i]
        HAF=[]
        HAFunique={}
        for i in haplotypes:
          HAFunique[i]=0
          for j in range(len(hapsInSubWin)):
            if hapsInSubWin[j][haplotypes[i][0]]==1:
              HAFunique[i]+=sum([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j]))])
          for j in range(len(haplotypes[i])):
            HAF.append(HAFunique[i])
        phi=[]
        kappa=[]
        SAFE=[]
        for i in range(len(hapsInSubWin)):
          phi.append(0)
          kappa.append([])
          phiDenom=0
          for j in haplotypes:
            phi[i]+=(int(list(j)[i])*HAFunique[j]*len(haplotypes[j]))
            if int(list(j)[i])==1 and HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
              kappa[i].append(HAFunique[j])
            phiDenom+=(HAFunique[j]*len(haplotypes[j]))
          phi[i]/=float(phiDenom)
          kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
          if dafs[snpIndicesInSubWins[subWinIndex]][i]==0 or dafs[snpIndicesInSubWins[subWinIndex]][i]==1:
           SAFE.append(0.0)
          else:
           SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafs[snpIndicesInSubWins[subWinIndex]][i]*(1-dafs[snpIndicesInSubWins[subWinIndex]][i]))))
        for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
          if i=="SFS":
            windowStats=dafs[snpIndicesInSubWins[subWinIndex]]
          elif i=="HAFunique":
            # windowStats=[eval(i)[x] for x in eval(i)]
            windowStats = list(HAFunique.values()) # SCOTT
          else:
            windowStats=eval(i)
          statVals[i+"-Mean"][instanceIndex].append(np.mean(windowStats))
          statVals[i+"-Median"][instanceIndex].append(np.median(windowStats))
          if(len(np.unique(windowStats,return_counts=True)[1])==1):
            statVals[i+"-Mode"][instanceIndex].append(windowStats[0])
          else:
            if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
              statVals[i+"-Mode"][instanceIndex].append(scipy.stats.mstats.mode(windowStats)[0][0])
            else:
              mode=min(windowStats)
              for j in range(1,51):
                if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
                  mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
              statVals[i+"-Mode"][instanceIndex].append(mode+((max(windowStats)-min(windowStats))/100))
          for j in quantiles:
            statVals[i+"-"+j][instanceIndex].append(np.percentile(windowStats,quantiles[j]))
          statVals[i+"-Max"][instanceIndex].append(max(windowStats))
          statVals[i+"-Var"][instanceIndex].append(np.var(windowStats))
          statVals[i+"-SD"][instanceIndex].append(np.std(windowStats))
          statVals[i+"-Skew"][instanceIndex].append(scipy.stats.skew(windowStats))
          statVals[i+"-Kurt"][instanceIndex].append(scipy.stats.kurtosis(windowStats))
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][instanceIndex])
  fvecFile.write("\t".join([str(x) for x in outVec])+"\n")

for subWinIndex in range(numSubWins):
  statFiles[subWinIndex].close()
fvecFile.close()
# sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, bunya template Neutral}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=8G
#SBATCH --job-name=[SAMPLE]
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_training_convert_to_FVs_Python3_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_training_convert_to_FVs_Python3_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/

msOut="[SAMPLE]"

srun python /home/uqsalle3/Programs/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spNeut_COMBINED_stats.txt\
  /scratch/project/chenoase/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell}

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spHard_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_10.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat

find ./ -maxdepth 1 -type f -name '*.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt



# Neutral

grep 'Neut' msOut_files.txt > temp.txt
grep -v 'COMBINED' temp.txt > msOut_files_Neutral.txt
rm temp.txt

while read p; do
  echo "$p"
  cp template_training_convert_to_FVs_Python3_Neutral.txt training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
done < msOut_files_Neutral.txt



# Function to process files based on the specified prefix
process_files() {
    local prefix="$1"
    local template="template_training_convert_to_FVs_Python3_${prefix}.txt"
    local output_file="msOut_files_${prefix}.txt"

    # Create a list of files based on the prefix
    grep "${prefix}_" msOut_files.txt > temp.txt
    grep -v 'COMBINED' temp.txt > "${output_file}"
    rm temp.txt

    # Process each file in the list
    while read -r p; do
        echo "$p"
        cp "${template}" "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh"
        replace_CMD="s/\[SAMPLE\]/${p}/g"
        perl -pi -e "$replace_CMD" "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh"
    done < "${output_file}"
}

# Loop through different prefixes
for i in {0..10}; do
    process_files "Hard_${i}"
done

for i in {0..10}; do
    process_files "Soft_${i}"
done

for i in {0..10}; do
    process_files "HardPartial_${i}"
done

for i in {0..10}; do
    process_files "SoftPartial_${i}"
done



find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*.slurm.sh' -print0 | xargs -0 chmod 755

```

```{shell, submit scripts Bunya}

# RUNNING
while read p; do
  echo "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh submitted"
  sbatch training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
  sleep 0.05
done < msOut_files_Neutral.txt

# Function to submit files based on the specified prefix
submit_jobs() {
    local prefix="$1"
    echo "training_convert_to_FVs_Python3_${prefix}.slurm.sh submitted"
    sbatch training_convert_to_FVs_Python3_${prefix}.slurm.sh
    sleep 0.05
}

training_convert_to_FVs_Python3_spHard_0_5.slurm.sh

# Loop through different prefixes

# RUNNING
for i in {1..2000}; do
    submit_jobs "spHard_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spSoft_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spHardPartial_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spSoftPartial_5_${i}"
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spHard_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spSoft_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spHardPartial_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spSoftPartial_${i}_${j}"
    done
done



ls ./spNeut_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_5_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spHard_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spSoft_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spHardPartial_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spSoftPartial_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_10_*.fvec | grep -v 'COMBINED' | wc -l

## rerun failed

grep 'error' ./slurm_training_convert_to_FVs_Python3_sp*.error | perl -ne 's/\.\/slurm_/sbatch\ /; s/\.msOut\.gz.*/\.slurm\.sh/; print $_;' | uniq > rerun_failed_training_convert_to_FVs_Python3.sh
cat rerun_failed_training_convert_to_FVs_Python3.sh

bash rerun_failed_training_convert_to_FVs_Python3.sh

```

```{shell, combined fvec}

process_files() {
  local PREFIX="$1"
  find ./ -maxdepth 1 -type f | grep -E "${PREFIX}_[0-9]+\.fvec" > temp.txt
  
  while read -r p; do
    wc ${p}
  done < temp.txt
  
  while read -r p; do
    awk 'NR == 2' ${p}
  done < temp.txt > temp.fvec

  # Extract the first line from ${PREFIX}_1.msOut
  head -n 1 "${PREFIX}_1.fvec" > header.txt

  # Concatenate the edited header with temp.fvec
  cat header.txt temp.fvec > "${PREFIX}_SINGLE_COMBINED.fvec"
}

rm -v ./spNeut_SINGLE_COMBINED.fvec
for PREFIX in spNeut; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spHard_*_SINGLE_COMBINED.fvec
for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_5 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spSoft_*_SINGLE_COMBINED.fvec
for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_5 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spHardPartial_*_SINGLE_COMBINED.fvec
for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_5 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spSoftPartial_*_SINGLE_COMBINED.fvec
for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_5 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done



find ./ -maxdepth 1 -type f -name '*_SINGLE_COMBINED.fvec' -exec wc {} \;

```

```{shell, transfer to Dell 2}

# constNe_intergenic

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/*_SINGLE_COMBINED.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat/*_SINGLE_COMBINED.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat/

# There should be 45 fvec files
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/*.fvec | wc -l
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat/*.fvec | wc -l

```

```{r, assess feature vectors}

# module load anaconda3
# source ~/conda-init
# conda activate r_env

library(tidyverse)
library(data.table)

# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic")
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat")

spHard_5 <- read_tsv("spHard_5_SINGLE_COMBINED.fvec")

spHard_5 %>% 
  mutate(sim=c(1:nrow(spHard_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

spSoft_5 <- read_tsv("spSoft_5_SINGLE_COMBINED.fvec")

spSoft_5 %>% 
  mutate(sim=c(1:nrow(spSoft_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

```

### Training - training_sample_FVs

```{python, training_sample_FVs.py}

# import sys,os,random
import sys # SCOTT
import os # SCOTT
import random # SCOTT

'''usage eg:
python2 training_sample_FVs.py spNeut.fvec spHard_ spSoft_ spPartialHard_ spPartialSoft_ 5 0,1,2,3,4,6,7,8,9,10 ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec
'''

if len(sys.argv)!=10:
  sys.exit("usage:\npython2 training_sample_FVs.py neutTrainingFileName hardTrainingFilesPrefix softTrainingFilesPrefix partialHardTrainingFilesPrefix partialSoftTrainingFilesPrefix sweepTrainingWindow linkedTrainingWindows sampledFVsDir sampledFVsFiles\n")
else:
  neutTrainingFileName, hardTrainingFilesPrefix, softTrainingFilesPrefix, partialHardTrainingFilesPrefix, partialSoftTrainingFilesPrefix, sweepTrainingWindow, linkedTrainingWindows, sampledFVsDir, sampledFVsFiles = sys.argv[1:]

neutTrainingFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spNeut.fvec"
hardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_"
softTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spSoft_"
partialHardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialHard_"
partialSoftTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialSoft_"
sweepTrainingWindow=5
linkedTrainingWindows=0,1,2,3,4,6,7,8,9,10
sampledFVsDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe"
sampledFVsFiles="neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec"

sweepFilePaths,linkedFilePaths = {},{}
for trainingFilePrefix in [hardTrainingFilesPrefix,softTrainingFilesPrefix,partialHardTrainingFilesPrefix,partialSoftTrainingFilesPrefix]:
  trainingSetDir="/".join(trainingFilePrefix.split("/")[:-1])
  trainingFilePrefixDirless=trainingFilePrefix.split("/")[-1]
  sweepFilePaths[trainingFilePrefix]=[]
  linkedFilePaths[trainingFilePrefix]=[]
  for fileName in os.listdir(trainingSetDir):
    if fileName.startswith(trainingFilePrefixDirless):
      winNum=int(fileName.split("_")[1].split(".")[0])
      if winNum==int(sweepTrainingWindow):
        sweepFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)
      elif winNum in [int(x) for x in linkedTrainingWindows.split(",")]:
        linkedFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)

def getExamplesFromFVFile(simFileName):
  try:
    simFile=open(simFileName)
    lines=[line.strip() for line in simFile.readlines()]
    header=lines[0]
    examples=lines[1:]
    simFile.close()
    return header,examples
  except Exception:
    return "",[]

def getExamplesFromFVFileLs(simFileLs):
  examples=[]
  keptHeader=""
  for filePath in simFileLs:
    header,currExamples=getExamplesFromFVFile(filePath)
    if header:
      keptHeader=header
    examples+=currExamples
  return keptHeader,examples

header,neutExamples=getExamplesFromFVFile(neutTrainingFileName)
hardHeader,hardExamples=getExamplesFromFVFileLs(sweepFilePaths[hardTrainingFilesPrefix])
linkedHardHeader,linkedHardExamples=getExamplesFromFVFileLs(linkedFilePaths[hardTrainingFilesPrefix])
softHeader,softExamples=getExamplesFromFVFileLs(sweepFilePaths[softTrainingFilesPrefix])
linkedSoftHeader,linkedSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[softTrainingFilesPrefix])
partialHardHeader,partialHardExamples=getExamplesFromFVFileLs(sweepFilePaths[partialHardTrainingFilesPrefix])
linkedPartialHardHeader,linkedPartialHardExamples=getExamplesFromFVFileLs(linkedFilePaths[partialHardTrainingFilesPrefix])
partialSoftHeader,partialSoftExamples=getExamplesFromFVFileLs(sweepFilePaths[partialSoftTrainingFilesPrefix])
linkedPartialSoftHeader,linkedPartialSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[partialSoftTrainingFilesPrefix])

def getMinButNonZeroExamples(lsLs):
  counts=[]
  for ls in lsLs:
    if len(ls)>0:
      counts.append(len(ls))
  if not counts:
    raise Exception
  return min(counts)

trainingSetLs=[hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
numExamplesToKeep=getMinButNonZeroExamples(trainingSetLs)
for i in range(len(trainingSetLs)):
  random.shuffle(trainingSetLs[i])
  trainingSetLs[i]=trainingSetLs[i][:numExamplesToKeep]
hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples=trainingSetLs

if sampledFVsDir.lower() in ["none","false","default"]:
  sampledFVsDir='./'
if sampledFVsFiles.lower() in ["none","false","default"]:
  sampledFVsFiles=["neut.fvec","hard.fvec","linkedHard.fvec","soft.fvec","linkedSoft.fvec","partialHard.fvec","linkedPartialHard.fvec","partialSoft.fvec","linkedPartialSoft.fvec"]
else:
  sampledFVsFiles=sampledFVsFiles.split(",")
  assert len(sampledFVsFiles)==9
outExamples=[neutExamples,hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
for i in range(len(sampledFVsFiles)):
  if outExamples[i]:
    outFile=open('/'.join((sampledFVsDir+'/'+sampledFVsFiles[i]).split('//')),"w")
    outFile.write("classLabel\t%s\n" %(hardHeader))
    for example in outExamples[i]:
      outFile.write("%s\t%s\n" %(sampledFVsFiles[i].replace(".fvec",""),example))
    outFile.close()

```

```{shell, test Dell 2}

# constNe_intergenic

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs

for file in *_SINGLE_COMBINED.fvec; do
  mv "$file" "${file/_SINGLE_COMBINED.fvec/.fvec}"
done

for file in spHardPartial_*.fvec; do
  mv "$file" "${file/spHardPartial_/spPartialHard_}"
done

for file in spSoftPartial_*.fvec; do
  mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
done

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

```

### Training

```{shell, test Dell 2}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic
mkdir -p FVs && cd ./FVs

# for i in 1 2 3 4 5 6 7 8 9 10; do
  time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    constNe.hdf5\
    constNe.json\
    constNe.npy > training_deep_learning_python3.log 2>&1
# done

tensorboard --logdir logs

# time on 40 cores ~12 minutes with 5GB RAM

```

#### Training with early stopping

```{shell, test Dell 2}

# module load anaconda3
# source ~/conda-init
# conda activate partialshic_py3
# 
# # expansionNe
# 
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
# mkdir -p FVs && cd ./FVs
# mkdir -p earlyStopping && cd ./earlyStopping
# rm ./*
# cp ../*.fvec
# 
# time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_test.hdf5\
#     expansionNe_earlyStopping_test.json\
#     expansionNe_earlyStopping_test.npy
# 
# for i in 1 2 3 4 5 6 7 8 9 10; do
#   time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_${i}.hdf5\
#     expansionNe_earlyStopping_${i}.json\
#     expansionNe_earlyStopping_${i}.npy > training_deep_learning_python3_earlyStopping_${i}.log 2>&1
# done
# 
# # time on 40 cores ~12 minutes with 5GB RAM

```

#### Training with early stopping and 10-fold CV

```{shell, test Dell 2}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
rm ./*
cp ../*.fvec ./

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping_kFold.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    constNe_earlyStopping_fold.hdf5\
    constNe_earlyStopping_fold.json\
    constNe_earlyStopping_fold.npy

tensorboard --logdir logs

# for i in 1 2 3 4 5 6 7 8 9 10; do
#   time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     constNe_earlyStopping_${i}.hdf5\
#     constNe_earlyStopping_${i}.json\
#     constNe_earlyStopping_${i}.npy > training_deep_learning_python3_earlyStopping_${i}.log 2>&1
# done

# time on 40 cores ~12 minutes with 5GB RAM

```

### Run testing

```{python}

import time
# startTime=time.clock()
startTime = time.perf_counter() # Python 3
import sys,os
import keras
import numpy as np
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import pandas as pd

'''usage eg:
python3 testing_deep_learning_classify.py training.npy ./ 11 89 ./ accuracy confusion_matrix.pdf
'''

if len(sys.argv)!=8:
  sys.exit("usage:\npython3 testing_deep_learning_classify.py classifierPickleFileName fvecDir numSubWins numSumStatsPerSubWin resultsDir accuracyFilesPrefix confusionMatrixFigFileName\n")
else:
  classifierPickleFileName, fvecDir, numSubWins, numSumStatsPerSubWin, resultsDir, accuracyFilesPrefix, confusionMatrixFigFileName = sys.argv[1:]

classifierPickleFileName='../FVs/constNe.npy'
fvecDir='./'
numSubWins='11'
numSumStatsPerSubWin='92'
resultsDir='./'
accuracyFilesPrefix='constNe_accuracy'
confusionMatrixFigFileName='constNe_confusion_matrix.pdf'

netlayers=keras.models.load_model(classifierPickleFileName)

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

testX={}

numSubWins=int(numSubWins)

# for testing for loop below
testSetFileName='spHard_0.fvec'
testExample=currTestData[0]
i=0

for testSetFileName in os.listdir(fvecDir):
  # testSetFile=open('/'.join((fvecDir+'/'+testSetFileName).split('//')))
  testSetFile=open(testSetFileName)
  currTestData=testSetFile.readlines()
  testSetFile.close()
  currTestData=currTestData[1:]
  testX[testSetFileName]=[]
  for testExample in currTestData:
    if not "nan" in testExample:
      testData=testExample.strip().split("\t")
      currVector=[]
      for i in range(len(testData)):
        currVector.append(float(testData[i]))
      testX[testSetFileName].append(currVector)
  testX[testSetFileName]=np.reshape(np.array(testX[testSetFileName]),(np.array(testX[testSetFileName]).shape[0],int(numSumStatsPerSubWin),numSubWins,1))

selVals={"Neutral":0,"Hard":1,"Soft":2,"HardPartial":3,"SoftPartial":4}

def getSelType(x):
  if "Neut" in x:
    return "Neutral"
  elif "PartialHard" in x:
    return "HardPartial"
  elif "PartialSoft" in x:
    return "SoftPartial"
  elif "Hard" in x:
    return "Hard"
  elif "Soft" in x:
    return "Soft"
  else:
    raise ValueError

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split()

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

outlinesH={}
accuracyOverall=0
accuracySpecific=0
accuracyBroad=0

# for testing loop below
testSetFileName='spNeut.fvec'
className='Neutral'
testExampleIndex=0

for testSetFileName in sorted(testX, key=lambda x: (selVals[getSelType(x)], int((x.split(".")[0]+"_0").split("_")[1]))):
  predictions=np.argmax(netlayers.predict(testX[testSetFileName]),axis=1)
  pred = netlayers.predict(testX[testSetFileName]) # SCOTT
  prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
  prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT
  prob_df['truth']=testSetFileName # SCOTT
  prob_df.to_csv(testSetFileName+'.prob.csv', index=False) # SCOTT
  currPreds={}
  for className in classOrder:
    currPreds[className]=0
  denom=float(len(testX[testSetFileName]))
  for testExampleIndex in range(len(predictions)):
    predictedClass=labelToClassName[predictions[testExampleIndex]]
    currPreds[predictedClass]+=1/denom
  if not testSetFileName=='spNeut.fvec':
    testSetFilePrefix=testSetFileName.split(".")[0].split("_")
    selType,selWin = getSelType(testSetFilePrefix[0]),testSetFilePrefix[1]
    selWin=int(selWin)
    key=(selType,selWin)
  else:
    key=('Neutral',0)
  outlinesH[key]=(testSetFileName,[currPreds[className] for className in classOrder])
  if key[0]=='Neutral' or key[1]==5:
    accuracyOverall+=currPreds[key[0]]
    accuracySpecific+=currPreds[key[0]]
  else:
    accuracyOverall+=currPreds[key[0]+'-linked']
  accuracyBroad+=currPreds[key[0]]
  if not key[0]=='Neutral':
    accuracyBroad+=currPreds[key[0]+'-linked']

if resultsDir.lower() in ["none","false","default"]:
  resultsDir='./'

if accuracyFilesPrefix.lower() in ["none","false","default"]:
  accuracyFilesPrefix='accuracy'

accuracyOverall=accuracyOverall/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_overall.txt').split('//')),'w')
fileName.write(str(accuracyOverall))
fileName.close()

accuracySpecific=accuracySpecific/5

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_specific.txt').split('//')),'w')
fileName.write(str(accuracySpecific))
fileName.close()

accuracyBroad=accuracyBroad/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_broad.txt').split('//')),'w')
fileName.write(str(accuracyBroad))
fileName.close()


rowLabels,data = [],[]

for selType in sorted(selVals, key=lambda x: selVals[x]):
 for selWin in range(numSubWins):
  if selType!="Neutral" or selWin==0:
    if "Neutral" in selType:
      rowLabels.append("Neutral")
    else:
      if selWin==5:
        rowLabels.append("%s sweep in focal window" %selType)
      else:
        diff=abs(selWin-5)
        if diff==1:
          plural=""
        else:
          plural="s"
        if selWin<5:
          direction="left"
        else:
          direction="right"
        rowLabels.append("%s sweep %s window%s to %s" %(selType, diff, plural, direction))
    vec=outlinesH[(selType,selWin)][1]
    data.append(vec)

data=np.array(data)

ax=plt.subplots()[1]
heatmap=ax.pcolor(data,cmap=plt.cm.Blues,vmin=0.0,vmax=1.0)
cbar=plt.colorbar(heatmap,cmap=plt.cm.Blues)
cbar.set_label('Fraction of simulations assigned to class',rotation=270,labelpad=20)
ax.set_xticks(np.arange(data.shape[1])+0.5,minor=False)
ax.set_yticks(np.arange(data.shape[0])+0.5,minor=False)
ax.invert_yaxis()
ax.xaxis.tick_top()
ax.axis('tight')
plt.tick_params(axis='y',which='both',right='off')
plt.tick_params(axis='x',which='both',direction='out')
ax.set_xticklabels(classOrder,minor=False,fontsize=9,rotation=45,ha="left")
ax.set_yticklabels(rowLabels,minor=False,fontsize=7)

for y in range(data.shape[0]):
  for x in range(data.shape[1]):
    val=data[y,x]
    val*=100
    if val>50:
      c='0.9'
    else:
      c='black'
    ax.text(x+0.5,y +0.5,'%.1f%%' % val,horizontalalignment='center',verticalalignment='center',color=c,fontsize=6)

if confusionMatrixFigFileName.lower() in ["none","false","default"]:
  confusionMatrixFigFileName='confusionmatrix.pdf'

plt.savefig('/'.join((resultsDir+'/'+confusionMatrixFigFileName).split('//')),bbox_inches='tight',dpi=600)

# sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(classifierPickleFileName,fvecDir,(time.clock()-startTime)))
sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### test on self

```{shell}

# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe
# mkdir -p test_self && cd test_self
# rm ./*
# cp ../FVs/*.fvec ./
# rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
# 
# python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
#  ../FVs/constNe.npy\
#  ./\
#  11\
#  92\
#  ./\
#  constNe_accuracy\
#  constNe_confusion_matrix.pdf
# 
# evince constNe_confusion_matrix.pdf

```

#### test on self-new data

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp ../../*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  for file in *_SINGLE_COMBINED.fvec; do
    mv "$file" "${file/_SINGLE_COMBINED.fvec/.fvec}"
  done
  for file in spHardPartial_*.fvec; do
    mv "$file" "${file/spHardPartial_/spPartialHard_}"
  done
  for file in spSoftPartial_*.fvec; do
    mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
  done
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince constNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/
  mkdir -p test_expansionNe_intergenic && cd test_expansionNe_intergenic
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on constNe

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/
  mkdir -p test_constNe && cd test_constNe
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic_lci

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/
  mkdir -p test_expansionNe_intergenic_lci && cd test_expansionNe_intergenic_lci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic_uci

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/
  mkdir -p test_expansionNe_intergenic_uci && cd test_expansionNe_intergenic_uci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on empirical

```{python}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell, OLD}

# export ss='expansionNe'
# 
# mkdir ${ss}_classifier
# 
# pred() {
#     echo ${1}
#     python3 ~/partialSHIC/empirical_deep_learning_classify.py \
#     ../${ss}/${ss}.npy \
#     ${1} \
#     11 92 \
#     ./${ss}_classifier/${1/all.fvec/bed}
# }
# 
# export -f pred
# 
# ls  ScA8VGg_[0-9]*.all.fvec | parallel -j 3 pred



# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors
# 
# mkdir -p pred_constNe && cd pred_constNe
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_MaskHets_110Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718.regular.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_MaskHets_110Lines.pred.bed
# 
# made predictions for 1323 total instances
# predicted 37 neutral regions (0.027967 of all classified regions)
# predicted 850 Hard sweep regions (0.642479 of all classified regions)
# predicted 391 Hard-linked sweep regions (0.295540 of all classified regions)
# predicted 6 Soft sweep regions (0.004535 of all classified regions)
# predicted 22 Soft-linked sweep regions (0.016629 of all classified regions)
# predicted 6 HardPartial sweep regions (0.004535 of all classified regions)
# predicted 7 HardPartial-linked sweep regions (0.005291 of all classified regions)
# predicted 3 SoftPartial sweep regions (0.002268 of all classified regions)
# predicted 1 SoftPartial-linked sweep regions (0.000756 of all classified regions)
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_noMaskHets_110Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_110Lines.noMaskHets.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_noMaskHets_110Lines.pred.bed
# 
# made predictions for 1389 total instances
# predicted 861 neutral regions (0.619870 of all classified regions)
# predicted 28 Hard sweep regions (0.020158 of all classified regions)
# predicted 56 Hard-linked sweep regions (0.040317 of all classified regions)
# predicted 158 Soft sweep regions (0.113751 of all classified regions)
# predicted 123 Soft-linked sweep regions (0.088553 of all classified regions)
# predicted 3 HardPartial sweep regions (0.002160 of all classified regions)
# predicted 14 HardPartial-linked sweep regions (0.010079 of all classified regions)
# predicted 55 SoftPartial sweep regions (0.039597 of all classified regions)
# predicted 91 SoftPartial-linked sweep regions (0.065515 of all classified regions)
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_top6mask_noMaskHets_110Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_110Lines.noMaskHets.top6mask.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_top6mask_noMaskHets_110Lines.pred.bed
# 
# made predictions for 1044 total instances
# predicted 848 neutral regions (0.812261 of all classified regions)
# predicted 6 Hard sweep regions (0.005747 of all classified regions)
# predicted 19 Hard-linked sweep regions (0.018199 of all classified regions)
# predicted 29 Soft sweep regions (0.027778 of all classified regions)
# predicted 32 Soft-linked sweep regions (0.030651 of all classified regions)
# predicted 5 HardPartial sweep regions (0.004789 of all classified regions)
# predicted 12 HardPartial-linked sweep regions (0.011494 of all classified regions)
# predicted 37 SoftPartial sweep regions (0.035441 of all classified regions)
# predicted 56 SoftPartial-linked sweep regions (0.053640 of all classified regions)
# 
# 
# 
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_MaskHets_55Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_55Lines.MaskHets.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_MaskHets_55Lines.pred.bed
# 
# made predictions for 1346 total instances
# predicted 168 neutral regions (0.124814 of all classified regions)
# predicted 483 Hard sweep regions (0.358841 of all classified regions)
# predicted 377 Hard-linked sweep regions (0.280089 of all classified regions)
# predicted 76 Soft sweep regions (0.056464 of all classified regions)
# predicted 59 Soft-linked sweep regions (0.043834 of all classified regions)
# predicted 43 HardPartial sweep regions (0.031947 of all classified regions)
# predicted 63 HardPartial-linked sweep regions (0.046805 of all classified regions)
# predicted 38 SoftPartial sweep regions (0.028232 of all classified regions)
# predicted 39 SoftPartial-linked sweep regions (0.028975 of all classified regions)
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_noMaskHets_55Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_55Lines.noMaskHets.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_noMaskHets_55Lines.pred.bed
# 
# made predictions for 1405 total instances
# predicted 991 neutral regions (0.705338 of all classified regions)
# predicted 28 Hard sweep regions (0.019929 of all classified regions)
# predicted 31 Hard-linked sweep regions (0.022064 of all classified regions)
# predicted 148 Soft sweep regions (0.105338 of all classified regions)
# predicted 134 Soft-linked sweep regions (0.095374 of all classified regions)
# predicted 1 HardPartial sweep regions (0.000712 of all classified regions)
# predicted 2 HardPartial-linked sweep regions (0.001423 of all classified regions)
# predicted 37 SoftPartial sweep regions (0.026335 of all classified regions)
# predicted 33 SoftPartial-linked sweep regions (0.023488 of all classified regions)
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_top6mask_noMaskHets_55Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_55Lines.noMaskHets.top6mask.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_top6mask_noMaskHets_55Lines.pred.bed
# 
# made predictions for 1044 total instances
# predicted 907 neutral regions (0.868774 of all classified regions)
# predicted 4 Hard sweep regions (0.003831 of all classified regions)
# predicted 17 Hard-linked sweep regions (0.016284 of all classified regions)
# predicted 24 Soft sweep regions (0.022989 of all classified regions)
# predicted 40 Soft-linked sweep regions (0.038314 of all classified regions)
# predicted 1 HardPartial sweep regions (0.000958 of all classified regions)
# predicted 3 HardPartial-linked sweep regions (0.002874 of all classified regions)
# predicted 22 SoftPartial sweep regions (0.021073 of all classified regions)
# predicted 26 SoftPartial-linked sweep regions (0.024904 of all classified regions)
# 
# 
# 
# # train_110Haplotypes.pred_genosRateMask_phased_220Haplotypes
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_110Lines.phased.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes.pred.bed
# 
# predicted 832 neutral regions (0.601156 of all classified regions)
# predicted 23 Hard sweep regions (0.016618 of all classified regions)
# predicted 74 Hard-linked sweep regions (0.053468 of all classified regions)
# predicted 102 Soft sweep regions (0.073699 of all classified regions)
# predicted 220 Soft-linked sweep regions (0.158960 of all classified regions)
# predicted 7 HardPartial sweep regions (0.005058 of all classified regions)
# predicted 19 HardPartial-linked sweep regions (0.013728 of all classified regions)
# predicted 39 SoftPartial sweep regions (0.028179 of all classified regions)
# predicted 68 SoftPartial-linked sweep regions (0.049133 of all classified regions)
# 
# 
# 
# # train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_MAX
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_110Lines.phased.inbred.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_MAX.pred.bed
# 
# predicted 400 neutral regions (0.290909 of all classified regions)
# predicted 57 Hard sweep regions (0.041455 of all classified regions)
# predicted 178 Hard-linked sweep regions (0.129455 of all classified regions)
# predicted 88 Soft sweep regions (0.064000 of all classified regions)
# predicted 176 Soft-linked sweep regions (0.128000 of all classified regions)
# predicted 65 HardPartial sweep regions (0.047273 of all classified regions)
# predicted 252 HardPartial-linked sweep regions (0.183273 of all classified regions)
# predicted 30 SoftPartial sweep regions (0.021818 of all classified regions)
# predicted 129 SoftPartial-linked sweep regions (0.093818 of all classified regions)



cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

mkdir -p pred_constNe && cd pred_constNe

# ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_542_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 2475 total instances
predicted 1255 neutral regions (0.507071 of all classified regions)
predicted 95 Hard sweep regions (0.038384 of all classified regions)
predicted 383 Hard-linked sweep regions (0.154747 of all classified regions)
predicted 113 Soft sweep regions (0.045657 of all classified regions)
predicted 277 Soft-linked sweep regions (0.111919 of all classified regions)
predicted 47 HardPartial sweep regions (0.018990 of all classified regions)
predicted 167 HardPartial-linked sweep regions (0.067475 of all classified regions)
predicted 25 SoftPartial sweep regions (0.010101 of all classified regions)
predicted 113 SoftPartial-linked sweep regions (0.045657 of all classified regions)

# ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_594_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 3117 total instances
predicted 2212 neutral regions (0.709657 of all classified regions)
predicted 71 Hard sweep regions (0.022778 of all classified regions)
predicted 224 Hard-linked sweep regions (0.071864 of all classified regions)
predicted 138 Soft sweep regions (0.044273 of all classified regions)
predicted 161 Soft-linked sweep regions (0.051652 of all classified regions)
predicted 63 HardPartial sweep regions (0.020212 of all classified regions)
predicted 178 HardPartial-linked sweep regions (0.057106 of all classified regions)
predicted 11 SoftPartial sweep regions (0.003529 of all classified regions)
predicted 59 SoftPartial-linked sweep regions (0.018928 of all classified regions)

# ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_628_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 4618 total instances
predicted 2726 neutral regions (0.590299 of all classified regions)
predicted 123 Hard sweep regions (0.026635 of all classified regions)
predicted 453 Hard-linked sweep regions (0.098094 of all classified regions)
predicted 227 Soft sweep regions (0.049155 of all classified regions)
predicted 514 Soft-linked sweep regions (0.111304 of all classified regions)
predicted 106 HardPartial sweep regions (0.022954 of all classified regions)
predicted 298 HardPartial-linked sweep regions (0.064530 of all classified regions)
predicted 18 SoftPartial sweep regions (0.003898 of all classified regions)
predicted 153 SoftPartial-linked sweep regions (0.033131 of all classified regions)

# ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_718_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 1375 total instances
predicted 434 neutral regions (0.315636 of all classified regions)
predicted 56 Hard sweep regions (0.040727 of all classified regions)
predicted 163 Hard-linked sweep regions (0.118545 of all classified regions)
predicted 84 Soft sweep regions (0.061091 of all classified regions)
predicted 176 Soft-linked sweep regions (0.128000 of all classified regions)
predicted 84 HardPartial sweep regions (0.061091 of all classified regions)
predicted 241 HardPartial-linked sweep regions (0.175273 of all classified regions)
predicted 22 SoftPartial sweep regions (0.016000 of all classified regions)
predicted 115 SoftPartial-linked sweep regions (0.083636 of all classified regions)

# ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_76_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 6080 total instances
predicted 3438 neutral regions (0.565461 of all classified regions)
predicted 145 Hard sweep regions (0.023849 of all classified regions)
predicted 556 Hard-linked sweep regions (0.091447 of all classified regions)
predicted 314 Soft sweep regions (0.051645 of all classified regions)
predicted 731 Soft-linked sweep regions (0.120230 of all classified regions)
predicted 146 HardPartial sweep regions (0.024013 of all classified regions)
predicted 467 HardPartial-linked sweep regions (0.076809 of all classified regions)
predicted 47 SoftPartial sweep regions (0.007730 of all classified regions)
predicted 236 SoftPartial-linked sweep regions (0.038816 of all classified regions)

# ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 3836 total instances
predicted 2166 neutral regions (0.564651 of all classified regions)
predicted 90 Hard sweep regions (0.023462 of all classified regions)
predicted 319 Hard-linked sweep regions (0.083160 of all classified regions)
predicted 200 Soft sweep regions (0.052138 of all classified regions)
predicted 447 Soft-linked sweep regions (0.116528 of all classified regions)
predicted 92 HardPartial sweep regions (0.023983 of all classified regions)
predicted 322 HardPartial-linked sweep regions (0.083942 of all classified regions)
predicted 38 SoftPartial sweep regions (0.009906 of all classified regions)
predicted 162 SoftPartial-linked sweep regions (0.042231 of all classified regions)

```

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/
mkdir -p pred_empirical && cd pred_empirical
rsync -ahPv /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/*_110Lines.phased.noPIRs.inbred.fvec ./

for CONTIG in 542 594 628 718 76 785; do
  for i in 1 2 3 4 5 6 7 8 9 10; do
    python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
      /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
      ./ScA8VGg_${CONTIG}_110Lines.phased.noPIRs.inbred.fvec\
      11\
      92\
      ScA8VGg_${CONTIG}.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_fold_${i}.pred.bed >\
      empirical_deep_learning_classify_ScA8VGg_${CONTIG}_fold_${i}.log 2>&1
  done
done

```

#### summary prediction

```{bash}

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical

```

```{r}

library(tidyverse)
library(data.table)
library(RColorBrewer)
# library(ggplot2)
# library(dplyr)
# library(tidyr)
library(cowplot)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/pred_empirical")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
  pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
  mutate(pred_ensemble = class[prob==max(prob)]) %>% 
  pivot_wider(names_from=class, values_from=prob)



# a1 <- pred %>% 
#   extract('V4','State','(.*)_ScA8VGg.*') %>% 
#   mutate(V1=gsub('chr','', V1))


# a1_df <- data.table(table(a1$State))
a1_df <- data.table(table(ensemble$pred_ensemble))
# hard_num <- pred_df[V1=="hard",N]
# soft_num <- pred_df[V1=="soft",N]
# hard_pct <- hard_num/(hard_num + soft_num)
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

aplot[V1 %in% c('HardPartial','HardPartial-linked'),lbl2:=lbl]


PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
    geom_bar(width=1, stat="identity", color="white") +
    coord_polar("y", start=0) +
    geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
                  y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), label=lbl2), 
              color="black", size=3) +
    scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                               my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                               "grey60")) +
    theme_void() + theme(legend.position = "none")



a1 <- data.table(ensemble)

a1[,chr:="xxxx"][contig=="chrScA8VGg_76",chr:="3R"][contig=="chrScA8VGg_718",chr:="2L"][contig=="chrScA8VGg_542",chr:="2L"][contig=="chrScA8VGg_594",chr:="X"][contig=="chrScA8VGg_628",chr:="3L"][contig=="chrScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$pred_ensemble)) -> a3
a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))

# ggsave("diploSHIC_proprotion_figure1.pdf",PP, width = 8, height = 4, 
#        dpi = 300, units = "in")


# chisquare test
library(rcompanion)

bc <- table(a2$chr, a2$pred_ensemble)

b2 <- cbind(rowSums(bc[,c(1,3)]), rowSums(bc[,c(2,4:9)]))
# 
# b1 <- cbind(bc,other = rowSums(bc[,2:9]))
# b2 <- b1[,c(1,10)]
chisq.test(b2)
b2 <- matrix(c(29, 43, 59, 46, 56, 3600, 3876, 4249, 5688, 4857), ncol=2)
colnames(b2) <- c('hard','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")
apply(b2,1,prop.table)

# c1 <- cbind(bc,other = rowSums(bc[,c(1:5,7:9)]))
# c2 <- c1[,c(6,10)
c2 <- cbind(rowSums(bc[,c(6,8)]), rowSums(bc[,c(1:5,7,9)]))
chisq.test(c2)

c2 <- matrix(c(381, 452, 503, 579, 615, 3248, 3467, 3805, 5155, 4298), ncol=2)
colnames(c2) <- c('soft','others')
rownames(c2) <- c('2L','2R','3L','3R','X')
pairwiseNominalIndependence(c2, fisher = F, gtest = F, chisq = T, method = "fdr")
apply(c2,1,prop.table)

ph <- cbind(bc[,3], rowSums(bc[,c(1:2,4:9)]))
apply(ph,1,prop.table)

```

```
                  V1     N         pct
1:               Hard   202 0.008976581 202
2:        Hard-linked  1319 0.058614407 1327
3:        HardPartial    31 0.001377594 22
4: HardPartial-linked    67 0.002977381
5:            Neutral  3192 0.141847754 3195
6:               Soft  2181 0.096920411 2183
7:        Soft-linked 14685 0.652579656 14694
8:        SoftPartial   349 0.015509043 360
9: SoftPartial-linked   477 0.021197174

```

#### prob pred

```{shell}

# module load anaconda3
# source ~/conda-init
# conda activate r_env
# 
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe

```

```{r}

# library(tidyverse)
# library(data.table)
# 
# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")
# 
# beds <- list.files(pattern = 'ScA8VGg_.*.csv$')
# 
# aa <- data.table()
# 
# for(ff in beds){
#     tmp <- fread(ff, header = TRUE, sep = ",")
#     aa <- rbind(aa, tmp)
# }
# 
# aa %>% 
#   mutate(window = c(1:nrow(aa))) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   filter(pred=="Neutral") %>% 
#   ggplot(aes(x=prob, fill=class)) + 
#   geom_density(alpha=0.5) + 
#   facet_wrap(~class, scale="free_y")
# 
# aa %>% 
#   mutate(window = c(1:nrow(aa))) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   filter(pred=="Soft-linked") %>% 
#   ggplot(aes(x=prob, fill=class)) + 
#   geom_density(alpha=0.5) + 
#   facet_wrap(~class, scale="free_y")
# 
# # Create a new column "prob" with the highest probability
# aa$prob <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   sorted_probs[1]
# })
# 
# # Create a new column "pred2" with the second highest probability class
# aa$pred2 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   names(sorted_probs)[2]
# })
# 
# # Create a new column "prob2" with the second highest probability
# aa$prob2 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   sorted_probs[2]
# })
# 
# # Create a new column "pred3" with the third highest probability class
# aa$pred3 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   names(sorted_probs)[3]
# })
# 
# # Create a new column "prob3" with the third highest probability
# aa$prob3 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   sorted_probs[3]
# })

```

# ######################## PartialSHIC - constNe Ne 1M

```
lapply(str_split(names(aa),'_'), function(s) s[1]) %>% unique() %>% unlist
[1] "pi"                 "thetaW"             "tajD"               "thetaH"
[5] "fayWuH"             "HapCount"           "H1"                 "H12"
[9] "H2/H1"              "ZnS"                "Omega"              "iHSMean"
[13] "iHSMax"             "iHSOutFrac"         "nSLMean"            "nSLMax"
[17] "nSLOutFrac"         "distVar"            "distSkew"           "distKurt"
[21] "HAF-Mean"           "HAF-Median"         "HAF-Mode"           "HAF-Lower95%"
[25] "HAF-Lower50%"       "HAF-Upper50%"       "HAF-Upper95%"       "HAF-Max"
[29] "HAF-Var"            "HAF-SD"             "HAF-Skew"           "HAF-Kurt"
[33] "HAFunique-Mean"     "HAFunique-Median"   "HAFunique-Mode"     "HAFunique-Lower95%"
[37] "HAFunique-Lower50%" "HAFunique-Upper50%" "HAFunique-Upper95%" "HAFunique-Max"
[41] "HAFunique-Var"      "HAFunique-SD"       "HAFunique-Skew"     "HAFunique-Kurt"
[45] "phi-Mean"           "phi-Median"         "phi-Mode"           "phi-Lower95%"
[49] "phi-Lower50%"       "phi-Upper50%"       "phi-Upper95%"       "phi-Max"
[53] "phi-Var"            "phi-SD"             "phi-Skew"           "phi-Kurt"
[57] "kappa-Mean"         "kappa-Median"       "kappa-Mode"         "kappa-Lower95%"
[61] "kappa-Lower50%"     "kappa-Upper50%"     "kappa-Upper95%"     "kappa-Max"
[65] "kappa-Var"          "kappa-SD"           "kappa-Skew"         "kappa-Kurt"
[69] "SFS-Mean"           "SFS-Median"         "SFS-Mode"           "SFS-Lower95%"
[73] "SFS-Lower50%"       "SFS-Upper50%"       "SFS-Upper95%"       "SFS-Max"
[77] "SFS-Var"            "SFS-SD"             "SFS-Skew"           "SFS-Kurt"
[81] "SAFE-Mean"          "SAFE-Median"        "SAFE-Mode"          "SAFE-Lower95%"
[85] "SAFE-Lower50%"      "SAFE-Upper50%"      "SAFE-Upper95%"      "SAFE-Max"
[89] "SAFE-Var"           "SAFE-SD"            "SAFE-Skew"          "SAFE-Kurt"
```

```
From Yiguan:

Source code change log:
    
    - training_sample_FVs.py:line67, fixed to 2000

    - fix bugs in empirical_convert_to_FVs.py:24-28 in parsing VCF file format. ancestral fasta file must split by chromosome
```

## Generate training data

* Previously run by Yiguan

```{bash, OLD}
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP
conda activate r_env
```

```{r, DISCOAL, OLD}
setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/discoal/constNe')
options(scipen = 999)
library("parallel")
library("stringr")

# DISCOAL = '~/discoal/discoal' 
DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal'
TIMES = 2000

Ne = 1*10^6
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3

# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len

# selection coefficient s: 0.0001-0.01
# strength of selection: alpha = 2*Ne*s
s_low = 0.00001
s_high = 0.01
Pa_low <- 2*Ne*s_low
Pa_high <- 2*Ne*s_high

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
Pu_high <- 0.01

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2

# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

# For NEUTRAL
# sample size = 220
# numReplicates = TIMES = 10
# nSites = len = 55000
# -Pt = low high (prior on theta)
# -Pre mean upperBound (prior on rho -- exponentially distributed but truncated at an upper bound)
# runNeutral <- str_glue("{DISCOAL} 220 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} | gzip > spNeut.msOut.gz &")
# system.time(runNeutral)
runNeutral <- function(i){
    outname <- str_glue("spNeut_{i-1}.msOut.gz")
    sm_cmd <- str_glue("{DISCOAL} 220 {TIMES/16} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} | gzip > {outname}")
    cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:16, runNeutral, mc.cores = 16)

# For complete HARD
# sample size = 110
# numReplicates = TIMES = 10
# nSites = len = 55000
# -Pt = low high (prior on theta)
# -Pre mean upperBound (prior on rho -- exponentially distributed but truncated at an upper bound)
# -ws tau (sweep happend tau generations ago- stochastic sweep)
# -Pa low high (prior on alpha)
# -Pu low high (prior on tau; sweep models only; still must use "-ws tau" and "tau" will be ignored)
# -x sweepSite (0-1)
runHard <- function(i){
    outname <- str_glue("spHard_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -x {sweep_pos[i]} | gzip > {outname}")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -x {sweep_pos[i]} | gzip > {outname}")
    }
    
    cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runHard, mc.cores = 11)


# For complete SOFT
# sample size = 110
# numReplicates = TIMES = 10
# nSites = len = 55000
# -Pt = low high (prior on theta)
# -Pre mean upperBound (prior on rho -- exponentially distributed but truncated at an upper bound)
# -ws tau (sweep happend tau generations ago- stochastic sweep)
# -Pa low high (prior on alpha)
# -Pu low high (prior on tau; sweep models only; still must use "-ws tau" and "tau" will be ignored)
# -Pf low high (prior on F0; sweep models only)
# -x sweepSite (0-1)
runSoft <- function(i){
    outname <- str_glue("spSoft_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0  -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname}")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0  -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname}")
    }
    # cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runSoft, mc.cores = 11)


# for partial HARD
runPHard <- function(i){
    outname <- str_glue("spPartialHard_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -x {sweep_pos[i]} | gzip > {outname}")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -x {sweep_pos[i]} | gzip > {outname}")
    }
    
    # cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runPHard, mc.cores = 11)


# for partial SOFT
runPSoft <- function(i){
    outname <- str_glue("spPartialSoft_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname}")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname}")
    }
    
    #cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runPSoft, mc.cores = 11)

```

* Bunya

```{shell, install discoal}

cd /scratch/project/chenoase/partialSHIC_DsGRP

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=1\
 --mem=2G\
 --job-name=uqsalle3_TEST\
 --time=00:30:00\
 --partition=general\
 --account=a_chenoweth srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

git clone https://github.com/kr-colab/discoal.git

cd /scratch/project/chenoase/partialSHIC_DsGRP/discoal

make discoal

```

* Dell 2 port from R to bash

```{shell}

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

# setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat')

DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal' 
TIMES = 2000

# Ne = 3607692
Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
s_low = 0.00001
s_high = 0.01
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.002771855 # 0.002771855*4*3607692 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
# bottleneck <- "-en 0.007292 0 1.5 -en 0.02135 0 0.547 -en 0.05208 0 0.208 -en 0.09271 0 0.0805"

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
# bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
cat("bottleneck: ", bottleneck, "\n")
cat("sweep_pos: ", sweep_pos, "\n")

```

* Back to Bunya

```{shell, bunya example}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name=Neut_sim
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/slurm_discoal_Neutral.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/slurm_discoal_Neutral.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/

DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper | gzip > spNeut_${TIME}.msOut.gz
done

$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper | gzip > spNeut_1.msOut.gz

```

```{shell, linked}

# sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_0.slurm.sh
# sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_1.slurm.sh
# sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_2.slurm.sh
# sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_3.slurm.sh
# sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_4.slurm.sh
# sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_6.slurm.sh
# sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_7.slurm.sh
# sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_8.slurm.sh
# sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_9.slurm.sh
# sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.slurm.sh > discoal_Hard_10.slurm.sh
# 
# sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_0.slurm.sh
# sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_1.slurm.sh
# sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_2.slurm.sh
# sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_3.slurm.sh
# sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_4.slurm.sh
# sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_6.slurm.sh
# sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_7.slurm.sh
# sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_8.slurm.sh
# sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_9.slurm.sh
# sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.slurm.sh > discoal_Soft_10.slurm.sh
# 
# sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_0.slurm.sh
# sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_1.slurm.sh
# sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_2.slurm.sh
# sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_3.slurm.sh
# sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_4.slurm.sh
# sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_6.slurm.sh
# sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_7.slurm.sh
# sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_8.slurm.sh
# sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_9.slurm.sh
# sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.slurm.sh > discoal_HardPartial_10.slurm.sh
# 
# sed 's/_5/_0/g; s/\[5\]/\[0\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_0.slurm.sh
# sed 's/_5/_1/g; s/\[5\]/\[1\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_1.slurm.sh
# sed 's/_5/_2/g; s/\[5\]/\[2\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_2.slurm.sh
# sed 's/_5/_3/g; s/\[5\]/\[3\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_3.slurm.sh
# sed 's/_5/_4/g; s/\[5\]/\[4\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_4.slurm.sh
# sed 's/_5/_6/g; s/\[5\]/\[6\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_6.slurm.sh
# sed 's/_5/_7/g; s/\[5\]/\[7\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_7.slurm.sh
# sed 's/_5/_8/g; s/\[5\]/\[8\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_8.slurm.sh
# sed 's/_5/_9/g; s/\[5\]/\[9\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_9.slurm.sh
# sed 's/_5/_10/g; s/\[5\]/\[10\]/g; s/time\=24/time\=12/; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.slurm.sh > discoal_SoftPartial_10.slurm.sh



sed 's/\[5\]/\[0\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_0.txt
sed 's/\[5\]/\[1\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_1.txt
sed 's/\[5\]/\[2\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_2.txt
sed 's/\[5\]/\[3\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_3.txt
sed 's/\[5\]/\[4\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_4.txt
sed 's/\[5\]/\[6\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_6.txt
sed 's/\[5\]/\[7\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_7.txt
sed 's/\[5\]/\[8\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_8.txt
sed 's/\[5\]/\[9\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_9.txt
sed 's/\[5\]/\[10\]/;' template_discoal_Hard_5.txt > template_discoal_Hard_10.txt

sed 's/\[5\]/\[0\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_0.txt
sed 's/\[5\]/\[1\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_1.txt
sed 's/\[5\]/\[2\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_2.txt
sed 's/\[5\]/\[3\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_3.txt
sed 's/\[5\]/\[4\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_4.txt
sed 's/\[5\]/\[6\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_6.txt
sed 's/\[5\]/\[7\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_7.txt
sed 's/\[5\]/\[8\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_8.txt
sed 's/\[5\]/\[9\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_9.txt
sed 's/\[5\]/\[10\]/;' template_discoal_Soft_5.txt > template_discoal_Soft_10.txt

sed 's/\[5\]/\[0\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_0.txt
sed 's/\[5\]/\[1\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_1.txt
sed 's/\[5\]/\[2\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_2.txt
sed 's/\[5\]/\[3\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_3.txt
sed 's/\[5\]/\[4\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_4.txt
sed 's/\[5\]/\[6\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_6.txt
sed 's/\[5\]/\[7\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_7.txt
sed 's/\[5\]/\[8\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_8.txt
sed 's/\[5\]/\[9\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_9.txt
sed 's/\[5\]/\[10\]/;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_10.txt

sed 's/\[5\]/\[0\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_0.txt
sed 's/\[5\]/\[1\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_1.txt
sed 's/\[5\]/\[2\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_2.txt
sed 's/\[5\]/\[3\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_3.txt
sed 's/\[5\]/\[4\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_4.txt
sed 's/\[5\]/\[6\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_6.txt
sed 's/\[5\]/\[7\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_7.txt
sed 's/\[5\]/\[8\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_8.txt
sed 's/\[5\]/\[9\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_9.txt
sed 's/\[5\]/\[10\]/;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_10.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/

## Neutral

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Neutral.txt discoal_Neutral_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Neutral_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Hard_5.txt discoal_Hard_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Hard_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Soft_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Soft_5.txt discoal_Soft_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Soft_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## HardPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_HardPartial_5.txt discoal_HardPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_HardPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## SoftPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_SoftPartial_5.txt discoal_SoftPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_SoftPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"

  for i in {1..400}; do
    echo "$i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## Soft-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## HardPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## SoftPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

```

```{shell, submit}

## Neutral

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Neutral.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral.txt

## Hard_5

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Hard_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5.txt

## Soft_5

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Soft_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5.txt

## HardPartial_5

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_HardPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5.txt

## SoftPartial_5

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_SoftPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5.txt

## Hard-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## Soft-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## HardPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## SoftPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done



ls ./spNeut_*.msOut.gz | wc -l
ls ./spHard_5_*.msOut.gz | wc -l
ls ./spSoft_5_*.msOut.gz | wc -l
ls ./spHardPartial_5_*.msOut.gz | wc -l
ls ./spSoftPartial_5_*.msOut.gz | wc -l



ls ./spHard_0_*.msOut.gz | wc -l
ls ./spHard_1_*.msOut.gz | wc -l
ls ./spHard_2_*.msOut.gz | wc -l
ls ./spHard_3_*.msOut.gz | wc -l
ls ./spHard_4_*.msOut.gz | wc -l
ls ./spHard_6_*.msOut.gz | wc -l
ls ./spHard_7_*.msOut.gz | wc -l
ls ./spHard_8_*.msOut.gz | wc -l
ls ./spHard_9_*.msOut.gz | wc -l
ls ./spHard_10_*.msOut.gz | wc -l



ls ./spSoft_0_*.msOut.gz | wc -l
ls ./spSoft_1_*.msOut.gz | wc -l
ls ./spSoft_2_*.msOut.gz | wc -l
ls ./spSoft_3_*.msOut.gz | wc -l
ls ./spSoft_4_*.msOut.gz | wc -l
ls ./spSoft_6_*.msOut.gz | wc -l
ls ./spSoft_7_*.msOut.gz | wc -l
ls ./spSoft_8_*.msOut.gz | wc -l
ls ./spSoft_9_*.msOut.gz | wc -l
ls ./spSoft_10_*.msOut.gz | wc -l



ls ./spHardPartial_0_*.msOut.gz | wc -l
ls ./spHardPartial_1_*.msOut.gz | wc -l
ls ./spHardPartial_2_*.msOut.gz | wc -l
ls ./spHardPartial_3_*.msOut.gz | wc -l
ls ./spHardPartial_4_*.msOut.gz | wc -l
ls ./spHardPartial_6_*.msOut.gz | wc -l
ls ./spHardPartial_7_*.msOut.gz | wc -l
ls ./spHardPartial_8_*.msOut.gz | wc -l
ls ./spHardPartial_9_*.msOut.gz | wc -l
ls ./spHardPartial_10_*.msOut.gz | wc -l



ls ./spSoftPartial_0_*.msOut.gz | wc -l
ls ./spSoftPartial_1_*.msOut.gz | wc -l
ls ./spSoftPartial_2_*.msOut.gz | wc -l
ls ./spSoftPartial_3_*.msOut.gz | wc -l
ls ./spSoftPartial_4_*.msOut.gz | wc -l
ls ./spSoftPartial_6_*.msOut.gz | wc -l
ls ./spSoftPartial_7_*.msOut.gz | wc -l
ls ./spSoftPartial_8_*.msOut.gz | wc -l
ls ./spSoftPartial_9_*.msOut.gz | wc -l
ls ./spSoftPartial_10_*.msOut.gz | wc -l

```

```{shell, check and fix empty simulation}

## Neutral

grep 'Out Of Memory' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt
grep ' MAXMUTS' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt
cat list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt > list_discoal_files_Neutral_RERUN_10.txt
wc -l list_discoal_files_Neutral_RERUN_10.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral_RERUN_10.txt

## Hard_5

grep 'Out Of Memory' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Hard_5_RERUN_OutOfMemory.txt list_discoal_files_Hard_5_RERUN_MAXMUTS.txt > list_discoal_files_Hard_5_RERUN.txt
wc -l list_discoal_files_Hard_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5_RERUN.txt

## Soft_5

rm list_discoal_files_Soft_5_RERUN_9.txt
grep 'Out Of Memory' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_OutOfMemory_9.txt
grep ' MAXMUTS' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_MAXMUTS_9.txt
cat list_discoal_files_Soft_5_RERUN_OutOfMemory_9.txt list_discoal_files_Soft_5_RERUN_MAXMUTS_9.txt > list_discoal_files_Soft_5_RERUN_9.txt
wc -l list_discoal_files_Soft_5_RERUN_9.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5_RERUN_9.txt

## HardPartial_5

rm list_discoal_files_HardPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_HardPartial_5_RERUN.txt
wc -l list_discoal_files_HardPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5_RERUN.txt

## SoftPartial_5

rm list_discoal_files_SoftPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_SoftPartial_5_RERUN.txt
wc -l list_discoal_files_SoftPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5_RERUN.txt

## check all

rm list_discoal_files_RERUN.txt
grep 'Out Of Memory' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' >  list_discoal_files_RERUN_MAXMUTS.txt
cat list_discoal_files_RERUN_OutOfMemory.txt list_discoal_files_RERUN_MAXMUTS.txt > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt



find ./ -maxdepth 1 -type f -name '*.msOut.gz' -size -21c



```

```{shell, combine simulations}

# Function to process and combine sweep files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail ./${PREFIX}_*.msOut -n +3 > temp1.msOut
    perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
    awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/2000/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

# for PREFIX in spNeut spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
#     process_and_combine "$PREFIX"
# done

for PREFIX in spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
    process_and_combine "$PREFIX"
done



# Function to process and combine linked files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail -n +3 ./${PREFIX}_*.msOut > temp1.msOut
    grep -v "$PREFIX" temp1.msOut > temp2.msOut
    awk 'BEGIN { RS="\n\n"; ORS="\n" } { print }' temp2.msOut > temp3.msOut
    
    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/400/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

# DONE
for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    process_and_combine "$PREFIX"
done

# DONE
for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    process_and_combine "$PREFIX"
done

# DONE
for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    process_and_combine "$PREFIX"
done

# RUNNING
for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    process_and_combine "$PREFIX"
done

# # mini for testing
# 
# gzip -d spHard_5_1.msOut.gz
# gzip -d spHard_5_2.msOut.gz
# gzip -d spHard_5_3.msOut.gz
# 
# # Concatenate files and remove headers
# tail spHard_5_*.msOut -n +3 > temp1.msOut
# perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
# awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut
# 
# # Extract the first two lines from ${PREFIX}_1.msOut
# head -n 2 "spHard_5_1.msOut" > header.txt
# 
# # Edit the header to change the third space-delimited element from 1 to 2000
# sed -i '1s/\<1\>/3/' header.txt
# 
# # Concatenate the edited header with temp3.msOut
# cat header.txt temp3.msOut > spHard_5_COMBINED_MINI.msOut
# 
# # Move and compress the combined file
# gzip spHard_5_COMBINED_MINI.msOut
# 
# # Compress files
# find ./ -maxdepth 1 -type f -name "spHard_5_*.msOut" -exec gzip {} \;
# 
# # Clean up temporary files
# rm temp1.msOut temp2.msOut

```

## Generate test data

* Bunya

```{shell, install discoal}

cd /scratch/project/chenoase/partialSHIC_DsGRP

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=1\
 --mem=2G\
 --job-name=uqsalle3_TEST\
 --time=00:30:00\
 --partition=general\
 --account=a_chenoweth srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

git clone https://github.com/kr-colab/discoal.git

cd /scratch/project/chenoase/partialSHIC_DsGRP/discoal

make discoal

```

* Dell 2 port from R to bash

```{shell}

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

# setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat')

DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal' 
TIMES = 2000

Ne = 3607692
# Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000002771855
# s_high = 0.01
s_high = 0.002771855
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.002771855 # 0.002771855*4*3607692 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
# bottleneck <- "-en 0.007292 0 1.5 -en 0.02135 0 0.547 -en 0.05208 0 0.208 -en 0.09271 0 0.0805"

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
# bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
cat("bottleneck: ", bottleneck, "\n")
cat("sweep_pos: ", sweep_pos, "\n")

```

* Back to Bunya

```{shell, bunya example}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name=Neut_sim
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat/slurm_discoal_Neutral.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat/slurm_discoal_Neutral.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat

DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper | gzip > spNeut_${TIME}.msOut.gz
done

$DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper | gzip > spNeut_1.msOut.gz

```

```{shell, linked}

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_10.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat

## Neutral (DONE)

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Neutral.txt discoal_Neutral_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Neutral_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard_5 (DONE)

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Hard_5.txt discoal_Hard_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Hard_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Soft_5 (DONE)

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Soft_5.txt discoal_Soft_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Soft_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## HardPartial_5 (DONE)

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_HardPartial_5.txt discoal_HardPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_HardPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## SoftPartial_5 (DONE)

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_SoftPartial_5.txt discoal_SoftPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_SoftPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## Soft-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## HardPartial-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## SoftPartial-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

```

```{shell, submit}

## Neutral (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Neutral.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral.txt

## Hard_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Hard_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5.txt

## Soft_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Soft_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5.txt

## HardPartial_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_HardPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5.txt

## SoftPartial_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_SoftPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5.txt

## Hard-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## Soft-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## HardPartial-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## SoftPartial-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done



ls ./spNeut_*.msOut.gz | wc -l
ls ./spHard_5_*.msOut.gz | wc -l
ls ./spSoft_5_*.msOut.gz | wc -l
ls ./spHardPartial_5_*.msOut.gz | wc -l
ls ./spSoftPartial_5_*.msOut.gz | wc -l



ls ./spHard_0_*.msOut.gz | wc -l
ls ./spHard_1_*.msOut.gz | wc -l
ls ./spHard_2_*.msOut.gz | wc -l
ls ./spHard_3_*.msOut.gz | wc -l
ls ./spHard_4_*.msOut.gz | wc -l
ls ./spHard_6_*.msOut.gz | wc -l
ls ./spHard_7_*.msOut.gz | wc -l
ls ./spHard_8_*.msOut.gz | wc -l
ls ./spHard_9_*.msOut.gz | wc -l
ls ./spHard_10_*.msOut.gz | wc -l



ls ./spSoft_0_*.msOut.gz | wc -l
ls ./spSoft_1_*.msOut.gz | wc -l
ls ./spSoft_2_*.msOut.gz | wc -l
ls ./spSoft_3_*.msOut.gz | wc -l
ls ./spSoft_4_*.msOut.gz | wc -l
ls ./spSoft_6_*.msOut.gz | wc -l
ls ./spSoft_7_*.msOut.gz | wc -l
ls ./spSoft_8_*.msOut.gz | wc -l
ls ./spSoft_9_*.msOut.gz | wc -l
ls ./spSoft_10_*.msOut.gz | wc -l



ls ./spHardPartial_0_*.msOut.gz | wc -l
ls ./spHardPartial_1_*.msOut.gz | wc -l
ls ./spHardPartial_2_*.msOut.gz | wc -l
ls ./spHardPartial_3_*.msOut.gz | wc -l
ls ./spHardPartial_4_*.msOut.gz | wc -l
ls ./spHardPartial_6_*.msOut.gz | wc -l
ls ./spHardPartial_7_*.msOut.gz | wc -l
ls ./spHardPartial_8_*.msOut.gz | wc -l
ls ./spHardPartial_9_*.msOut.gz | wc -l
ls ./spHardPartial_10_*.msOut.gz | wc -l



ls ./spSoftPartial_0_*.msOut.gz | wc -l
ls ./spSoftPartial_1_*.msOut.gz | wc -l
ls ./spSoftPartial_2_*.msOut.gz | wc -l
ls ./spSoftPartial_3_*.msOut.gz | wc -l
ls ./spSoftPartial_4_*.msOut.gz | wc -l
ls ./spSoftPartial_6_*.msOut.gz | wc -l
ls ./spSoftPartial_7_*.msOut.gz | wc -l
ls ./spSoftPartial_8_*.msOut.gz | wc -l
ls ./spSoftPartial_9_*.msOut.gz | wc -l
ls ./spSoftPartial_10_*.msOut.gz | wc -l

```

```{shell, check and fix empty simulation}

## Neutral (DONE)

rm list_discoal_files_Neutral_RERUN_10.txt
grep 'Out Of Memory' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt
grep ' MAXMUTS' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt
cat list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt > list_discoal_files_Neutral_RERUN_10.txt
wc -l list_discoal_files_Neutral_RERUN_10.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral_RERUN_10.txt

## Hard_5 (DONE)

rm list_discoal_files_Hard_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Hard_5_RERUN_OutOfMemory.txt list_discoal_files_Hard_5_RERUN_MAXMUTS.txt > list_discoal_files_Hard_5_RERUN.txt
wc -l list_discoal_files_Hard_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5_RERUN.txt

## Soft_5 (DONE)

rm list_discoal_files_Soft_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Soft_5_RERUN_OutOfMemory.txt list_discoal_files_Soft_5_RERUN_MAXMUTS.txt > list_discoal_files_Soft_5_RERUN.txt
wc -l list_discoal_files_Soft_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5_RERUN.txt

## HardPartial_5 (DONE)

rm list_discoal_files_HardPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_HardPartial_5_RERUN.txt
wc -l list_discoal_files_HardPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5_RERUN.txt

## SoftPartial_5 (DONE)

rm list_discoal_files_SoftPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_SoftPartial_5_RERUN.txt
wc -l list_discoal_files_SoftPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5_RERUN.txt

## check all

rm list_discoal_files_RERUN.txt
grep 'Out Of Memory' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' >  list_discoal_files_RERUN_MAXMUTS.txt
cat list_discoal_files_RERUN_OutOfMemory.txt list_discoal_files_RERUN_MAXMUTS.txt > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt


rm list_discoal_files_RERUN.txt
find ./ -maxdepth 1 -type f -name '*.msOut.gz' -size -21c | sed 's/\.\/sp/discoal_/; s/msOut\.gz/slurm\.sh/; s/Neut/Neutral/;' > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt

```

```{shell, combine simulations}

# Function to process and combine sweep files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail ./${PREFIX}_*.msOut -n +3 > temp1.msOut
    perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
    awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/2000/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spHard_5; do
    process_and_combine "$PREFIX"
done

for PREFIX in spNeut spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
    process_and_combine "$PREFIX"
done



# Function to process and combine linked files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail -n +3 ./${PREFIX}_*.msOut > temp1.msOut
    grep -v "$PREFIX" temp1.msOut > temp2.msOut
    awk 'BEGIN { RS="\n\n"; ORS="\n" } { print }' temp2.msOut > temp3.msOut
    
    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/400/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    process_and_combine "$PREFIX"
done

# # mini for testing
# 
# gzip -d spHard_5_1.msOut.gz
# gzip -d spHard_5_2.msOut.gz
# gzip -d spHard_5_3.msOut.gz
# 
# # Concatenate files and remove headers
# tail spHard_5_*.msOut -n +3 > temp1.msOut
# perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
# awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut
# 
# # Extract the first two lines from ${PREFIX}_1.msOut
# head -n 2 "spHard_5_1.msOut" > header.txt
# 
# # Edit the header to change the third space-delimited element from 1 to 2000
# sed -i '1s/\<1\>/3/' header.txt
# 
# # Concatenate the edited header with temp3.msOut
# cat header.txt temp3.msOut > spHard_5_COMBINED_MINI.msOut
# 
# # Move and compress the combined file
# gzip spHard_5_COMBINED_MINI.msOut
# 
# # Compress files
# find ./ -maxdepth 1 -type f -name "spHard_5_*.msOut" -exec gzip {} \;
# 
# # Clean up temporary files
# rm temp1.msOut temp2.msOut

```

## Run training

### Training - calcStatsAndDafForEachSnp

```{python, calcStatsAndDafForEachSnpSingleMsFile_Python3}

import sys
import allel
import random
import numpy as np
from msTools import *
from fvTools import *
import time

'''usage example
python calcStatsAndDafForEachSnpSingleMsFile.py /san/data/dan/simulations/discoal_multipopStuff/spatialSVMSims/trainingSets/equilibNeut.msout.gz 110000 iHS nSL
'''

trainingDataFileName, totalPhysLen = sys.argv[1:3]
totalPhysLen = int(totalPhysLen)

hapArraysIn, positionArrays = msOutToHaplotypeArrayIn(trainingDataFileName, totalPhysLen)
numInstances = len(hapArraysIn)

header = "daf\tiHS\tnSL"
#print header # Python 2
print(header) # Python 3

dafs = []
#start = time.clock() # Python 2
start = time.perf_counter() # Python 3
numInstancesDone = 0

numSnpsDone = 0
for instanceIndex in range(numInstances):
    if instanceIndex % 10 == 0:
        sys.stderr.write("done wtih %d of %d instances (%d SNPs)\n" %(instanceIndex, numInstances, numSnpsDone))
    haps = allel.HaplotypeArray(hapArraysIn[instanceIndex], dtype='i1')
    genos = haps.to_genotypes(ploidy=2)
    ac = genos.count_alleles()
    sampleSizes = [sum(x) for x in ac]
    assert len(set(sampleSizes)) == 1
    dafs = ac[:,1]/float(sampleSizes[0])
    ihsVals = allel.stats.selection.ihs(haps, positionArrays[instanceIndex], use_threads=True, include_edges=False)
    nslVals = allel.stats.selection.nsl(haps, use_threads=True)
    assert len(dafs) == len(ihsVals) == len(nslVals)
    numSnpsDone += len(dafs)
    for i in range(len(dafs)):
        #print "%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i]) # Python 2
        print ("%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i])) #Python 3
    if numSnpsDone > 5000000:
        break

```

```{shell, bunya template}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=32G
#SBATCH --job-name=[SAMPLE]
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/contsNe_intergenic/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/

msOut="[SAMPLE]"

srun python /home/uqsalle3/Programs/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/

find ./ -maxdepth 1 -type f -name '*_COMBINED.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_calcStatsAndDafForEachSnpSingleMsFile_Python3.txt calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
done < msOut_files.txt

find ./ -maxdepth 1 -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*.slurm.sh' -print0 | xargs -0 chmod 755

ls ./calcStatsAndDafForEachSnpSingleMsFile_Python3_*.slurm.sh

```

```{shell, submit scripts Bunya}

while read p; do
  echo "calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh submitted"
  sbatch calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
  sleep 0.25
done < msOut_files.txt

```

### Training - training_convert_to_FVs

```{python, training_convert_to_FVs_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
from msTools import *
from fvTools import *
import random
import allel
import numpy as np
import math

# '''usage eg:
# pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
# python2 training_convert_to_FVs.py spNeut.msOut.gz 2L,2R,3L,3R 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.fa ./ spNeut.msOut.fvec
# '''
# 
# if len(sys.argv)!=12:
#   sys.exit("usage:\npython2 training_convert_to_FVs.py trainingDataFileName chrArmsForMasking subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName statDir fvecFileName\n")
# else:
#   trainingDataFileName, chrArmsForMasking, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, statDir, fvecFileName = sys.argv[1:]

trainingDataFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5.msOut.gz"
chrArmsForMasking="ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140"
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_stats.txt"
maskFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta"
# ancestralArmFaFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/top6.anc.fa"
ancestralArmFaFileName="none"
statDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/"
fvecFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_test.fvec"

subWinSize,numSubWins = int(subWinSize),int(numSubWins)
assert subWinSize>0 and numSubWins>1

totalPhysLen=(subWinSize*numSubWins)

hapArraysIn,positionArrays=msOutToHaplotypeArrayIn(trainingDataFileName,totalPhysLen)

chrArmsForMasking=chrArmsForMasking.split(",")

unmaskedFracCutoff,pMisPol=float(unmaskedFracCutoff),float(pMisPol)

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")


if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")


standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none", "false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*totalPhysLen
  sys.stderr.write("Warning: not doing any masking! (i.e. mask.fa file for the chr arm with all masked sites N'ed out, or at least the reference with Ns, is not provided)\n")
  maskFileName=False
else:
  if ancestralArmFaFileName.lower() in ["none", "false"]:
    maskData=readMaskDataForTraining(maskFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  else:
    maskData=readMaskAndAncDataForTraining(maskFileName,ancestralArmFaFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  if len(maskData)<len(hapArraysIn):
    sys.stderr.write("Warning: not enough windows from masked data (needed %d; got %d); will draw with replacement!!\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=True
  else:
    sys.stderr.write("enough windows from masked data (needed %d; got %d); will draw without replacement.\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=False


# for i in range(len(maskData)):
#   print(str(i) + " " + str(len(maskData[i])) + " " + str(sum(maskData[i])))

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

header=[]

for statName in statNames:
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName, i))

header="\t".join(header)

statVals={}

for statName in statNames:
  statVals[statName]=[]

def getSubWinBounds(subWinSize,totalPhysLen):
  subWinStart=1-subWinSize
  subWinEnd=0
  subWinBounds=[]
  for i in range(0,(numSubWins-1)):
    subWinStart+=subWinSize
    subWinEnd+=subWinSize
    subWinBounds.append((subWinStart,subWinEnd))
  subWinStart+=subWinSize
  subWinEnd=totalPhysLen
  subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,totalPhysLen)

def getSnpIndicesInSubWins(subWinBounds,positions):
  snpIndicesInSubWins=[]
  for subWinIndex in range(len(subWinBounds)):
    snpIndicesInSubWins.append([])
  subWinIndex=0
  for i in range(len(positions)):
    while not (positions[i]>=subWinBounds[subWinIndex][0] and positions[i]<=subWinBounds[subWinIndex][1]):
      subWinIndex+=1
    snpIndicesInSubWins[subWinIndex].append(i)
  return snpIndicesInSubWins

quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

if statDir.lower() in ["none","false","default"]:
  statDir='./'

statFiles=[]

for subWinIndex in range(numSubWins):
  statFileName='/'.join(("%s/%s.subWin%d.stats" %(statDir, trainingDataFileName.split("/")[-1].replace(".msOut.gz",""), subWinIndex)).split('//'))
  statFiles.append(open(statFileName,"w"))
  statFiles[-1].write("\t".join(statNames)+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  fvecFileName="%s.fvec" %trainingDataFileName.split("/")[-1].replace(".msOut.gz","")

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

# for testing for loop below only
instanceIndex=0
statName='pi'

for instanceIndex in range(len(hapArraysIn)):
  print("Starting index " + str(instanceIndex))
  for statName in statNames:
    statVals[statName].append([])
  if maskFileName:
    if drawWithReplacement:
      unmasked=random.choice(maskData)
    else:
      unmasked=maskData[instanceIndex]
    assert len(unmasked)==totalPhysLen
    print(str(sum(unmasked)) + " Loci unmasked")
  snpIndicesToKeep=[x for x in range(len(positionArrays[instanceIndex])) if unmasked[positionArrays[instanceIndex][x]-1]]
  if len(snpIndicesToKeep)==0:
    for subWinIndex in range(numSubWins):
      for statName in statNames:
        print("No SNPs in BIGwindow, appendStatValsForMonomorphic")
        appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  else:
    haps=allel.HaplotypeArray(hapArraysIn[instanceIndex],dtype='i1').subset(sel0=snpIndicesToKeep)
    if pMisPol>0:
      misPolarizeCorrectionIndex=np.random.binomial(1,pMisPol,len(haps))
      for i in range(len(misPolarizeCorrectionIndex)):
        if misPolarizeCorrectionIndex[i]==1:
          for j in range(len(haps[i])):
            if haps[i][j]==0:
              haps[i][j]=1
            else:
              haps[i][j]=0
    genos=haps.to_genotypes(ploidy=2)
    alleleCounts=genos.count_alleles()
    positions=[positionArrays[instanceIndex][x] for x in snpIndicesToKeep]
    precomputedStats={}
    sampleSizes=[sum(x) for x in alleleCounts]
    assert len(set(sampleSizes))==1
    dafs=alleleCounts[:,1]/float(sampleSizes[0])
    ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
    nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
    nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
    # sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    if nonNanCount==0:
      precomputedStats["iHS"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["iHS"].append([])
    else:
      ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
      precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)
    nslVals=allel.stats.selection.nsl(haps,use_threads=False)
    nonNanCount=[x for x in np.isnan(nslVals)].count(False)
    # sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    if nonNanCount==0:
      precomputedStats["nSL"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["nSL"].append([])
    else:
      nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
      precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)
    snpIndicesInSubWins=getSnpIndicesInSubWins(subWinBounds,positions)
    for subWinIndex in range(numSubWins):
      subWinStart,subWinEnd = subWinBounds[subWinIndex]
      unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
      assert unmaskedFrac>=unmaskedFracCutoff
      if len(snpIndicesInSubWins[subWinIndex])==0:
        print("No SNPS in " + str(subWinIndex) + ", appendStatValsForMonomorphic")
        for statName in statNames:
          appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      else:
        print(str(snpIndicesInSubWins[subWinIndex])) + " SNPs in " + str(subWinIndex) + ", calcAndAppendStatVal")
        hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
        for statName in [x for x in statNames if x[0:3]!="HAF" and x[0:3]!="phi" and x[0:3]!="kap" and x[0:3]!="SFS" and x[0:3]!="SAF"]:
          calcAndAppendStatVal(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, instanceIndex, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        haplotypes={}
        for i in range(len(hapsInSubWin[0])):
          haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
          haplotype="".join(str(x) for x in haplotype)
          if haplotype in haplotypes:
            haplotypes[haplotype].append(i)
          else:
            haplotypes[haplotype]=[i]
        HAF=[]
        HAFunique={}
        for i in haplotypes:
          HAFunique[i]=0
          for j in range(len(hapsInSubWin)):
            if hapsInSubWin[j][haplotypes[i][0]]==1:
              HAFunique[i]+=sum([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j]))])
          for j in range(len(haplotypes[i])):
            HAF.append(HAFunique[i])
        phi=[]
        kappa=[]
        SAFE=[]
        for i in range(len(hapsInSubWin)):
          phi.append(0)
          kappa.append([])
          phiDenom=0
          for j in haplotypes:
            phi[i]+=(int(list(j)[i])*HAFunique[j]*len(haplotypes[j]))
            if int(list(j)[i])==1 and HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
              kappa[i].append(HAFunique[j])
            phiDenom+=(HAFunique[j]*len(haplotypes[j]))
          phi[i]/=float(phiDenom)
          kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
          if dafs[snpIndicesInSubWins[subWinIndex]][i]==0 or dafs[snpIndicesInSubWins[subWinIndex]][i]==1:
           SAFE.append(0.0)
          else:
           SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafs[snpIndicesInSubWins[subWinIndex]][i]*(1-dafs[snpIndicesInSubWins[subWinIndex]][i]))))
        for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
          if i=="SFS":
            windowStats=dafs[snpIndicesInSubWins[subWinIndex]]
          elif i=="HAFunique":
            # windowStats=[eval(i)[x] for x in eval(i)]
            windowStats = list(HAFunique.values()) # SCOTT
          else:
            windowStats=eval(i)
          statVals[i+"-Mean"][instanceIndex].append(np.mean(windowStats))
          statVals[i+"-Median"][instanceIndex].append(np.median(windowStats))
          if(len(np.unique(windowStats,return_counts=True)[1])==1):
            statVals[i+"-Mode"][instanceIndex].append(windowStats[0])
          else:
            if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
              statVals[i+"-Mode"][instanceIndex].append(scipy.stats.mstats.mode(windowStats)[0][0])
            else:
              mode=min(windowStats)
              for j in range(1,51):
                if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
                  mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
              statVals[i+"-Mode"][instanceIndex].append(mode+((max(windowStats)-min(windowStats))/100))
          for j in quantiles:
            statVals[i+"-"+j][instanceIndex].append(np.percentile(windowStats,quantiles[j]))
          statVals[i+"-Max"][instanceIndex].append(max(windowStats))
          statVals[i+"-Var"][instanceIndex].append(np.var(windowStats))
          statVals[i+"-SD"][instanceIndex].append(np.std(windowStats))
          statVals[i+"-Skew"][instanceIndex].append(scipy.stats.skew(windowStats))
          statVals[i+"-Kurt"][instanceIndex].append(scipy.stats.kurtosis(windowStats))
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][instanceIndex])
  fvecFile.write("\t".join([str(x) for x in outVec])+"\n")

for subWinIndex in range(numSubWins):
  statFiles[subWinIndex].close()
fvecFile.close()
# sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, bunya template Neutral}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=8G
#SBATCH --job-name=[SAMPLE]
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_training_convert_to_FVs_Python3_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/slurm_training_convert_to_FVs_Python3_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/

msOut="[SAMPLE]"

srun python /home/uqsalle3/Programs/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spNeut_COMBINED_stats.txt\
  /scratch/project/chenoase/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell}

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spHard_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_10.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat

find ./ -maxdepth 1 -type f -name '*.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt



# Neutral

grep 'Neut' msOut_files.txt > temp.txt
grep -v 'COMBINED' temp.txt > msOut_files_Neutral.txt
rm temp.txt

while read p; do
  echo "$p"
  cp template_training_convert_to_FVs_Python3_Neutral.txt training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
done < msOut_files_Neutral.txt



# Function to process files based on the specified prefix
process_files() {
    local prefix="$1"
    local template="template_training_convert_to_FVs_Python3_${prefix}.txt"
    local output_file="msOut_files_${prefix}.txt"

    # Create a list of files based on the prefix
    grep "${prefix}_" msOut_files.txt > temp.txt
    grep -v 'COMBINED' temp.txt > "${output_file}"
    rm temp.txt

    # Process each file in the list
    while read -r p; do
        echo "$p"
        cp "${template}" "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh"
        replace_CMD="s/\[SAMPLE\]/${p}/g"
        perl -pi -e "$replace_CMD" "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh"
    done < "${output_file}"
}

# Loop through different prefixes
for i in {0..10}; do
    process_files "Hard_${i}"
done

for i in {0..10}; do
    process_files "Soft_${i}"
done

for i in {0..10}; do
    process_files "HardPartial_${i}"
done

for i in {0..10}; do
    process_files "SoftPartial_${i}"
done



find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*.slurm.sh' -print0 | xargs -0 chmod 755

```

```{shell, submit scripts Bunya}

# RUNNING
while read p; do
  echo "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh submitted"
  sbatch training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
  sleep 0.05
done < msOut_files_Neutral.txt

# Function to submit files based on the specified prefix
submit_jobs() {
    local prefix="$1"
    echo "training_convert_to_FVs_Python3_${prefix}.slurm.sh submitted"
    sbatch training_convert_to_FVs_Python3_${prefix}.slurm.sh
    sleep 0.05
}

training_convert_to_FVs_Python3_spHard_0_5.slurm.sh

# Loop through different prefixes

# RUNNING
for i in {1..2000}; do
    submit_jobs "spHard_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spSoft_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spHardPartial_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spSoftPartial_5_${i}"
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spHard_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spSoft_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spHardPartial_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spSoftPartial_${i}_${j}"
    done
done



ls ./spNeut_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_5_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spHard_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spSoft_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spHardPartial_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spSoftPartial_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_10_*.fvec | grep -v 'COMBINED' | wc -l

## rerun failed

grep 'error' ./slurm_training_convert_to_FVs_Python3_sp*.error | perl -ne 's/\.\/slurm_/sbatch\ /; s/\.msOut\.gz.*/\.slurm\.sh/; print $_;' | uniq > rerun_failed_training_convert_to_FVs_Python3.sh
cat rerun_failed_training_convert_to_FVs_Python3.sh

bash rerun_failed_training_convert_to_FVs_Python3.sh

```

```{shell, combined fvec}

process_files() {
  local PREFIX="$1"
  find ./ -maxdepth 1 -type f | grep -E "${PREFIX}_[0-9]+\.fvec" > temp.txt
  
  while read -r p; do
    wc ${p}
  done < temp.txt
  
  while read -r p; do
    awk 'NR == 2' ${p}
  done < temp.txt > temp.fvec

  # Extract the first line from ${PREFIX}_1.msOut
  head -n 1 "${PREFIX}_1.fvec" > header.txt

  # Concatenate the edited header with temp.fvec
  cat header.txt temp.fvec > "${PREFIX}_SINGLE_COMBINED.fvec"
}

rm -v ./spNeut_SINGLE_COMBINED.fvec
for PREFIX in spNeut; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spHard_*_SINGLE_COMBINED.fvec
for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_5 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spSoft_*_SINGLE_COMBINED.fvec
for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_5 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spHardPartial_*_SINGLE_COMBINED.fvec
for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_5 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spSoftPartial_*_SINGLE_COMBINED.fvec
for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_5 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done



find ./ -maxdepth 1 -type f -name '*_SINGLE_COMBINED.fvec' -exec wc {} \;

```

```{shell, transfer to Dell 2}

# constNe_intergenic

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/*_SINGLE_COMBINED.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat/*_SINGLE_COMBINED.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat/

# There should be 45 fvec files
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/*.fvec | wc -l
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat/*.fvec | wc -l

```

```{r, assess feature vectors}

# module load anaconda3
# source ~/conda-init
# conda activate r_env

library(tidyverse)
library(data.table)

# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic")
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat")

spHard_5 <- read_tsv("spHard_5_SINGLE_COMBINED.fvec")

spHard_5 %>% 
  mutate(sim=c(1:nrow(spHard_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

spSoft_5 <- read_tsv("spSoft_5_SINGLE_COMBINED.fvec")

spSoft_5 %>% 
  mutate(sim=c(1:nrow(spSoft_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

```

### Training - training_sample_FVs

```{python, training_sample_FVs.py}

# import sys,os,random
import sys # SCOTT
import os # SCOTT
import random # SCOTT

'''usage eg:
python2 training_sample_FVs.py spNeut.fvec spHard_ spSoft_ spPartialHard_ spPartialSoft_ 5 0,1,2,3,4,6,7,8,9,10 ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec
'''

if len(sys.argv)!=10:
  sys.exit("usage:\npython2 training_sample_FVs.py neutTrainingFileName hardTrainingFilesPrefix softTrainingFilesPrefix partialHardTrainingFilesPrefix partialSoftTrainingFilesPrefix sweepTrainingWindow linkedTrainingWindows sampledFVsDir sampledFVsFiles\n")
else:
  neutTrainingFileName, hardTrainingFilesPrefix, softTrainingFilesPrefix, partialHardTrainingFilesPrefix, partialSoftTrainingFilesPrefix, sweepTrainingWindow, linkedTrainingWindows, sampledFVsDir, sampledFVsFiles = sys.argv[1:]

neutTrainingFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spNeut.fvec"
hardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_"
softTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spSoft_"
partialHardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialHard_"
partialSoftTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialSoft_"
sweepTrainingWindow=5
linkedTrainingWindows=0,1,2,3,4,6,7,8,9,10
sampledFVsDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe"
sampledFVsFiles="neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec"

sweepFilePaths,linkedFilePaths = {},{}
for trainingFilePrefix in [hardTrainingFilesPrefix,softTrainingFilesPrefix,partialHardTrainingFilesPrefix,partialSoftTrainingFilesPrefix]:
  trainingSetDir="/".join(trainingFilePrefix.split("/")[:-1])
  trainingFilePrefixDirless=trainingFilePrefix.split("/")[-1]
  sweepFilePaths[trainingFilePrefix]=[]
  linkedFilePaths[trainingFilePrefix]=[]
  for fileName in os.listdir(trainingSetDir):
    if fileName.startswith(trainingFilePrefixDirless):
      winNum=int(fileName.split("_")[1].split(".")[0])
      if winNum==int(sweepTrainingWindow):
        sweepFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)
      elif winNum in [int(x) for x in linkedTrainingWindows.split(",")]:
        linkedFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)

def getExamplesFromFVFile(simFileName):
  try:
    simFile=open(simFileName)
    lines=[line.strip() for line in simFile.readlines()]
    header=lines[0]
    examples=lines[1:]
    simFile.close()
    return header,examples
  except Exception:
    return "",[]

def getExamplesFromFVFileLs(simFileLs):
  examples=[]
  keptHeader=""
  for filePath in simFileLs:
    header,currExamples=getExamplesFromFVFile(filePath)
    if header:
      keptHeader=header
    examples+=currExamples
  return keptHeader,examples

header,neutExamples=getExamplesFromFVFile(neutTrainingFileName)
hardHeader,hardExamples=getExamplesFromFVFileLs(sweepFilePaths[hardTrainingFilesPrefix])
linkedHardHeader,linkedHardExamples=getExamplesFromFVFileLs(linkedFilePaths[hardTrainingFilesPrefix])
softHeader,softExamples=getExamplesFromFVFileLs(sweepFilePaths[softTrainingFilesPrefix])
linkedSoftHeader,linkedSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[softTrainingFilesPrefix])
partialHardHeader,partialHardExamples=getExamplesFromFVFileLs(sweepFilePaths[partialHardTrainingFilesPrefix])
linkedPartialHardHeader,linkedPartialHardExamples=getExamplesFromFVFileLs(linkedFilePaths[partialHardTrainingFilesPrefix])
partialSoftHeader,partialSoftExamples=getExamplesFromFVFileLs(sweepFilePaths[partialSoftTrainingFilesPrefix])
linkedPartialSoftHeader,linkedPartialSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[partialSoftTrainingFilesPrefix])

def getMinButNonZeroExamples(lsLs):
  counts=[]
  for ls in lsLs:
    if len(ls)>0:
      counts.append(len(ls))
  if not counts:
    raise Exception
  return min(counts)

trainingSetLs=[hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
numExamplesToKeep=getMinButNonZeroExamples(trainingSetLs)
for i in range(len(trainingSetLs)):
  random.shuffle(trainingSetLs[i])
  trainingSetLs[i]=trainingSetLs[i][:numExamplesToKeep]
hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples=trainingSetLs

if sampledFVsDir.lower() in ["none","false","default"]:
  sampledFVsDir='./'
if sampledFVsFiles.lower() in ["none","false","default"]:
  sampledFVsFiles=["neut.fvec","hard.fvec","linkedHard.fvec","soft.fvec","linkedSoft.fvec","partialHard.fvec","linkedPartialHard.fvec","partialSoft.fvec","linkedPartialSoft.fvec"]
else:
  sampledFVsFiles=sampledFVsFiles.split(",")
  assert len(sampledFVsFiles)==9
outExamples=[neutExamples,hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
for i in range(len(sampledFVsFiles)):
  if outExamples[i]:
    outFile=open('/'.join((sampledFVsDir+'/'+sampledFVsFiles[i]).split('//')),"w")
    outFile.write("classLabel\t%s\n" %(hardHeader))
    for example in outExamples[i]:
      outFile.write("%s\t%s\n" %(sampledFVsFiles[i].replace(".fvec",""),example))
    outFile.close()

```

```{shell, test Dell 2}

# constNe_intergenic

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs

for file in *_SINGLE_COMBINED.fvec; do
  mv "$file" "${file/_SINGLE_COMBINED.fvec/.fvec}"
done

for file in spHardPartial_*.fvec; do
  mv "$file" "${file/spHardPartial_/spPartialHard_}"
done

for file in spSoftPartial_*.fvec; do
  mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
done

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

```

### Training

```{shell, test Dell 2}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe
mkdir -p FVs && cd ./FVs

# for i in 1 2 3 4 5 6 7 8 9 10; do
  time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    constNe.hdf5\
    constNe.json\
    constNe.npy > training_deep_learning_python3.log 2>&1
# done

tensorboard --logdir logs

# time on 40 cores ~12 minutes with 5GB RAM

```

#### Training with early stopping

```{shell, test Dell 2}

# module load anaconda3
# source ~/conda-init
# conda activate partialshic_py3
# 
# # expansionNe
# 
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
# mkdir -p FVs && cd ./FVs
# mkdir -p earlyStopping && cd ./earlyStopping
# rm ./*
# cp ../*.fvec
# 
# time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_test.hdf5\
#     expansionNe_earlyStopping_test.json\
#     expansionNe_earlyStopping_test.npy
# 
# for i in 1 2 3 4 5 6 7 8 9 10; do
#   time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_${i}.hdf5\
#     expansionNe_earlyStopping_${i}.json\
#     expansionNe_earlyStopping_${i}.npy > training_deep_learning_python3_earlyStopping_${i}.log 2>&1
# done
# 
# # time on 40 cores ~12 minutes with 5GB RAM

```

#### Training with early stopping and 10-fold CV

```{shell, test Dell 2}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
rm ./*
cp ../*.fvec ./

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping_kFold.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    constNe_earlyStopping_fold.hdf5\
    constNe_earlyStopping_fold.json\
    constNe_earlyStopping_fold.npy

tensorboard --logdir logs

# for i in 1 2 3 4 5 6 7 8 9 10; do
#   time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     constNe_earlyStopping_${i}.hdf5\
#     constNe_earlyStopping_${i}.json\
#     constNe_earlyStopping_${i}.npy > training_deep_learning_python3_earlyStopping_${i}.log 2>&1
# done

# time on 40 cores ~12 minutes with 5GB RAM

```

### Run testing

```{python}

import time
# startTime=time.clock()
startTime = time.perf_counter() # Python 3
import sys,os
import keras
import numpy as np
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import pandas as pd

'''usage eg:
python3 testing_deep_learning_classify.py training.npy ./ 11 89 ./ accuracy confusion_matrix.pdf
'''

if len(sys.argv)!=8:
  sys.exit("usage:\npython3 testing_deep_learning_classify.py classifierPickleFileName fvecDir numSubWins numSumStatsPerSubWin resultsDir accuracyFilesPrefix confusionMatrixFigFileName\n")
else:
  classifierPickleFileName, fvecDir, numSubWins, numSumStatsPerSubWin, resultsDir, accuracyFilesPrefix, confusionMatrixFigFileName = sys.argv[1:]

classifierPickleFileName='../FVs/constNe.npy'
fvecDir='./'
numSubWins='11'
numSumStatsPerSubWin='92'
resultsDir='./'
accuracyFilesPrefix='constNe_accuracy'
confusionMatrixFigFileName='constNe_confusion_matrix.pdf'

netlayers=keras.models.load_model(classifierPickleFileName)

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

testX={}

numSubWins=int(numSubWins)

# for testing for loop below
testSetFileName='spHard_0.fvec'
testExample=currTestData[0]
i=0

for testSetFileName in os.listdir(fvecDir):
  # testSetFile=open('/'.join((fvecDir+'/'+testSetFileName).split('//')))
  testSetFile=open(testSetFileName)
  currTestData=testSetFile.readlines()
  testSetFile.close()
  currTestData=currTestData[1:]
  testX[testSetFileName]=[]
  for testExample in currTestData:
    if not "nan" in testExample:
      testData=testExample.strip().split("\t")
      currVector=[]
      for i in range(len(testData)):
        currVector.append(float(testData[i]))
      testX[testSetFileName].append(currVector)
  testX[testSetFileName]=np.reshape(np.array(testX[testSetFileName]),(np.array(testX[testSetFileName]).shape[0],int(numSumStatsPerSubWin),numSubWins,1))

selVals={"Neutral":0,"Hard":1,"Soft":2,"HardPartial":3,"SoftPartial":4}

def getSelType(x):
  if "Neut" in x:
    return "Neutral"
  elif "PartialHard" in x:
    return "HardPartial"
  elif "PartialSoft" in x:
    return "SoftPartial"
  elif "Hard" in x:
    return "Hard"
  elif "Soft" in x:
    return "Soft"
  else:
    raise ValueError

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split()

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

outlinesH={}
accuracyOverall=0
accuracySpecific=0
accuracyBroad=0

# for testing loop below
testSetFileName='spNeut.fvec'
className='Neutral'
testExampleIndex=0

for testSetFileName in sorted(testX, key=lambda x: (selVals[getSelType(x)], int((x.split(".")[0]+"_0").split("_")[1]))):
  predictions=np.argmax(netlayers.predict(testX[testSetFileName]),axis=1)
  pred = netlayers.predict(testX[testSetFileName]) # SCOTT
  prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
  prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT
  prob_df['truth']=testSetFileName # SCOTT
  prob_df.to_csv(testSetFileName+'.prob.csv', index=False) # SCOTT
  currPreds={}
  for className in classOrder:
    currPreds[className]=0
  denom=float(len(testX[testSetFileName]))
  for testExampleIndex in range(len(predictions)):
    predictedClass=labelToClassName[predictions[testExampleIndex]]
    currPreds[predictedClass]+=1/denom
  if not testSetFileName=='spNeut.fvec':
    testSetFilePrefix=testSetFileName.split(".")[0].split("_")
    selType,selWin = getSelType(testSetFilePrefix[0]),testSetFilePrefix[1]
    selWin=int(selWin)
    key=(selType,selWin)
  else:
    key=('Neutral',0)
  outlinesH[key]=(testSetFileName,[currPreds[className] for className in classOrder])
  if key[0]=='Neutral' or key[1]==5:
    accuracyOverall+=currPreds[key[0]]
    accuracySpecific+=currPreds[key[0]]
  else:
    accuracyOverall+=currPreds[key[0]+'-linked']
  accuracyBroad+=currPreds[key[0]]
  if not key[0]=='Neutral':
    accuracyBroad+=currPreds[key[0]+'-linked']

if resultsDir.lower() in ["none","false","default"]:
  resultsDir='./'

if accuracyFilesPrefix.lower() in ["none","false","default"]:
  accuracyFilesPrefix='accuracy'

accuracyOverall=accuracyOverall/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_overall.txt').split('//')),'w')
fileName.write(str(accuracyOverall))
fileName.close()

accuracySpecific=accuracySpecific/5

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_specific.txt').split('//')),'w')
fileName.write(str(accuracySpecific))
fileName.close()

accuracyBroad=accuracyBroad/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_broad.txt').split('//')),'w')
fileName.write(str(accuracyBroad))
fileName.close()


rowLabels,data = [],[]

for selType in sorted(selVals, key=lambda x: selVals[x]):
 for selWin in range(numSubWins):
  if selType!="Neutral" or selWin==0:
    if "Neutral" in selType:
      rowLabels.append("Neutral")
    else:
      if selWin==5:
        rowLabels.append("%s sweep in focal window" %selType)
      else:
        diff=abs(selWin-5)
        if diff==1:
          plural=""
        else:
          plural="s"
        if selWin<5:
          direction="left"
        else:
          direction="right"
        rowLabels.append("%s sweep %s window%s to %s" %(selType, diff, plural, direction))
    vec=outlinesH[(selType,selWin)][1]
    data.append(vec)

data=np.array(data)

ax=plt.subplots()[1]
heatmap=ax.pcolor(data,cmap=plt.cm.Blues,vmin=0.0,vmax=1.0)
cbar=plt.colorbar(heatmap,cmap=plt.cm.Blues)
cbar.set_label('Fraction of simulations assigned to class',rotation=270,labelpad=20)
ax.set_xticks(np.arange(data.shape[1])+0.5,minor=False)
ax.set_yticks(np.arange(data.shape[0])+0.5,minor=False)
ax.invert_yaxis()
ax.xaxis.tick_top()
ax.axis('tight')
plt.tick_params(axis='y',which='both',right='off')
plt.tick_params(axis='x',which='both',direction='out')
ax.set_xticklabels(classOrder,minor=False,fontsize=9,rotation=45,ha="left")
ax.set_yticklabels(rowLabels,minor=False,fontsize=7)

for y in range(data.shape[0]):
  for x in range(data.shape[1]):
    val=data[y,x]
    val*=100
    if val>50:
      c='0.9'
    else:
      c='black'
    ax.text(x+0.5,y +0.5,'%.1f%%' % val,horizontalalignment='center',verticalalignment='center',color=c,fontsize=6)

if confusionMatrixFigFileName.lower() in ["none","false","default"]:
  confusionMatrixFigFileName='confusionmatrix.pdf'

plt.savefig('/'.join((resultsDir+'/'+confusionMatrixFigFileName).split('//')),bbox_inches='tight',dpi=600)

# sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(classifierPickleFileName,fvecDir,(time.clock()-startTime)))
sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### test on self

```{shell}

# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe
# mkdir -p test_self && cd test_self
# rm ./*
# cp ../FVs/*.fvec ./
# rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
# 
# python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
#  ../FVs/constNe.npy\
#  ./\
#  11\
#  92\
#  ./\
#  constNe_accuracy\
#  constNe_confusion_matrix.pdf
# 
# evince constNe_confusion_matrix.pdf

```

#### test on self-new data

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/test_self_newDat
  mkdir -p FVs_earlyStopping && cd FVs_earlyStopping
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp ../../*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince constNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/
  mkdir -p test_expansionNe_intergenic && cd test_expansionNe_intergenic
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on constNe_intergenic

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/
  mkdir -p test_constNe_intergenic && cd test_constNe_intergenic
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic_lci

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/
  mkdir -p test_expansionNe_intergenic_lci && cd test_expansionNe_intergenic_lci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic_uci

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/
  mkdir -p test_expansionNe_intergenic_uci && cd test_expansionNe_intergenic_uci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on empirical

```{python}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell, OLD}

# export ss='expansionNe'
# 
# mkdir ${ss}_classifier
# 
# pred() {
#     echo ${1}
#     python3 ~/partialSHIC/empirical_deep_learning_classify.py \
#     ../${ss}/${ss}.npy \
#     ${1} \
#     11 92 \
#     ./${ss}_classifier/${1/all.fvec/bed}
# }
# 
# export -f pred
# 
# ls  ScA8VGg_[0-9]*.all.fvec | parallel -j 3 pred



# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors
# 
# mkdir -p pred_constNe && cd pred_constNe
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_MaskHets_110Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718.regular.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_MaskHets_110Lines.pred.bed
# 
# made predictions for 1323 total instances
# predicted 37 neutral regions (0.027967 of all classified regions)
# predicted 850 Hard sweep regions (0.642479 of all classified regions)
# predicted 391 Hard-linked sweep regions (0.295540 of all classified regions)
# predicted 6 Soft sweep regions (0.004535 of all classified regions)
# predicted 22 Soft-linked sweep regions (0.016629 of all classified regions)
# predicted 6 HardPartial sweep regions (0.004535 of all classified regions)
# predicted 7 HardPartial-linked sweep regions (0.005291 of all classified regions)
# predicted 3 SoftPartial sweep regions (0.002268 of all classified regions)
# predicted 1 SoftPartial-linked sweep regions (0.000756 of all classified regions)
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_noMaskHets_110Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_110Lines.noMaskHets.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_noMaskHets_110Lines.pred.bed
# 
# made predictions for 1389 total instances
# predicted 861 neutral regions (0.619870 of all classified regions)
# predicted 28 Hard sweep regions (0.020158 of all classified regions)
# predicted 56 Hard-linked sweep regions (0.040317 of all classified regions)
# predicted 158 Soft sweep regions (0.113751 of all classified regions)
# predicted 123 Soft-linked sweep regions (0.088553 of all classified regions)
# predicted 3 HardPartial sweep regions (0.002160 of all classified regions)
# predicted 14 HardPartial-linked sweep regions (0.010079 of all classified regions)
# predicted 55 SoftPartial sweep regions (0.039597 of all classified regions)
# predicted 91 SoftPartial-linked sweep regions (0.065515 of all classified regions)
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_top6mask_noMaskHets_110Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_110Lines.noMaskHets.top6mask.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_top6mask_noMaskHets_110Lines.pred.bed
# 
# made predictions for 1044 total instances
# predicted 848 neutral regions (0.812261 of all classified regions)
# predicted 6 Hard sweep regions (0.005747 of all classified regions)
# predicted 19 Hard-linked sweep regions (0.018199 of all classified regions)
# predicted 29 Soft sweep regions (0.027778 of all classified regions)
# predicted 32 Soft-linked sweep regions (0.030651 of all classified regions)
# predicted 5 HardPartial sweep regions (0.004789 of all classified regions)
# predicted 12 HardPartial-linked sweep regions (0.011494 of all classified regions)
# predicted 37 SoftPartial sweep regions (0.035441 of all classified regions)
# predicted 56 SoftPartial-linked sweep regions (0.053640 of all classified regions)
# 
# 
# 
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_MaskHets_55Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_55Lines.MaskHets.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_MaskHets_55Lines.pred.bed
# 
# made predictions for 1346 total instances
# predicted 168 neutral regions (0.124814 of all classified regions)
# predicted 483 Hard sweep regions (0.358841 of all classified regions)
# predicted 377 Hard-linked sweep regions (0.280089 of all classified regions)
# predicted 76 Soft sweep regions (0.056464 of all classified regions)
# predicted 59 Soft-linked sweep regions (0.043834 of all classified regions)
# predicted 43 HardPartial sweep regions (0.031947 of all classified regions)
# predicted 63 HardPartial-linked sweep regions (0.046805 of all classified regions)
# predicted 38 SoftPartial sweep regions (0.028232 of all classified regions)
# predicted 39 SoftPartial-linked sweep regions (0.028975 of all classified regions)
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_noMaskHets_55Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_55Lines.noMaskHets.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_genosRateMask_noMaskHets_55Lines.pred.bed
# 
# made predictions for 1405 total instances
# predicted 991 neutral regions (0.705338 of all classified regions)
# predicted 28 Hard sweep regions (0.019929 of all classified regions)
# predicted 31 Hard-linked sweep regions (0.022064 of all classified regions)
# predicted 148 Soft sweep regions (0.105338 of all classified regions)
# predicted 134 Soft-linked sweep regions (0.095374 of all classified regions)
# predicted 1 HardPartial sweep regions (0.000712 of all classified regions)
# predicted 2 HardPartial-linked sweep regions (0.001423 of all classified regions)
# predicted 37 SoftPartial sweep regions (0.026335 of all classified regions)
# predicted 33 SoftPartial-linked sweep regions (0.023488 of all classified regions)
# 
# # train_geonsRateMask_noMaskHets_55Lines.pred_top6mask_noMaskHets_55Lines
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_55Lines.noMaskHets.top6mask.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_noMaskHets_55Lines.pred_top6mask_noMaskHets_55Lines.pred.bed
# 
# made predictions for 1044 total instances
# predicted 907 neutral regions (0.868774 of all classified regions)
# predicted 4 Hard sweep regions (0.003831 of all classified regions)
# predicted 17 Hard-linked sweep regions (0.016284 of all classified regions)
# predicted 24 Soft sweep regions (0.022989 of all classified regions)
# predicted 40 Soft-linked sweep regions (0.038314 of all classified regions)
# predicted 1 HardPartial sweep regions (0.000958 of all classified regions)
# predicted 3 HardPartial-linked sweep regions (0.002874 of all classified regions)
# predicted 22 SoftPartial sweep regions (0.021073 of all classified regions)
# predicted 26 SoftPartial-linked sweep regions (0.024904 of all classified regions)
# 
# 
# 
# # train_110Haplotypes.pred_genosRateMask_phased_220Haplotypes
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_110Lines.phased.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes.pred.bed
# 
# predicted 832 neutral regions (0.601156 of all classified regions)
# predicted 23 Hard sweep regions (0.016618 of all classified regions)
# predicted 74 Hard-linked sweep regions (0.053468 of all classified regions)
# predicted 102 Soft sweep regions (0.073699 of all classified regions)
# predicted 220 Soft-linked sweep regions (0.158960 of all classified regions)
# predicted 7 HardPartial sweep regions (0.005058 of all classified regions)
# predicted 19 HardPartial-linked sweep regions (0.013728 of all classified regions)
# predicted 39 SoftPartial sweep regions (0.028179 of all classified regions)
# predicted 68 SoftPartial-linked sweep regions (0.049133 of all classified regions)
# 
# 
# 
# # train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_MAX
# 
# python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
#  /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
#  ../ScA8VGg_718_110Lines.phased.inbred.fvec\
#  11\
#  92\
#  ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_MAX.pred.bed
# 
# predicted 400 neutral regions (0.290909 of all classified regions)
# predicted 57 Hard sweep regions (0.041455 of all classified regions)
# predicted 178 Hard-linked sweep regions (0.129455 of all classified regions)
# predicted 88 Soft sweep regions (0.064000 of all classified regions)
# predicted 176 Soft-linked sweep regions (0.128000 of all classified regions)
# predicted 65 HardPartial sweep regions (0.047273 of all classified regions)
# predicted 252 HardPartial-linked sweep regions (0.183273 of all classified regions)
# predicted 30 SoftPartial sweep regions (0.021818 of all classified regions)
# predicted 129 SoftPartial-linked sweep regions (0.093818 of all classified regions)



cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

mkdir -p pred_constNe && cd pred_constNe

# ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_542_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 2475 total instances
predicted 1255 neutral regions (0.507071 of all classified regions)
predicted 95 Hard sweep regions (0.038384 of all classified regions)
predicted 383 Hard-linked sweep regions (0.154747 of all classified regions)
predicted 113 Soft sweep regions (0.045657 of all classified regions)
predicted 277 Soft-linked sweep regions (0.111919 of all classified regions)
predicted 47 HardPartial sweep regions (0.018990 of all classified regions)
predicted 167 HardPartial-linked sweep regions (0.067475 of all classified regions)
predicted 25 SoftPartial sweep regions (0.010101 of all classified regions)
predicted 113 SoftPartial-linked sweep regions (0.045657 of all classified regions)

# ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_594_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 3117 total instances
predicted 2212 neutral regions (0.709657 of all classified regions)
predicted 71 Hard sweep regions (0.022778 of all classified regions)
predicted 224 Hard-linked sweep regions (0.071864 of all classified regions)
predicted 138 Soft sweep regions (0.044273 of all classified regions)
predicted 161 Soft-linked sweep regions (0.051652 of all classified regions)
predicted 63 HardPartial sweep regions (0.020212 of all classified regions)
predicted 178 HardPartial-linked sweep regions (0.057106 of all classified regions)
predicted 11 SoftPartial sweep regions (0.003529 of all classified regions)
predicted 59 SoftPartial-linked sweep regions (0.018928 of all classified regions)

# ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_628_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 4618 total instances
predicted 2726 neutral regions (0.590299 of all classified regions)
predicted 123 Hard sweep regions (0.026635 of all classified regions)
predicted 453 Hard-linked sweep regions (0.098094 of all classified regions)
predicted 227 Soft sweep regions (0.049155 of all classified regions)
predicted 514 Soft-linked sweep regions (0.111304 of all classified regions)
predicted 106 HardPartial sweep regions (0.022954 of all classified regions)
predicted 298 HardPartial-linked sweep regions (0.064530 of all classified regions)
predicted 18 SoftPartial sweep regions (0.003898 of all classified regions)
predicted 153 SoftPartial-linked sweep regions (0.033131 of all classified regions)

# ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_718_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 1375 total instances
predicted 434 neutral regions (0.315636 of all classified regions)
predicted 56 Hard sweep regions (0.040727 of all classified regions)
predicted 163 Hard-linked sweep regions (0.118545 of all classified regions)
predicted 84 Soft sweep regions (0.061091 of all classified regions)
predicted 176 Soft-linked sweep regions (0.128000 of all classified regions)
predicted 84 HardPartial sweep regions (0.061091 of all classified regions)
predicted 241 HardPartial-linked sweep regions (0.175273 of all classified regions)
predicted 22 SoftPartial sweep regions (0.016000 of all classified regions)
predicted 115 SoftPartial-linked sweep regions (0.083636 of all classified regions)

# ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_76_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 6080 total instances
predicted 3438 neutral regions (0.565461 of all classified regions)
predicted 145 Hard sweep regions (0.023849 of all classified regions)
predicted 556 Hard-linked sweep regions (0.091447 of all classified regions)
predicted 314 Soft sweep regions (0.051645 of all classified regions)
predicted 731 Soft-linked sweep regions (0.120230 of all classified regions)
predicted 146 HardPartial sweep regions (0.024013 of all classified regions)
predicted 467 HardPartial-linked sweep regions (0.076809 of all classified regions)
predicted 47 SoftPartial sweep regions (0.007730 of all classified regions)
predicted 236 SoftPartial-linked sweep regions (0.038816 of all classified regions)

# ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy\
 ../ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 3836 total instances
predicted 2166 neutral regions (0.564651 of all classified regions)
predicted 90 Hard sweep regions (0.023462 of all classified regions)
predicted 319 Hard-linked sweep regions (0.083160 of all classified regions)
predicted 200 Soft sweep regions (0.052138 of all classified regions)
predicted 447 Soft-linked sweep regions (0.116528 of all classified regions)
predicted 92 HardPartial sweep regions (0.023983 of all classified regions)
predicted 322 HardPartial-linked sweep regions (0.083942 of all classified regions)
predicted 38 SoftPartial sweep regions (0.009906 of all classified regions)
predicted 162 SoftPartial-linked sweep regions (0.042231 of all classified regions)

```

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/
mkdir -p pred_empirical && cd pred_empirical
rsync -ahPv /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/*_110Lines.phased.noPIRs.inbred.fvec ./

for CONTIG in 542 594 628 718 76 785; do
  for i in 1 2 3 4 5 6 7 8 9 10; do
    python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
      /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
      ./ScA8VGg_${CONTIG}_110Lines.phased.noPIRs.inbred.fvec\
      11\
      92\
      ScA8VGg_${CONTIG}.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_fold_${i}.pred.bed >\
      empirical_deep_learning_classify_ScA8VGg_${CONTIG}_fold_${i}.log 2>&1
  done
done

```

#### summary prediction

```{bash}

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical

```

```{r}

library(tidyverse)
library(data.table)
library(RColorBrewer)
# library(ggplot2)
# library(dplyr)
# library(tidyr)
library(cowplot)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/pred_empirical")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
  pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
  mutate(pred_ensemble = class[prob==max(prob)]) %>% 
  pivot_wider(names_from=class, values_from=prob)



# a1 <- pred %>% 
#   extract('V4','State','(.*)_ScA8VGg.*') %>% 
#   mutate(V1=gsub('chr','', V1))


# a1_df <- data.table(table(a1$State))
a1_df <- data.table(table(ensemble$pred_ensemble))
# hard_num <- pred_df[V1=="hard",N]
# soft_num <- pred_df[V1=="soft",N]
# hard_pct <- hard_num/(hard_num + soft_num)
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

aplot[V1 %in% c('HardPartial','HardPartial-linked'),lbl2:=lbl]


PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
    geom_bar(width=1, stat="identity", color="white") +
    coord_polar("y", start=0) +
    geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
                  y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), label=lbl2), 
              color="black", size=3) +
    scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                               my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                               "grey60")) +
    theme_void() + theme(legend.position = "none")



a1 <- data.table(ensemble)

a1[,chr:="xxxx"][contig=="chrScA8VGg_76",chr:="3R"][contig=="chrScA8VGg_718",chr:="2L"][contig=="chrScA8VGg_542",chr:="2L"][contig=="chrScA8VGg_594",chr:="X"][contig=="chrScA8VGg_628",chr:="3L"][contig=="chrScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$pred_ensemble)) -> a3
a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))

# ggsave("diploSHIC_proprotion_figure1.pdf",PP, width = 8, height = 4, 
#        dpi = 300, units = "in")


# chisquare test
library(rcompanion)

bc <- table(a2$chr, a2$pred_ensemble)

b2 <- cbind(rowSums(bc[,c(1,3)]), rowSums(bc[,c(2,4:9)]))
# 
# b1 <- cbind(bc,other = rowSums(bc[,2:9]))
# b2 <- b1[,c(1,10)]
chisq.test(b2)
b2 <- matrix(c(29, 43, 59, 46, 56, 3600, 3876, 4249, 5688, 4857), ncol=2)
colnames(b2) <- c('hard','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")
apply(b2,1,prop.table)

# c1 <- cbind(bc,other = rowSums(bc[,c(1:5,7:9)]))
# c2 <- c1[,c(6,10)
c2 <- cbind(rowSums(bc[,c(6,8)]), rowSums(bc[,c(1:5,7,9)]))
chisq.test(c2)

c2 <- matrix(c(381, 452, 503, 579, 615, 3248, 3467, 3805, 5155, 4298), ncol=2)
colnames(c2) <- c('soft','others')
rownames(c2) <- c('2L','2R','3L','3R','X')
pairwiseNominalIndependence(c2, fisher = F, gtest = F, chisq = T, method = "fdr")
apply(c2,1,prop.table)

ph <- cbind(bc[,3], rowSums(bc[,c(1:2,4:9)]))
apply(ph,1,prop.table)

```

```
                  V1     N         pct
1:               Hard   202 0.008976581 202
2:        Hard-linked  1319 0.058614407 1327
3:        HardPartial    31 0.001377594 22
4: HardPartial-linked    67 0.002977381
5:            Neutral  3192 0.141847754 3195
6:               Soft  2181 0.096920411 2183
7:        Soft-linked 14685 0.652579656 14694
8:        SoftPartial   349 0.015509043 360
9: SoftPartial-linked   477 0.021197174

```

#### prob pred

```{shell}

# module load anaconda3
# source ~/conda-init
# conda activate r_env
# 
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe

```

```{r}

# library(tidyverse)
# library(data.table)
# 
# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")
# 
# beds <- list.files(pattern = 'ScA8VGg_.*.csv$')
# 
# aa <- data.table()
# 
# for(ff in beds){
#     tmp <- fread(ff, header = TRUE, sep = ",")
#     aa <- rbind(aa, tmp)
# }
# 
# aa %>% 
#   mutate(window = c(1:nrow(aa))) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   filter(pred=="Neutral") %>% 
#   ggplot(aes(x=prob, fill=class)) + 
#   geom_density(alpha=0.5) + 
#   facet_wrap(~class, scale="free_y")
# 
# aa %>% 
#   mutate(window = c(1:nrow(aa))) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   filter(pred=="Soft-linked") %>% 
#   ggplot(aes(x=prob, fill=class)) + 
#   geom_density(alpha=0.5) + 
#   facet_wrap(~class, scale="free_y")
# 
# # Create a new column "prob" with the highest probability
# aa$prob <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   sorted_probs[1]
# })
# 
# # Create a new column "pred2" with the second highest probability class
# aa$pred2 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   names(sorted_probs)[2]
# })
# 
# # Create a new column "prob2" with the second highest probability
# aa$prob2 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   sorted_probs[2]
# })
# 
# # Create a new column "pred3" with the third highest probability class
# aa$pred3 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   names(sorted_probs)[3]
# })
# 
# # Create a new column "prob3" with the third highest probability
# aa$prob3 <- apply(aa[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   sorted_probs[3]
# })

```

# ######################## PartialSHIC - expansionNe intergenic LCI

## Generate training data

* Bunya

```{shell, install discoal}

cd /scratch/project/chenoase/partialSHIC_DsGRP

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=1\
 --mem=2G\
 --job-name=uqsalle3_TEST\
 --time=00:30:00\
 --partition=general\
 --account=a_chenoweth srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

git clone https://github.com/kr-colab/discoal.git

cd /scratch/project/chenoase/partialSHIC_DsGRP/discoal

make discoal

```

* Dell 2 port from R to bash

```{shell}

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

# setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat')

DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal' 
TIMES = 2000

Ne = 1041174
# Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000009604543
# s_high = 0.01
s_high = 0.009604543
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.009604543 # 0.009604543*4*1041174 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
# bottleneck <- "-en 0.007292 0 1.5 -en 0.02135 0 0.547 -en 0.05208 0 0.208 -en 0.09271 0 0.0805"

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
# bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
cat("bottleneck: ", bottleneck, "\n")
cat("sweep_pos: ", sweep_pos, "\n")

```

```{r}
TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.000009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"
```


* Back to Bunya

```{shell, bunya example}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name=Neut_sim
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/slurm_discoal_Neutral.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/slurm_discoal_Neutral.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/

DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell}

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_10.txt

```

```{shell, create scripts}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/
  
## Neutral

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Neutral.txt discoal_Neutral_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Neutral_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Hard_5.txt discoal_Hard_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Hard_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Soft_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Soft_5.txt discoal_Soft_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Soft_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## HardPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_HardPartial_5.txt discoal_HardPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_HardPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## SoftPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_SoftPartial_5.txt discoal_SoftPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_SoftPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## Soft-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## HardPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## SoftPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

```

```{shell, submit}

## Neutral

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Neutral.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral.txt

## Hard_5

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Hard_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5.txt

## Soft_5

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Soft_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5.txt

## HardPartial_5

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_HardPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5.txt

## SoftPartial_5

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_SoftPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5.txt

## Hard-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## Soft-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## HardPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## SoftPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done



ls ./spNeut_*.msOut.gz | wc -l
ls ./spHard_5_*.msOut.gz | wc -l
ls ./spSoft_5_*.msOut.gz | wc -l
ls ./spHardPartial_5_*.msOut.gz | wc -l
ls ./spSoftPartial_5_*.msOut.gz | wc -l



ls ./spHard_0_*.msOut.gz | wc -l
ls ./spHard_1_*.msOut.gz | wc -l
ls ./spHard_2_*.msOut.gz | wc -l
ls ./spHard_3_*.msOut.gz | wc -l
ls ./spHard_4_*.msOut.gz | wc -l
ls ./spHard_6_*.msOut.gz | wc -l
ls ./spHard_7_*.msOut.gz | wc -l
ls ./spHard_8_*.msOut.gz | wc -l
ls ./spHard_9_*.msOut.gz | wc -l
ls ./spHard_10_*.msOut.gz | wc -l



ls ./spSoft_0_*.msOut.gz | wc -l
ls ./spSoft_1_*.msOut.gz | wc -l
ls ./spSoft_2_*.msOut.gz | wc -l
ls ./spSoft_3_*.msOut.gz | wc -l
ls ./spSoft_4_*.msOut.gz | wc -l
ls ./spSoft_6_*.msOut.gz | wc -l
ls ./spSoft_7_*.msOut.gz | wc -l
ls ./spSoft_8_*.msOut.gz | wc -l
ls ./spSoft_9_*.msOut.gz | wc -l
ls ./spSoft_10_*.msOut.gz | wc -l



ls ./spHardPartial_0_*.msOut.gz | wc -l
ls ./spHardPartial_1_*.msOut.gz | wc -l
ls ./spHardPartial_2_*.msOut.gz | wc -l
ls ./spHardPartial_3_*.msOut.gz | wc -l
ls ./spHardPartial_4_*.msOut.gz | wc -l
ls ./spHardPartial_6_*.msOut.gz | wc -l
ls ./spHardPartial_7_*.msOut.gz | wc -l
ls ./spHardPartial_8_*.msOut.gz | wc -l
ls ./spHardPartial_9_*.msOut.gz | wc -l
ls ./spHardPartial_10_*.msOut.gz | wc -l



ls ./spSoftPartial_0_*.msOut.gz | wc -l
ls ./spSoftPartial_1_*.msOut.gz | wc -l
ls ./spSoftPartial_2_*.msOut.gz | wc -l
ls ./spSoftPartial_3_*.msOut.gz | wc -l
ls ./spSoftPartial_4_*.msOut.gz | wc -l
ls ./spSoftPartial_6_*.msOut.gz | wc -l
ls ./spSoftPartial_7_*.msOut.gz | wc -l
ls ./spSoftPartial_8_*.msOut.gz | wc -l
ls ./spSoftPartial_9_*.msOut.gz | wc -l
ls ./spSoftPartial_10_*.msOut.gz | wc -l

```

```{shell, check and fix empty simulation}

## Neutral

rm list_discoal_files_Neutral_RERUN_10.txt
grep 'Out Of Memory' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt
grep ' MAXMUTS' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt
cat list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt > list_discoal_files_Neutral_RERUN_10.txt
wc -l list_discoal_files_Neutral_RERUN_10.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral_RERUN_10.txt

## Hard_5

rm list_discoal_files_Hard_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Hard_5_RERUN_OutOfMemory.txt list_discoal_files_Hard_5_RERUN_MAXMUTS.txt > list_discoal_files_Hard_5_RERUN.txt
wc -l list_discoal_files_Hard_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5_RERUN.txt

## Soft_5

rm list_discoal_files_Soft_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Soft_5_RERUN_OutOfMemory.txt list_discoal_files_Soft_5_RERUN_MAXMUTS.txt > list_discoal_files_Soft_5_RERUN.txt
wc -l list_discoal_files_Soft_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5_RERUN.txt

## HardPartial_5

rm list_discoal_files_HardPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_HardPartial_5_RERUN.txt
wc -l list_discoal_files_HardPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5_RERUN.txt

## SoftPartial_5

rm list_discoal_files_SoftPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_SoftPartial_5_RERUN.txt
wc -l list_discoal_files_SoftPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5_RERUN.txt

## check all

rm list_discoal_files_RERUN.txt
grep 'Out Of Memory' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' >  list_discoal_files_RERUN_MAXMUTS.txt
cat list_discoal_files_RERUN_OutOfMemory.txt list_discoal_files_RERUN_MAXMUTS.txt > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt


rm list_discoal_files_RERUN.txt
find ./ -maxdepth 1 -type f -name '*.msOut.gz' -size -21c | sed 's/\.\/sp/discoal_/; s/msOut\.gz/slurm\.sh/; s/Neut/Neutral/;' > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt

```

```{shell, combine simulations}

# Function to process and combine sweep files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail ./${PREFIX}_*.msOut -n +3 > temp1.msOut
    perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
    awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/2000/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spNeut spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
    process_and_combine "$PREFIX"
done



# Function to process and combine linked files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail -n +3 ./${PREFIX}_*.msOut > temp1.msOut
    grep -v "$PREFIX" temp1.msOut > temp2.msOut
    awk 'BEGIN { RS="\n\n"; ORS="\n" } { print }' temp2.msOut > temp3.msOut
    
    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/400/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    process_and_combine "$PREFIX"
done

# # mini for testing
# 
# gzip -d spHard_5_1.msOut.gz
# gzip -d spHard_5_2.msOut.gz
# gzip -d spHard_5_3.msOut.gz
# 
# # Concatenate files and remove headers
# tail spHard_5_*.msOut -n +3 > temp1.msOut
# perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
# awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut
# 
# # Extract the first two lines from ${PREFIX}_1.msOut
# head -n 2 "spHard_5_1.msOut" > header.txt
# 
# # Edit the header to change the third space-delimited element from 1 to 2000
# sed -i '1s/\<1\>/3/' header.txt
# 
# # Concatenate the edited header with temp3.msOut
# cat header.txt temp3.msOut > spHard_5_COMBINED_MINI.msOut
# 
# # Move and compress the combined file
# gzip spHard_5_COMBINED_MINI.msOut
# 
# # Compress files
# find ./ -maxdepth 1 -type f -name "spHard_5_*.msOut" -exec gzip {} \;
# 
# # Clean up temporary files
# rm temp1.msOut temp2.msOut

```

## Generate test data

* Bunya

```{shell, install discoal}

cd /scratch/project/chenoase/partialSHIC_DsGRP

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=1\
 --mem=2G\
 --job-name=uqsalle3_TEST\
 --time=00:30:00\
 --partition=general\
 --account=a_chenoweth srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

git clone https://github.com/kr-colab/discoal.git

cd /scratch/project/chenoase/partialSHIC_DsGRP/discoal

make discoal

```

* Dell 2 port from R to bash

```{shell}

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

# setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat')

DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal' 
TIMES = 2000

Ne = 1041174
# Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000009604543
# s_high = 0.01
s_high = 0.009604543
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.009604543 # 0.009604543*4*1041174 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
# bottleneck <- "-en 0.007292 0 1.5 -en 0.02135 0 0.547 -en 0.05208 0 0.208 -en 0.09271 0 0.0805"

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
# bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
cat("bottleneck: ", bottleneck, "\n")
cat("sweep_pos: ", sweep_pos, "\n")

```
* Back to Bunya

```{shell, bunya example}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name=Neut_sim
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat/slurm_discoal_Neutral.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat/slurm_discoal_Neutral.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat/

DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell}

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_10.txt

```

```{shell, create scripts}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat/
  
## Neutral

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Neutral.txt discoal_Neutral_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Neutral_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Hard_5.txt discoal_Hard_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Hard_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Soft_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Soft_5.txt discoal_Soft_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Soft_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## HardPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_HardPartial_5.txt discoal_HardPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_HardPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## SoftPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_SoftPartial_5.txt discoal_SoftPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_SoftPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## Soft-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## HardPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## SoftPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

```

```{shell, submit}

## Neutral (RUNNING)

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Neutral.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_Neutral.txt

## Hard_5 (RUNNING)

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Hard_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_Hard_5.txt

## Soft_5 (RUNNING)

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Soft_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_Soft_5.txt

## HardPartial_5 (RUNNING)

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_HardPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_HardPartial_5.txt

## SoftPartial_5 (RUNNING)

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_SoftPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_SoftPartial_5.txt

## Hard-linked (RUNNING)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.05
  done < list_discoal_files_${PREFIX}.txt
done

## Soft-linked (RUNNING)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.05
  done < list_discoal_files_${PREFIX}.txt
done

## HardPartial-linked (RUNNING)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.05
  done < list_discoal_files_${PREFIX}.txt
done

## SoftPartial-linked (RUNNING)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.05
  done < list_discoal_files_${PREFIX}.txt
done



ls ./spNeut_*.msOut.gz | wc -l
ls ./spHard_5_*.msOut.gz | wc -l
ls ./spSoft_5_*.msOut.gz | wc -l
ls ./spHardPartial_5_*.msOut.gz | wc -l
ls ./spSoftPartial_5_*.msOut.gz | wc -l



ls ./spHard_0_*.msOut.gz | wc -l
ls ./spHard_1_*.msOut.gz | wc -l
ls ./spHard_2_*.msOut.gz | wc -l
ls ./spHard_3_*.msOut.gz | wc -l
ls ./spHard_4_*.msOut.gz | wc -l
ls ./spHard_6_*.msOut.gz | wc -l
ls ./spHard_7_*.msOut.gz | wc -l
ls ./spHard_8_*.msOut.gz | wc -l
ls ./spHard_9_*.msOut.gz | wc -l
ls ./spHard_10_*.msOut.gz | wc -l



ls ./spSoft_0_*.msOut.gz | wc -l
ls ./spSoft_1_*.msOut.gz | wc -l
ls ./spSoft_2_*.msOut.gz | wc -l
ls ./spSoft_3_*.msOut.gz | wc -l
ls ./spSoft_4_*.msOut.gz | wc -l
ls ./spSoft_6_*.msOut.gz | wc -l
ls ./spSoft_7_*.msOut.gz | wc -l
ls ./spSoft_8_*.msOut.gz | wc -l
ls ./spSoft_9_*.msOut.gz | wc -l
ls ./spSoft_10_*.msOut.gz | wc -l



ls ./spHardPartial_0_*.msOut.gz | wc -l
ls ./spHardPartial_1_*.msOut.gz | wc -l
ls ./spHardPartial_2_*.msOut.gz | wc -l
ls ./spHardPartial_3_*.msOut.gz | wc -l
ls ./spHardPartial_4_*.msOut.gz | wc -l
ls ./spHardPartial_6_*.msOut.gz | wc -l
ls ./spHardPartial_7_*.msOut.gz | wc -l
ls ./spHardPartial_8_*.msOut.gz | wc -l
ls ./spHardPartial_9_*.msOut.gz | wc -l
ls ./spHardPartial_10_*.msOut.gz | wc -l



ls ./spSoftPartial_0_*.msOut.gz | wc -l
ls ./spSoftPartial_1_*.msOut.gz | wc -l
ls ./spSoftPartial_2_*.msOut.gz | wc -l
ls ./spSoftPartial_3_*.msOut.gz | wc -l
ls ./spSoftPartial_4_*.msOut.gz | wc -l
ls ./spSoftPartial_6_*.msOut.gz | wc -l
ls ./spSoftPartial_7_*.msOut.gz | wc -l
ls ./spSoftPartial_8_*.msOut.gz | wc -l
ls ./spSoftPartial_9_*.msOut.gz | wc -l
ls ./spSoftPartial_10_*.msOut.gz | wc -l

```

```{shell, check and fix empty simulation}

## Neutral

rm list_discoal_files_Neutral.txt
grep 'Out Of Memory' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_MAXMUTS.txt
cat list_discoal_files_Neutral_RERUN_OutOfMemory.txt list_discoal_files_Neutral_RERUN_MAXMUTS.txt > list_discoal_files_Neutral_RERUN.txt
wc -l list_discoal_files_Neutral_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral_RERUN_10.txt

## Hard_5

rm list_discoal_files_Hard_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Hard_5_RERUN_OutOfMemory.txt list_discoal_files_Hard_5_RERUN_MAXMUTS.txt > list_discoal_files_Hard_5_RERUN.txt
wc -l list_discoal_files_Hard_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5_RERUN.txt

## Soft_5

rm list_discoal_files_Soft_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Soft_5_RERUN_OutOfMemory.txt list_discoal_files_Soft_5_RERUN_MAXMUTS.txt > list_discoal_files_Soft_5_RERUN.txt
wc -l list_discoal_files_Soft_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5_RERUN.txt

## HardPartial_5

rm list_discoal_files_HardPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_HardPartial_5_RERUN.txt
wc -l list_discoal_files_HardPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5_RERUN.txt

## SoftPartial_5

rm list_discoal_files_SoftPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_SoftPartial_5_RERUN.txt
wc -l list_discoal_files_SoftPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5_RERUN.txt

## check all

rm list_discoal_files_RERUN.txt
grep 'Out Of Memory' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' >  list_discoal_files_RERUN_MAXMUTS.txt
cat list_discoal_files_RERUN_OutOfMemory.txt list_discoal_files_RERUN_MAXMUTS.txt > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt


rm list_discoal_files_RERUN.txt
find ./ -maxdepth 1 -type f -name '*.msOut.gz' -size -21c | sed 's/\.\/sp/discoal_/; s/msOut\.gz/slurm\.sh/; s/Neut/Neutral/;' > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_RERUN.txt

```

```{shell, combine simulations}

# Function to process and combine sweep files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail ./${PREFIX}_*.msOut -n +3 > temp1.msOut
    perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
    awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/2000/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spNeut spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
    process_and_combine "$PREFIX"
done



# Function to process and combine linked files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail -n +3 ./${PREFIX}_*.msOut > temp1.msOut
    grep -v "$PREFIX" temp1.msOut > temp2.msOut
    awk 'BEGIN { RS="\n\n"; ORS="\n" } { print }' temp2.msOut > temp3.msOut
    
    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/400/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    process_and_combine "$PREFIX"
done

# # mini for testing
# 
# gzip -d spHard_5_1.msOut.gz
# gzip -d spHard_5_2.msOut.gz
# gzip -d spHard_5_3.msOut.gz
# 
# # Concatenate files and remove headers
# tail spHard_5_*.msOut -n +3 > temp1.msOut
# perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
# awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut
# 
# # Extract the first two lines from ${PREFIX}_1.msOut
# head -n 2 "spHard_5_1.msOut" > header.txt
# 
# # Edit the header to change the third space-delimited element from 1 to 2000
# sed -i '1s/\<1\>/3/' header.txt
# 
# # Concatenate the edited header with temp3.msOut
# cat header.txt temp3.msOut > spHard_5_COMBINED_MINI.msOut
# 
# # Move and compress the combined file
# gzip spHard_5_COMBINED_MINI.msOut
# 
# # Compress files
# find ./ -maxdepth 1 -type f -name "spHard_5_*.msOut" -exec gzip {} \;
# 
# # Clean up temporary files
# rm temp1.msOut temp2.msOut

```

## Run training

```{python, training_deep_learning_python3.py}

# import time
# # startTime=time.clock() # Python 2
# startTime = time.perf_counter() # Python 3
# import sys
# import numpy as np
# np.random.seed(123)
# from sklearn.model_selection import train_test_split
# # from keras.utils import np_utils
# from keras.utils import to_categorical # SCOTT
# from keras.models import Sequential
# from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
# from keras import optimizers
# from keras.callbacks import EarlyStopping,ModelCheckpoint
# 
# '''usage eg:
# python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
# '''
# 
# if len(sys.argv)!=9:
#   sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
# else:
#   fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]
# 
# fvecDir='./'
# fvecFiles='neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec'
# numSubWins='11'
# numSumStatsPerSubWin='92'
# validationSize='0.1'
# weightsFileName='constNe.hdf5'
# jsonFileName='constNe.json'
# npyFileName='constNe.npy'
# 
# if fvecDir.lower() in ["none","false","default"]:
#   fvecDir='./'
# 
# if fvecFiles.lower() in ["none","false","default"]:
#   fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
# else:
#   fvecFiles=fvecFiles.split(",")
#   assert len(fvecFiles)==9
# 
# numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)
# 
# if validationSize.lower() in ["none","false","default"]:
#   validationSize=0.1
# else:
#   validationSize=float(validationSize)
# 
# sweeps=[]
# 
# for i in range(0,9):
#  sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
#  if i>0:
#   assert len(sweeps[0])==len(sweeps[i])
# 
# sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))
# 
# sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)
# 
# models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))
# 
# sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=42)
# 
# # models=np_utils.to_categorical(models,9)
# models=to_categorical(models,9) # SCOTT
# 
# # models_val=np_utils.to_categorical(models_val,9)
# models_val=to_categorical(models_val,9) # SCOTT
# 
# netlayers=Sequential()
# netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
# netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
# netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
# netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
# netlayers.add(Dropout(0.25))
# netlayers.add(Flatten())
# netlayers.add(Dense(512, activation='relu'))
# netlayers.add(Dropout(0.5))
# netlayers.add(Dense(128, activation='relu'))
# netlayers.add(Dropout(0.5))
# netlayers.add(Dense(9, activation='softmax'))
# netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
# 
# checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')
# 
# netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)
# 
# netlayers_json=netlayers.to_json()
# 
# with open(jsonFileName,"w") as json_file:
#   json_file.write(netlayers_json)
# 
# netlayers.save(npyFileName)
# 
# sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))

```

### Training - calcStatsAndDafForEachSnp

```{python, calcStatsAndDafForEachSnpSingleMsFile_Python3}

import sys
import allel
import random
import numpy as np
from msTools import *
from fvTools import *
import time

'''usage example
python calcStatsAndDafForEachSnpSingleMsFile.py /san/data/dan/simulations/discoal_multipopStuff/spatialSVMSims/trainingSets/equilibNeut.msout.gz 110000 iHS nSL
'''

trainingDataFileName, totalPhysLen = sys.argv[1:3]
totalPhysLen = int(totalPhysLen)

hapArraysIn, positionArrays = msOutToHaplotypeArrayIn(trainingDataFileName, totalPhysLen)
numInstances = len(hapArraysIn)

header = "daf\tiHS\tnSL"
#print header # Python 2
print(header) # Python 3

dafs = []
#start = time.clock() # Python 2
start = time.perf_counter() # Python 3
numInstancesDone = 0

numSnpsDone = 0
for instanceIndex in range(numInstances):
    if instanceIndex % 10 == 0:
        sys.stderr.write("done wtih %d of %d instances (%d SNPs)\n" %(instanceIndex, numInstances, numSnpsDone))
    haps = allel.HaplotypeArray(hapArraysIn[instanceIndex], dtype='i1')
    genos = haps.to_genotypes(ploidy=2)
    ac = genos.count_alleles()
    sampleSizes = [sum(x) for x in ac]
    assert len(set(sampleSizes)) == 1
    dafs = ac[:,1]/float(sampleSizes[0])
    ihsVals = allel.stats.selection.ihs(haps, positionArrays[instanceIndex], use_threads=True, include_edges=False)
    nslVals = allel.stats.selection.nsl(haps, use_threads=True)
    assert len(dafs) == len(ihsVals) == len(nslVals)
    numSnpsDone += len(dafs)
    for i in range(len(dafs)):
        #print "%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i]) # Python 2
        print ("%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i])) #Python 3
    if numSnpsDone > 5000000:
        break

```

```{shell, bunya template}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=32G
#SBATCH --job-name=[SAMPLE]
#SBATCH --time=01:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/

msOut="[SAMPLE]"

srun python /home/uqsalle3/Programs/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/

find ./ -maxdepth 1 -type f -name '*_COMBINED.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_calcStatsAndDafForEachSnpSingleMsFile_Python3.txt calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
done < msOut_files.txt

find ./ -maxdepth 1 -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*.slurm.sh' -print0 | xargs -0 chmod 755

ls ./calcStatsAndDafForEachSnpSingleMsFile_Python3_*.slurm.sh

```

```{shell, submit scripts Bunya}

while read p; do
  echo "calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh submitted"
  sbatch calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
  sleep 0.05
done < msOut_files.txt

```

### Training - training_convert_to_FVs

```{python, training_convert_to_FVs_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
from msTools import *
from fvTools import *
import random
import allel
import numpy as np
import math

# '''usage eg:
# pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
# python2 training_convert_to_FVs.py spNeut.msOut.gz 2L,2R,3L,3R 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.fa ./ spNeut.msOut.fvec
# '''
# 
# if len(sys.argv)!=12:
#   sys.exit("usage:\npython2 training_convert_to_FVs.py trainingDataFileName chrArmsForMasking subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName statDir fvecFileName\n")
# else:
#   trainingDataFileName, chrArmsForMasking, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, statDir, fvecFileName = sys.argv[1:]

trainingDataFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5.msOut.gz"
chrArmsForMasking="ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140"
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_stats.txt"
maskFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta"
# ancestralArmFaFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/top6.anc.fa"
ancestralArmFaFileName="none"
statDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/"
fvecFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_test.fvec"

subWinSize,numSubWins = int(subWinSize),int(numSubWins)
assert subWinSize>0 and numSubWins>1

totalPhysLen=(subWinSize*numSubWins)

hapArraysIn,positionArrays=msOutToHaplotypeArrayIn(trainingDataFileName,totalPhysLen)

chrArmsForMasking=chrArmsForMasking.split(",")

unmaskedFracCutoff,pMisPol=float(unmaskedFracCutoff),float(pMisPol)

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")


if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")


standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none", "false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*totalPhysLen
  sys.stderr.write("Warning: not doing any masking! (i.e. mask.fa file for the chr arm with all masked sites N'ed out, or at least the reference with Ns, is not provided)\n")
  maskFileName=False
else:
  if ancestralArmFaFileName.lower() in ["none", "false"]:
    maskData=readMaskDataForTraining(maskFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  else:
    maskData=readMaskAndAncDataForTraining(maskFileName,ancestralArmFaFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  if len(maskData)<len(hapArraysIn):
    sys.stderr.write("Warning: not enough windows from masked data (needed %d; got %d); will draw with replacement!!\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=True
  else:
    sys.stderr.write("enough windows from masked data (needed %d; got %d); will draw without replacement.\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=False


# for i in range(len(maskData)):
#   print(str(i) + " " + str(len(maskData[i])) + " " + str(sum(maskData[i])))

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

header=[]

for statName in statNames:
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName, i))

header="\t".join(header)

statVals={}

for statName in statNames:
  statVals[statName]=[]

def getSubWinBounds(subWinSize,totalPhysLen):
  subWinStart=1-subWinSize
  subWinEnd=0
  subWinBounds=[]
  for i in range(0,(numSubWins-1)):
    subWinStart+=subWinSize
    subWinEnd+=subWinSize
    subWinBounds.append((subWinStart,subWinEnd))
  subWinStart+=subWinSize
  subWinEnd=totalPhysLen
  subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,totalPhysLen)

def getSnpIndicesInSubWins(subWinBounds,positions):
  snpIndicesInSubWins=[]
  for subWinIndex in range(len(subWinBounds)):
    snpIndicesInSubWins.append([])
  subWinIndex=0
  for i in range(len(positions)):
    while not (positions[i]>=subWinBounds[subWinIndex][0] and positions[i]<=subWinBounds[subWinIndex][1]):
      subWinIndex+=1
    snpIndicesInSubWins[subWinIndex].append(i)
  return snpIndicesInSubWins

quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

if statDir.lower() in ["none","false","default"]:
  statDir='./'

statFiles=[]

for subWinIndex in range(numSubWins):
  statFileName='/'.join(("%s/%s.subWin%d.stats" %(statDir, trainingDataFileName.split("/")[-1].replace(".msOut.gz",""), subWinIndex)).split('//'))
  statFiles.append(open(statFileName,"w"))
  statFiles[-1].write("\t".join(statNames)+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  fvecFileName="%s.fvec" %trainingDataFileName.split("/")[-1].replace(".msOut.gz","")

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

# for testing for loop below only
instanceIndex=0
statName='pi'

for instanceIndex in range(len(hapArraysIn)):
  print("Starting index " + str(instanceIndex))
  for statName in statNames:
    statVals[statName].append([])
  if maskFileName:
    if drawWithReplacement:
      unmasked=random.choice(maskData)
    else:
      unmasked=maskData[instanceIndex]
    assert len(unmasked)==totalPhysLen
    print(str(sum(unmasked)) + " Loci unmasked")
  snpIndicesToKeep=[x for x in range(len(positionArrays[instanceIndex])) if unmasked[positionArrays[instanceIndex][x]-1]]
  if len(snpIndicesToKeep)==0:
    for subWinIndex in range(numSubWins):
      for statName in statNames:
        print("No SNPs in BIGwindow, appendStatValsForMonomorphic")
        appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  else:
    haps=allel.HaplotypeArray(hapArraysIn[instanceIndex],dtype='i1').subset(sel0=snpIndicesToKeep)
    if pMisPol>0:
      misPolarizeCorrectionIndex=np.random.binomial(1,pMisPol,len(haps))
      for i in range(len(misPolarizeCorrectionIndex)):
        if misPolarizeCorrectionIndex[i]==1:
          for j in range(len(haps[i])):
            if haps[i][j]==0:
              haps[i][j]=1
            else:
              haps[i][j]=0
    genos=haps.to_genotypes(ploidy=2)
    alleleCounts=genos.count_alleles()
    positions=[positionArrays[instanceIndex][x] for x in snpIndicesToKeep]
    precomputedStats={}
    sampleSizes=[sum(x) for x in alleleCounts]
    assert len(set(sampleSizes))==1
    dafs=alleleCounts[:,1]/float(sampleSizes[0])
    ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
    nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
    nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
    # sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    if nonNanCount==0:
      precomputedStats["iHS"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["iHS"].append([])
    else:
      ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
      precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)
    nslVals=allel.stats.selection.nsl(haps,use_threads=False)
    nonNanCount=[x for x in np.isnan(nslVals)].count(False)
    # sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    if nonNanCount==0:
      precomputedStats["nSL"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["nSL"].append([])
    else:
      nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
      precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)
    snpIndicesInSubWins=getSnpIndicesInSubWins(subWinBounds,positions)
    for subWinIndex in range(numSubWins):
      subWinStart,subWinEnd = subWinBounds[subWinIndex]
      unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
      assert unmaskedFrac>=unmaskedFracCutoff
      if len(snpIndicesInSubWins[subWinIndex])==0:
        print("No SNPS in " + str(subWinIndex) + ", appendStatValsForMonomorphic")
        for statName in statNames:
          appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      else:
        print(str(snpIndicesInSubWins[subWinIndex])) + " SNPs in " + str(subWinIndex) + ", calcAndAppendStatVal")
        hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
        for statName in [x for x in statNames if x[0:3]!="HAF" and x[0:3]!="phi" and x[0:3]!="kap" and x[0:3]!="SFS" and x[0:3]!="SAF"]:
          calcAndAppendStatVal(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, instanceIndex, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        haplotypes={}
        for i in range(len(hapsInSubWin[0])):
          haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
          haplotype="".join(str(x) for x in haplotype)
          if haplotype in haplotypes:
            haplotypes[haplotype].append(i)
          else:
            haplotypes[haplotype]=[i]
        HAF=[]
        HAFunique={}
        for i in haplotypes:
          HAFunique[i]=0
          for j in range(len(hapsInSubWin)):
            if hapsInSubWin[j][haplotypes[i][0]]==1:
              HAFunique[i]+=sum([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j]))])
          for j in range(len(haplotypes[i])):
            HAF.append(HAFunique[i])
        phi=[]
        kappa=[]
        SAFE=[]
        for i in range(len(hapsInSubWin)):
          phi.append(0)
          kappa.append([])
          phiDenom=0
          for j in haplotypes:
            phi[i]+=(int(list(j)[i])*HAFunique[j]*len(haplotypes[j]))
            if int(list(j)[i])==1 and HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
              kappa[i].append(HAFunique[j])
            phiDenom+=(HAFunique[j]*len(haplotypes[j]))
          phi[i]/=float(phiDenom)
          kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
          if dafs[snpIndicesInSubWins[subWinIndex]][i]==0 or dafs[snpIndicesInSubWins[subWinIndex]][i]==1:
           SAFE.append(0.0)
          else:
           SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafs[snpIndicesInSubWins[subWinIndex]][i]*(1-dafs[snpIndicesInSubWins[subWinIndex]][i]))))
        for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
          if i=="SFS":
            windowStats=dafs[snpIndicesInSubWins[subWinIndex]]
          elif i=="HAFunique":
            # windowStats=[eval(i)[x] for x in eval(i)]
            windowStats = list(HAFunique.values()) # SCOTT
          else:
            windowStats=eval(i)
          statVals[i+"-Mean"][instanceIndex].append(np.mean(windowStats))
          statVals[i+"-Median"][instanceIndex].append(np.median(windowStats))
          if(len(np.unique(windowStats,return_counts=True)[1])==1):
            statVals[i+"-Mode"][instanceIndex].append(windowStats[0])
          else:
            if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
              statVals[i+"-Mode"][instanceIndex].append(scipy.stats.mstats.mode(windowStats)[0][0])
            else:
              mode=min(windowStats)
              for j in range(1,51):
                if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
                  mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
              statVals[i+"-Mode"][instanceIndex].append(mode+((max(windowStats)-min(windowStats))/100))
          for j in quantiles:
            statVals[i+"-"+j][instanceIndex].append(np.percentile(windowStats,quantiles[j]))
          statVals[i+"-Max"][instanceIndex].append(max(windowStats))
          statVals[i+"-Var"][instanceIndex].append(np.var(windowStats))
          statVals[i+"-SD"][instanceIndex].append(np.std(windowStats))
          statVals[i+"-Skew"][instanceIndex].append(scipy.stats.skew(windowStats))
          statVals[i+"-Kurt"][instanceIndex].append(scipy.stats.kurtosis(windowStats))
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][instanceIndex])
  fvecFile.write("\t".join([str(x) for x in outVec])+"\n")

for subWinIndex in range(numSubWins):
  statFiles[subWinIndex].close()
fvecFile.close()
# sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, bunya template Neutral}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=8G
#SBATCH --job-name=[SAMPLE]
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/slurm_training_convert_to_FVs_Python3_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/slurm_training_convert_to_FVs_Python3_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/

msOut="[SAMPLE]"

srun python /home/uqsalle3/Programs/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spNeut_COMBINED_stats.txt\
  /scratch/project/chenoase/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell}

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spHard_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_10.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/

find ./ -maxdepth 1 -type f -name '*.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt



# Neutral

grep 'Neut' msOut_files.txt > temp.txt
grep -v 'COMBINED' temp.txt > msOut_files_Neutral.txt
rm temp.txt

while read p; do
  echo "$p"
  cp template_training_convert_to_FVs_Python3_Neutral.txt training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
done < msOut_files_Neutral.txt



# Function to process files based on the specified prefix
process_files() {
    local prefix="$1"
    local template="template_training_convert_to_FVs_Python3_${prefix}.txt"
    local output_file="msOut_files_${prefix}.txt"

    # Create a list of files based on the prefix
    grep "${prefix}_" msOut_files.txt > temp.txt
    grep -v 'COMBINED' temp.txt > "${output_file}"
    rm temp.txt

    # Process each file in the list
    while read -r p; do
        echo "$p"
        cp "${template}" "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh"
        replace_CMD="s/\[SAMPLE\]/${p}/g"
        perl -pi -e "$replace_CMD" "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh"
    done < "${output_file}"
}

# Loop through different prefixes
for i in {0..10}; do
    process_files "Hard_${i}"
done

for i in {0..10}; do
    process_files "Soft_${i}"
done

for i in {0..10}; do
    process_files "HardPartial_${i}"
done

for i in {0..10}; do
    process_files "SoftPartial_${i}"
done



find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*.slurm.sh' -print0 | xargs -0 chmod 755

```

```{shell, submit scripts Bunya}

# RUNNING
while read p; do
  echo "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh submitted"
  sbatch training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
  sleep 0.05
done < msOut_files_Neutral.txt

# Function to submit files based on the specified prefix
submit_jobs() {
    local prefix="$1"
    echo "training_convert_to_FVs_Python3_${prefix}.slurm.sh submitted"
    sbatch training_convert_to_FVs_Python3_${prefix}.slurm.sh
    sleep 0.05
}

# Loop through different prefixes

# RUNNING
for i in {1..2000}; do
    submit_jobs "spHard_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spSoft_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spHardPartial_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spSoftPartial_5_${i}"
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spHard_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spSoft_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spHardPartial_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spSoftPartial_${i}_${j}"
    done
done



ls ./spNeut_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_5_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spHard_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spSoft_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spHardPartial_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spSoftPartial_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_10_*.fvec | grep -v 'COMBINED' | wc -l

## rerun failed all

rm rerun_failed_training_convert_to_FVs_Python3.sh
grep 'error' ./slurm_training_convert_to_FVs_Python3_sp*.error | perl -ne 's/\.\/slurm_/sbatch\ /; s/\.msOut\.gz.*/\.slurm\.sh/; print $_;' | uniq > rerun_failed_training_convert_to_FVs_Python3.sh
cat rerun_failed_training_convert_to_FVs_Python3.sh

bash rerun_failed_training_convert_to_FVs_Python3.sh

```

```{shell, combined fvec}

process_files() {
  local PREFIX="$1"
  find ./ -maxdepth 1 -type f | grep -E "${PREFIX}_[0-9]+\.fvec" > temp.txt
  
  while read -r p; do
    wc ${p}
  done < temp.txt
  
  while read -r p; do
    awk 'NR == 2' ${p}
  done < temp.txt > temp.fvec

  # Extract the first line from ${PREFIX}_1.msOut
  head -n 1 "${PREFIX}_1.fvec" > header.txt

  # Concatenate the edited header with temp.fvec
  cat header.txt temp.fvec > "${PREFIX}_SINGLE_COMBINED.fvec"
}

rm -v ./spNeut_SINGLE_COMBINED.fvec
for PREFIX in spNeut; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spHard_*_SINGLE_COMBINED.fvec
for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_5 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spSoft_*_SINGLE_COMBINED.fvec
for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_5 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spHardPartial_*_SINGLE_COMBINED.fvec
for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_5 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spSoftPartial_*_SINGLE_COMBINED.fvec
for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_5 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done



find ./ -maxdepth 1 -type f -name '*_SINGLE_COMBINED.fvec' -exec wc {} \;

```

```{shell, transfer to Dell 2}

# constNe_intergenic

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/*_SINGLE_COMBINED.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat/*_SINGLE_COMBINED.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat/

# There should be 45 fvec files
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/*.fvec | wc -l
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat/*.fvec | wc -l

```

```{r, assess feature vectors}

# module load anaconda3
# source ~/conda-init
# conda activate r_env

library(tidyverse)
library(data.table)

# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic")
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat")

spHard_5 <- read_tsv("spHard_5_SINGLE_COMBINED.fvec")

spHard_5 %>% 
  mutate(sim=c(1:nrow(spHard_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

spSoft_5 <- read_tsv("spSoft_5_SINGLE_COMBINED.fvec")

spSoft_5 %>% 
  mutate(sim=c(1:nrow(spSoft_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

```

### Training - training_sample_FVs

```{python, training_sample_FVs.py}

# import sys,os,random
import sys # SCOTT
import os # SCOTT
import random # SCOTT

'''usage eg:
python2 training_sample_FVs.py spNeut.fvec spHard_ spSoft_ spPartialHard_ spPartialSoft_ 5 0,1,2,3,4,6,7,8,9,10 ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec
'''

if len(sys.argv)!=10:
  sys.exit("usage:\npython2 training_sample_FVs.py neutTrainingFileName hardTrainingFilesPrefix softTrainingFilesPrefix partialHardTrainingFilesPrefix partialSoftTrainingFilesPrefix sweepTrainingWindow linkedTrainingWindows sampledFVsDir sampledFVsFiles\n")
else:
  neutTrainingFileName, hardTrainingFilesPrefix, softTrainingFilesPrefix, partialHardTrainingFilesPrefix, partialSoftTrainingFilesPrefix, sweepTrainingWindow, linkedTrainingWindows, sampledFVsDir, sampledFVsFiles = sys.argv[1:]

neutTrainingFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spNeut.fvec"
hardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_"
softTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spSoft_"
partialHardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialHard_"
partialSoftTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialSoft_"
sweepTrainingWindow=5
linkedTrainingWindows=0,1,2,3,4,6,7,8,9,10
sampledFVsDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe"
sampledFVsFiles="neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec"

sweepFilePaths,linkedFilePaths = {},{}
for trainingFilePrefix in [hardTrainingFilesPrefix,softTrainingFilesPrefix,partialHardTrainingFilesPrefix,partialSoftTrainingFilesPrefix]:
  trainingSetDir="/".join(trainingFilePrefix.split("/")[:-1])
  trainingFilePrefixDirless=trainingFilePrefix.split("/")[-1]
  sweepFilePaths[trainingFilePrefix]=[]
  linkedFilePaths[trainingFilePrefix]=[]
  for fileName in os.listdir(trainingSetDir):
    if fileName.startswith(trainingFilePrefixDirless):
      winNum=int(fileName.split("_")[1].split(".")[0])
      if winNum==int(sweepTrainingWindow):
        sweepFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)
      elif winNum in [int(x) for x in linkedTrainingWindows.split(",")]:
        linkedFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)

def getExamplesFromFVFile(simFileName):
  try:
    simFile=open(simFileName)
    lines=[line.strip() for line in simFile.readlines()]
    header=lines[0]
    examples=lines[1:]
    simFile.close()
    return header,examples
  except Exception:
    return "",[]

def getExamplesFromFVFileLs(simFileLs):
  examples=[]
  keptHeader=""
  for filePath in simFileLs:
    header,currExamples=getExamplesFromFVFile(filePath)
    if header:
      keptHeader=header
    examples+=currExamples
  return keptHeader,examples

header,neutExamples=getExamplesFromFVFile(neutTrainingFileName)
hardHeader,hardExamples=getExamplesFromFVFileLs(sweepFilePaths[hardTrainingFilesPrefix])
linkedHardHeader,linkedHardExamples=getExamplesFromFVFileLs(linkedFilePaths[hardTrainingFilesPrefix])
softHeader,softExamples=getExamplesFromFVFileLs(sweepFilePaths[softTrainingFilesPrefix])
linkedSoftHeader,linkedSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[softTrainingFilesPrefix])
partialHardHeader,partialHardExamples=getExamplesFromFVFileLs(sweepFilePaths[partialHardTrainingFilesPrefix])
linkedPartialHardHeader,linkedPartialHardExamples=getExamplesFromFVFileLs(linkedFilePaths[partialHardTrainingFilesPrefix])
partialSoftHeader,partialSoftExamples=getExamplesFromFVFileLs(sweepFilePaths[partialSoftTrainingFilesPrefix])
linkedPartialSoftHeader,linkedPartialSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[partialSoftTrainingFilesPrefix])

def getMinButNonZeroExamples(lsLs):
  counts=[]
  for ls in lsLs:
    if len(ls)>0:
      counts.append(len(ls))
  if not counts:
    raise Exception
  return min(counts)

trainingSetLs=[hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
numExamplesToKeep=getMinButNonZeroExamples(trainingSetLs)
for i in range(len(trainingSetLs)):
  random.shuffle(trainingSetLs[i])
  trainingSetLs[i]=trainingSetLs[i][:numExamplesToKeep]
hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples=trainingSetLs

if sampledFVsDir.lower() in ["none","false","default"]:
  sampledFVsDir='./'
if sampledFVsFiles.lower() in ["none","false","default"]:
  sampledFVsFiles=["neut.fvec","hard.fvec","linkedHard.fvec","soft.fvec","linkedSoft.fvec","partialHard.fvec","linkedPartialHard.fvec","partialSoft.fvec","linkedPartialSoft.fvec"]
else:
  sampledFVsFiles=sampledFVsFiles.split(",")
  assert len(sampledFVsFiles)==9
outExamples=[neutExamples,hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
for i in range(len(sampledFVsFiles)):
  if outExamples[i]:
    outFile=open('/'.join((sampledFVsDir+'/'+sampledFVsFiles[i]).split('//')),"w")
    outFile.write("classLabel\t%s\n" %(hardHeader))
    for example in outExamples[i]:
      outFile.write("%s\t%s\n" %(sampledFVsFiles[i].replace(".fvec",""),example))
    outFile.close()

```

```{shell, test Dell 2}

# expansionNe_intergenic_lci

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs

for file in *_SINGLE_COMBINED.fvec; do
  mv "$file" "${file/_SINGLE_COMBINED.fvec/.fvec}"
done

for file in spHardPartial_*.fvec; do
  mv "$file" "${file/spHardPartial_/spPartialHard_}"
done

for file in spSoftPartial_*.fvec; do
  mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
done

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

```

### Training

```{shell, test Dell 2}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

# expansionNe_intergenic_lci

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci
mkdir -p FVs && cd ./FVs

# for i in 1 2 3 4 5 6 7 8 9 10; do
  time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    expansionNe_intergenic_lci.hdf5\
    expansionNe_intergenic_lci.json\
    expansionNe_intergenic_lci.npy > training_deep_learning_python3.log 2>&1
# done

tensorboard --logdir logs

# time on 40 cores ~12 minutes with 5GB RAM

```

#### Training with early stopping

```{shell, test Dell 2}

# module load anaconda3
# source ~/conda-init
# conda activate partialshic_py3
# 
# # expansionNe
# 
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
# mkdir -p FVs && cd ./FVs
# mkdir -p earlyStopping && cd ./earlyStopping
# rm ./*
# cp ../*.fvec
# 
# time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_test.hdf5\
#     expansionNe_earlyStopping_test.json\
#     expansionNe_earlyStopping_test.npy
# 
# for i in 1 2 3 4 5 6 7 8 9 10; do
#   time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_${i}.hdf5\
#     expansionNe_earlyStopping_${i}.json\
#     expansionNe_earlyStopping_${i}.npy > training_deep_learning_python3_earlyStopping_${i}.log 2>&1
# done
# 
# # time on 40 cores ~12 minutes with 5GB RAM

```

#### Training with early stopping and 10-fold CV

```{shell, test Dell 2}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
rm ./*
cp ../*.fvec ./

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping_kFold.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    expansionNe_earlyStopping_fold.hdf5\
    expansionNe_earlyStopping_fold.json\
    expansionNe_earlyStopping_fold.npy

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
tensorboard --logdir logs

# for i in 1 2 3 4 5 6 7 8 9 10; do
#   time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     constNe_earlyStopping_${i}.hdf5\
#     constNe_earlyStopping_${i}.json\
#     constNe_earlyStopping_${i}.npy > training_deep_learning_python3_earlyStopping_${i}.log 2>&1
# done

# time on 40 cores ~12 minutes with 5GB RAM

```

### Run testing

#### test on self

```{shell}

# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
# mkdir -p test_self && cd test_self
# rm ./*
# cp ../FVs/*.fvec ./
# # cp ../../expansionNe/FVs/*.fvec ./
# rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
# 
# for file in spHardPartial_*.fvec; do
#   mv "$file" "${file/spHardPartial_/spPartialHard_}"
# done
# 
# for file in spSoftPartial_*.fvec; do
#   mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
# done
# 
# python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
#  ../FVs/expansionNe.npy\
#  ./\
#  11\
#  92\
#  ./\
#  expansionNe_accuracy\
#  expansionNe_confusion_matrix.pdf
# 
# evince expansionNe_confusion_matrix.pdf

```

#### test on self-new data

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp ../../*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  for file in *_SINGLE_COMBINED.fvec; do
    mv "$file" "${file/_SINGLE_COMBINED.fvec/.fvec}"
  done
  for file in spHardPartial_*.fvec; do
    mv "$file" "${file/spHardPartial_/spPartialHard_}"
  done
  for file in spSoftPartial_*.fvec; do
    mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
  done
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince constNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/
  mkdir -p test_expansionNe_intergenic && cd test_expansionNe_intergenic
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on constNe

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/
  mkdir -p test_constNe && cd test_constNe
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on constNe_intergenic

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/
  mkdir -p test_constNe_intergenic && cd test_constNe_intergenic
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic_uci

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/
  mkdir -p test_expansionNe_intergenic_uci && cd test_expansionNe_intergenic_uci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on empirical

```{python}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell, OLD}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

mkdir -p pred_expansionNe_intergenic && cd pred_expansionNe_intergenic

# ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_542_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_542_${i}.log 2>&1
done

# made predictions for 2475 total instances
# predicted 560 neutral regions (0.226263 of all classified regions)
# predicted 81 Hard sweep regions (0.032727 of all classified regions)
# predicted 292 Hard-linked sweep regions (0.117980 of all classified regions)
# predicted 136 Soft sweep regions (0.054949 of all classified regions)
# predicted 1190 Soft-linked sweep regions (0.480808 of all classified regions)
# predicted 8 HardPartial sweep regions (0.003232 of all classified regions)
# predicted 24 HardPartial-linked sweep regions (0.009697 of all classified regions)
# predicted 33 SoftPartial sweep regions (0.013333 of all classified regions)
# predicted 151 SoftPartial-linked sweep regions (0.061010 of all classified regions)

# made predictions for 2475 total instances
# predicted 821 neutral regions (0.331717 of all classified regions)
# predicted 43 Hard sweep regions (0.017374 of all classified regions)
# predicted 403 Hard-linked sweep regions (0.162828 of all classified regions)
# predicted 99 Soft sweep regions (0.040000 of all classified regions)
# predicted 866 Soft-linked sweep regions (0.349899 of all classified regions)
# predicted 9 HardPartial sweep regions (0.003636 of all classified regions)
# predicted 52 HardPartial-linked sweep regions (0.021010 of all classified regions)
# predicted 5 SoftPartial sweep regions (0.002020 of all classified regions)
# predicted 177 SoftPartial-linked sweep regions (0.071515 of all classified regions)

# ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_594_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_594_${i}.log 2>&1
done

# made predictions for 3117 total instances
# predicted 1121 neutral regions (0.359641 of all classified regions)
# predicted 31 Hard sweep regions (0.009945 of all classified regions)
# predicted 112 Hard-linked sweep regions (0.035932 of all classified regions)
# predicted 260 Soft sweep regions (0.083414 of all classified regions)
# predicted 1375 Soft-linked sweep regions (0.441129 of all classified regions)
# predicted 15 HardPartial sweep regions (0.004812 of all classified regions)
# predicted 29 HardPartial-linked sweep regions (0.009304 of all classified regions)
# predicted 33 SoftPartial sweep regions (0.010587 of all classified regions)
# predicted 141 SoftPartial-linked sweep regions (0.045236 of all classified regions)

made predictions for 3117 total instances
predicted 1474 neutral regions (0.472891 of all classified regions)
predicted 19 Hard sweep regions (0.006096 of all classified regions)
predicted 148 Hard-linked sweep regions (0.047482 of all classified regions)
predicted 169 Soft sweep regions (0.054219 of all classified regions)
predicted 1121 Soft-linked sweep regions (0.359641 of all classified regions)
predicted 13 HardPartial sweep regions (0.004171 of all classified regions)
predicted 47 HardPartial-linked sweep regions (0.015079 of all classified regions)
predicted 3 SoftPartial sweep regions (0.000962 of all classified regions)
predicted 123 SoftPartial-linked sweep regions (0.039461 of all classified regions)

# ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_628_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_628_${i}.log 2>&1
done

# made predictions for 4618 total instances
# predicted 1276 neutral regions (0.276310 of all classified regions)
# predicted 53 Hard sweep regions (0.011477 of all classified regions)
# predicted 277 Hard-linked sweep regions (0.059983 of all classified regions)
# predicted 328 Soft sweep regions (0.071026 of all classified regions)
# predicted 2287 Soft-linked sweep regions (0.495236 of all classified regions)
# predicted 10 HardPartial sweep regions (0.002165 of all classified regions)
# predicted 36 HardPartial-linked sweep regions (0.007796 of all classified regions)
# predicted 72 SoftPartial sweep regions (0.015591 of all classified regions)
# predicted 279 SoftPartial-linked sweep regions (0.060416 of all classified regions)

made predictions for 4618 total instances
predicted 1926 neutral regions (0.417064 of all classified regions)
predicted 44 Hard sweep regions (0.009528 of all classified regions)
predicted 388 Hard-linked sweep regions (0.084019 of all classified regions)
predicted 237 Soft sweep regions (0.051321 of all classified regions)
predicted 1704 Soft-linked sweep regions (0.368991 of all classified regions)
predicted 21 HardPartial sweep regions (0.004547 of all classified regions)
predicted 56 HardPartial-linked sweep regions (0.012126 of all classified regions)
predicted 10 SoftPartial sweep regions (0.002165 of all classified regions)
predicted 232 SoftPartial-linked sweep regions (0.050238 of all classified regions)

# ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_718_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_718_${i}.log 2>&1
done

# made predictions for 1375 total instances
# predicted 220 neutral regions (0.160000 of all classified regions)
# predicted 29 Hard sweep regions (0.021091 of all classified regions)
# predicted 97 Hard-linked sweep regions (0.070545 of all classified regions)
# predicted 116 Soft sweep regions (0.084364 of all classified regions)
# predicted 704 Soft-linked sweep regions (0.512000 of all classified regions)
# predicted 1 HardPartial sweep regions (0.000727 of all classified regions)
# predicted 24 HardPartial-linked sweep regions (0.017455 of all classified regions)
# predicted 29 SoftPartial sweep regions (0.021091 of all classified regions)
# predicted 155 SoftPartial-linked sweep regions (0.112727 of all classified regions)

made predictions for 1375 total instances
predicted 288 neutral regions (0.209455 of all classified regions)
predicted 20 Hard sweep regions (0.014545 of all classified regions)
predicted 173 Hard-linked sweep regions (0.125818 of all classified regions)
predicted 88 Soft sweep regions (0.064000 of all classified regions)
predicted 573 Soft-linked sweep regions (0.416727 of all classified regions)
predicted 12 HardPartial sweep regions (0.008727 of all classified regions)
predicted 54 HardPartial-linked sweep regions (0.039273 of all classified regions)
predicted 8 SoftPartial sweep regions (0.005818 of all classified regions)
predicted 159 SoftPartial-linked sweep regions (0.115636 of all classified regions)

# ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_76_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_76_${i}.log 2>&1
done

# made predictions for 6080 total instances
# predicted 1648 neutral regions (0.271053 of all classified regions)
# predicted 57 Hard sweep regions (0.009375 of all classified regions)
# predicted 219 Hard-linked sweep regions (0.036020 of all classified regions)
# predicted 474 Soft sweep regions (0.077961 of all classified regions)
# predicted 3178 Soft-linked sweep regions (0.522697 of all classified regions)
# predicted 3 HardPartial sweep regions (0.000493 of all classified regions)
# predicted 49 HardPartial-linked sweep regions (0.008059 of all classified regions)
# predicted 88 SoftPartial sweep regions (0.014474 of all classified regions)
# predicted 364 SoftPartial-linked sweep regions (0.059868 of all classified regions)

made predictions for 6080 total instances
predicted 2419 neutral regions (0.397862 of all classified regions)
predicted 57 Hard sweep regions (0.009375 of all classified regions)
predicted 368 Hard-linked sweep regions (0.060526 of all classified regions)
predicted 332 Soft sweep regions (0.054605 of all classified regions)
predicted 2420 Soft-linked sweep regions (0.398026 of all classified regions)
predicted 19 HardPartial sweep regions (0.003125 of all classified regions)
predicted 78 HardPartial-linked sweep regions (0.012829 of all classified regions)
predicted 14 SoftPartial sweep regions (0.002303 of all classified regions)
predicted 373 SoftPartial-linked sweep regions (0.061349 of all classified regions)

# ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

for i in $(seq 1 10); do
  python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe_${i}.npy\
    ../ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec\
    11\
    92\
    ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_${i}.pred.bed >\
    empirical_deep_learning_classify_ScA8VGg_785_${i}.log 2>&1
done

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/expansionNe2.npy\
 ../ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred2.pred.bed

# made predictions for 3836 total instances
# predicted 1085 neutral regions (0.282847 of all classified regions)
# predicted 33 Hard sweep regions (0.008603 of all classified regions)
# predicted 145 Hard-linked sweep regions (0.037800 of all classified regions)
# predicted 315 Soft sweep regions (0.082117 of all classified regions)
# predicted 1903 Soft-linked sweep regions (0.496090 of all classified regions)
# predicted 9 HardPartial sweep regions (0.002346 of all classified regions)
# predicted 33 HardPartial-linked sweep regions (0.008603 of all classified regions)
# predicted 61 SoftPartial sweep regions (0.015902 of all classified regions)
# predicted 252 SoftPartial-linked sweep regions (0.065693 of all classified regions)

made predictions for 3836 total instances
predicted 1521 neutral regions (0.396507 of all classified regions)
predicted 19 Hard sweep regions (0.004953 of all classified regions)
predicted 229 Hard-linked sweep regions (0.059698 of all classified regions)
predicted 206 Soft sweep regions (0.053702 of all classified regions)
predicted 1523 Soft-linked sweep regions (0.397028 of all classified regions)
predicted 14 HardPartial sweep regions (0.003650 of all classified regions)
predicted 73 HardPartial-linked sweep regions (0.019030 of all classified regions)
predicted 11 SoftPartial sweep regions (0.002868 of all classified regions)
predicted 240 SoftPartial-linked sweep regions (0.062565 of all classified regions)

```

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/
mkdir -p pred_empirical && cd pred_empirical
rsync -ahPv /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/*_110Lines.phased.noPIRs.inbred.fvec ./

for CONTIG in 542 594 628 718 76 785; do
  for i in 1 2 3 4 5 6 7 8 9 10; do
    python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
      /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
      ./ScA8VGg_${CONTIG}_110Lines.phased.noPIRs.inbred.fvec\
      11\
      92\
      ScA8VGg_${CONTIG}.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_fold_${i}.pred.bed >\
      empirical_deep_learning_classify_ScA8VGg_${CONTIG}_fold_${i}.log 2>&1
  done
done

```

#### summary prediction

```{bash}

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical

```

```{r}

library(tidyverse)
library(data.table)
library(RColorBrewer)
# library(ggplot2)
# library(dplyr)
# library(tidyr)
library(cowplot)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
  pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
  mutate(pred_ensemble = class[prob==max(prob)]) %>% 
  pivot_wider(names_from=class, values_from=prob)



# a1 <- pred %>% 
#   extract('V4','State','(.*)_ScA8VGg.*') %>% 
#   mutate(V1=gsub('chr','', V1))


# a1_df <- data.table(table(a1$State))
a1_df <- data.table(table(ensemble$pred_ensemble))
# hard_num <- pred_df[V1=="hard",N]
# soft_num <- pred_df[V1=="soft",N]
# hard_pct <- hard_num/(hard_num + soft_num)
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

aplot[V1 %in% c('HardPartial','HardPartial-linked'),lbl2:=lbl]


PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
    geom_bar(width=1, stat="identity", color="white") +
    coord_polar("y", start=0) +
    geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
                  y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), label=lbl2), 
              color="black", size=3) +
    scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                               my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                               "grey60")) +
    theme_void() + theme(legend.position = "none")



a1 <- data.table(ensemble)

a1[,chr:="xxxx"][contig=="chrScA8VGg_76",chr:="3R"][contig=="chrScA8VGg_718",chr:="2L"][contig=="chrScA8VGg_542",chr:="2L"][contig=="chrScA8VGg_594",chr:="X"][contig=="chrScA8VGg_628",chr:="3L"][contig=="chrScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$pred_ensemble)) -> a3
a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))

# ggsave("diploSHIC_proprotion_figure1.pdf",PP, width = 8, height = 4, 
#        dpi = 300, units = "in")


# chisquare test
library(rcompanion)

bc <- table(a2$chr, a2$pred_ensemble)

b2 <- cbind(rowSums(bc[,c(1,3)]), rowSums(bc[,c(2,4:9)]))
# 
# b1 <- cbind(bc,other = rowSums(bc[,2:9]))
# b2 <- b1[,c(1,10)]
chisq.test(b2)
b2 <- matrix(c(29, 43, 59, 46, 56, 3600, 3876, 4249, 5688, 4857), ncol=2)
colnames(b2) <- c('hard','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")
apply(b2,1,prop.table)

# c1 <- cbind(bc,other = rowSums(bc[,c(1:5,7:9)]))
# c2 <- c1[,c(6,10)
c2 <- cbind(rowSums(bc[,c(6,8)]), rowSums(bc[,c(1:5,7,9)]))
chisq.test(c2)

c2 <- matrix(c(381, 452, 503, 579, 615, 3248, 3467, 3805, 5155, 4298), ncol=2)
colnames(c2) <- c('soft','others')
rownames(c2) <- c('2L','2R','3L','3R','X')
pairwiseNominalIndependence(c2, fisher = F, gtest = F, chisq = T, method = "fdr")
apply(c2,1,prop.table)

ph <- cbind(bc[,3], rowSums(bc[,c(1:2,4:9)]))
apply(ph,1,prop.table)

```

# ######################## PartialSHIC - expansionNe intergenic UCI

## Generate training data

* Bunya

```{shell, install discoal}

cd /scratch/project/chenoase/partialSHIC_DsGRP

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=1\
 --mem=2G\
 --job-name=uqsalle3_TEST\
 --time=00:30:00\
 --partition=general\
 --account=a_chenoweth srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

git clone https://github.com/kr-colab/discoal.git

cd /scratch/project/chenoase/partialSHIC_DsGRP/discoal

make discoal

```

* Dell 2 port from R to bash

```{shell}

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

# setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat')

DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal' 
TIMES = 2000

Ne = 5182173
# Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000001929692
# s_high = 0.01
s_high = 0.001929692
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

20000/(2*Ne)

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.001929692 # 0.001929692*4*5182173 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
# bottleneck <- "-en 0.007292 0 1.5 -en 0.02135 0 0.547 -en 0.05208 0 0.208 -en 0.09271 0 0.0805"

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
# bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
cat("bottleneck: ", bottleneck, "\n")
cat("sweep_pos: ", sweep_pos, "\n")

```

```{r}
TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.000009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"
```


* Back to Bunya

```{shell, bunya example}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name=Neut_sim
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/slurm_discoal_Neutral.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/slurm_discoal_Neutral.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/

DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell}

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_10.txt

```

```{shell, create scripts}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/
  
## Neutral

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Neutral.txt discoal_Neutral_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Neutral_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Hard_5.txt discoal_Hard_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Hard_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Soft_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Soft_5.txt discoal_Soft_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Soft_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## HardPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_HardPartial_5.txt discoal_HardPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_HardPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## SoftPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_SoftPartial_5.txt discoal_SoftPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_SoftPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## Soft-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## HardPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## SoftPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

```

```{shell, submit}

## Neutral (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Neutral.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral.txt

## Hard_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Hard_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5.txt

## Soft_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Soft_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5.txt

## HardPartial_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_HardPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5.txt

## SoftPartial_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_SoftPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5.txt

## Hard-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## Soft-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## HardPartial-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done

## SoftPartial-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.25
  done < list_discoal_files_${PREFIX}.txt
done



ls ./spNeut_*.msOut.gz | wc -l
ls ./spHard_5_*.msOut.gz | wc -l
ls ./spSoft_5_*.msOut.gz | wc -l
ls ./spHardPartial_5_*.msOut.gz | wc -l
ls ./spSoftPartial_5_*.msOut.gz | wc -l



ls ./spHard_0_*.msOut.gz | wc -l
ls ./spHard_1_*.msOut.gz | wc -l
ls ./spHard_2_*.msOut.gz | wc -l
ls ./spHard_3_*.msOut.gz | wc -l
ls ./spHard_4_*.msOut.gz | wc -l
ls ./spHard_6_*.msOut.gz | wc -l
ls ./spHard_7_*.msOut.gz | wc -l
ls ./spHard_8_*.msOut.gz | wc -l
ls ./spHard_9_*.msOut.gz | wc -l
ls ./spHard_10_*.msOut.gz | wc -l



ls ./spSoft_0_*.msOut.gz | wc -l
ls ./spSoft_1_*.msOut.gz | wc -l
ls ./spSoft_2_*.msOut.gz | wc -l
ls ./spSoft_3_*.msOut.gz | wc -l
ls ./spSoft_4_*.msOut.gz | wc -l
ls ./spSoft_6_*.msOut.gz | wc -l
ls ./spSoft_7_*.msOut.gz | wc -l
ls ./spSoft_8_*.msOut.gz | wc -l
ls ./spSoft_9_*.msOut.gz | wc -l
ls ./spSoft_10_*.msOut.gz | wc -l



ls ./spHardPartial_0_*.msOut.gz | wc -l
ls ./spHardPartial_1_*.msOut.gz | wc -l
ls ./spHardPartial_2_*.msOut.gz | wc -l
ls ./spHardPartial_3_*.msOut.gz | wc -l
ls ./spHardPartial_4_*.msOut.gz | wc -l
ls ./spHardPartial_6_*.msOut.gz | wc -l
ls ./spHardPartial_7_*.msOut.gz | wc -l
ls ./spHardPartial_8_*.msOut.gz | wc -l
ls ./spHardPartial_9_*.msOut.gz | wc -l
ls ./spHardPartial_10_*.msOut.gz | wc -l



ls ./spSoftPartial_0_*.msOut.gz | wc -l
ls ./spSoftPartial_1_*.msOut.gz | wc -l
ls ./spSoftPartial_2_*.msOut.gz | wc -l
ls ./spSoftPartial_3_*.msOut.gz | wc -l
ls ./spSoftPartial_4_*.msOut.gz | wc -l
ls ./spSoftPartial_6_*.msOut.gz | wc -l
ls ./spSoftPartial_7_*.msOut.gz | wc -l
ls ./spSoftPartial_8_*.msOut.gz | wc -l
ls ./spSoftPartial_9_*.msOut.gz | wc -l
ls ./spSoftPartial_10_*.msOut.gz | wc -l

```

```{shell, check and fix empty simulation}

## Neutral

rm list_discoal_files_Neutral_RERUN_10.txt
grep 'Out Of Memory' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt
grep ' MAXMUTS' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt
cat list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt > list_discoal_files_Neutral_RERUN_10.txt
wc -l list_discoal_files_Neutral_RERUN_10.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral_RERUN_10.txt

## Hard_5

rm list_discoal_files_Hard_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Hard_5_RERUN_OutOfMemory.txt list_discoal_files_Hard_5_RERUN_MAXMUTS.txt > list_discoal_files_Hard_5_RERUN.txt
wc -l list_discoal_files_Hard_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5_RERUN.txt

## Soft_5

rm list_discoal_files_Soft_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Soft_5_RERUN_OutOfMemory.txt list_discoal_files_Soft_5_RERUN_MAXMUTS.txt > list_discoal_files_Soft_5_RERUN.txt
wc -l list_discoal_files_Soft_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5_RERUN.txt

## HardPartial_5

rm list_discoal_files_HardPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_HardPartial_5_RERUN.txt
wc -l list_discoal_files_HardPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5_RERUN.txt

## SoftPartial_5

rm list_discoal_files_SoftPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_SoftPartial_5_RERUN.txt
wc -l list_discoal_files_SoftPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5_RERUN.txt

## check all

rm list_discoal_files_RERUN.txt
grep 'Out Of Memory' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' >  list_discoal_files_RERUN_MAXMUTS.txt
cat list_discoal_files_RERUN_OutOfMemory.txt list_discoal_files_RERUN_MAXMUTS.txt > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt


rm list_discoal_files_RERUN.txt
find ./ -maxdepth 1 -type f -name '*.msOut.gz' -size -21c | sed 's/\.\/sp/discoal_/; s/msOut\.gz/slurm\.sh/; s/Neut/Neutral/;' > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt

```

```{shell, combine simulations}

# Function to process and combine sweep files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail ./${PREFIX}_*.msOut -n +3 > temp1.msOut
    perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
    awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/2000/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spNeut spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
    process_and_combine "$PREFIX"
done



# Function to process and combine linked files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail -n +3 ./${PREFIX}_*.msOut > temp1.msOut
    grep -v "$PREFIX" temp1.msOut > temp2.msOut
    awk 'BEGIN { RS="\n\n"; ORS="\n" } { print }' temp2.msOut > temp3.msOut
    
    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/400/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    process_and_combine "$PREFIX"
done

# # mini for testing
# 
# gzip -d spHard_5_1.msOut.gz
# gzip -d spHard_5_2.msOut.gz
# gzip -d spHard_5_3.msOut.gz
# 
# # Concatenate files and remove headers
# tail spHard_5_*.msOut -n +3 > temp1.msOut
# perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
# awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut
# 
# # Extract the first two lines from ${PREFIX}_1.msOut
# head -n 2 "spHard_5_1.msOut" > header.txt
# 
# # Edit the header to change the third space-delimited element from 1 to 2000
# sed -i '1s/\<1\>/3/' header.txt
# 
# # Concatenate the edited header with temp3.msOut
# cat header.txt temp3.msOut > spHard_5_COMBINED_MINI.msOut
# 
# # Move and compress the combined file
# gzip spHard_5_COMBINED_MINI.msOut
# 
# # Compress files
# find ./ -maxdepth 1 -type f -name "spHard_5_*.msOut" -exec gzip {} \;
# 
# # Clean up temporary files
# rm temp1.msOut temp2.msOut

```

## Generate test data

* Bunya

```{shell, install discoal}

cd /scratch/project/chenoase/partialSHIC_DsGRP

salloc\
 --nodes=1\
 --ntasks-per-node=1\
 --cpus-per-task=1\
 --mem=2G\
 --job-name=uqsalle3_TEST\
 --time=00:30:00\
 --partition=general\
 --account=a_chenoweth srun\
 --export=PATH,TERM,HOME,LANG\
 --pty /bin/bash -l

git clone https://github.com/kr-colab/discoal.git

cd /scratch/project/chenoase/partialSHIC_DsGRP/discoal

make discoal

```

* Dell 2 port from R to bash

```{shell}

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

# setwd('/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/test_self_newDat')

DISCOAL = '/home/uqsalle3/shared/Scott_Allen/discoal/discoal' 
TIMES = 2000

Ne = 1041174
# Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000009604543
# s_high = 0.01
s_high = 0.009604543
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.009604543 # 0.009604543*4*1041174 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
# bottleneck <- "-en 0.007292 0 1.5 -en 0.02135 0 0.547 -en 0.05208 0 0.208 -en 0.09271 0 0.0805"

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
# bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway
cat("bottleneck: ", bottleneck, "\n")
cat("sweep_pos: ", sweep_pos, "\n")

```
* Back to Bunya

```{shell, bunya example}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name=Neut_sim
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat/slurm_discoal_Neutral.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat/slurm_discoal_Neutral.error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_lci/test_self_newDat/

DISCOAL="/scratch/project/chenoase/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell}

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Hard_5.txt > template_discoal_Hard_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_Soft_5.txt > template_discoal_Soft_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_HardPartial_5.txt > template_discoal_HardPartial_10.txt

sed 's/_5/_0/g; s/\[5\]/\[0\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_0.txt
sed 's/_5/_1/g; s/\[5\]/\[1\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_1.txt
sed 's/_5/_2/g; s/\[5\]/\[2\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_2.txt
sed 's/_5/_3/g; s/\[5\]/\[3\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_3.txt
sed 's/_5/_4/g; s/\[5\]/\[4\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_4.txt
sed 's/_5/_6/g; s/\[5\]/\[6\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_6.txt
sed 's/_5/_7/g; s/\[5\]/\[7\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_7.txt
sed 's/_5/_8/g; s/\[5\]/\[8\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_8.txt
sed 's/_5/_9/g; s/\[5\]/\[9\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_9.txt
sed 's/_5/_10/g; s/\[5\]/\[10\]/g;' template_discoal_SoftPartial_5.txt > template_discoal_SoftPartial_10.txt

```

```{shell, create scripts}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/test_self_newDat/
  
## Neutral

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Neutral.txt discoal_Neutral_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Neutral_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Hard_5.txt discoal_Hard_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Hard_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Soft_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_Soft_5.txt discoal_Soft_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_Soft_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## HardPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_HardPartial_5.txt discoal_HardPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_HardPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## SoftPartial_5

for i in {1..2000}; do
  echo "$i"
  cp template_discoal_SoftPartial_5.txt discoal_SoftPartial_5_${i}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${i}/g"
  perl -pi -e "$replace_CMD" discoal_SoftPartial_5_${i}.slurm.sh
done

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' -print0 | xargs -0 chmod 755

## Hard-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## Soft-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## HardPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

## SoftPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"

  for i in {1..400}; do
    echo "$PREFIX $i"
    cp template_discoal_${PREFIX}.txt discoal_${PREFIX}_${i}.slurm.sh
    replace_CMD="s/\[SAMPLE\]/${i}/g"
    perl -pi -e "$replace_CMD" discoal_${PREFIX}_${i}.slurm.sh
  done

  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" -print0 | xargs -0 chmod 755
done

```

```{shell, submit}

## Neutral (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Neutral_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Neutral.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_Neutral.txt

## Hard_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Hard_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Hard_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_Hard_5.txt

## Soft_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_Soft_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_Soft_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_Soft_5.txt

## HardPartial_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_HardPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_HardPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_HardPartial_5.txt

## SoftPartial_5 (DONE)

find ./ -maxdepth 1 -type f -name 'discoal_SoftPartial_5_*.slurm.sh' | sed 's/\.\///' | sort -V > list_discoal_files_SoftPartial_5.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_discoal_files_SoftPartial_5.txt

## Hard-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Hard_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.05
  done < list_discoal_files_${PREFIX}.txt
done

## Soft-linked (DONE)

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="Soft_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.05
  done < list_discoal_files_${PREFIX}.txt
done

## HardPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="HardPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.05
  done < list_discoal_files_${PREFIX}.txt
done

## SoftPartial-linked

for j in 0 1 2 3 4 6 7 8 9 10; do
  PREFIX="SoftPartial_${j}"
  find ./ -maxdepth 1 -type f -name "discoal_${PREFIX}_*.slurm.sh" | sed 's/\.\///' | sort -V > list_discoal_files_${PREFIX}.txt
  while read p; do
    echo "${p} submitted"
    sbatch ${p}
    sleep 0.05
  done < list_discoal_files_${PREFIX}.txt
done



ls ./spNeut_*.msOut.gz | wc -l
ls ./spHard_5_*.msOut.gz | wc -l
ls ./spSoft_5_*.msOut.gz | wc -l
ls ./spHardPartial_5_*.msOut.gz | wc -l
ls ./spSoftPartial_5_*.msOut.gz | wc -l



ls ./spHard_0_*.msOut.gz | wc -l
ls ./spHard_1_*.msOut.gz | wc -l
ls ./spHard_2_*.msOut.gz | wc -l
ls ./spHard_3_*.msOut.gz | wc -l
ls ./spHard_4_*.msOut.gz | wc -l
ls ./spHard_6_*.msOut.gz | wc -l
ls ./spHard_7_*.msOut.gz | wc -l
ls ./spHard_8_*.msOut.gz | wc -l
ls ./spHard_9_*.msOut.gz | wc -l
ls ./spHard_10_*.msOut.gz | wc -l



ls ./spSoft_0_*.msOut.gz | wc -l
ls ./spSoft_1_*.msOut.gz | wc -l
ls ./spSoft_2_*.msOut.gz | wc -l
ls ./spSoft_3_*.msOut.gz | wc -l
ls ./spSoft_4_*.msOut.gz | wc -l
ls ./spSoft_6_*.msOut.gz | wc -l
ls ./spSoft_7_*.msOut.gz | wc -l
ls ./spSoft_8_*.msOut.gz | wc -l
ls ./spSoft_9_*.msOut.gz | wc -l
ls ./spSoft_10_*.msOut.gz | wc -l



ls ./spHardPartial_0_*.msOut.gz | wc -l
ls ./spHardPartial_1_*.msOut.gz | wc -l
ls ./spHardPartial_2_*.msOut.gz | wc -l
ls ./spHardPartial_3_*.msOut.gz | wc -l
ls ./spHardPartial_4_*.msOut.gz | wc -l
ls ./spHardPartial_6_*.msOut.gz | wc -l
ls ./spHardPartial_7_*.msOut.gz | wc -l
ls ./spHardPartial_8_*.msOut.gz | wc -l
ls ./spHardPartial_9_*.msOut.gz | wc -l
ls ./spHardPartial_10_*.msOut.gz | wc -l



ls ./spSoftPartial_0_*.msOut.gz | wc -l
ls ./spSoftPartial_1_*.msOut.gz | wc -l
ls ./spSoftPartial_2_*.msOut.gz | wc -l
ls ./spSoftPartial_3_*.msOut.gz | wc -l
ls ./spSoftPartial_4_*.msOut.gz | wc -l
ls ./spSoftPartial_6_*.msOut.gz | wc -l
ls ./spSoftPartial_7_*.msOut.gz | wc -l
ls ./spSoftPartial_8_*.msOut.gz | wc -l
ls ./spSoftPartial_9_*.msOut.gz | wc -l
ls ./spSoftPartial_10_*.msOut.gz | wc -l

```

```{shell, check and fix empty simulation}

## Neutral

rm list_discoal_files_Neutral_RERUN_10.txt
grep 'Out Of Memory' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt
grep ' MAXMUTS' slurm_discoal_Neutral_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt
cat list_discoal_files_Neutral_RERUN_OutOfMemory_10.txt list_discoal_files_Neutral_RERUN_MAXMUTS_10.txt > list_discoal_files_Neutral_RERUN_10.txt
wc -l list_discoal_files_Neutral_RERUN_10.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Neutral_RERUN_10.txt

## Hard_5

rm list_discoal_files_Hard_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Hard_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Hard_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Hard_5_RERUN_OutOfMemory.txt list_discoal_files_Hard_5_RERUN_MAXMUTS.txt > list_discoal_files_Hard_5_RERUN.txt
wc -l list_discoal_files_Hard_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Hard_5_RERUN.txt

## Soft_5

rm list_discoal_files_Soft_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_Soft_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_Soft_5_RERUN_MAXMUTS.txt
cat list_discoal_files_Soft_5_RERUN_OutOfMemory.txt list_discoal_files_Soft_5_RERUN_MAXMUTS.txt > list_discoal_files_Soft_5_RERUN.txt
wc -l list_discoal_files_Soft_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_Soft_5_RERUN.txt

## HardPartial_5

rm list_discoal_files_HardPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_HardPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_HardPartial_5_RERUN_OutOfMemory.txt list_discoal_files_HardPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_HardPartial_5_RERUN.txt
wc -l list_discoal_files_HardPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_HardPartial_5_RERUN.txt

## SoftPartial_5

rm list_discoal_files_SoftPartial_5_RERUN.txt
grep 'Out Of Memory' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_SoftPartial_5_*.error | sed 's/slurm_//;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt
cat list_discoal_files_SoftPartial_5_RERUN_OutOfMemory.txt list_discoal_files_SoftPartial_5_RERUN_MAXMUTS.txt > list_discoal_files_SoftPartial_5_RERUN.txt
wc -l list_discoal_files_SoftPartial_5_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_SoftPartial_5_RERUN.txt

## check all

rm list_discoal_files_RERUN.txt
grep 'Out Of Memory' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' > list_discoal_files_RERUN_OutOfMemory.txt
grep ' MAXMUTS' slurm_discoal_*.error | sed 's/slurm_//;' | perl -ne 's/\.\.error.*/\.slurm\.sh/; print $_;' | perl -ne 's/\.error.*/\.slurm\.sh/; print $_;' >  list_discoal_files_RERUN_MAXMUTS.txt
cat list_discoal_files_RERUN_OutOfMemory.txt list_discoal_files_RERUN_MAXMUTS.txt > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt


rm list_discoal_files_RERUN.txt
find ./ -maxdepth 1 -type f -name '*.msOut.gz' -size -21c | sed 's/\.\/sp/discoal_/; s/msOut\.gz/slurm\.sh/; s/Neut/Neutral/;' > list_discoal_files_RERUN.txt
wc -l list_discoal_files_RERUN.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.25
done < list_discoal_files_RERUN.txt

```

```{shell, combine simulations}

# Function to process and combine sweep files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail ./${PREFIX}_*.msOut -n +3 > temp1.msOut
    perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
    awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut

    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/2000/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spNeut spHard_5 spSoft_5 spHardPartial_5 spSoftPartial_5; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoftPartial_5; do
    process_and_combine "$PREFIX"
done



# Function to process and combine linked files

process_and_combine() {
    PREFIX=$1
    echo "Processing files for $PREFIX"
    
    # Decompress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut.gz" -exec gzip -d {} \;
    
    # Concatenate files and remove headers
    tail -n +3 ./${PREFIX}_*.msOut > temp1.msOut
    grep -v "$PREFIX" temp1.msOut > temp2.msOut
    awk 'BEGIN { RS="\n\n"; ORS="\n" } { print }' temp2.msOut > temp3.msOut
    
    # Extract the first two lines from ${PREFIX}_1.msOut
    head -n 2 "${PREFIX}_1.msOut" > header.txt
    
    # Edit the header to change the third space-delimited element from 1 to 2000
    sed -i '1s/\<1\>/400/' header.txt
    
    # Concatenate the edited header with temp3.msOut
    cat header.txt temp3.msOut > ${PREFIX}_COMBINED.msOut
    
    # Move and compress the combined file
    gzip ${PREFIX}_COMBINED.msOut
    
    # Compress files
    find ./ -maxdepth 1 -type f -name "${PREFIX}_*.msOut" -exec gzip {} \;
    
    # Clean up temporary files
    rm temp1.msOut temp2.msOut
}

# Loop through prefixes

for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    process_and_combine "$PREFIX"
done

for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    process_and_combine "$PREFIX"
done

# # mini for testing
# 
# gzip -d spHard_5_1.msOut.gz
# gzip -d spHard_5_2.msOut.gz
# gzip -d spHard_5_3.msOut.gz
# 
# # Concatenate files and remove headers
# tail spHard_5_*.msOut -n +3 > temp1.msOut
# perl -ne 's/\=\=.*\n//; print $_;' temp1.msOut > temp2.msOut
# awk '/./ { e=0 } /^$/ { e += 1 } e <= 1' < temp2.msOut > temp3.msOut
# 
# # Extract the first two lines from ${PREFIX}_1.msOut
# head -n 2 "spHard_5_1.msOut" > header.txt
# 
# # Edit the header to change the third space-delimited element from 1 to 2000
# sed -i '1s/\<1\>/3/' header.txt
# 
# # Concatenate the edited header with temp3.msOut
# cat header.txt temp3.msOut > spHard_5_COMBINED_MINI.msOut
# 
# # Move and compress the combined file
# gzip spHard_5_COMBINED_MINI.msOut
# 
# # Compress files
# find ./ -maxdepth 1 -type f -name "spHard_5_*.msOut" -exec gzip {} \;
# 
# # Clean up temporary files
# rm temp1.msOut temp2.msOut

```

## Run training

```{python, training_deep_learning_python3.py}

# import time
# # startTime=time.clock() # Python 2
# startTime = time.perf_counter() # Python 3
# import sys
# import numpy as np
# np.random.seed(123)
# from sklearn.model_selection import train_test_split
# # from keras.utils import np_utils
# from keras.utils import to_categorical # SCOTT
# from keras.models import Sequential
# from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
# from keras import optimizers
# from keras.callbacks import EarlyStopping,ModelCheckpoint
# 
# '''usage eg:
# python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
# '''
# 
# if len(sys.argv)!=9:
#   sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
# else:
#   fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]
# 
# fvecDir='./'
# fvecFiles='neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec'
# numSubWins='11'
# numSumStatsPerSubWin='92'
# validationSize='0.1'
# weightsFileName='constNe.hdf5'
# jsonFileName='constNe.json'
# npyFileName='constNe.npy'
# 
# if fvecDir.lower() in ["none","false","default"]:
#   fvecDir='./'
# 
# if fvecFiles.lower() in ["none","false","default"]:
#   fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
# else:
#   fvecFiles=fvecFiles.split(",")
#   assert len(fvecFiles)==9
# 
# numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)
# 
# if validationSize.lower() in ["none","false","default"]:
#   validationSize=0.1
# else:
#   validationSize=float(validationSize)
# 
# sweeps=[]
# 
# for i in range(0,9):
#  sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
#  if i>0:
#   assert len(sweeps[0])==len(sweeps[i])
# 
# sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))
# 
# sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)
# 
# models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))
# 
# sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=42)
# 
# # models=np_utils.to_categorical(models,9)
# models=to_categorical(models,9) # SCOTT
# 
# # models_val=np_utils.to_categorical(models_val,9)
# models_val=to_categorical(models_val,9) # SCOTT
# 
# netlayers=Sequential()
# netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
# netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
# netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
# netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
# netlayers.add(Dropout(0.25))
# netlayers.add(Flatten())
# netlayers.add(Dense(512, activation='relu'))
# netlayers.add(Dropout(0.5))
# netlayers.add(Dense(128, activation='relu'))
# netlayers.add(Dropout(0.5))
# netlayers.add(Dense(9, activation='softmax'))
# netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
# 
# checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')
# 
# netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)
# 
# netlayers_json=netlayers.to_json()
# 
# with open(jsonFileName,"w") as json_file:
#   json_file.write(netlayers_json)
# 
# netlayers.save(npyFileName)
# 
# sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))

```

### Training - calcStatsAndDafForEachSnp

```{python, calcStatsAndDafForEachSnpSingleMsFile_Python3}

import sys
import allel
import random
import numpy as np
from msTools import *
from fvTools import *
import time

'''usage example
python calcStatsAndDafForEachSnpSingleMsFile.py /san/data/dan/simulations/discoal_multipopStuff/spatialSVMSims/trainingSets/equilibNeut.msout.gz 110000 iHS nSL
'''

trainingDataFileName, totalPhysLen = sys.argv[1:3]
totalPhysLen = int(totalPhysLen)

hapArraysIn, positionArrays = msOutToHaplotypeArrayIn(trainingDataFileName, totalPhysLen)
numInstances = len(hapArraysIn)

header = "daf\tiHS\tnSL"
#print header # Python 2
print(header) # Python 3

dafs = []
#start = time.clock() # Python 2
start = time.perf_counter() # Python 3
numInstancesDone = 0

numSnpsDone = 0
for instanceIndex in range(numInstances):
    if instanceIndex % 10 == 0:
        sys.stderr.write("done wtih %d of %d instances (%d SNPs)\n" %(instanceIndex, numInstances, numSnpsDone))
    haps = allel.HaplotypeArray(hapArraysIn[instanceIndex], dtype='i1')
    genos = haps.to_genotypes(ploidy=2)
    ac = genos.count_alleles()
    sampleSizes = [sum(x) for x in ac]
    assert len(set(sampleSizes)) == 1
    dafs = ac[:,1]/float(sampleSizes[0])
    ihsVals = allel.stats.selection.ihs(haps, positionArrays[instanceIndex], use_threads=True, include_edges=False)
    nslVals = allel.stats.selection.nsl(haps, use_threads=True)
    assert len(dafs) == len(ihsVals) == len(nslVals)
    numSnpsDone += len(dafs)
    for i in range(len(dafs)):
        #print "%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i]) # Python 2
        print ("%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i])) #Python 3
    if numSnpsDone > 5000000:
        break

```

```{shell, bunya template}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=32G
#SBATCH --job-name=[SAMPLE]
#SBATCH --time=01:00:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/slurm_calcStatsAndDafForEachSnpSingleMsFile_Python3_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/

msOut="[SAMPLE]"

srun python /home/uqsalle3/Programs/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/

find ./ -maxdepth 1 -type f -name '*_COMBINED.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_calcStatsAndDafForEachSnpSingleMsFile_Python3.txt calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
done < msOut_files.txt

find ./ -maxdepth 1 -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*.slurm.sh' -print0 | xargs -0 chmod 755

ls ./calcStatsAndDafForEachSnpSingleMsFile_Python3_*.slurm.sh

```

```{shell, submit scripts Bunya}

while read p; do
  echo "calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh submitted"
  sbatch calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.slurm.sh
  sleep 0.25
done < msOut_files.txt

```

### Training - training_convert_to_FVs

```{python, training_convert_to_FVs_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
from msTools import *
from fvTools import *
import random
import allel
import numpy as np
import math

# '''usage eg:
# pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
# python2 training_convert_to_FVs.py spNeut.msOut.gz 2L,2R,3L,3R 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.fa ./ spNeut.msOut.fvec
# '''
# 
# if len(sys.argv)!=12:
#   sys.exit("usage:\npython2 training_convert_to_FVs.py trainingDataFileName chrArmsForMasking subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName statDir fvecFileName\n")
# else:
#   trainingDataFileName, chrArmsForMasking, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, statDir, fvecFileName = sys.argv[1:]

trainingDataFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5.msOut.gz"
chrArmsForMasking="ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140"
subWinSize=5000
numSubWins=11
unmaskedFracCutoff=0.25
pMisPol=0.003
partialStatAndDafFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_stats.txt"
maskFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta"
# ancestralArmFaFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/top6.anc.fa"
ancestralArmFaFileName="none"
statDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/"
fvecFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_5_test.fvec"

subWinSize,numSubWins = int(subWinSize),int(numSubWins)
assert subWinSize>0 and numSubWins>1

totalPhysLen=(subWinSize*numSubWins)

hapArraysIn,positionArrays=msOutToHaplotypeArrayIn(trainingDataFileName,totalPhysLen)

chrArmsForMasking=chrArmsForMasking.split(",")

unmaskedFracCutoff,pMisPol=float(unmaskedFracCutoff),float(pMisPol)

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")


if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")


standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none", "false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*totalPhysLen
  sys.stderr.write("Warning: not doing any masking! (i.e. mask.fa file for the chr arm with all masked sites N'ed out, or at least the reference with Ns, is not provided)\n")
  maskFileName=False
else:
  if ancestralArmFaFileName.lower() in ["none", "false"]:
    maskData=readMaskDataForTraining(maskFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  else:
    maskData=readMaskAndAncDataForTraining(maskFileName,ancestralArmFaFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  if len(maskData)<len(hapArraysIn):
    sys.stderr.write("Warning: not enough windows from masked data (needed %d; got %d); will draw with replacement!!\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=True
  else:
    sys.stderr.write("enough windows from masked data (needed %d; got %d); will draw without replacement.\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=False


# for i in range(len(maskData)):
#   print(str(i) + " " + str(len(maskData[i])) + " " + str(sum(maskData[i])))

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

header=[]

for statName in statNames:
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName, i))

header="\t".join(header)

statVals={}

for statName in statNames:
  statVals[statName]=[]

def getSubWinBounds(subWinSize,totalPhysLen):
  subWinStart=1-subWinSize
  subWinEnd=0
  subWinBounds=[]
  for i in range(0,(numSubWins-1)):
    subWinStart+=subWinSize
    subWinEnd+=subWinSize
    subWinBounds.append((subWinStart,subWinEnd))
  subWinStart+=subWinSize
  subWinEnd=totalPhysLen
  subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,totalPhysLen)

def getSnpIndicesInSubWins(subWinBounds,positions):
  snpIndicesInSubWins=[]
  for subWinIndex in range(len(subWinBounds)):
    snpIndicesInSubWins.append([])
  subWinIndex=0
  for i in range(len(positions)):
    while not (positions[i]>=subWinBounds[subWinIndex][0] and positions[i]<=subWinBounds[subWinIndex][1]):
      subWinIndex+=1
    snpIndicesInSubWins[subWinIndex].append(i)
  return snpIndicesInSubWins

quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

if statDir.lower() in ["none","false","default"]:
  statDir='./'

statFiles=[]

for subWinIndex in range(numSubWins):
  statFileName='/'.join(("%s/%s.subWin%d.stats" %(statDir, trainingDataFileName.split("/")[-1].replace(".msOut.gz",""), subWinIndex)).split('//'))
  statFiles.append(open(statFileName,"w"))
  statFiles[-1].write("\t".join(statNames)+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  fvecFileName="%s.fvec" %trainingDataFileName.split("/")[-1].replace(".msOut.gz","")

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

# for testing for loop below only
instanceIndex=0
statName='pi'

for instanceIndex in range(len(hapArraysIn)):
  print("Starting index " + str(instanceIndex))
  for statName in statNames:
    statVals[statName].append([])
  if maskFileName:
    if drawWithReplacement:
      unmasked=random.choice(maskData)
    else:
      unmasked=maskData[instanceIndex]
    assert len(unmasked)==totalPhysLen
    print(str(sum(unmasked)) + " Loci unmasked")
  snpIndicesToKeep=[x for x in range(len(positionArrays[instanceIndex])) if unmasked[positionArrays[instanceIndex][x]-1]]
  if len(snpIndicesToKeep)==0:
    for subWinIndex in range(numSubWins):
      for statName in statNames:
        print("No SNPs in BIGwindow, appendStatValsForMonomorphic")
        appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  else:
    haps=allel.HaplotypeArray(hapArraysIn[instanceIndex],dtype='i1').subset(sel0=snpIndicesToKeep)
    if pMisPol>0:
      misPolarizeCorrectionIndex=np.random.binomial(1,pMisPol,len(haps))
      for i in range(len(misPolarizeCorrectionIndex)):
        if misPolarizeCorrectionIndex[i]==1:
          for j in range(len(haps[i])):
            if haps[i][j]==0:
              haps[i][j]=1
            else:
              haps[i][j]=0
    genos=haps.to_genotypes(ploidy=2)
    alleleCounts=genos.count_alleles()
    positions=[positionArrays[instanceIndex][x] for x in snpIndicesToKeep]
    precomputedStats={}
    sampleSizes=[sum(x) for x in alleleCounts]
    assert len(set(sampleSizes))==1
    dafs=alleleCounts[:,1]/float(sampleSizes[0])
    ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
    nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
    nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
    # sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    if nonNanCount==0:
      precomputedStats["iHS"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["iHS"].append([])
    else:
      ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
      precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)
    nslVals=allel.stats.selection.nsl(haps,use_threads=False)
    nonNanCount=[x for x in np.isnan(nslVals)].count(False)
    # sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    if nonNanCount==0:
      precomputedStats["nSL"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["nSL"].append([])
    else:
      nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
      precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)
    snpIndicesInSubWins=getSnpIndicesInSubWins(subWinBounds,positions)
    for subWinIndex in range(numSubWins):
      subWinStart,subWinEnd = subWinBounds[subWinIndex]
      unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
      assert unmaskedFrac>=unmaskedFracCutoff
      if len(snpIndicesInSubWins[subWinIndex])==0:
        print("No SNPS in " + str(subWinIndex) + ", appendStatValsForMonomorphic")
        for statName in statNames:
          appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      else:
        print(str(snpIndicesInSubWins[subWinIndex])) + " SNPs in " + str(subWinIndex) + ", calcAndAppendStatVal")
        hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
        for statName in [x for x in statNames if x[0:3]!="HAF" and x[0:3]!="phi" and x[0:3]!="kap" and x[0:3]!="SFS" and x[0:3]!="SAF"]:
          calcAndAppendStatVal(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, instanceIndex, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        haplotypes={}
        for i in range(len(hapsInSubWin[0])):
          haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
          haplotype="".join(str(x) for x in haplotype)
          if haplotype in haplotypes:
            haplotypes[haplotype].append(i)
          else:
            haplotypes[haplotype]=[i]
        HAF=[]
        HAFunique={}
        for i in haplotypes:
          HAFunique[i]=0
          for j in range(len(hapsInSubWin)):
            if hapsInSubWin[j][haplotypes[i][0]]==1:
              HAFunique[i]+=sum([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j]))])
          for j in range(len(haplotypes[i])):
            HAF.append(HAFunique[i])
        phi=[]
        kappa=[]
        SAFE=[]
        for i in range(len(hapsInSubWin)):
          phi.append(0)
          kappa.append([])
          phiDenom=0
          for j in haplotypes:
            phi[i]+=(int(list(j)[i])*HAFunique[j]*len(haplotypes[j]))
            if int(list(j)[i])==1 and HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
              kappa[i].append(HAFunique[j])
            phiDenom+=(HAFunique[j]*len(haplotypes[j]))
          phi[i]/=float(phiDenom)
          kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
          if dafs[snpIndicesInSubWins[subWinIndex]][i]==0 or dafs[snpIndicesInSubWins[subWinIndex]][i]==1:
           SAFE.append(0.0)
          else:
           SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafs[snpIndicesInSubWins[subWinIndex]][i]*(1-dafs[snpIndicesInSubWins[subWinIndex]][i]))))
        for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
          if i=="SFS":
            windowStats=dafs[snpIndicesInSubWins[subWinIndex]]
          elif i=="HAFunique":
            # windowStats=[eval(i)[x] for x in eval(i)]
            windowStats = list(HAFunique.values()) # SCOTT
          else:
            windowStats=eval(i)
          statVals[i+"-Mean"][instanceIndex].append(np.mean(windowStats))
          statVals[i+"-Median"][instanceIndex].append(np.median(windowStats))
          if(len(np.unique(windowStats,return_counts=True)[1])==1):
            statVals[i+"-Mode"][instanceIndex].append(windowStats[0])
          else:
            if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
              statVals[i+"-Mode"][instanceIndex].append(scipy.stats.mstats.mode(windowStats)[0][0])
            else:
              mode=min(windowStats)
              for j in range(1,51):
                if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
                  mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
              statVals[i+"-Mode"][instanceIndex].append(mode+((max(windowStats)-min(windowStats))/100))
          for j in quantiles:
            statVals[i+"-"+j][instanceIndex].append(np.percentile(windowStats,quantiles[j]))
          statVals[i+"-Max"][instanceIndex].append(max(windowStats))
          statVals[i+"-Var"][instanceIndex].append(np.var(windowStats))
          statVals[i+"-SD"][instanceIndex].append(np.std(windowStats))
          statVals[i+"-Skew"][instanceIndex].append(scipy.stats.skew(windowStats))
          statVals[i+"-Kurt"][instanceIndex].append(scipy.stats.kurtosis(windowStats))
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][instanceIndex])
  fvecFile.write("\t".join([str(x) for x in outVec])+"\n")

for subWinIndex in range(numSubWins):
  statFiles[subWinIndex].close()
fvecFile.close()
# sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, bunya template Neutral}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=8G
#SBATCH --job-name=[SAMPLE]
#SBATCH --time=00:15:00
#SBATCH --partition=general
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/slurm_training_convert_to_FVs_Python3_[SAMPLE].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/slurm_training_convert_to_FVs_Python3_[SAMPLE].error

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/

msOut="[SAMPLE]"

srun python /home/uqsalle3/Programs/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spNeut_COMBINED_stats.txt\
  /scratch/project/chenoase/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell}

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Hard_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spHard_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHard_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Hard_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_Soft_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoft_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_Soft_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_HardPartial_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spHardPartial_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_HardPartial_10.txt

cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_0.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_1.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_2.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_3.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_4.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_5.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_6.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_7.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_8.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_9.txt
cp template_training_convert_to_FVs_Python3_Neutral.txt template_training_convert_to_FVs_Python3_SoftPartial_10.txt

sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_0_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_0.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_1_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_1.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_2_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_2.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_3_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_3.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_4_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_4.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_5_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_5.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_6_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_6.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_7_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_7.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_8_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_8.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_9_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_9.txt
sed -i 's/spNeut_COMBINED_stats\.txt/spSoftPartial_10_COMBINED_stats\.txt/' template_training_convert_to_FVs_Python3_SoftPartial_10.txt

```

```{shell, create scripts Bunya}

cd /scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/

find ./ -maxdepth 1 -type f -name '*.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt



# Neutral

grep 'Neut' msOut_files.txt > temp.txt
grep -v 'COMBINED' temp.txt > msOut_files_Neutral.txt
rm temp.txt

while read p; do
  echo "$p"
  cp template_training_convert_to_FVs_Python3_Neutral.txt training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
done < msOut_files_Neutral.txt



# Function to process files based on the specified prefix
process_files() {
    local prefix="$1"
    local template="template_training_convert_to_FVs_Python3_${prefix}.txt"
    local output_file="msOut_files_${prefix}.txt"

    # Create a list of files based on the prefix
    grep "${prefix}_" msOut_files.txt > temp.txt
    grep -v 'COMBINED' temp.txt > "${output_file}"
    rm temp.txt

    # Process each file in the list
    while read -r p; do
        echo "$p"
        cp "${template}" "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh"
        replace_CMD="s/\[SAMPLE\]/${p}/g"
        perl -pi -e "$replace_CMD" "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh"
    done < "${output_file}"
}

# Loop through different prefixes
for i in {0..10}; do
    process_files "Hard_${i}"
done

for i in {0..10}; do
    process_files "Soft_${i}"
done

for i in {0..10}; do
    process_files "HardPartial_${i}"
done

for i in {0..10}; do
    process_files "SoftPartial_${i}"
done



find ./ -maxdepth 1 -type f -name 'training_convert_to_FVs_Python3_*.slurm.sh' -print0 | xargs -0 chmod 755

```

```{shell, submit scripts Bunya}

# RUNNING
while read p; do
  echo "training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh submitted"
  sbatch training_convert_to_FVs_Python3_${p/.msOut.gz/}.slurm.sh
  sleep 0.05
done < msOut_files_Neutral.txt

# Function to submit files based on the specified prefix
submit_jobs() {
    local prefix="$1"
    echo "training_convert_to_FVs_Python3_${prefix}.slurm.sh submitted"
    sbatch training_convert_to_FVs_Python3_${prefix}.slurm.sh
    sleep 0.05
}

# Loop through different prefixes

# RUNNING
for i in {1..2000}; do
    submit_jobs "spHard_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spSoft_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spHardPartial_5_${i}"
done

# RUNNING
for i in {1..2000}; do
    submit_jobs "spSoftPartial_5_${i}"
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spHard_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spSoft_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spHardPartial_${i}_${j}"
    done
done

# RUNNING
for i in 0 1 2 3 4 6 7 8 9 10; do
    for j in {1..400}; do
      submit_jobs "spSoftPartial_${i}_${j}"
    done
done



ls ./spNeut_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_5_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_5_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spHard_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHard_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spSoft_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoft_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spHardPartial_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spHardPartial_10_*.fvec | grep -v 'COMBINED' | wc -l

ls ./spSoftPartial_0_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_1_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_2_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_3_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_4_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_6_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_7_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_8_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_9_*.fvec | grep -v 'COMBINED' | wc -l
ls ./spSoftPartial_10_*.fvec | grep -v 'COMBINED' | wc -l

## rerun failed

grep 'error' ./slurm_training_convert_to_FVs_Python3_sp*.error | perl -ne 's/\.\/slurm_/sbatch\ /; s/\.msOut\.gz.*/\.slurm\.sh/; print $_;' | uniq > rerun_failed_training_convert_to_FVs_Python3.sh
cat rerun_failed_training_convert_to_FVs_Python3.sh

bash rerun_failed_training_convert_to_FVs_Python3.sh

#!/bin/bash

prefix="spSoftPartial_5_"
suffix=".fvec"
expected_count=2000

# Generate the list of expected filenames
expected_files=()
for ((i=1; i<=$expected_count; i++)); do
  expected_files+=("${prefix}${i}${suffix}")
done

# Get the actual list of filenames
actual_files=$(ls -1 "${prefix}"*"${suffix}")

# Find the missing file
for expected_file in "${expected_files[@]}"; do
  if ! echo "${actual_files}" | grep -q "${expected_file}"; then
    echo "Missing file: ${expected_file}"
  fi
done

```

```{shell, combined fvec}

process_files() {
  local PREFIX="$1"
  find ./ -maxdepth 1 -type f | grep -E "${PREFIX}_[0-9]+\.fvec" > temp.txt
  
  while read -r p; do
    wc ${p}
  done < temp.txt
  
  while read -r p; do
    awk 'NR == 2' ${p}
  done < temp.txt > temp.fvec

  # Extract the first line from ${PREFIX}_1.msOut
  head -n 1 "${PREFIX}_1.fvec" > header.txt

  # Concatenate the edited header with temp.fvec
  cat header.txt temp.fvec > "${PREFIX}_SINGLE_COMBINED.fvec"
}

rm -v ./spNeut_SINGLE_COMBINED.fvec
for PREFIX in spNeut; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spHard_*_SINGLE_COMBINED.fvec
for PREFIX in spHard_0 spHard_1 spHard_2 spHard_3 spHard_4 spHard_5 spHard_6 spHard_7 spHard_8 spHard_9 spHard_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spSoft_*_SINGLE_COMBINED.fvec
for PREFIX in spSoft_0 spSoft_1 spSoft_2 spSoft_3 spSoft_4 spSoft_5 spSoft_6 spSoft_7 spSoft_8 spSoft_9 spSoft_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spHardPartial_*_SINGLE_COMBINED.fvec
for PREFIX in spHardPartial_0 spHardPartial_1 spHardPartial_2 spHardPartial_3 spHardPartial_4 spHardPartial_5 spHardPartial_6 spHardPartial_7 spHardPartial_8 spHardPartial_9 spHardPartial_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done

rm -v ./spSoftPartial_*_SINGLE_COMBINED.fvec
for PREFIX in spSoftPartial_0 spSoftPartial_1 spSoftPartial_2 spSoftPartial_3 spSoftPartial_4 spSoftPartial_5 spSoftPartial_6 spSoftPartial_7 spSoftPartial_8 spSoftPartial_9 spSoftPartial_10; do
    echo ${PREFIX}
    process_files "$PREFIX"
done



find ./ -maxdepth 1 -type f -name '*_SINGLE_COMBINED.fvec' -exec wc {} \;

```

```{shell, transfer to Dell 2}

# constNe_intergenic

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/*_SINGLE_COMBINED.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/

mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/test_self_newDat
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/test_self_newDat

rsync -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/expansionNe_intergenic_uci/test_self_newDat/*_SINGLE_COMBINED.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/test_self_newDat/

# There should be 45 fvec files
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/*.fvec | wc -l
ls /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/test_self_newDat/*.fvec | wc -l

```

```{r, assess feature vectors}

# module load anaconda3
# source ~/conda-init
# conda activate r_env

library(tidyverse)
library(data.table)

# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic")
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/test_self_newDat")

spHard_5 <- read_tsv("spHard_5_SINGLE_COMBINED.fvec")

spHard_5 %>% 
  mutate(sim=c(1:nrow(spHard_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

spSoft_5 <- read_tsv("spSoft_5_SINGLE_COMBINED.fvec")

spSoft_5 %>% 
  mutate(sim=c(1:nrow(spSoft_5))) %>% 
  pivot_longer(pi_win0:`SAFE-Kurt_win10`, names_to="stat_win", values_to="value") %>% 
  separate(stat_win, into=c("stat", "win"), sep="_", remove=TRUE) %>% 
  mutate(win2 = str_replace(win, "win", "")) %>% 
  mutate(win2 = as.integer(win2)) %>% 
  group_by(stat, win2) %>% 
  summarise(meanValue = mean(value)) %>% 
  ggplot(aes(x=win2, y=meanValue, col=stat)) + 
  geom_line()

```

### Training - training_sample_FVs

```{python, training_sample_FVs.py}

# import sys,os,random
import sys # SCOTT
import os # SCOTT
import random # SCOTT

'''usage eg:
python2 training_sample_FVs.py spNeut.fvec spHard_ spSoft_ spPartialHard_ spPartialSoft_ 5 0,1,2,3,4,6,7,8,9,10 ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec
'''

if len(sys.argv)!=10:
  sys.exit("usage:\npython2 training_sample_FVs.py neutTrainingFileName hardTrainingFilesPrefix softTrainingFilesPrefix partialHardTrainingFilesPrefix partialSoftTrainingFilesPrefix sweepTrainingWindow linkedTrainingWindows sampledFVsDir sampledFVsFiles\n")
else:
  neutTrainingFileName, hardTrainingFilesPrefix, softTrainingFilesPrefix, partialHardTrainingFilesPrefix, partialSoftTrainingFilesPrefix, sweepTrainingWindow, linkedTrainingWindows, sampledFVsDir, sampledFVsFiles = sys.argv[1:]

neutTrainingFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spNeut.fvec"
hardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_"
softTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spSoft_"
partialHardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialHard_"
partialSoftTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialSoft_"
sweepTrainingWindow=5
linkedTrainingWindows=0,1,2,3,4,6,7,8,9,10
sampledFVsDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe"
sampledFVsFiles="neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec"

sweepFilePaths,linkedFilePaths = {},{}
for trainingFilePrefix in [hardTrainingFilesPrefix,softTrainingFilesPrefix,partialHardTrainingFilesPrefix,partialSoftTrainingFilesPrefix]:
  trainingSetDir="/".join(trainingFilePrefix.split("/")[:-1])
  trainingFilePrefixDirless=trainingFilePrefix.split("/")[-1]
  sweepFilePaths[trainingFilePrefix]=[]
  linkedFilePaths[trainingFilePrefix]=[]
  for fileName in os.listdir(trainingSetDir):
    if fileName.startswith(trainingFilePrefixDirless):
      winNum=int(fileName.split("_")[1].split(".")[0])
      if winNum==int(sweepTrainingWindow):
        sweepFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)
      elif winNum in [int(x) for x in linkedTrainingWindows.split(",")]:
        linkedFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)

def getExamplesFromFVFile(simFileName):
  try:
    simFile=open(simFileName)
    lines=[line.strip() for line in simFile.readlines()]
    header=lines[0]
    examples=lines[1:]
    simFile.close()
    return header,examples
  except Exception:
    return "",[]

def getExamplesFromFVFileLs(simFileLs):
  examples=[]
  keptHeader=""
  for filePath in simFileLs:
    header,currExamples=getExamplesFromFVFile(filePath)
    if header:
      keptHeader=header
    examples+=currExamples
  return keptHeader,examples

header,neutExamples=getExamplesFromFVFile(neutTrainingFileName)
hardHeader,hardExamples=getExamplesFromFVFileLs(sweepFilePaths[hardTrainingFilesPrefix])
linkedHardHeader,linkedHardExamples=getExamplesFromFVFileLs(linkedFilePaths[hardTrainingFilesPrefix])
softHeader,softExamples=getExamplesFromFVFileLs(sweepFilePaths[softTrainingFilesPrefix])
linkedSoftHeader,linkedSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[softTrainingFilesPrefix])
partialHardHeader,partialHardExamples=getExamplesFromFVFileLs(sweepFilePaths[partialHardTrainingFilesPrefix])
linkedPartialHardHeader,linkedPartialHardExamples=getExamplesFromFVFileLs(linkedFilePaths[partialHardTrainingFilesPrefix])
partialSoftHeader,partialSoftExamples=getExamplesFromFVFileLs(sweepFilePaths[partialSoftTrainingFilesPrefix])
linkedPartialSoftHeader,linkedPartialSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[partialSoftTrainingFilesPrefix])

def getMinButNonZeroExamples(lsLs):
  counts=[]
  for ls in lsLs:
    if len(ls)>0:
      counts.append(len(ls))
  if not counts:
    raise Exception
  return min(counts)

trainingSetLs=[hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
numExamplesToKeep=getMinButNonZeroExamples(trainingSetLs)
for i in range(len(trainingSetLs)):
  random.shuffle(trainingSetLs[i])
  trainingSetLs[i]=trainingSetLs[i][:numExamplesToKeep]
hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples=trainingSetLs

if sampledFVsDir.lower() in ["none","false","default"]:
  sampledFVsDir='./'
if sampledFVsFiles.lower() in ["none","false","default"]:
  sampledFVsFiles=["neut.fvec","hard.fvec","linkedHard.fvec","soft.fvec","linkedSoft.fvec","partialHard.fvec","linkedPartialHard.fvec","partialSoft.fvec","linkedPartialSoft.fvec"]
else:
  sampledFVsFiles=sampledFVsFiles.split(",")
  assert len(sampledFVsFiles)==9
outExamples=[neutExamples,hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
for i in range(len(sampledFVsFiles)):
  if outExamples[i]:
    outFile=open('/'.join((sampledFVsDir+'/'+sampledFVsFiles[i]).split('//')),"w")
    outFile.write("classLabel\t%s\n" %(hardHeader))
    for example in outExamples[i]:
      outFile.write("%s\t%s\n" %(sampledFVsFiles[i].replace(".fvec",""),example))
    outFile.close()

```

```{shell, test Dell 2}

# expansionNe_intergenic_uci

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs

for file in *_SINGLE_COMBINED.fvec; do
  mv "$file" "${file/_SINGLE_COMBINED.fvec/.fvec}"
done

for file in spHardPartial_*.fvec; do
  mv "$file" "${file/spHardPartial_/spPartialHard_}"
done

for file in spSoftPartial_*.fvec; do
  mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
done

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

```

### Training

```{shell, test Dell 2}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

# expansionNe_intergenic_uci

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci
mkdir -p FVs && cd ./FVs

# for i in 1 2 3 4 5 6 7 8 9 10; do
  time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    expansionNe_intergenic_uci.hdf5\
    expansionNe_intergenic_uci.json\
    expansionNe_intergenic_uci.npy
# done

tensorboard --logdir logs

# time on 40 cores ~12 minutes with 5GB RAM

```

#### Training with early stopping

```{shell, test Dell 2}

# module load anaconda3
# source ~/conda-init
# conda activate partialshic_py3
# 
# # expansionNe
# 
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
# mkdir -p FVs && cd ./FVs
# mkdir -p earlyStopping && cd ./earlyStopping
# rm ./*
# cp ../*.fvec
# 
# time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_test.hdf5\
#     expansionNe_earlyStopping_test.json\
#     expansionNe_earlyStopping_test.npy
# 
# for i in 1 2 3 4 5 6 7 8 9 10; do
#   time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     expansionNe_earlyStopping_${i}.hdf5\
#     expansionNe_earlyStopping_${i}.json\
#     expansionNe_earlyStopping_${i}.npy > training_deep_learning_python3_earlyStopping_${i}.log 2>&1
# done
# 
# # time on 40 cores ~12 minutes with 5GB RAM

```

#### Training with early stopping and 10-fold CV

```{shell, test Dell 2}

module load anaconda3
source ~/conda-init
conda activate partialshic_py3

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
rm ./*
cp ../*.fvec ./

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping_kFold.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    expansionNe_earlyStopping_fold.hdf5\
    expansionNe_earlyStopping_fold.json\
    expansionNe_earlyStopping_fold.npy

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
tensorboard --logdir logs

# for i in 1 2 3 4 5 6 7 8 9 10; do
#   time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3_earlyStopping.py\
#     ./\
#     neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
#     11\
#     92\
#     0.1\
#     constNe_earlyStopping_${i}.hdf5\
#     constNe_earlyStopping_${i}.json\
#     constNe_earlyStopping_${i}.npy > training_deep_learning_python3_earlyStopping_${i}.log 2>&1
# done

# time on 40 cores ~12 minutes with 5GB RAM

```

### Run testing

#### test on self

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic
mkdir -p test_self && cd test_self
rm ./*
cp ../FVs/*.fvec ./
# cp ../../expansionNe/FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

for file in spHardPartial_*.fvec; do
  mv "$file" "${file/spHardPartial_/spPartialHard_}"
done

for file in spSoftPartial_*.fvec; do
  mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
done

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/expansionNe.npy\
 ./\
 11\
 92\
 ./\
 expansionNe_accuracy\
 expansionNe_confusion_matrix.pdf

evince expansionNe_confusion_matrix.pdf

```

#### test on self-new data

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/test_self_newDat
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp ../../*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  for file in *_SINGLE_COMBINED.fvec; do
    mv "$file" "${file/_SINGLE_COMBINED.fvec/.fvec}"
  done
  for file in spHardPartial_*.fvec; do
    mv "$file" "${file/spHardPartial_/spPartialHard_}"
  done
  for file in spSoftPartial_*.fvec; do
    mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
  done
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince constNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/
  mkdir -p test_expansionNe_intergenic && cd test_expansionNe_intergenic
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on constNe

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/
  mkdir -p test_constNe && cd test_constNe
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on constNe_intergenic

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/
  mkdir -p test_constNe_intergenic && cd test_constNe_intergenic
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe_intergenic/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on expansionNe_intergenic_lci

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/
  mkdir -p test_expansionNe_intergenic_lci && cd test_expansionNe_intergenic_lci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
    /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    expansionNe_accuracy_${i}\
    expansionNe_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

evince expansionNe_confusion_matrix.pdf

```

#### test on empirical

```{python}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/
mkdir -p pred_empirical && cd pred_empirical
rsync -ahPv /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/*_110Lines.phased.noPIRs.inbred.fvec ./

for CONTIG in 542 594 628 718 76 785; do
  for i in 1 2 3 4 5 6 7 8 9 10; do
    python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
      /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/FVs/earlyStopping/fold_${i}_expansionNe_earlyStopping_fold.npy\
      ./ScA8VGg_${CONTIG}_110Lines.phased.noPIRs.inbred.fvec\
      11\
      92\
      ScA8VGg_${CONTIG}.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_fold_${i}.pred.bed >\
      empirical_deep_learning_classify_ScA8VGg_${CONTIG}_fold_${i}.log 2>&1
  done
done

```

# PartialSHIC - expansionNe

## Generate training data

```{r}
#setwd('~/myData/phd/p2/discoal_simu')
options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '~/discoal/discoal' 
TIMES = 2000

Ne = 1*10^6
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
s_low = 0.00001
s_high = 0.01
Pa_low <- 2*Ne*s_low
Pa_high <- 2*Ne*s_high

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
Pu_high <- 0.01

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
# bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe
bottleneck <- "-en 0.00015 0 1.2 -en 0.00075 0 1.6 -en 0.0045 0 1.2 -en 0.006 0 0.7 -en 0.03 0 0.4 -en 0.11 0 0.24 -en 0.175 0 0.2" # expansion, based on stairway

# N0=5e6
# expansion = '-en {3e3/(4*N0)} 0 {6e6/N0} -en {15e3/(4*N0)} 0 {8e6/N0} -en {90e3/(4*N0)} 0 {6e6/N0} -en {120e3/(4*N0)} 0 {3.5e6/N0} -en {600e3/(4*N0)} 0 {2e6/N0} -en {2200e3/(4*N0)} 0 {1.2e6/N0} -en {3500e3/(4*N0)} 0 {1e6/N0}'


sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 
               0.60, 0.68, 0.77, 0.86, 0.95)
# for NEUTRAL
runNeutral <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} | gzip > spNeut.msOut.gz &")
system(runNeutral)  

# for complete HARD
runHard <- function(i){
    outname <- str_glue("spHard_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }
    #cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runHard, mc.cores = 11)


# for complete SOFT
runSoft <- function(i){
    outname <- str_glue("spSoft_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0  -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")}
    else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0  -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }   
    #cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runSoft, mc.cores = 11)


# for partial HARD
runPHard <- function(i){
    outname <- str_glue("spPartialHard_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -x {sweep_pos[i]} | gzip > {outname} &")  
    }
    #cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runPHard, mc.cores = 11)


# for partial SOFT
runPSoft <- function(i){
    outname <- str_glue("spPartialSoft_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }
    
    # cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runPSoft, mc.cores = 11)

```
    
## Run training

* Already run

```{python, training_deep_learning_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import numpy as np
np.random.seed(123)
from sklearn.model_selection import train_test_split
# from keras.utils import np_utils
from keras.utils import to_categorical # SCOTT
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
from keras import optimizers
from keras.callbacks import EarlyStopping,ModelCheckpoint

'''usage eg:
python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
'''

if len(sys.argv)!=9:
  sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
else:
  fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]

fvecDir='./'
fvecFiles='neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec'
numSubWins='11'
numSumStatsPerSubWin='92'
validationSize='0.1'
weightsFileName='constNe.hdf5'
jsonFileName='constNe.json'
npyFileName='constNe.npy'

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

if fvecFiles.lower() in ["none","false","default"]:
  fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
else:
  fvecFiles=fvecFiles.split(",")
  assert len(fvecFiles)==9

numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

if validationSize.lower() in ["none","false","default"]:
  validationSize=0.1
else:
  validationSize=float(validationSize)

sweeps=[]

for i in range(0,9):
 sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
 if i>0:
  assert len(sweeps[0])==len(sweeps[i])

sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))

sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)

models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))

sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=42)

# models=np_utils.to_categorical(models,9)
models=to_categorical(models,9) # SCOTT

# models_val=np_utils.to_categorical(models_val,9)
models_val=to_categorical(models_val,9) # SCOTT

netlayers=Sequential()
netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
netlayers.add(Dropout(0.25))
netlayers.add(Flatten())
netlayers.add(Dense(512, activation='relu'))
netlayers.add(Dropout(0.5))
netlayers.add(Dense(128, activation='relu'))
netlayers.add(Dropout(0.5))
netlayers.add(Dense(9, activation='softmax'))
netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')

netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)

netlayers_json=netlayers.to_json()

with open(jsonFileName,"w") as json_file:
  json_file.write(netlayers_json)

netlayers.save(npyFileName)

sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))

```

```{shell, test Dell 2}

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 constNe.hdf5\
 constNe.json\
 constNe.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# expansionNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 expansionNe.hdf5\
 expansionNe.json\
 expansionNe.npy 

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 expansionNe2.hdf5\
 expansionNe2.json\
 expansionNe2.npy 

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 expansionNe3.hdf5\
 expansionNe3.json\
 expansionNe3.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# longShallow

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 longShallow.hdf5\
 longShallow.json\
 longShallow.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# shortSevere

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 shortSevere.hdf5\
 shortSevere.json\
 shortSevere.npy 

# time on 40 cores ~12 minutes with 5GB RAM

```

## Run testing

### test on self

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe
mkdir -p test_self && cd test_self
rm ./*
cp ../FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/expansionNe.npy\
 ./\
 11\
 92\
 ./\
 expansionNe_accuracy_test\
 expansionNe_confusion_matrix_test.pdf

evince expansionNe_confusion_matrix.pdf

```

### test on self-new data

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/test_self_newDat
mkdir -p FVs && cd FVs
cp ../*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../../FVs/expansionNe.npy\
 ./\
 11\
 92\
 ./\
 expansionNe_accuracy\
 expansionNe_confusion_matrix.pdf

evince expansionNe_confusion_matrix.pdf



mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/test_self_newDat_2
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/test_self_newDat_2
mkdir -p FVs && cd FVs
rm -v ./*
cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/test_self_newDat/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../../FVs/expansionNe2.npy\
 ./\
 11\
 92\
 ./\
 expansionNe_accuracy_2\
 expansionNe_confusion_matrix_2.pdf

evince expansionNe_confusion_matrix_2.pdf



mkdir -p /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/test_self_newDat_3
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/test_self_newDat_3
mkdir -p FVs && cd FVs
rm -v ./*
cp /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/test_self_newDat/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../../FVs/expansionNe3.npy\
 ./\
 11\
 92\
 ./\
 expansionNe_accuracy_3\
 expansionNe_confusion_matrix_3.pdf

evince expansionNe_confusion_matrix_3.pdf

```

### test on constNe

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/
mkdir -p test_constNe && cd test_constNe
cp ../../constNe/FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/expansionNe.npy\
 ./\
 11\
 92\
 ./\
 ExpansionNe_on_constNe\
 ExpansionNe_on_constNe_confusion_matrix.pdf

evince ExpansionNe_on_constNe_confusion_matrix.pdf

```
    
### test on shortSevere

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/
mkdir -p test_shortSevere && cd test_shortSevere
cp ../../shortSevere/FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/expansionNe.npy\
 ./\
 11\
 92\
 ./\
 ExpansionNe_on_ShortSevere\
 ExpansionNe_on_ShortSevere_confusion_matrix.pdf

evince ExpansionNe_on_ShortSevere_confusion_matrix.pdf

```

### test on longShallow

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/
mkdir -p test_longShallow && cd test_longShallow
cp ../../longShallow/FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/expansionNe.npy\
 ./\
 11\
 92\
 ./\
 ExpansionNe_on_longShallow\
 ExpansionNe_on_longShallow_confusion_matrix.pdf

evince ExpansionNe_on_longShallow_confusion_matrix.pdf

```

### test on empirical

```{python}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

mkdir -p pred_expansionNe && cd pred_expansionNe

# ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/expansionNe.npy\
 ../ScA8VGg_542_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 2475 total instances
predicted 560 neutral regions (0.226263 of all classified regions)
predicted 81 Hard sweep regions (0.032727 of all classified regions)
predicted 292 Hard-linked sweep regions (0.117980 of all classified regions)
predicted 136 Soft sweep regions (0.054949 of all classified regions)
predicted 1190 Soft-linked sweep regions (0.480808 of all classified regions)
predicted 8 HardPartial sweep regions (0.003232 of all classified regions)
predicted 24 HardPartial-linked sweep regions (0.009697 of all classified regions)
predicted 33 SoftPartial sweep regions (0.013333 of all classified regions)
predicted 151 SoftPartial-linked sweep regions (0.061010 of all classified regions)

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/expansionNe2.npy\
 ../ScA8VGg_542_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred2.pred.bed

made predictions for 2475 total instances
predicted 707 neutral regions (0.285657 of all classified regions)
predicted 59 Hard sweep regions (0.023838 of all classified regions)
predicted 240 Hard-linked sweep regions (0.096970 of all classified regions)
predicted 211 Soft sweep regions (0.085253 of all classified regions)
predicted 1127 Soft-linked sweep regions (0.455354 of all classified regions)
predicted 12 HardPartial sweep regions (0.004848 of all classified regions)
predicted 18 HardPartial-linked sweep regions (0.007273 of all classified regions)
predicted 28 SoftPartial sweep regions (0.011313 of all classified regions)
predicted 73 SoftPartial-linked sweep regions (0.029495 of all classified regions)

# ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/expansionNe.npy\
 ../ScA8VGg_594_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 3117 total instances
predicted 1121 neutral regions (0.359641 of all classified regions)
predicted 31 Hard sweep regions (0.009945 of all classified regions)
predicted 112 Hard-linked sweep regions (0.035932 of all classified regions)
predicted 260 Soft sweep regions (0.083414 of all classified regions)
predicted 1375 Soft-linked sweep regions (0.441129 of all classified regions)
predicted 15 HardPartial sweep regions (0.004812 of all classified regions)
predicted 29 HardPartial-linked sweep regions (0.009304 of all classified regions)
predicted 33 SoftPartial sweep regions (0.010587 of all classified regions)
predicted 141 SoftPartial-linked sweep regions (0.045236 of all classified regions)

# ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/expansionNe.npy\
 ../ScA8VGg_628_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 4618 total instances
predicted 1276 neutral regions (0.276310 of all classified regions)
predicted 53 Hard sweep regions (0.011477 of all classified regions)
predicted 277 Hard-linked sweep regions (0.059983 of all classified regions)
predicted 328 Soft sweep regions (0.071026 of all classified regions)
predicted 2287 Soft-linked sweep regions (0.495236 of all classified regions)
predicted 10 HardPartial sweep regions (0.002165 of all classified regions)
predicted 36 HardPartial-linked sweep regions (0.007796 of all classified regions)
predicted 72 SoftPartial sweep regions (0.015591 of all classified regions)
predicted 279 SoftPartial-linked sweep regions (0.060416 of all classified regions)

# ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/expansionNe.npy\
 ../ScA8VGg_718_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 1375 total instances
predicted 220 neutral regions (0.160000 of all classified regions)
predicted 29 Hard sweep regions (0.021091 of all classified regions)
predicted 97 Hard-linked sweep regions (0.070545 of all classified regions)
predicted 116 Soft sweep regions (0.084364 of all classified regions)
predicted 704 Soft-linked sweep regions (0.512000 of all classified regions)
predicted 1 HardPartial sweep regions (0.000727 of all classified regions)
predicted 24 HardPartial-linked sweep regions (0.017455 of all classified regions)
predicted 29 SoftPartial sweep regions (0.021091 of all classified regions)
predicted 155 SoftPartial-linked sweep regions (0.112727 of all classified regions)

# ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/expansionNe.npy\
 ../ScA8VGg_76_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 6080 total instances
predicted 1648 neutral regions (0.271053 of all classified regions)
predicted 57 Hard sweep regions (0.009375 of all classified regions)
predicted 219 Hard-linked sweep regions (0.036020 of all classified regions)
predicted 474 Soft sweep regions (0.077961 of all classified regions)
predicted 3178 Soft-linked sweep regions (0.522697 of all classified regions)
predicted 3 HardPartial sweep regions (0.000493 of all classified regions)
predicted 49 HardPartial-linked sweep regions (0.008059 of all classified regions)
predicted 88 SoftPartial sweep regions (0.014474 of all classified regions)
predicted 364 SoftPartial-linked sweep regions (0.059868 of all classified regions)

# ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe/FVs/expansionNe.npy\
 ../ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 3836 total instances
predicted 1085 neutral regions (0.282847 of all classified regions)
predicted 33 Hard sweep regions (0.008603 of all classified regions)
predicted 145 Hard-linked sweep regions (0.037800 of all classified regions)
predicted 315 Soft sweep regions (0.082117 of all classified regions)
predicted 1903 Soft-linked sweep regions (0.496090 of all classified regions)
predicted 9 HardPartial sweep regions (0.002346 of all classified regions)
predicted 33 HardPartial-linked sweep regions (0.008603 of all classified regions)
predicted 61 SoftPartial sweep regions (0.015902 of all classified regions)
predicted 252 SoftPartial-linked sweep regions (0.065693 of all classified regions)

```

### summary prediction

```{r}

library(data.table)
library(RColorBrewer)
library(ggplot2)
library(dplyr)
library(tidyr)
library(cowplot)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

beds <- list.files(pattern = 'ScA8VGg_.*.bed$')

aa <- data.table()

for(ff in beds){
    tmp <- fread(ff, header = FALSE, sep = "\t")
    aa <- rbind(aa, tmp)
}


a1 <- aa %>% 
  extract('V4','State','(.*)_ScA8VGg.*') %>% 
  mutate(V1=gsub('chr','', V1))


a1_df <- data.table(table(a1$State))
# hard_num <- aa_df[V1=="hard",N]
# soft_num <- aa_df[V1=="soft",N]
# hard_pct <- hard_num/(hard_num + soft_num)
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

aplot[V1 %in% c('HardPartial','HardPartial-linked'),lbl2:=lbl]


PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
    geom_bar(width=1, stat="identity", color="white") +
    coord_polar("y", start=0) +
    geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
                  y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), label=lbl2), 
              color="black", size=3) +
    scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                               my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                               "grey60")) +
    theme_void() + theme(legend.position = "none")



a1 <- data.table(a1)

a1[,chr:="xxxx"][V1=="ScA8VGg_76",chr:="3R"][V1=="ScA8VGg_718",
                                        chr:="2L"][V1=="ScA8VGg_542",chr:="2L"][V1=="ScA8VGg_594",
                                                                                     chr:="X"][V1=="ScA8VGg_628",chr:="3L"][V1=="ScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$State)) -> a3
a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))

# ggsave("diploSHIC_proprotion_figure1.pdf",PP, width = 8, height = 4, 
#        dpi = 300, units = "in")


# chisquare test
library(rcompanion)

bc <- table(a2$chr, a2$State)

b2 <- cbind(rowSums(bc[,c(1,3)]), rowSums(bc[,c(2,4:9)]))
# 
# b1 <- cbind(bc,other = rowSums(bc[,2:9]))
# b2 <- b1[,c(1,10)]
chisq.test(b2)
b2 <- matrix(c(29, 43, 59, 46, 56, 3600, 3876, 4249, 5688, 4857), ncol=2)
colnames(b2) <- c('hard','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")
apply(b2,1,prop.table)

# c1 <- cbind(bc,other = rowSums(bc[,c(1:5,7:9)]))
# c2 <- c1[,c(6,10)
c2 <- cbind(rowSums(bc[,c(6,8)]), rowSums(bc[,c(1:5,7,9)]))
chisq.test(c2)

c2 <- matrix(c(381, 452, 503, 579, 615, 3248, 3467, 3805, 5155, 4298), ncol=2)
colnames(c2) <- c('soft','others')
rownames(c2) <- c('2L','2R','3L','3R','X')
pairwiseNominalIndependence(c2, fisher = F, gtest = F, chisq = T, method = "fdr")
apply(c2,1,prop.table)

ph <- cbind(bc[,3], rowSums(bc[,c(1:2,4:9)]))
apply(ph,1,prop.table)

```

```
                  V1     N         pct
1:               Hard   202 0.008976581 202
2:        Hard-linked  1319 0.058614407 1327
3:        HardPartial    31 0.001377594 22
4: HardPartial-linked    67 0.002977381
5:            Neutral  3192 0.141847754 3195
6:               Soft  2181 0.096920411 2183
7:        Soft-linked 14685 0.652579656 14694
8:        SoftPartial   349 0.015509043 360
9: SoftPartial-linked   477 0.021197174

```

### prob pred

```{bash}

module load anaconda3
source ~/conda-init
conda activate r_env

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe

```

```{r}

library(tidyverse)
library(data.table)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

beds <- list.files(pattern = 'ScA8VGg_.*.csv$')

aa <- data.table()

for(ff in beds){
    tmp <- fread(ff, header = TRUE, sep = ",")
    aa <- rbind(aa, tmp)
}

aa %>% 
  mutate(window = c(1:nrow(aa))) %>% 
  pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
  filter(pred=="Neutral") %>% 
  ggplot(aes(x=prob, fill=class)) + 
  geom_density(alpha=0.5) + 
  facet_wrap(~class, scale="free_y")

aa %>% 
  mutate(window = c(1:nrow(aa))) %>% 
  pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
  filter(pred=="Soft-linked") %>% 
  ggplot(aes(x=prob, fill=class)) + 
  geom_density(alpha=0.5) + 
  facet_wrap(~class, scale="free_y")

# Create a new column "prob" with the highest probability
aa$prob <- apply(aa[,1:9], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  sorted_probs[1]
})

# Create a new column "pred2" with the second highest probability class
aa$pred2 <- apply(aa[,1:9], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[2]
})

# Create a new column "prob2" with the second highest probability
aa$prob2 <- apply(aa[,1:9], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  sorted_probs[2]
})

# Create a new column "pred3" with the third highest probability class
aa$pred3 <- apply(aa[,1:9], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[3]
})

# Create a new column "prob3" with the third highest probability
aa$prob3 <- apply(aa[,1:9], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  sorted_probs[3]
})

```

# PartialSHIC - longShallow

## Generate training data

```{r}
#setwd('~/myData/phd/p2/discoal_simu')
options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '~/discoal/discoal' 
TIMES = 2000

Ne = 1*10^6
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
s_low = 0.00001
s_high = 0.01
Pa_low <- 2*Ne*s_low
Pa_high <- 2*Ne*s_high

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
Pu_high <- 0.01

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
#bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe




sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 
               0.60, 0.68, 0.77, 0.86, 0.95)
# for NEUTRAL
runNeutral <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} | gzip > spNeut.msOut.gz &")
system(runNeutral)  

# for complete HARD
runHard <- function(i){
    outname <- str_glue("spHard_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }
    
    #cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runHard, mc.cores = 11)


# for complete SOFT
runSoft <- function(i){
    outname <- str_glue("spSoft_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0  -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0  -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }
    
    # cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runSoft, mc.cores = 11)


# for partial HARD
runPHard <- function(i){
    outname <- str_glue("spPartialHard_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }
    
    #cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runPHard, mc.cores = 11)


# for partial SOFT
runPSoft <- function(i){
    outname <- str_glue("spPartialSoft_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")   
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }
    
    #cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runPSoft, mc.cores = 11)

```

## Run training

* Already run

```{python, training_deep_learning_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import numpy as np
np.random.seed(123)
from sklearn.model_selection import train_test_split
# from keras.utils import np_utils
from keras.utils import to_categorical # SCOTT
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
from keras import optimizers
from keras.callbacks import EarlyStopping,ModelCheckpoint

'''usage eg:
python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
'''

if len(sys.argv)!=9:
  sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
else:
  fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]

fvecDir='./'
fvecFiles='neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec'
numSubWins='11'
numSumStatsPerSubWin='92'
validationSize='0.1'
weightsFileName='constNe.hdf5'
jsonFileName='constNe.json'
npyFileName='constNe.npy'

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

if fvecFiles.lower() in ["none","false","default"]:
  fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
else:
  fvecFiles=fvecFiles.split(",")
  assert len(fvecFiles)==9

numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

if validationSize.lower() in ["none","false","default"]:
  validationSize=0.1
else:
  validationSize=float(validationSize)

sweeps=[]

for i in range(0,9):
 sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
 if i>0:
  assert len(sweeps[0])==len(sweeps[i])

sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))

sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)

models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))

sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=42)

# models=np_utils.to_categorical(models,9)
models=to_categorical(models,9) # SCOTT

# models_val=np_utils.to_categorical(models_val,9)
models_val=to_categorical(models_val,9) # SCOTT

netlayers=Sequential()
netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
netlayers.add(Dropout(0.25))
netlayers.add(Flatten())
netlayers.add(Dense(512, activation='relu'))
netlayers.add(Dropout(0.5))
netlayers.add(Dense(128, activation='relu'))
netlayers.add(Dropout(0.5))
netlayers.add(Dense(9, activation='softmax'))
netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')

netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)

netlayers_json=netlayers.to_json()

with open(jsonFileName,"w") as json_file:
  json_file.write(netlayers_json)

netlayers.save(npyFileName)

sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))

```

```{shell, test Dell 2}

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 constNe.hdf5\
 constNe.json\
 constNe.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# expansionNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 expansionNe.hdf5\
 expansionNe.json\
 expansionNe.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# longShallow

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 longShallow.hdf5\
 longShallow.json\
 longShallow.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# shortSevere

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 shortSevere.hdf5\
 shortSevere.json\
 shortSevere.npy 

# time on 40 cores ~12 minutes with 5GB RAM

```

## Run testing

### test on self

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow
mkdir -p test_self && cd test_self
rm ./*
cp ../FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/longShallow.npy\
 ./\
 11\
 92\
 ./\
 longShallow_accuracy\
 longShallow_confusion_matrix.pdf

evince longShallow_confusion_matrix.pdf

```

### test on self-new data

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/test_self_newDat
mkdir -p FVs && cd FVs
cp ../*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../../FVs/longShallow.npy\
 ./\
 11\
 92\
 ./\
 longShallow_accuracy\
 longShallow_confusion_matrix.pdf

evince longShallow_confusion_matrix.pdf

```

### test on constNe

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/
mkdir -p test_constNe && cd test_constNe
cp ../../constNe/FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/longShallow.npy\
 ./\
 11\
 92\
 ./\
 longShallow_on_constNe\
 longShallow_on_constNe_confusion_matrix.pdf

evince longShallow_on_constNe_confusion_matrix.pdf

```
    
### test on shortSevere

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/
mkdir -p test_shortSevere && cd test_shortSevere
cp ../../shortSevere/FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/longShallow.npy\
 ./\
 11\
 92\
 ./\
 longShallow_on_ShortSevere\
 longShallow_on_ShortSevere_confusion_matrix.pdf

evince longShallow_on_ShortSevere_confusion_matrix.pdf

```

### test on expansionNe

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/
mkdir -p test_expansionNe && cd test_expansionNe
cp ../../expansionNe/FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/longShallow.npy\
 ./\
 11\
 92\
 ./\
 longShallow_on_expansionNe\
 longShallow_on_expansionNe_confusion_matrix.pdf

evince longShallow_on_expansionNe_confusion_matrix.pdf

```

### test on empirical

```{python}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

mkdir -p pred_longShallow && cd pred_longShallow

# ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/longShallow.npy\
 ../ScA8VGg_542_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 2475 total instances
predicted 1615 neutral regions (0.652525 of all classified regions)
predicted 96 Hard sweep regions (0.038788 of all classified regions)
predicted 322 Hard-linked sweep regions (0.130101 of all classified regions)
predicted 94 Soft sweep regions (0.037980 of all classified regions)
predicted 265 Soft-linked sweep regions (0.107071 of all classified regions)
predicted 10 HardPartial sweep regions (0.004040 of all classified regions)
predicted 48 HardPartial-linked sweep regions (0.019394 of all classified regions)
predicted 3 SoftPartial sweep regions (0.001212 of all classified regions)
predicted 22 SoftPartial-linked sweep regions (0.008889 of all classified regions)

# ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/longShallow.npy\
 ../ScA8VGg_594_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 3117 total instances
predicted 2606 neutral regions (0.836060 of all classified regions)
predicted 32 Hard sweep regions (0.010266 of all classified regions)
predicted 113 Hard-linked sweep regions (0.036253 of all classified regions)
predicted 132 Soft sweep regions (0.042348 of all classified regions)
predicted 203 Soft-linked sweep regions (0.065127 of all classified regions)
predicted 10 HardPartial sweep regions (0.003208 of all classified regions)
predicted 16 HardPartial-linked sweep regions (0.005133 of all classified regions)
predicted 3 SoftPartial sweep regions (0.000962 of all classified regions)
predicted 2 SoftPartial-linked sweep regions (0.000642 of all classified regions)

# ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/longShallow.npy\
 ../ScA8VGg_628_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 4618 total instances
predicted 3419 neutral regions (0.740364 of all classified regions)
predicted 95 Hard sweep regions (0.020572 of all classified regions)
predicted 306 Hard-linked sweep regions (0.066262 of all classified regions)
predicted 236 Soft sweep regions (0.051104 of all classified regions)
predicted 471 Soft-linked sweep regions (0.101992 of all classified regions)
predicted 11 HardPartial sweep regions (0.002382 of all classified regions)
predicted 50 HardPartial-linked sweep regions (0.010827 of all classified regions)
predicted 12 SoftPartial sweep regions (0.002599 of all classified regions)
predicted 18 SoftPartial-linked sweep regions (0.003898 of all classified regions)

# ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/longShallow.npy\
 ../ScA8VGg_718_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 1375 total instances
predicted 718 neutral regions (0.522182 of all classified regions)
predicted 54 Hard sweep regions (0.039273 of all classified regions)
predicted 128 Hard-linked sweep regions (0.093091 of all classified regions)
predicted 104 Soft sweep regions (0.075636 of all classified regions)
predicted 259 Soft-linked sweep regions (0.188364 of all classified regions)
predicted 14 HardPartial sweep regions (0.010182 of all classified regions)
predicted 49 HardPartial-linked sweep regions (0.035636 of all classified regions)
predicted 17 SoftPartial sweep regions (0.012364 of all classified regions)
predicted 32 SoftPartial-linked sweep regions (0.023273 of all classified regions)

# ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/longShallow.npy\
 ../ScA8VGg_76_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 6080 total instances
predicted 4565 neutral regions (0.750822 of all classified regions)
predicted 102 Hard sweep regions (0.016776 of all classified regions)
predicted 268 Hard-linked sweep regions (0.044079 of all classified regions)
predicted 312 Soft sweep regions (0.051316 of all classified regions)
predicted 696 Soft-linked sweep regions (0.114474 of all classified regions)
predicted 4 HardPartial sweep regions (0.000658 of all classified regions)
predicted 91 HardPartial-linked sweep regions (0.014967 of all classified regions)
predicted 8 SoftPartial sweep regions (0.001316 of all classified regions)
predicted 34 SoftPartial-linked sweep regions (0.005592 of all classified regions)

# ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow/FVs/longShallow.npy\
 ../ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 3836 total instances
predicted 2905 neutral regions (0.757299 of all classified regions)
predicted 58 Hard sweep regions (0.015120 of all classified regions)
predicted 171 Hard-linked sweep regions (0.044578 of all classified regions)
predicted 213 Soft sweep regions (0.055527 of all classified regions)
predicted 415 Soft-linked sweep regions (0.108186 of all classified regions)
predicted 7 HardPartial sweep regions (0.001825 of all classified regions)
predicted 37 HardPartial-linked sweep regions (0.009645 of all classified regions)
predicted 11 SoftPartial sweep regions (0.002868 of all classified regions)
predicted 19 SoftPartial-linked sweep regions (0.004953 of all classified regions)

```

# PartialSHIC - shortSevere

## Generate training data

```{r}
#setwd('~/myData/phd/p2/discoal_simu')
options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '~/discoal/discoal'
# for neutral; for sweep: TIMEs/5, must (TIMEs/5)*11 > TIMEs
# I modified the sampling function in training_sample_FVs.py to 
# make the final training size the same (TIMES) for each sweep state
TIMES = 2000

Ne = 1*10^6
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
s_low = 0.00001
s_high = 0.01
Pa_low <- 2*Ne*s_low
Pa_high <- 2*Ne*s_high

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
Pu_high <- 0.01

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

#bottleneck <- "-en 0.16 0 0.589 -en 0.216 0 1.47" # long shallow
bottleneck <- "-en 0.33 0 0.033 -en 0.34 0 1.00" # short severe




sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 
               0.60, 0.68, 0.77, 0.86, 0.95)
# for NEUTRAL
runNeutral <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} | gzip > spNeut.msOut.gz &")
system(runNeutral)  

# for complete HARD
runHard <- function(i){
    outname <- str_glue("spHard_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }
    
    # cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runHard, mc.cores = 11)


# for complete SOFT
runSoft <- function(i){
    outname <- str_glue("spSoft_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0  -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0  -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }
    
    #cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runSoft, mc.cores = 11)


# for partial HARD
runPHard <- function(i){
    outname <- str_glue("spPartialHard_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }
    
    # cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runPHard, mc.cores = 11)


# for partial SOFT
runPSoft <- function(i){
    outname <- str_glue("spPartialSoft_{i-1}.msOut.gz")
    if(i==6){
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }else{
        sm_cmd <- str_glue("{DISCOAL} 110 {TIMES/5} {len} -Pt {Pt_low} {Pt_high} -Pre {Pre_mean} {Pre_upper} {bottleneck} -ws 0 -Pa {Pa_low} {Pa_high} -Pu {Pu_low} {Pu_high} -Pc {Pc_low} {Pc_high} -Pf {Pf_low} {Pf_high} -x {sweep_pos[i]} | gzip > {outname} &")
    }
    
    # cat(sm_cmd,"\n")
    system(sm_cmd)
}
mclapply(1:11, runPSoft, mc.cores = 11)

```

## Run training

* Already run

```{python, training_deep_learning_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import numpy as np
np.random.seed(123)
from sklearn.model_selection import train_test_split
# from keras.utils import np_utils
from keras.utils import to_categorical # SCOTT
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
from keras import optimizers
from keras.callbacks import EarlyStopping,ModelCheckpoint

'''usage eg:
python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
'''

if len(sys.argv)!=9:
  sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
else:
  fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]

fvecDir='./'
fvecFiles='neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec'
numSubWins='11'
numSumStatsPerSubWin='92'
validationSize='0.1'
weightsFileName='constNe.hdf5'
jsonFileName='constNe.json'
npyFileName='constNe.npy'

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

if fvecFiles.lower() in ["none","false","default"]:
  fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
else:
  fvecFiles=fvecFiles.split(",")
  assert len(fvecFiles)==9

numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

if validationSize.lower() in ["none","false","default"]:
  validationSize=0.1
else:
  validationSize=float(validationSize)

sweeps=[]

for i in range(0,9):
 sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
 if i>0:
  assert len(sweeps[0])==len(sweeps[i])

sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))

sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)

models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))

sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=42)

# models=np_utils.to_categorical(models,9)
models=to_categorical(models,9) # SCOTT

# models_val=np_utils.to_categorical(models_val,9)
models_val=to_categorical(models_val,9) # SCOTT

netlayers=Sequential()
netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
netlayers.add(Dropout(0.25))
netlayers.add(Flatten())
netlayers.add(Dense(512, activation='relu'))
netlayers.add(Dropout(0.5))
netlayers.add(Dense(128, activation='relu'))
netlayers.add(Dropout(0.5))
netlayers.add(Dense(9, activation='softmax'))
netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')

netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)

netlayers_json=netlayers.to_json()

with open(jsonFileName,"w") as json_file:
  json_file.write(netlayers_json)

netlayers.save(npyFileName)

sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))

```

```{shell, test Dell 2}

# constNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 constNe.hdf5\
 constNe.json\
 constNe.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# expansionNe

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 expansionNe.hdf5\
 expansionNe.json\
 expansionNe.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# longShallow

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/longShallow
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 longShallow.hdf5\
 longShallow.json\
 longShallow.npy 

# time on 40 cores ~12 minutes with 5GB RAM

# shortSevere

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere
mkdir -p FVs && cd ./FVs

time python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/training_deep_learning_python3.py\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
 11\
 92\
 0.1\
 shortSevere.hdf5\
 shortSevere.json\
 shortSevere.npy 

# time on 40 cores ~12 minutes with 5GB RAM

```

## Run testing

### test on self

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere
mkdir -p test_self && cd test_self
rm ./*
cp ../FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/shortSevere.npy\
 ./\
 11\
 92\
 ./\
 shortSevere_accuracy\
 shortSevere_confusion_matrix.pdf

evince shortSevere_confusion_matrix.pdf

```

### test on self-new data

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/test_self_newDat
mkdir -p FVs && cd FVs
cp ../*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../../FVs/shortSevere.npy\
 ./\
 11\
 92\
 ./\
 shortSevere_accuracy\
 shortSevere_confusion_matrix.pdf

evince shortSevere_confusion_matrix.pdf

```

### test on constNe

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/
mkdir -p test_constNe && cd test_constNe
cp ../../constNe/FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/shortSevere.npy\
 ./\
 11\
 92\
 ./\
 shortSevere_on_constNe\
 shortSevere_on_constNe_confusion_matrix.pdf

evince shortSevere_on_constNe_confusion_matrix.pdf

```
    
### test on longShallow

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/
mkdir -p test_longShallow && cd test_longShallow
cp ../../longShallow/FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/shortSevere.npy\
 ./\
 11\
 92\
 ./\
 shortSevere_on_longShallow\
 shortSevere_on_longShallow_confusion_matrix.pdf

evince shortSevere_on_longShallow_confusion_matrix.pdf

```

### test on expansionNe

```{bash}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/
mkdir -p test_expansionNe && cd test_expansionNe
cp ../../expansionNe/FVs/*.fvec ./
rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec

python /home/uqsalle3/shared/Scott_Allen/partialSHIC/testing_deep_learning_classify_python3.py\
 ../FVs/shortSevere.npy\
 ./\
 11\
 92\
 ./\
 shortSevere_on_expansionNe\
 shortSevere_on_expansionNe_confusion_matrix.pdf

evince shortSevere_on_expansionNe_confusion_matrix.pdf

```

### test on empirical

```{python}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors

mkdir -p pred_shortSevere && cd pred_shortSevere

# ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/shortSevere.npy\
 ../ScA8VGg_542_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_542.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 2475 total instances
predicted 671 neutral regions (0.271111 of all classified regions)
predicted 68 Hard sweep regions (0.027475 of all classified regions)
predicted 417 Hard-linked sweep regions (0.168485 of all classified regions)
predicted 154 Soft sweep regions (0.062222 of all classified regions)
predicted 813 Soft-linked sweep regions (0.328485 of all classified regions)
predicted 69 HardPartial sweep regions (0.027879 of all classified regions)
predicted 137 HardPartial-linked sweep regions (0.055354 of all classified regions)
predicted 42 SoftPartial sweep regions (0.016970 of all classified regions)
predicted 104 SoftPartial-linked sweep regions (0.042020 of all classified regions)

# ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/shortSevere.npy\
 ../ScA8VGg_594_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_594.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 3117 total instances
predicted 1004 neutral regions (0.322105 of all classified regions)
predicted 54 Hard sweep regions (0.017324 of all classified regions)
predicted 256 Hard-linked sweep regions (0.082130 of all classified regions)
predicted 286 Soft sweep regions (0.091755 of all classified regions)
predicted 1030 Soft-linked sweep regions (0.330446 of all classified regions)
predicted 137 HardPartial sweep regions (0.043953 of all classified regions)
predicted 195 HardPartial-linked sweep regions (0.062560 of all classified regions)
predicted 46 SoftPartial sweep regions (0.014758 of all classified regions)
predicted 109 SoftPartial-linked sweep regions (0.034970 of all classified regions)

# ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/shortSevere.npy\
 ../ScA8VGg_628_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_628.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 4618 total instances
predicted 1327 neutral regions (0.287354 of all classified regions)
predicted 102 Hard sweep regions (0.022087 of all classified regions)
predicted 502 Hard-linked sweep regions (0.108705 of all classified regions)
predicted 336 Soft sweep regions (0.072759 of all classified regions)
predicted 1551 Soft-linked sweep regions (0.335860 of all classified regions)
predicted 190 HardPartial sweep regions (0.041143 of all classified regions)
predicted 323 HardPartial-linked sweep regions (0.069944 of all classified regions)
predicted 89 SoftPartial sweep regions (0.019272 of all classified regions)
predicted 198 SoftPartial-linked sweep regions (0.042876 of all classified regions)

# ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/shortSevere.npy\
 ../ScA8VGg_718_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_718.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 1375 total instances
predicted 227 neutral regions (0.165091 of all classified regions)
predicted 36 Hard sweep regions (0.026182 of all classified regions)
predicted 176 Hard-linked sweep regions (0.128000 of all classified regions)
predicted 94 Soft sweep regions (0.068364 of all classified regions)
predicted 468 Soft-linked sweep regions (0.340364 of all classified regions)
predicted 51 HardPartial sweep regions (0.037091 of all classified regions)
predicted 146 HardPartial-linked sweep regions (0.106182 of all classified regions)
predicted 28 SoftPartial sweep regions (0.020364 of all classified regions)
predicted 149 SoftPartial-linked sweep regions (0.108364 of all classified regions)

# ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/shortSevere.npy\
 ../ScA8VGg_76_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_76.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 6080 total instances
predicted 1734 neutral regions (0.285197 of all classified regions)
predicted 97 Hard sweep regions (0.015954 of all classified regions)
predicted 478 Hard-linked sweep regions (0.078618 of all classified regions)
predicted 465 Soft sweep regions (0.076480 of all classified regions)
predicted 2260 Soft-linked sweep regions (0.371711 of all classified regions)
predicted 209 HardPartial sweep regions (0.034375 of all classified regions)
predicted 383 HardPartial-linked sweep regions (0.062993 of all classified regions)
predicted 116 SoftPartial sweep regions (0.019079 of all classified regions)
predicted 338 SoftPartial-linked sweep regions (0.055592 of all classified regions)

# ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred

python3 /home/uqsalle3/shared/Scott_Allen/partialSHIC/empirical_deep_learning_classify_python3.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/shortSevere/FVs/shortSevere.npy\
 ../ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec\
 11\
 92\
 ScA8VGg_785.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed

made predictions for 3836 total instances
predicted 1125 neutral regions (0.293274 of all classified regions)
predicted 54 Hard sweep regions (0.014077 of all classified regions)
predicted 299 Hard-linked sweep regions (0.077946 of all classified regions)
predicted 308 Soft sweep regions (0.080292 of all classified regions)
predicted 1353 Soft-linked sweep regions (0.352711 of all classified regions)
predicted 145 HardPartial sweep regions (0.037800 of all classified regions)
predicted 268 HardPartial-linked sweep regions (0.069864 of all classified regions)
predicted 72 SoftPartial sweep regions (0.018770 of all classified regions)
predicted 212 SoftPartial-linked sweep regions (0.055266 of all classified regions)

```

# ######################## Figure 2 ROC sweep vs other

```{shell}

module load anaconda3
source ~/conda-init
# conda activate r_env
conda activate r_env_4.3.1

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/

```

## train constNe

```{r}

library(data.table)
library(stringr)
library(dplyr)
library(tidyr)
library(readr)
library(plotROC)
library(pROC)
library(cowplot)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/")

ffs <- list.files(pattern = '*.prob.csv', recursive = TRUE)
ffs_test_self_newDat <- ffs[grep("test_self_newDat/FVs_earlyStopping/", ffs)]
ffs_test_expansionNe_intergenic <- ffs[grep("test_expansionNe_intergenic/FVs/", ffs)]
ffs_test_expansionNe_intergenic_lci <- ffs[grep("test_expansionNe_intergenic_lci/FVs/", ffs)]
ffs_test_expansionNe_intergenic_uci <- ffs[grep("test_expansionNe_intergenic_uci/FVs/", ffs)]



constNe_test_self_newDat <- data.table()

for(ff in ffs_test_self_newDat){
  fold <- gsub("test_self_newDat/FVs_earlyStopping/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs_earlyStopping/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "constNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  constNe_test_self_newDat <- rbind(constNe_test_self_newDat, tmp)
}

ensemble_test_self_newDat <- constNe_test_self_newDat %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_self_newDat$pred_ensemble <- apply(ensemble_test_self_newDat[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_self_newDat %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)




constNe_test_expansionNe_intergenic <- data.table()

for(ff in ffs_test_expansionNe_intergenic){
  fold <- gsub("test_expansionNe_intergenic/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "constNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  constNe_test_expansionNe_intergenic <- rbind(constNe_test_expansionNe_intergenic, tmp)
}

ensemble_test_expansionNe_intergenic <- constNe_test_expansionNe_intergenic %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_expansionNe_intergenic$pred_ensemble <- apply(ensemble_test_expansionNe_intergenic[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_expansionNe_intergenic %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



constNe_test_expansionNe_intergenic_lci <- data.table()

for(ff in ffs_test_expansionNe_intergenic_lci){
  fold <- gsub("test_expansionNe_intergenic_lci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "constNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  constNe_test_expansionNe_intergenic_lci <- rbind(constNe_test_expansionNe_intergenic_lci, tmp)
}

ensemble_test_expansionNe_intergenic_lci <- constNe_test_expansionNe_intergenic_lci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_expansionNe_intergenic_lci$pred_ensemble <- apply(ensemble_test_expansionNe_intergenic_lci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_expansionNe_intergenic_lci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



constNe_test_expansionNe_intergenic_uci <- data.table()

for(ff in ffs_test_expansionNe_intergenic_uci){
  fold <- gsub("test_expansionNe_intergenic_uci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "constNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  constNe_test_expansionNe_intergenic_uci <- rbind(constNe_test_expansionNe_intergenic_uci, tmp)
}

ensemble_test_expansionNe_intergenic_uci <- constNe_test_expansionNe_intergenic_uci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_expansionNe_intergenic_uci$pred_ensemble <- apply(ensemble_test_expansionNe_intergenic_uci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_expansionNe_intergenic_uci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



constNe <- rbind(
  ensemble_test_self_newDat,
  ensemble_test_expansionNe_intergenic, 
  ensemble_test_expansionNe_intergenic_lci, 
  ensemble_test_expansionNe_intergenic_uci)

constNe <- constNe %>% 
  filter(ivali != "test_self") %>% 
  mutate(ivali = case_when(
    ivali=="test_self_newDat" ~ "constNe/constNe", 
    ivali=="test_expansionNe_intergenic" ~ "constNe/expansionNe", 
    ivali=="test_expansionNe_intergenic_lci" ~ "constNe/expansionNe_lci", 
    ivali=="test_expansionNe_intergenic_uci" ~ "constNe/expansionNe_uci"
  ))

constNe <- data.table(constNe)

constNe[truth %like% '5', predClass:='sweep']
constNe[! truth %like% '5', predClass:='other']
constNe[,probSweep:=Hard+Soft+HardPartial+SoftPartial]
constNe <- constNe %>% mutate(probSweep = probSweep/10)

constNe <- constNe %>% 
  mutate(true_labels = ifelse(predClass=="sweep", 1, 0))

auc(roc(constNe$true_labels[constNe$ivali=="constNe/constNe"], constNe$probSweep[constNe$ivali=="constNe/constNe"]))
auc(roc(constNe$true_labels[constNe$ivali=="constNe/expansionNe"], constNe$probSweep[constNe$ivali=="constNe/expansionNe"]))
auc(roc(constNe$true_labels[constNe$ivali=="constNe/expansionNe_lci"], constNe$probSweep[constNe$ivali=="constNe/expansionNe_lci"]))
auc(roc(constNe$true_labels[constNe$ivali=="constNe/expansionNe_uci"], constNe$probSweep[constNe$ivali=="constNe/expansionNe_uci"]))

# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.9087
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.8984
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.8867
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.8959

P1 <- ggplot(data=constNe, aes(d=predClass, m=probSweep, color=ivali)) + 
    geom_roc(labels=FALSE, pointsize=0) + 
    geom_abline(intercept=0, slope=1, lty="dashed", colour="grey") + 
    xlab("False positive rate") + 
    ylab("True positive rate") +
    scale_color_discrete(name='Test demogrpahy (AUC)', labels=c(
        'constNe (90.9%)',
        'SP2 median (89.8%)',
        'SP2 2.5th (88.7%)',
        'SP2 97.5th (89.6%)'
    )) +
    theme_classic() + 
    theme(legend.position = c(0.7, 0.2), 
          axis.text=element_text(size=10), 
          legend.title.align=0.1,
          axis.title=element_text(size=10), 
          legend.text=element_text(size=9), 
          legend.title=element_text(size=10), 
          plot.margin = margin(t = 20, b = 20))

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")
write_tsv(constNe, "ensemble_train_constNe_test_ALL.tsv")

```

## train expansionNe_intergenic

```{r}

# train expansionNe

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/")

ffs <- list.files(pattern = '*.prob.csv', recursive = TRUE)
ffs_test_self_newDat <- ffs[grep("test_self_newDat/FVs/", ffs)]
ffs_test_constNe <- ffs[grep("test_constNe/FVs/", ffs)]
ffs_test_expansionNe_intergenic_lci <- ffs[grep("test_expansionNe_intergenic_lci/FVs/", ffs)]
ffs_test_expansionNe_intergenic_uci <- ffs[grep("test_expansionNe_intergenic_uci/FVs/", ffs)]



expansionNe_intergenic_test_self_newDat <- data.table()

for(ff in ffs_test_self_newDat){
  fold <- gsub("test_self_newDat/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_intergenic_test_self_newDat <- rbind(expansionNe_intergenic_test_self_newDat, tmp)
}

# ensemble_test_self_newDat <- expansionNe_intergenic_test_self_newDat %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_self_newDat <- expansionNe_intergenic_test_self_newDat %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_self_newDat$pred_ensemble <- apply(ensemble_test_self_newDat[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_self_newDat %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_intergenic_test_constNe <- data.table()

for(ff in ffs_test_constNe){
  fold <- gsub("test_constNe/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_intergenic_test_constNe <- rbind(expansionNe_intergenic_test_constNe, tmp)
}

# ensemble_test_constNe <- expansionNe_intergenic_test_constNe %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_constNe <- expansionNe_intergenic_test_constNe %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_constNe$pred_ensemble <- apply(ensemble_test_constNe[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_constNe %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_intergenic_test_expansionNe_intergenic_lci <- data.table()

for(ff in ffs_test_expansionNe_intergenic_lci){
  fold <- gsub("test_expansionNe_intergenic_lci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_intergenic_test_expansionNe_intergenic_lci <- rbind(expansionNe_intergenic_test_expansionNe_intergenic_lci, tmp)
}

# ensemble_test_expansionNe_intergenic_lci <- expansionNe_intergenic_test_expansionNe_intergenic_lci %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_expansionNe_intergenic_lci <- expansionNe_intergenic_test_expansionNe_intergenic_lci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_expansionNe_intergenic_lci$pred_ensemble <- apply(ensemble_test_expansionNe_intergenic_lci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_expansionNe_intergenic_lci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_intergenic_test_expansionNe_intergenic_uci <- data.table()

for(ff in ffs_test_expansionNe_intergenic_uci){
  fold <- gsub("test_expansionNe_intergenic_uci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_intergenic_test_expansionNe_intergenic_uci <- rbind(expansionNe_intergenic_test_expansionNe_intergenic_uci, tmp)
}

# ensemble_test_expansionNe_intergenic_uci <- expansionNe_intergenic_test_expansionNe_intergenic_uci %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_expansionNe_intergenic_uci <- expansionNe_intergenic_test_expansionNe_intergenic_uci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_expansionNe_intergenic_uci$pred_ensemble <- apply(ensemble_test_expansionNe_intergenic_uci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_expansionNe_intergenic_uci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe <- rbind(
  ensemble_test_self_newDat,
  ensemble_test_constNe, 
  ensemble_test_expansionNe_intergenic_lci, 
  ensemble_test_expansionNe_intergenic_uci)

expansionNe <- expansionNe %>% 
  filter(ivali != "test_self") %>% 
  mutate(ivali = case_when(
    ivali=="test_self_newDat" ~ "expansionNe/expansionNe", 
    ivali=="test_constNe" ~ "expansionNe/constNe", 
    ivali=="test_expansionNe_intergenic_lci" ~ "expansionNe/expansionNe_lci", 
    ivali=="test_expansionNe_intergenic_uci" ~ "expansionNe/expansionNe_uci"
  ))

expansionNe <- data.table(expansionNe)

expansionNe[truth %like% '5', predClass:='sweep']
expansionNe[! truth %like% '5', predClass:='other']
expansionNe[,probSweep:=Hard+Soft+HardPartial+SoftPartial]
expansionNe <- expansionNe %>% mutate(probSweep = probSweep/10)

expansionNe <- expansionNe %>% 
  mutate(true_labels = ifelse(predClass=="sweep", 1, 0))



auc(roc(expansionNe$true_labels[expansionNe$ivali=="expansionNe/constNe"], 
        expansionNe$probSweep[expansionNe$ivali=="expansionNe/constNe"]))

auc(roc(expansionNe$true_labels[expansionNe$ivali=="expansionNe/expansionNe"], 
        expansionNe$probSweep[expansionNe$ivali=="expansionNe/expansionNe"]))

auc(roc(expansionNe$true_labels[expansionNe$ivali=="expansionNe/expansionNe_lci"], 
        expansionNe$probSweep[expansionNe$ivali=="expansionNe/expansionNe_lci"]))

auc(roc(expansionNe$true_labels[expansionNe$ivali=="expansionNe/expansionNe_uci"], 
        expansionNe$probSweep[expansionNe$ivali=="expansionNe/expansionNe_uci"]))

# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.8925
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.9568
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.9264
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.961

P2 <- ggplot(data=expansionNe, aes(d=predClass, m=probSweep, color=ivali)) + 
    geom_roc(labels=FALSE, pointsize=0) + 
    geom_abline(intercept=0, slope=1, lty="dashed", colour="grey") + 
    xlab("False positive rate") + 
    ylab("True positive rate") +
    scale_color_discrete(name='Test demography (AUC)', labels=c(
        'constNe (89.3%)',
        'SP2 median (95.7%)',
        'SP2 2.5th (92.6%)',
        'SP2 97.5th (96.1%)'
    )) +
    theme_classic() + 
    theme(legend.position = c(0.7, 0.2), 
          axis.text=element_text(size=10), 
          legend.title.align=0.1,
          axis.title=element_text(size=10), 
          legend.text=element_text(size=9), 
          legend.title=element_text(size=10), 
          plot.margin = margin(t = 20, b = 20))

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")
write_tsv(expansionNe, "ensemble_train_expansionNe_intergenic_test_ALL.tsv")

```

## train expansionNe_lci

```{r}

# train expansionNe_lci

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/")

ffs <- list.files(pattern = '*.prob.csv', recursive = TRUE)
ffs_test_self_newDat <- ffs[grep("test_self_newDat/FVs/", ffs)]
ffs_test_constNe <- ffs[grep("test_constNe/FVs/", ffs)]
ffs_test_expansionNe_intergenic <- ffs[grep("test_expansionNe_intergenic/FVs/", ffs)]
ffs_test_expansionNe_intergenic_uci <- ffs[grep("test_expansionNe_intergenic_uci/FVs/", ffs)]



expansionNe_lci_intergenic_test_self_newDat <- data.table()

for(ff in ffs_test_self_newDat){
  fold <- gsub("test_self_newDat/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_lci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_lci_intergenic_test_self_newDat <- rbind(expansionNe_lci_intergenic_test_self_newDat, tmp)
}

# ensemble_test_self_newDat <- expansionNe_lci_intergenic_test_self_newDat %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_self_newDat <- expansionNe_lci_intergenic_test_self_newDat %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_self_newDat$pred_ensemble <- apply(ensemble_test_self_newDat[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_self_newDat %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_intergenic_lci_test_constNe <- data.table()

for(ff in ffs_test_constNe){
  fold <- gsub("test_constNe/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_lci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_intergenic_lci_test_constNe <- rbind(expansionNe_intergenic_lci_test_constNe, tmp)
}

# ensemble_test_constNe <- expansionNe_intergenic_lci_test_constNe %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_constNe <- expansionNe_intergenic_lci_test_constNe %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_constNe$pred_ensemble <- apply(ensemble_test_constNe[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_constNe %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_intergenic_lci_test_expansionNe_intergenic <- data.table()

for(ff in ffs_test_expansionNe_intergenic){
  fold <- gsub("test_expansionNe_intergenic/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_lci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_intergenic_lci_test_expansionNe_intergenic <- rbind(expansionNe_intergenic_lci_test_expansionNe_intergenic, tmp)
}

# ensemble_test_expansionNe_intergenic <- expansionNe_intergenic_lci_test_expansionNe_intergenic %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_expansionNe_intergenic <- expansionNe_intergenic_lci_test_expansionNe_intergenic %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_expansionNe_intergenic$pred_ensemble <- apply(ensemble_test_expansionNe_intergenic[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_expansionNe_intergenic %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_intergenic_lci_test_expansionNe_intergenic_uci <- data.table()

for(ff in ffs_test_expansionNe_intergenic_uci){
  fold <- gsub("test_expansionNe_intergenic_uci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_intergenic_lci_test_expansionNe_intergenic_uci <- rbind(expansionNe_intergenic_lci_test_expansionNe_intergenic_uci, tmp)
}

# ensemble_test_expansionNe_intergenic_uci <- expansionNe_intergenic_lci_test_expansionNe_intergenic_uci %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_expansionNe_intergenic_uci <- expansionNe_intergenic_lci_test_expansionNe_intergenic_uci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_expansionNe_intergenic_uci$pred_ensemble <- apply(ensemble_test_expansionNe_intergenic_uci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_expansionNe_intergenic_uci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_lci <- rbind(
  ensemble_test_self_newDat,
  ensemble_test_constNe, 
  ensemble_test_expansionNe_intergenic, 
  ensemble_test_expansionNe_intergenic_uci)

expansionNe_lci <- expansionNe_lci %>% 
  filter(ivali != "test_self") %>% 
  mutate(ivali = case_when(
    ivali=="test_self_newDat" ~ "expansionNe_lci/expansionNe_lci", 
    ivali=="test_constNe" ~ "expansionNe_lci/constNe", 
    ivali=="test_expansionNe_intergenic" ~ "expansionNe_lci/expansionNe", 
    ivali=="test_expansionNe_intergenic_uci" ~ "expansionNe_lci/expansionNe_uci"
  ))

expansionNe_lci <- data.table(expansionNe_lci)

expansionNe_lci[truth %like% '5', predClass:='sweep']
expansionNe_lci[! truth %like% '5', predClass:='other']
expansionNe_lci[,probSweep:=Hard+Soft+HardPartial+SoftPartial]
expansionNe_lci <- expansionNe_lci %>% mutate(probSweep = probSweep/10)

expansionNe_lci <- expansionNe_lci %>% 
  mutate(true_labels = ifelse(predClass=="sweep", 1, 0))



auc(roc(expansionNe_lci$true_labels[expansionNe_lci$ivali=="expansionNe_lci/constNe"], 
        expansionNe_lci$probSweep[expansionNe_lci$ivali=="expansionNe_lci/constNe"]))

auc(roc(expansionNe_lci$true_labels[expansionNe_lci$ivali=="expansionNe_lci/expansionNe"], 
        expansionNe_lci$probSweep[expansionNe_lci$ivali=="expansionNe_lci/expansionNe"]))

auc(roc(expansionNe_lci$true_labels[expansionNe_lci$ivali=="expansionNe_lci/expansionNe_lci"], 
        expansionNe_lci$probSweep[expansionNe_lci$ivali=="expansionNe_lci/expansionNe_lci"]))

auc(roc(expansionNe_lci$true_labels[expansionNe_lci$ivali=="expansionNe_lci/expansionNe_uci"], 
        expansionNe_lci$probSweep[expansionNe_lci$ivali=="expansionNe_lci/expansionNe_uci"]))

# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.8957
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.9529
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.9253
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.9548

P3 <- ggplot(data=expansionNe_lci, aes(d=predClass, m=probSweep, color=ivali)) + 
    geom_roc(labels=FALSE, pointsize=0) + 
    geom_abline(intercept=0, slope=1, lty="dashed", colour="grey") + 
    xlab("False positive rate") + 
    ylab("True positive rate") +
    scale_color_discrete(name='Test demography (AUC)', labels=c(
        'constNe (89.6%)',
        'SP2 median (95.3%)',
        'SP2 2.5th (92.5%)',
        'SP2 97.5th (95.5%)'
    )) +
    theme_classic() + 
    theme(legend.position = c(0.7, 0.2), 
          axis.text=element_text(size=10), 
          legend.title.align=0.1,
          axis.title=element_text(size=10), 
          legend.text=element_text(size=9), 
          legend.title=element_text(size=10), 
          plot.margin = margin(t = 20, b = 20))

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")
write_tsv(expansionNe_lci, "ensemble_train_expansionNe_intergenic_lci_test_ALL.tsv")

```

## train expansionNe_uci

```{r}

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/")

ffs <- list.files(pattern = '*.prob.csv', recursive = TRUE)
ffs_test_self_newDat <- ffs[grep("test_self_newDat/FVs/", ffs)]
ffs_test_constNe <- ffs[grep("test_constNe/FVs/", ffs)]
ffs_test_expansionNe_intergenic <- ffs[grep("test_expansionNe_intergenic/FVs/", ffs)]
ffs_test_expansionNe_intergenic_lci <- ffs[grep("test_expansionNe_intergenic_lci/FVs/", ffs)]



expansionNe_uci_intergenic_test_self_newDat <- data.table()

for(ff in ffs_test_self_newDat){
  fold <- gsub("test_self_newDat/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_uci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_uci_intergenic_test_self_newDat <- rbind(expansionNe_uci_intergenic_test_self_newDat, tmp)
}

# ensemble_test_self_newDat <- expansionNe_uci_intergenic_test_self_newDat %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_self_newDat <- expansionNe_uci_intergenic_test_self_newDat %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_self_newDat$pred_ensemble <- apply(ensemble_test_self_newDat[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_self_newDat %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_intergenic_uci_test_constNe <- data.table()

for(ff in ffs_test_constNe){
  fold <- gsub("test_constNe/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_uci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_intergenic_uci_test_constNe <- rbind(expansionNe_intergenic_uci_test_constNe, tmp)
}

# ensemble_test_constNe <- expansionNe_intergenic_uci_test_constNe %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_constNe <- expansionNe_intergenic_uci_test_constNe %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_constNe$pred_ensemble <- apply(ensemble_test_constNe[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_constNe %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_intergenic_uci_test_expansionNe_intergenic <- data.table()

for(ff in ffs_test_expansionNe_intergenic){
  fold <- gsub("test_expansionNe_intergenic/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_uci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_intergenic_uci_test_expansionNe_intergenic <- rbind(expansionNe_intergenic_uci_test_expansionNe_intergenic, tmp)
}

# ensemble_test_expansionNe_intergenic <- expansionNe_intergenic_uci_test_expansionNe_intergenic %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_expansionNe_intergenic <- expansionNe_intergenic_uci_test_expansionNe_intergenic %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_expansionNe_intergenic$pred_ensemble <- apply(ensemble_test_expansionNe_intergenic[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_expansionNe_intergenic %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_intergenic_uci_test_expansionNe_intergenic_lci <- data.table()

for(ff in ffs_test_expansionNe_intergenic_lci){
  fold <- gsub("test_expansionNe_intergenic_lci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_uci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_intergenic_uci_test_expansionNe_intergenic_lci <- rbind(expansionNe_intergenic_uci_test_expansionNe_intergenic_lci, tmp)
}

# ensemble_test_expansionNe_intergenic_lci <- expansionNe_intergenic_uci_test_expansionNe_intergenic_lci %>% 
#   group_by(truth, iclassifier, ivali, sim) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob) %>% 
#   ungroup()

ensemble_test_expansionNe_intergenic_lci <- expansionNe_intergenic_uci_test_expansionNe_intergenic_lci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_expansionNe_intergenic_lci$pred_ensemble <- apply(ensemble_test_expansionNe_intergenic_lci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_expansionNe_intergenic_lci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_uci <- rbind(
  ensemble_test_self_newDat,
  ensemble_test_constNe, 
  ensemble_test_expansionNe_intergenic, 
  ensemble_test_expansionNe_intergenic_lci)

expansionNe_uci <- expansionNe_uci %>% 
  filter(ivali != "test_self") %>% 
  mutate(ivali = case_when(
    ivali=="test_self_newDat" ~ "expansionNe_uci/expansionNe_uci", 
    ivali=="test_constNe" ~ "expansionNe_uci/constNe", 
    ivali=="test_expansionNe_intergenic" ~ "expansionNe_uci/expansionNe", 
    ivali=="test_expansionNe_intergenic_lci" ~ "expansionNe_uci/expansionNe_lci"
  ))

expansionNe_uci <- data.table(expansionNe_uci)

expansionNe_uci[truth %like% '5', predClass:='sweep']
expansionNe_uci[! truth %like% '5', predClass:='other']
expansionNe_uci[,probSweep:=Hard+Soft+HardPartial+SoftPartial]
expansionNe_uci <- expansionNe_uci %>% mutate(probSweep = probSweep/10)

expansionNe_uci <- expansionNe_uci %>% 
  mutate(true_labels = ifelse(predClass=="sweep", 1, 0))



auc(roc(expansionNe_uci$true_labels[expansionNe_uci$ivali=="expansionNe_uci/constNe"], 
        expansionNe_uci$probSweep[expansionNe_uci$ivali=="expansionNe_uci/constNe"]))

auc(roc(expansionNe_uci$true_labels[expansionNe_uci$ivali=="expansionNe_uci/expansionNe"], 
        expansionNe_uci$probSweep[expansionNe_uci$ivali=="expansionNe_uci/expansionNe"]))

auc(roc(expansionNe_uci$true_labels[expansionNe_uci$ivali=="expansionNe_uci/expansionNe_lci"], 
        expansionNe_uci$probSweep[expansionNe_uci$ivali=="expansionNe_uci/expansionNe_lci"]))

auc(roc(expansionNe_uci$true_labels[expansionNe_uci$ivali=="expansionNe_uci/expansionNe_uci"], 
        expansionNe_uci$probSweep[expansionNe_uci$ivali=="expansionNe_uci/expansionNe_uci"]))

# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.8851
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.9539
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.9191
# Setting levels: control = 0, case = 1
# Setting direction: controls < cases
# Area under the curve: 0.9644

P4 <- ggplot(data=expansionNe_uci, aes(d=predClass, m=probSweep, color=ivali)) + 
    geom_roc(labels=FALSE, pointsize=0) + 
    geom_abline(intercept=0, slope=1, lty="dashed", colour="grey") + 
    xlab("False positive rate") + 
    ylab("True positive rate") +
    scale_color_discrete(name='Test demography (AUC)', labels=c(
        'constNe (88.5%)',
        'SP2 median (95.4%)',
        'SP2 2.5th (91.9%)',
        'SP2 97.5th (96.4%)'
    )) +
    theme_classic() + 
    theme(legend.position = c(0.7, 0.2), 
          axis.text=element_text(size=10), 
          legend.title.align=0.1,
          axis.title=element_text(size=10), 
          legend.text=element_text(size=9), 
          legend.title=element_text(size=10), 
          plot.margin = margin(t = 20, b = 20))

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")
write_tsv(expansionNe_uci, "ensemble_train_expansionNe_intergenic_uci_test_ALL.tsv")

```

## plot grid

```{r}

# plot grid

plot_grid(P1, P2, P3, P4, 
          nrow=2, 
          labels=c('A) Train demography constNe',
                   'B) Train demography SP2 median', 
                   'C) Train demography SP2 2.5th quantile',
                   'D) Train demography SP2 97.5th quantile'), 
          label_size = 10, 
          hjust = 0, 
          vjust = 1)

# save pdf

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_2_ROC_SCOTT_4.pdf', width=10, height=9, unit='in', dpi=600)

# save png

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_2_ROC_SCOTT_4.png', width=10, height=9, unit='in', dpi=600)

```

```{powershell, scp}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Figure_2_ROC_SCOTT_4.p* ./

```

# ######################## Figure X Confusion matrix Analysis

```{shell}

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/

```

```{r, OLD}

# library(tidyverse)
# # library(dplyr)
# # library(stringr)
# # library(ggplot2)
# library(data.table)
# 
# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")
# 
# # list files
# ffs <- list.files(pattern = '*.prob.csv', recursive = TRUE)
# # files to ignore
# temp1 <- grep("Empirical_Feature_Vectors", ffs)
# temp2 <- grep("/test_self/", ffs)
# indices_to_remove <- c(temp1, temp2)
# ffs <- ffs[-indices_to_remove]
# # split into test self and the rest
# temp3 <- grep("/test_self_newDat/FVs/", ffs)
# ffs_test_self <- ffs[temp3]
# ffs <- ffs[-temp3]
# rm(temp1, temp2, indices_to_remove, temp3)
# 
# # import the rest and add columns with mutate
# probs1 <- data.table()
# 
# for(ff in ffs){
#   tmp <- fread(ff, header = TRUE, sep = ',') %>%
#     mutate(
#       train = str_split(ff, '/')[[1]][1], 
#       test = str_split(ff, '/')[[1]][2], 
#       truth = str_split(ff, '/')[[1]][3] %>% str_replace(., "sp", "") %>% str_replace(., ".fvec.prob.csv", "")) %>% 
#     mutate(window = str_split(truth, "_")[[1]][2])
#   probs1 <- rbind(probs1, tmp)
# }
# 
# # import test self and add columns with mutate
# probs2 <- data.table()
# 
# for(ff in ffs_test_self){
#   tmp <- fread(ff, header = TRUE, sep = ',') %>%
#     mutate(
#       train = str_split(ff, '/')[[1]][1], 
#       test = str_split(ff, '/')[[1]][2], 
#       truth = str_split(ff, '/')[[1]][4] %>% str_replace(., "sp", "") %>% str_replace(., ".fvec.prob.csv", "")) %>% 
#     mutate(window = str_split(truth, "_")[[1]][2])
#   probs2 <- rbind(probs2, tmp)
# }
# 
# # combind test self and the rest
# probs <- bind_rows(probs1, probs2)
# rm(tmp, probs1, probs2)
# 
# # fix Neut window
# probs <- probs %>% 
#   mutate(window = ifelse(truth=="Neut", 5, window))
# 
# # create truth2 to match predicted class
# probs <- probs %>% 
#   mutate(truth2 = case_when(
#     str_detect(truth, "^Hard_") & window!=5 ~ "Hard-linked", 
#     str_detect(truth, "^Hard_") & window==5 ~ "Hard", 
#     str_detect(truth, "^Soft_") & window!=5 ~ "Soft-linked", 
#     str_detect(truth, "^Soft_") & window==5 ~ "Soft",
#     str_detect(truth, "^PartialHard_") & window!=5 ~ "HardPartial-linked", 
#     str_detect(truth, "^PartialHard_") & window==5 ~ "HardPartial", 
#     str_detect(truth, "^PartialSoft_") & window!=5 ~ "SoftPartial-linked", 
#     str_detect(truth, "^PartialSoft_") & window==5 ~ "SoftPartial", 
#     str_detect(truth, "^Neut") ~ "Neutral"
#   )) 
# 
# # Create a new column "pred" with the highest probability class
# probs$pred <- apply(probs[,1:9], 1, function(row) {
#   sorted_probs <- sort(row, decreasing = TRUE)
#   names(sorted_probs)[1]
# })
# 
# # select needed columns
# probs <- probs %>% 
#   select(train, test, truth, truth2, pred, window)
# 
# # standardise test name
# probs <- probs %>% 
#   mutate(test = ifelse(test=="test_expansion", "test_expansionNe", test))
# 
# # standardise window
# probs <- probs %>% 
#   mutate(window = case_when(
#     window==5 ~ 100,
#     window==0 ~ 5, 
#     window==1 ~ 4, 
#     window==2 ~ 3, 
#     window==3 ~ 2, 
#     window==4 ~ 1, 
#     window==10 ~ 5, 
#     window==9 ~ 4, 
#     window==8 ~ 3, 
#     window==7 ~ 2, 
#     window==6 ~ 1, 
#   ))
# 
# # calculate accuracy
# 
# probs_OG <- probs
# 
# probs_windows <- probs %>% 
#   filter(window!=100) %>% 
#   group_by(train, test, truth2, pred, window) %>%
#   summarise(n=n()) %>% 
#   mutate(accuracy=n/800)
# 
# # temp <- probs_windows %>%
# #   filter(window!=100) %>%
# #   group_by(train, test, truth2, window) %>%
# #   summarise(simulations=sum(n), 
# #             accuracy=n/sum(n))
# # 
# # probs_windows$accuracy <- temp$accuracy
# 
# probs_windows <- probs_windows %>% ungroup()
# 
# probs <- probs %>% 
#   group_by(train, test, truth2, pred) %>% 
#   summarise(n=n())
# 
# temp <- probs %>% 
#   group_by(train, test, truth2) %>% 
#   summarise(accuracy=n/sum(n))
# 
# probs$accuracy <- temp$accuracy
# 
# probs <- probs %>% ungroup()
# 
# # calculate accuracy no expansionNe
# 
# probs_windows_noExp <- probs_OG %>% 
#   filter(window!=100) %>% 
#   filter(train!="expansionNe") %>% 
#   filter(test!="test_expansionNe") %>% 
#   group_by(train, test, truth2, pred, window) %>%
#   summarise(n=n()) %>% 
#   mutate(accuracy=n/800)
# 
# # temp <- probs_windows %>%
# #   filter(window!=100) %>%
# #   group_by(train, test, truth2, window) %>%
# #   summarise(simulations=sum(n), 
# #             accuracy=n/sum(n))
# # 
# # probs_windows$accuracy <- temp$accuracy
# 
# probs_windows_noExp <- probs_windows_noExp %>% ungroup()
# 
# probs_noExp <- probs_OG %>% 
#   filter(train!="expansionNe") %>% 
#   filter(test!="test_expansionNe") %>% 
#   group_by(train, test, truth2, pred) %>% 
#   summarise(n=n())
# 
# temp <- probs_noExp %>% 
#   group_by(train, test, truth2) %>% 
#   summarise(accuracy=n/sum(n))
# 
# probs_noExp$accuracy <- temp$accuracy
# 
# probs_noExp <- probs_noExp %>% ungroup()

```

```{r}

library(tidyverse)
library(data.table)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ensemble_constNe <- fread("ensemble_train_constNe_test_ALL.tsv")
ensemble_expansionNe <- fread("ensemble_train_expansionNe_intergenic_test_ALL.tsv")
ensemble_expansionNe_lci <- fread("ensemble_train_expansionNe_intergenic_lci_test_ALL.tsv")
ensemble_expansionNe_uci <- fread("ensemble_train_expansionNe_intergenic_uci_test_ALL.tsv")

probs <- rbind(
  ensemble_constNe, 
  ensemble_expansionNe, 
  ensemble_expansionNe_lci, 
  ensemble_expansionNe_uci
  )

probs <- probs %>% mutate(window = gsub(".*_", "", truth))

# fix Neut window
probs <- probs %>% mutate(window = ifelse(truth=="spNeut", 5, window))

# create truth2 to match predicted class
probs <- probs %>% 
  mutate(truth2 = case_when(
    str_detect(truth, "^spHard_") & window!=5 ~ "Hard-linked", 
    str_detect(truth, "^spHard_") & window==5 ~ "Hard", 
    str_detect(truth, "^spSoft_") & window!=5 ~ "Soft-linked", 
    str_detect(truth, "^spSoft_") & window==5 ~ "Soft",
    str_detect(truth, "^spPartialHard_") & window!=5 ~ "HardPartial-linked", 
    str_detect(truth, "^spPartialHard_") & window==5 ~ "HardPartial", 
    str_detect(truth, "^spPartialSoft_") & window!=5 ~ "SoftPartial-linked", 
    str_detect(truth, "^spPartialSoft_") & window==5 ~ "SoftPartial", 
    str_detect(truth, "^spNeut") ~ "Neutral"
  )) 

# Create a new column "pred" with the highest probability class
probs$pred <- apply(probs[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

# select needed columns
probs <- probs %>% 
  rename(train = iclassifier, test = ivali) %>% 
  select(train, test, truth, truth2, pred, window)

# standardise test name
probs <- probs %>% 
  mutate(test = ifelse(test=="test_expansion", "test_expansionNe", test))

# standardise window
probs <- probs %>% 
  mutate(window = case_when(
    window==5 ~ 100,
    window==0 ~ 5, 
    window==1 ~ 4, 
    window==2 ~ 3, 
    window==3 ~ 2, 
    window==4 ~ 1, 
    window==10 ~ 5, 
    window==9 ~ 4, 
    window==8 ~ 3, 
    window==7 ~ 2, 
    window==6 ~ 1, 
  ))

# calculate accuracy

probs_OG <- probs

probs_windows <- probs %>% 
  filter(window!=100) %>% 
  group_by(train, test, truth2, pred, window) %>%
  summarise(n=n()) %>% 
  mutate(accuracy=n/800)

# temp <- probs_windows %>%
#   filter(window!=100) %>%
#   group_by(train, test, truth2, window) %>%
#   summarise(simulations=sum(n), 
#             accuracy=n/sum(n))
# 
# probs_windows$accuracy <- temp$accuracy

probs_windows <- probs_windows %>% ungroup()

probs <- probs %>% 
  group_by(train, test, truth2, pred) %>% 
  summarise(n=n())

temp <- probs %>% 
  group_by(train, test, truth2) %>% 
  summarise(accuracy=n/sum(n))

probs$accuracy <- temp$accuracy

probs <- probs %>% ungroup()

# # calculate accuracy no expansionNe
# 
# probs_windows_noExp <- probs_OG %>% 
#   filter(window!=100) %>% 
#   filter(train!="expansionNe") %>% 
#   filter(test!="test_expansionNe") %>% 
#   group_by(train, test, truth2, pred, window) %>%
#   summarise(n=n()) %>% 
#   mutate(accuracy=n/800)
# 
# # temp <- probs_windows %>%
# #   filter(window!=100) %>%
# #   group_by(train, test, truth2, window) %>%
# #   summarise(simulations=sum(n), 
# #             accuracy=n/sum(n))
# # 
# # probs_windows$accuracy <- temp$accuracy
# 
# probs_windows_noExp <- probs_windows_noExp %>% ungroup()
# 
# probs_noExp <- probs_OG %>% 
#   filter(train!="expansionNe") %>% 
#   filter(test!="test_expansionNe") %>% 
#   group_by(train, test, truth2, pred) %>% 
#   summarise(n=n())
# 
# temp <- probs_noExp %>% 
#   group_by(train, test, truth2) %>% 
#   summarise(accuracy=n/sum(n))
# 
# probs_noExp$accuracy <- temp$accuracy
# 
# probs_noExp <- probs_noExp %>% ungroup()

```

## Hard sweeps

* not needed for fig

```{r, fig.width=7, fig.height=7}

probs %>%
  dplyr::filter(truth2=="Hard") %>%
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive",
    pred!=truth2 ~ "False positive"
  )) %>%
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>%
  ggplot2::ggplot(aes(x=pred, y=accuracy, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  facet_wrap(
    ~test,
    # labeller = labeller(
    #   test = c(
    #     `constNe_constNe/constNe` = "train constNe\ntest constNe",
    #     constNe_constNe/expansionNe = "train constNe\ntest expansionNe",
    #     constNe_test_longShallow = "train constNe\ntest longShallow",
    #     constNe_test_shortSevere = "train constNe\ntest shortSevere",
    #     expansionNe_test_constNe = "train expansionNe\ntest constNe",
    #     expansionNe_test_self_newDat = "train expansionNe\ntest expansionNe",
    #     expansionNe_test_longShallow = "train expansionNe\ntest longShallow",
    #     expansionNe_test_shortSevere = "train expansionNe\ntest shortSevere",
    #     longShallow_test_constNe = "train longShallow\ntest constNe",
    #     longShallow_test_expansionNe = "train longShallow\ntest expansionNe",
    #     longShallow_test_self_newDat = "train longShallow\ntest longShallow",
    #     longShallow_test_shortSevere = "train longShallow\ntest shortSevere",
    #     shortSevere_test_constNe = "train shortSevere\ntest constNe",
    #     shortSevere_test_expansionNe = "train shortSevere\ntest expansionNe",
    #     shortSevere_test_longShallow = "train shortSevere\ntest longShallow",
    #     shortSevere_test_self_newDat = "train shortSevere\ntest shortSevere"
    #   )
    # )
  ) +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Simulated Hard",
       x = "Prediction",
       y = "Proportion",
       fill = "")

```

* not needed for fig

```{r, fig.width=7, fig.height=7}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialSoft.col <- RColorBrewer::brewer.pal(9,"Purples")
my.hard.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHard.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>%
  dplyr::filter(truth2=="Hard") %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Hard" ~ "True positive",
    TRUE ~ "False positive")) %>%
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  scale_fill_manual(values=c(my.soft.col[6], my.hard.col[6])) +
  theme_classic() +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Hard",
       x = "Prediction",
       y = "Proportion (median +/- interquartile range)",
       fill = "")

```

* needed for fig

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialSoft.col <- RColorBrewer::brewer.pal(9,"Purples")
my.hard.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHard.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>% 
  dplyr::filter(truth2=="Hard") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Hard" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
  scale_fill_manual(values=c(my.soft.col[6], my.hard.col[6])) + 
  theme_classic() + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Hard",
       x = "Prediction", 
       y = "Proportion (median +/- IQR)", 
       fill = "") -> 
hard_error

hard_error



# probs_noExp %>% 
#   dplyr::filter(truth2=="Hard") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   dplyr::summarise(median = median(accuracy), 
#                    iqr = IQR(accuracy), 
#                    lowerbar = median - iqr, 
#                    upperbar = median + iqr) %>% 
#   dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
#                 upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
#   dplyr::mutate(Classification = dplyr::case_when(
#     pred=="Hard" ~ "True positive", 
#     TRUE ~ "False positive")) %>% 
#   ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
#   geom_bar(stat="identity", position="identity") + 
#   geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
#   scale_fill_manual(values=c(my.soft.col[6], my.hard.col[6])) + 
#   theme_classic() + 
#   theme(legend.position="bottom", 
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
#         strip.text = element_text(size = 8)) + 
#   guides(fill = guide_legend(reverse = TRUE)) + 
#   labs(title = "Hard",
#        x = "Prediction", 
#        y = "Proportion (median +/- IQR)", 
#        fill = "") -> 
# hard_error_noExp
# 
# hard_error_noExp

```

```{r}

probs %>% 
  dplyr::filter(truth2=="Hard") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  # dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
  #               upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Hard" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

```

## Soft sweeps

* not needed for fig

```{r, fig.width=7, fig.height=7}

probs %>%
  dplyr::filter(truth2=="Soft") %>%
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive",
    pred!=truth2 ~ "False positive"
  )) %>%
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>%
  ggplot2::ggplot(aes(x=pred, y=accuracy, fill=Classification)) +
  geom_bar(stat="identity") +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  facet_wrap(
    ~test,
    # labeller = labeller(
    #   train_test = c(
    #     constNe_test_self_newDat = "train constNe\ntest constNe",
    #     constNe_test_expansionNe = "train constNe\ntest expansionNe",
    #     constNe_test_longShallow = "train constNe\ntest longShallow",
    #     constNe_test_shortSevere = "train constNe\ntest shortSevere",
    #     expansionNe_test_constNe = "train expansionNe\ntest constNe",
    #     expansionNe_test_self_newDat = "train expansionNe\ntest expansionNe",
    #     expansionNe_test_longShallow = "train expansionNe\ntest longShallow",
    #     expansionNe_test_shortSevere = "train expansionNe\ntest shortSevere",
    #     longShallow_test_constNe = "train longShallow\ntest constNe",
    #     longShallow_test_expansionNe = "train longShallow\ntest expansionNe",
    #     longShallow_test_self_newDat = "train longShallow\ntest longShallow",
    #     longShallow_test_shortSevere = "train longShallow\ntest shortSevere",
    #     shortSevere_test_constNe = "train shortSevere\ntest constNe",
    #     shortSevere_test_expansionNe = "train shortSevere\ntest expansionNe",
    #     shortSevere_test_longShallow = "train shortSevere\ntest longShallow",
    #     shortSevere_test_self_newDat = "train shortSevere\ntest shortSevere"
    #   )
    # )
  ) +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Simulated Soft",
       x = "Prediction",
       y = "Proportion",
       fill = "")

```

* not needed for fig

```{r, fig.width=7, fig.height=7}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialSoft.col <- RColorBrewer::brewer.pal(9,"Purples")
my.Soft.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialSoft.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>%
  dplyr::filter(truth2=="Soft") %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Soft" ~ "True positive",
    TRUE ~ "False positive")) %>%
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  scale_fill_manual(values=c(my.soft.col[6], my.Soft.col[6])) +
  theme_classic() +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Soft",
       x = "Prediction",
       y = "Proportion (median +/- interquartile range)",
       fill = "")

```

* needed for fig

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialSoft.col <- RColorBrewer::brewer.pal(9,"Purples")
my.Soft.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialSoft.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>% 
  dplyr::filter(truth2=="Soft") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Soft" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
  scale_fill_manual(values=c(my.soft.col[6], my.Soft.col[6])) + 
  theme_classic() + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Soft", 
       x = "Prediction", 
       y = "Proportion (median +/- IQR)", 
       fill = "") -> 
soft_error

soft_error



# probs_noExp %>% 
#   dplyr::filter(truth2=="Soft") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   dplyr::summarise(median = median(accuracy), 
#                    iqr = IQR(accuracy), 
#                    lowerbar = median - iqr, 
#                    upperbar = median + iqr) %>% 
#   dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
#                 upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
#   dplyr::mutate(Classification = dplyr::case_when(
#     pred=="Soft" ~ "True positive", 
#     TRUE ~ "False positive")) %>% 
#   ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
#   geom_bar(stat="identity", position="identity") + 
#   geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
#   scale_fill_manual(values=c(my.soft.col[6], my.Soft.col[6])) + 
#   theme_classic() + 
#   theme(legend.position="bottom", 
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
#         strip.text = element_text(size = 8)) + 
#   guides(fill = guide_legend(reverse = TRUE)) + 
#   labs(title = "Soft", 
#        x = "Prediction", 
#        y = "Proportion (median +/- IQR)", 
#        fill = "") -> 
# soft_error_noExp
# 
# soft_error_noExp

```

```{r}

probs %>% 
  dplyr::filter(truth2=="Soft") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  # dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
  #               upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Soft" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

```

```{r}

probs %>% 
  dplyr::filter(truth2=="Soft") %>% 
  # dplyr::filter(pred!="Soft") %>% 
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>% 
  dplyr::filter(train_test!="constNe_expansionNe") %>% 
  dplyr::filter(train_test!="longShallow_expansionNe") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

```{r}

probs %>% 
  dplyr::filter(truth2=="Soft") %>% 
  # dplyr::filter(pred!="Soft") %>% 
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>% 
  dplyr::filter(train_test=="constNe_expansionNe" | train_test=="longShallow_expansionNe") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

## HardPartial

```{r, fig.width=7, fig.height=7}

probs %>%
  dplyr::filter(truth2=="HardPartial") %>%
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive",
    pred!=truth2 ~ "False positive"
  )) %>%
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>%
  ggplot2::ggplot(aes(x=pred, y=accuracy, fill=Classification)) +
  geom_bar(stat="identity") +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  facet_wrap(
    ~test,
    # labeller = labeller(
    #   train_test = c(
    #     constNe_test_self_newDat = "train constNe\ntest constNe",
    #     constNe_test_expansionNe = "train constNe\ntest expansionNe",
    #     constNe_test_longShallow = "train constNe\ntest longShallow",
    #     constNe_test_shortSevere = "train constNe\ntest shortSevere",
    #     expansionNe_test_constNe = "train expansionNe\ntest constNe",
    #     expansionNe_test_self_newDat = "train expansionNe\ntest expansionNe",
    #     expansionNe_test_longShallow = "train expansionNe\ntest longShallow",
    #     expansionNe_test_shortSevere = "train expansionNe\ntest shortSevere",
    #     longShallow_test_constNe = "train longShallow\ntest constNe",
    #     longShallow_test_expansionNe = "train longShallow\ntest expansionNe",
    #     longShallow_test_self_newDat = "train longShallow\ntest longShallow",
    #     longShallow_test_shortSevere = "train longShallow\ntest shortSevere",
    #     shortSevere_test_constNe = "train shortSevere\ntest constNe",
    #     shortSevere_test_expansionNe = "train shortSevere\ntest expansionNe",
    #     shortSevere_test_longShallow = "train shortSevere\ntest longShallow",
    #     shortSevere_test_self_newDat = "train shortSevere\ntest shortSevere"
    #   )
    # )
  ) +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Simulated HardPartial",
       x = "Prediction",
       y = "Proportion",
       fill = "")

```

```{r, fig.width=7, fig.height=7}

# my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
# my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
# my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
# my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')
# 
# probs %>% 
#   dplyr::filter(truth2=="HardPartial") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   dplyr::summarise(median = median(accuracy), 
#                    iqr = IQR(accuracy), 
#                    lowerbar = median - iqr, 
#                    upperbar = median + iqr) %>% 
#   dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
#                 upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
#   dplyr::mutate(Classification = dplyr::case_when(
#     pred=="HardPartial" ~ "True positive", 
#     TRUE ~ "False positive")) %>% 
#   ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
#   geom_bar(stat="identity", position="identity") + 
#   geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
#   scale_fill_manual(values=c(my.soft.col[6], my.HardPartial.col[6])) + 
#   theme_classic() + 
#   theme(legend.position="bottom", 
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
#         strip.text = element_text(size = 8)) + 
#   guides(fill = guide_legend(reverse = TRUE)) + 
#   labs(title = "HardPartial", 
#        x = "Prediction", 
#        y = "Proportion (median +/- interquartile range)", 
#        fill = "")

```

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>% 
  dplyr::filter(truth2=="HardPartial") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="HardPartial" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
  scale_fill_manual(values=c(my.soft.col[6], my.HardPartial.col[6])) + 
  theme_classic() + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "HardPartial", 
       x = "Prediction", 
       y = "Proportion (median +/- IQR)", 
       fill = "") -> 
hardpartial_error

hardpartial_error



# probs_noExp %>% 
#   dplyr::filter(truth2=="HardPartial") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   dplyr::summarise(median = median(accuracy), 
#                    iqr = IQR(accuracy), 
#                    lowerbar = median - iqr, 
#                    upperbar = median + iqr) %>% 
#   dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
#                 upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
#   dplyr::mutate(Classification = dplyr::case_when(
#     pred=="HardPartial" ~ "True positive", 
#     TRUE ~ "False positive")) %>% 
#   ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
#   geom_bar(stat="identity", position="identity") + 
#   geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
#   scale_fill_manual(values=c(my.soft.col[6], my.HardPartial.col[6])) + 
#   theme_classic() + 
#   theme(legend.position="bottom", 
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
#         strip.text = element_text(size = 8)) + 
#   guides(fill = guide_legend(reverse = TRUE)) + 
#   labs(title = "HardPartial", 
#        x = "Prediction", 
#        y = "Proportion (median +/- IQR)", 
#        fill = "") -> 
# hardpartial_error_noExp
# 
# hardpartial_error_noExp

```

```{r}

probs %>% 
  dplyr::filter(truth2=="HardPartial") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  # dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
  #               upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="HardPartial" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

```

## SoftPartial

```{r, fig.width=7, fig.height=7}

probs %>%
  dplyr::filter(truth2=="SoftPartial") %>%
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive",
    pred!=truth2 ~ "False positive"
  )) %>%
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>%
  ggplot2::ggplot(aes(x=pred, y=accuracy, fill=Classification)) +
  geom_bar(stat="identity") +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  facet_wrap(
    ~test,
    # labeller = labeller(
    #   train_test = c(
    #     constNe_test_self_newDat = "train constNe\ntest constNe",
    #     constNe_test_expansionNe = "train constNe\ntest expansionNe",
    #     constNe_test_longShallow = "train constNe\ntest longShallow",
    #     constNe_test_shortSevere = "train constNe\ntest shortSevere",
    #     expansionNe_test_constNe = "train expansionNe\ntest constNe",
    #     expansionNe_test_self_newDat = "train expansionNe\ntest expansionNe",
    #     expansionNe_test_longShallow = "train expansionNe\ntest longShallow",
    #     expansionNe_test_shortSevere = "train expansionNe\ntest shortSevere",
    #     longShallow_test_constNe = "train longShallow\ntest constNe",
    #     longShallow_test_expansionNe = "train longShallow\ntest expansionNe",
    #     longShallow_test_self_newDat = "train longShallow\ntest longShallow",
    #     longShallow_test_shortSevere = "train longShallow\ntest shortSevere",
    #     shortSevere_test_constNe = "train shortSevere\ntest constNe",
    #     shortSevere_test_expansionNe = "train shortSevere\ntest expansionNe",
    #     shortSevere_test_longShallow = "train shortSevere\ntest longShallow",
    #     shortSevere_test_self_newDat = "train shortSevere\ntest shortSevere"
    #   )
    # )
  ) +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Simulated SoftPartial",
       x = "Prediction",
       y = "Proportion",
       fill = "")

```

```{r, fig.width=7, fig.height=7}

# my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
# my.partialSoftPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
# my.SoftPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
# my.partialSoftPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')
# 
# probs %>% 
#   dplyr::filter(truth2=="SoftPartial") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   dplyr::summarise(median = median(accuracy), 
#                    iqr = IQR(accuracy), 
#                    lowerbar = median - iqr, 
#                    upperbar = median + iqr) %>% 
#   dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
#                 upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
#   dplyr::mutate(Classification = dplyr::case_when(
#     pred=="SoftPartial" ~ "True positive", 
#     TRUE ~ "False positive")) %>% 
#   ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
#   geom_bar(stat="identity", position="identity") + 
#   geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
#   scale_fill_manual(values=c(my.soft.col[6], my.SoftPartial.col[6])) + 
#   theme_classic() + 
#   theme(legend.position="bottom", 
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
#         strip.text = element_text(size = 8)) + 
#   guides(fill = guide_legend(reverse = TRUE)) + 
#   labs(title = "SoftPartial", 
#        x = "Prediction", 
#        y = "Proportion (median +/- interquartile range)", 
#        fill = "")

```

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialSoftPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.SoftPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialSoftPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>% 
  dplyr::filter(truth2=="SoftPartial") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="SoftPartial" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
  scale_fill_manual(values=c(my.soft.col[6], my.SoftPartial.col[6])) + 
  theme_classic() + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "SoftPartial", 
       x = "Prediction", 
       y = "Proportion (median +/- IQR)", 
       fill = "") -> 
softpartial_error

softpartial_error



# probs_noExp %>% 
#   dplyr::filter(truth2=="SoftPartial") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   dplyr::summarise(median = median(accuracy), 
#                    iqr = IQR(accuracy), 
#                    lowerbar = median - iqr, 
#                    upperbar = median + iqr) %>% 
#   dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
#                 upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
#   dplyr::mutate(Classification = dplyr::case_when(
#     pred=="SoftPartial" ~ "True positive", 
#     TRUE ~ "False positive")) %>% 
#   ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
#   geom_bar(stat="identity", position="identity") + 
#   geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
#   scale_fill_manual(values=c(my.soft.col[6], my.SoftPartial.col[6])) + 
#   theme_classic() + 
#   theme(legend.position="bottom", 
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
#         strip.text = element_text(size = 8)) + 
#   guides(fill = guide_legend(reverse = TRUE)) + 
#   labs(title = "SoftPartial", 
#        x = "Prediction", 
#        y = "Proportion (median +/- IQR)", 
#        fill = "") -> 
# softpartial_error_noExp
# 
# softpartial_error_noExp

```

```{r}

probs %>% 
  dplyr::filter(truth2=="SoftPartial") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  # dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
  #               upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="SoftPartial" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

```

```{r}

probs %>% 
  dplyr::filter(truth2=="SoftPartial") %>% 
  # dplyr::filter(pred!="SoftPartial") %>% 
  dplyr::filter(train=="expansionNe") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

```{r}

probs %>% 
  dplyr::filter(truth2=="SoftPartial") %>% 
  # dplyr::filter(pred!="SoftPartial") %>% 
  dplyr::filter(train!="expansionNe") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

## Hard-linked

```{r, fig.width=7, fig.height=7}

# probs %>%
#   dplyr::filter(truth2=="Hard-linked") %>%
#   dplyr::mutate(train_test = paste(train, test, sep="_")) %>%
#   dplyr::select(truth2, pred, train_test, accuracy) %>%
#   dplyr::group_by(train_test, truth2, pred) %>%
#   skimr::skim() %>%
#   as.data.frame() %>%
#   dplyr::mutate(Classification = case_when(
#     pred==truth2 ~ "True positive",
#     pred!=truth2 ~ "False positive"
#   )) %>%
#   ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) +
#   geom_bar(stat="identity", position="identity") +
#   geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) +
#   facet_wrap(
#     ~test,
#     # labeller = labeller(
#     #   train_test = c(
#     #     constNe_test_self_newDat = "train constNe\ntest constNe",
#     #     constNe_test_expansionNe = "train constNe\ntest expansionNe",
#     #     constNe_test_longShallow = "train constNe\ntest longShallow",
#     #     constNe_test_shortSevere = "train constNe\ntest shortSevere",
#     #     expansionNe_test_constNe = "train expansionNe\ntest constNe",
#     #     expansionNe_test_self_newDat = "train expansionNe\ntest expansionNe",
#     #     expansionNe_test_longShallow = "train expansionNe\ntest longShallow",
#     #     expansionNe_test_shortSevere = "train expansionNe\ntest shortSevere",
#     #     longShallow_test_constNe = "train longShallow\ntest constNe",
#     #     longShallow_test_expansionNe = "train longShallow\ntest expansionNe",
#     #     longShallow_test_self_newDat = "train longShallow\ntest longShallow",
#     #     longShallow_test_shortSevere = "train longShallow\ntest shortSevere",
#     #     shortSevere_test_constNe = "train shortSevere\ntest constNe",
#     #     shortSevere_test_expansionNe = "train shortSevere\ntest expansionNe",
#     #     shortSevere_test_longShallow = "train shortSevere\ntest longShallow",
#     #     shortSevere_test_self_newDat = "train shortSevere\ntest shortSevere"
#     #   )
#     # )
#   ) +
#   theme(legend.position="bottom",
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8),
#         strip.text = element_text(size = 8)) +
#   guides(fill = guide_legend(reverse = TRUE)) +
#   labs(title = "Simulated Hard-linked",
#        x = "Prediction",
#        y = "Proportion",
#        fill = "")

```

```{r, fig.width=7, fig.height=7}

# probs %>% 
#   dplyr::filter(truth2=="Hard-linked") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   skimr::skim() %>% 
#   as.data.frame() %>% 
#   dplyr::mutate(Classification = case_when(
#     pred=="Hard-linked" ~ "True positive", 
#     TRUE ~ "False positive"
#   )) %>% 
#   ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
#   geom_bar(stat="identity", position="identity") + 
#   geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
#   theme(legend.position="bottom", 
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
#         strip.text = element_text(size = 8)) + 
#   guides(fill = guide_legend(reverse = TRUE)) + 
#   labs(title = "Hard-linked", 
#        x = "Prediction", 
#        y = "Proportion", 
#        fill = "")

```

```{r, fig.width=4, fig.height=4}

# probs %>% 
#   dplyr::filter(truth2=="Hard-linked") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   skimr::skim() %>% 
#   as.data.frame() %>% 
#   dplyr::mutate(Classification = case_when(
#     pred=="Hard-linked" ~ "True positive", 
#     TRUE ~ "False positive"
#   )) %>% 
#   ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
#   geom_bar(stat="identity", position="identity") + 
#   geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
#   theme(legend.position="bottom", 
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
#         strip.text = element_text(size = 8)) + 
#   guides(fill = guide_legend(reverse = TRUE)) + 
#   labs(title = "Hard-linked", 
#        x = "Prediction", 
#        y = "Proportion", 
#        fill = "") -> 
# hardlinked_error
# 
# hardlinked_error

```

```{r, fig.width=7, fig.height=7}

# probs_windows %>%
#   dplyr::filter(truth2=="Hard-linked") %>%
#   dplyr::mutate(truth_window = paste(truth2, window, sep="_")) %>%
#   dplyr::select(truth_window, pred, accuracy) %>%
#   dplyr::group_by(pred, truth_window) %>%
#   skimr::skim() %>%
#   as.data.frame() %>%
#   dplyr::mutate(Classification = case_when(
#     pred=="Hard-linked" ~ "True positive",
#     TRUE ~ "False positive"
#   )) %>%
#   ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) +
#   geom_bar(stat="identity", position="identity") +
#   geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) +
#   facet_wrap(~truth_window) +
#   theme(legend.position="bottom",
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8),
#         strip.text = element_text(size = 8)) +
#   guides(fill = guide_legend(reverse = TRUE)) +
#   labs(title = "Hard-linked",
#        x = "Prediction",
#        y = "Proportion",
#        fill = "")

```

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

# Initialize a list to store the plot objects
hardlinked_error_list <- list()

# Initialise list for distince
dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")

# Loop through the window values and create the plots
for (i in 1:5) {
  plot <- probs_windows %>%
    dplyr::filter(truth2 == "Hard-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "Hard-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
    geom_bar(stat = "identity", position = "identity") +
    geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
    scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
    scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
    theme_classic() +
    theme(
      legend.position = "bottom",
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      strip.text = element_text(size = 8)
    ) +
    guides(fill = guide_legend(reverse = TRUE)) +
    labs(
      title = paste("Hard-linked", dist_list[i], "from target", sep=" "),
      x = "Prediction",
      y = "Proportion (median +/- IQR)",
      fill = ""
    )

  # Add the plot to the list
  hardlinked_error_list[[i]] <- plot
}



# # Initialize a list to store the plot objects
# hardlinked_error_list_noExp <- list()
# 
# # Initialise list for distince
# dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")
# 
# # Loop through the window values and create the plots
# for (i in 1:5) {
#   plot <- probs_windows_noExp %>%
#     dplyr::filter(truth2 == "Hard-linked") %>%
#     dplyr::filter(window == i) %>%
#     dplyr::select(pred, accuracy) %>%
#     dplyr::group_by(pred) %>%
#     dplyr::summarise(median = median(accuracy),
#                      iqr = IQR(accuracy),
#                      lowerbar = median - iqr,
#                      upperbar = median + iqr) %>%
#     dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
#                   upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
#     dplyr::mutate(Classification = dplyr::case_when(
#       pred == "Hard-linked" ~ "True positive",
#       TRUE ~ "False positive"
#     )) %>%
#     ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
#     geom_bar(stat = "identity", position = "identity") +
#     geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#     scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
#     scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
#     theme_classic() +
#     theme(
#       legend.position = "bottom",
#       axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
#       strip.text = element_text(size = 8)
#     ) +
#     guides(fill = guide_legend(reverse = TRUE)) +
#     labs(
#       title = paste("Hard-linked", dist_list[i], "from target", sep=" "),
#       x = "Prediction",
#       y = "Proportion (median +/- IQR)",
#       fill = ""
#     )
# 
#   # Add the plot to the list
#   hardlinked_error_list_noExp[[i]] <- plot
# }

```

```{r, fig.width=4, fig.height=4}

# Access the individual plots using plot_list[[1]], plot_list[[2]], etc.
hardlinked_error_list

```

```{r}

tbl_list = list()

for (i in 1:5) {
  tbl <- probs_windows %>%
    dplyr::filter(truth2 == "Hard-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "Hard-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    dplyr::arrange(desc(median))
  tbl_list[[i]] <- tbl
}

tbl_list

```

```{r, fig.width=7, fig.height=7}

probs_windows %>%
  dplyr::filter(truth2=="Hard-linked") %>%
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive",
    pred!=truth2 ~ "False positive"
  )) %>%
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>%
  ggplot2::ggplot(aes(x=pred, y=accuracy, fill=window, group=window)) +
  geom_bar(stat="identity", position="identity") +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  facet_wrap(~window) +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Simulated Hard-linked",
       x = "Prediction",
       y = "Proportion",
       fill = "")

```

```{r}

probs_windows %>%
  dplyr::filter(truth2=="Hard-linked") %>%
  dplyr::filter(window==1) %>%
  # dplyr::filter(pred!="Hard-linked") %>%
  dplyr::filter(train=="constNe") %>%
  dplyr::filter(test=="test_constNe") %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  skimr::skim() %>%
  as.data.frame() %>%
  arrange(desc(numeric.mean))

```

```{r}

probs %>% 
  dplyr::filter(truth2=="Hard-linked") %>% 
  # dplyr::filter(pred!="Hard-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

## Soft-linked

```{r, fig.width=7, fig.height=7}

probs %>% 
  dplyr::filter(truth2=="Soft-linked") %>% 
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>% 
  dplyr::select(truth2, pred, train_test, accuracy) %>% 
  dplyr::group_by(train_test, truth2, pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive", 
    pred!=truth2 ~ "False positive"
  )) %>% 
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
  facet_wrap(
    ~train_test, 
    labeller = labeller(
      train_test = c(
        constNe_test_self_newDat = "train constNe\ntest constNe",
        constNe_test_expansionNe = "train constNe\ntest expansionNe",
        constNe_test_longShallow = "train constNe\ntest longShallow", 
        constNe_test_shortSevere = "train constNe\ntest shortSevere", 
        expansionNe_test_constNe = "train expansionNe\ntest constNe",
        expansionNe_test_self_newDat = "train expansionNe\ntest expansionNe",
        expansionNe_test_longShallow = "train expansionNe\ntest longShallow", 
        expansionNe_test_shortSevere = "train expansionNe\ntest shortSevere", 
        longShallow_test_constNe = "train longShallow\ntest constNe",
        longShallow_test_expansionNe = "train longShallow\ntest expansionNe",
        longShallow_test_self_newDat = "train longShallow\ntest longShallow", 
        longShallow_test_shortSevere = "train longShallow\ntest shortSevere", 
        shortSevere_test_constNe = "train shortSevere\ntest constNe",
        shortSevere_test_expansionNe = "train shortSevere\ntest expansionNe",
        shortSevere_test_longShallow = "train shortSevere\ntest longShallow", 
        shortSevere_test_self_newDat = "train shortSevere\ntest shortSevere"
      )
    )
  ) +
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Simulated Soft-linked", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "")

```

```{r, fig.width=7, fig.height=7}

probs %>% 
  dplyr::filter(truth2=="Soft-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  dplyr::mutate(Classification = case_when(
    pred=="Soft-linked" ~ "True positive", 
    TRUE ~ "False positive"
  )) %>% 
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Soft-linked", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "")

```

```{r, fig.width=4, fig.height=4}

probs %>% 
  dplyr::filter(truth2=="Soft-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  dplyr::mutate(Classification = case_when(
    pred=="Soft-linked" ~ "True positive", 
    TRUE ~ "False positive"
  )) %>% 
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Soft-linked", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "") -> 
softlinked_error

softlinked_error

```

```{r, fig.width=7, fig.height=7}

probs_windows %>%
  dplyr::filter(truth2=="Soft-linked") %>%
  dplyr::mutate(truth_window = paste(truth2, window, sep="_")) %>%
  dplyr::select(truth_window, pred, accuracy) %>%
  dplyr::group_by(pred, truth_window) %>%
  skimr::skim() %>%
  as.data.frame() %>%
  dplyr::mutate(Classification = case_when(
    pred=="Soft-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>%
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) +
  facet_wrap(~truth_window) +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Soft-linked",
       x = "Prediction",
       y = "Proportion",
       fill = "")

```

```{r}

tbl_list = list()

for (i in 1:5) {
  tbl <- probs_windows %>%
    dplyr::filter(truth2 == "Soft-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "HardPartial-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    dplyr::arrange(desc(median))
  tbl_list[[i]] <- tbl
}
tbl_list

```

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

# Initialize a list to store the plot objects
softlinked_error_list <- list()

# Initialise list for distince
dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")

# Loop through the window values and create the plots
for (i in 1:5) {
  plot <- probs_windows %>%
    dplyr::filter(truth2 == "Soft-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "Soft-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
    geom_bar(stat = "identity", position = "identity") +
    geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
    scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
    scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
    theme_classic() +
    theme(
      legend.position = "bottom",
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      strip.text = element_text(size = 8)
    ) +
    guides(fill = guide_legend(reverse = TRUE)) +
    labs(
      title = paste("Soft-linked", dist_list[i], "from target", sep=" "),
      x = "Prediction",
      y = "Proportion (median +/- IQR)",
      fill = ""
    )

  # Add the plot to the list
  softlinked_error_list[[i]] <- plot
}



# # Initialize a list to store the plot objects
# softlinked_error_list_noExp <- list()
# 
# # Initialise list for distince
# dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")
# 
# # Loop through the window values and create the plots
# for (i in 1:5) {
#   plot <- probs_windows_noExp %>%
#     dplyr::filter(truth2 == "Soft-linked") %>%
#     dplyr::filter(window == i) %>%
#     dplyr::select(pred, accuracy) %>%
#     dplyr::group_by(pred) %>%
#     dplyr::summarise(median = median(accuracy),
#                      iqr = IQR(accuracy),
#                      lowerbar = median - iqr,
#                      upperbar = median + iqr) %>%
#     dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
#                   upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
#     dplyr::mutate(Classification = dplyr::case_when(
#       pred == "Soft-linked" ~ "True positive",
#       TRUE ~ "False positive"
#     )) %>%
#     ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
#     geom_bar(stat = "identity", position = "identity") +
#     geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#     scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
#     scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
#     theme_classic() +
#     theme(
#       legend.position = "bottom",
#       axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
#       strip.text = element_text(size = 8)
#     ) +
#     guides(fill = guide_legend(reverse = TRUE)) +
#     labs(
#       title = paste("Soft-linked", dist_list[i], "from target", sep=" "),
#       x = "Prediction",
#       y = "Proportion (median +/- IQR)",
#       fill = ""
#     )
# 
#   # Add the plot to the list
#   softlinked_error_list_noExp[[i]] <- plot
# }

```

```{r, fig.width=4, fig.height=4}

# Access the individual plots using plot_list[[1]], plot_list[[2]], etc.
softlinked_error_list

```

```{r, fig.width=7, fig.height=7}

# notLinked %>% 
#   dplyr::bind_rows(isLinked) %>% 
#   dplyr::filter(truth2=="Soft-linked") %>% 
#   dplyr::mutate(Classification = case_when(
#     pred==truth2 ~ "True positive", 
#     pred!=truth2 ~ "False positive"
#   )) %>% 
#   dplyr::mutate(train_test = paste(train, test, sep="_")) %>% 
#   ggplot2::ggplot(aes(x=pred, y=accuracy, fill=window, group=window)) + 
#   geom_bar(stat="identity", position="identity") +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
#   facet_wrap(~window) +
#   theme(legend.position="bottom", 
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
#         strip.text = element_text(size = 8)) + 
#   guides(fill = guide_legend(reverse = TRUE)) + 
#   labs(title = "Simulated Soft-linked", 
#        x = "Prediction", 
#        y = "Proportion", 
#        fill = "")

```

```{r}

probs %>% 
  dplyr::filter(truth2=="Soft-linked") %>% 
  # dplyr::filter(pred!="Soft-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

```{r}

probs %>% 
  dplyr::filter(truth2=="Soft-linked") %>% 
  # dplyr::filter(pred!="Soft-linked") %>% 
  dplyr::filter(train=="longShallow") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

```{r}

probs %>% 
  dplyr::filter(truth2=="Soft-linked") %>% 
  # dplyr::filter(pred!="Soft-linked") %>% 
  dplyr::filter(train!="longShallow") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

## HardPartial-linked

```{r, fig.width=7, fig.height=7}

probs %>% 
  dplyr::filter(truth2=="HardPartial-linked") %>% 
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>% 
  dplyr::select(truth2, pred, train_test, accuracy) %>% 
  dplyr::group_by(train_test, truth2, pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive", 
    pred!=truth2 ~ "False positive"
  )) %>% 
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
  facet_wrap(
    ~train_test, 
    labeller = labeller(
      train_test = c(
        constNe_test_self_newDat = "train constNe\ntest constNe",
        constNe_test_expansionNe = "train constNe\ntest expansionNe",
        constNe_test_longShallow = "train constNe\ntest longShallow", 
        constNe_test_shortSevere = "train constNe\ntest shortSevere", 
        expansionNe_test_constNe = "train expansionNe\ntest constNe",
        expansionNe_test_self_newDat = "train expansionNe\ntest expansionNe",
        expansionNe_test_longShallow = "train expansionNe\ntest longShallow", 
        expansionNe_test_shortSevere = "train expansionNe\ntest shortSevere", 
        longShallow_test_constNe = "train longShallow\ntest constNe",
        longShallow_test_expansionNe = "train longShallow\ntest expansionNe",
        longShallow_test_self_newDat = "train longShallow\ntest longShallow", 
        longShallow_test_shortSevere = "train longShallow\ntest shortSevere", 
        shortSevere_test_constNe = "train shortSevere\ntest constNe",
        shortSevere_test_expansionNe = "train shortSevere\ntest expansionNe",
        shortSevere_test_longShallow = "train shortSevere\ntest longShallow", 
        shortSevere_test_self_newDat = "train shortSevere\ntest shortSevere"
      )
    )
  ) +
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Simulated HardPartial-linked", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "")

```

```{r, fig.width=7, fig.height=7}

probs %>% 
  dplyr::filter(truth2=="HardPartial-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  dplyr::mutate(Classification = case_when(
    pred=="HardPartial-linked" ~ "True positive", 
    TRUE ~ "False positive"
  )) %>% 
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "HardPartial-linked", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "")

```

```{r, fig.width=4, fig.height=4}

probs %>% 
  dplyr::filter(truth2=="HardPartial-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  dplyr::mutate(Classification = case_when(
    pred=="HardPartial-linked" ~ "True positive", 
    TRUE ~ "False positive"
  )) %>% 
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "HardPartial-linked", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "") -> 
hardpartiallinked_error

hardpartiallinked_error

```

```{r, fig.width=7, fig.height=7}

probs_windows %>%
  dplyr::filter(truth2=="HardPartial-linked") %>%
  dplyr::mutate(truth_window = paste(truth2, window, sep="_")) %>%
  dplyr::select(truth_window, pred, accuracy) %>%
  dplyr::group_by(pred, truth_window) %>%
  skimr::skim() %>%
  as.data.frame() %>%
  dplyr::mutate(Classification = case_when(
    pred=="HardPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>%
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) +
  facet_wrap(~truth_window) +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "HardPartial-linked",
       x = "Prediction",
       y = "Proportion",
       fill = "")

```

```{r}

tbl_list = list()

for (i in 1:5) {
  tbl <- probs_windows %>%
    dplyr::filter(truth2 == "HardPartial-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "HardPartial-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    dplyr::arrange(desc(median))
  tbl_list[[i]] <- tbl
}
tbl_list

```

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

# Initialize a list to store the plot objects
hardpartiallinked_error_list <- list()

# Initialise list for distince
dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")

# Loop through the window values and create the plots
for (i in 1:5) {
  plot <- probs_windows %>%
    dplyr::filter(truth2 == "HardPartial-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "HardPartial-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
    geom_bar(stat = "identity", position = "identity") +
    geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
    scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
    scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
    theme_classic() +
    theme(
      legend.position = "bottom",
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      strip.text = element_text(size = 8)
    ) +
    guides(fill = guide_legend(reverse = TRUE)) +
    labs(
      title = paste("HardPartial-linked", dist_list[i], "from target", sep=" "),
      x = "Prediction",
      y = "Proportion (median +/- IQR)",
      fill = ""
    )

  # Add the plot to the list
  hardpartiallinked_error_list[[i]] <- plot
}



# # Initialize a list to store the plot objects
# hardpartiallinked_error_list_noExp <- list()
# 
# # Initialise list for distince
# dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")
# 
# # Loop through the window values and create the plots
# for (i in 1:5) {
#   plot <- probs_windows_noExp %>%
#     dplyr::filter(truth2 == "HardPartial-linked") %>%
#     dplyr::filter(window == i) %>%
#     dplyr::select(pred, accuracy) %>%
#     dplyr::group_by(pred) %>%
#     dplyr::summarise(median = median(accuracy),
#                      iqr = IQR(accuracy),
#                      lowerbar = median - iqr,
#                      upperbar = median + iqr) %>%
#     dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
#                   upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
#     dplyr::mutate(Classification = dplyr::case_when(
#       pred == "HardPartial-linked" ~ "True positive",
#       TRUE ~ "False positive"
#     )) %>%
#     ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
#     geom_bar(stat = "identity", position = "identity") +
#     geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#     scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
#     scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
#     theme_classic() +
#     theme(
#       legend.position = "bottom",
#       axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
#       strip.text = element_text(size = 8)
#     ) +
#     guides(fill = guide_legend(reverse = TRUE)) +
#     labs(
#       title = paste("HardPartial-linked", dist_list[i], "from target", sep=" "),
#       x = "Prediction",
#       y = "Proportion (median +/- IQR)",
#       fill = ""
#     )
# 
#   # Add the plot to the list
#   hardpartiallinked_error_list_noExp[[i]] <- plot
# }

```

```{r, fig.width=4, fig.height=4}

# Access the individual plots using plot_list[[1]], plot_list[[2]], etc.
hardpartiallinked_error_list

```

```{r}

probs %>% 
  dplyr::filter(truth2=="HardPartial-linked") %>% 
  # dplyr::filter(pred!="HardPartial-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

## SoftPartial-linked

```{r, fig.width=7, fig.height=7}

probs %>% 
  dplyr::filter(truth2=="SoftPartial-linked") %>% 
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>% 
  dplyr::select(truth2, pred, train_test, accuracy) %>% 
  dplyr::group_by(train_test, truth2, pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive", 
    pred!=truth2 ~ "False positive"
  )) %>% 
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
  facet_wrap(
    ~train_test, 
    labeller = labeller(
      train_test = c(
        constNe_test_self_newDat = "train constNe\ntest constNe",
        constNe_test_expansionNe = "train constNe\ntest expansionNe",
        constNe_test_longShallow = "train constNe\ntest longShallow", 
        constNe_test_shortSevere = "train constNe\ntest shortSevere", 
        expansionNe_test_constNe = "train expansionNe\ntest constNe",
        expansionNe_test_self_newDat = "train expansionNe\ntest expansionNe",
        expansionNe_test_longShallow = "train expansionNe\ntest longShallow", 
        expansionNe_test_shortSevere = "train expansionNe\ntest shortSevere", 
        longShallow_test_constNe = "train longShallow\ntest constNe",
        longShallow_test_expansionNe = "train longShallow\ntest expansionNe",
        longShallow_test_self_newDat = "train longShallow\ntest longShallow", 
        longShallow_test_shortSevere = "train longShallow\ntest shortSevere", 
        shortSevere_test_constNe = "train shortSevere\ntest constNe",
        shortSevere_test_expansionNe = "train shortSevere\ntest expansionNe",
        shortSevere_test_longShallow = "train shortSevere\ntest longShallow", 
        shortSevere_test_self_newDat = "train shortSevere\ntest shortSevere"
      )
    )
  ) +
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Simulated SoftPartial-linked", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "")

```

```{r, fig.width=7, fig.height=7}

probs %>% 
  dplyr::filter(truth2=="SoftPartial-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  dplyr::mutate(Classification = case_when(
    pred=="SoftPartial-linked" ~ "True positive", 
    TRUE ~ "False positive"
  )) %>% 
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "SoftPartial-linked", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "")

```

```{r, fig.width=4, fig.height=4}

probs %>% 
  dplyr::filter(truth2=="SoftPartial-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  dplyr::mutate(Classification = case_when(
    pred=="SoftPartial-linked" ~ "True positive", 
    TRUE ~ "False positive"
  )) %>% 
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "SoftPartial-linked", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "") -> 
softpartiallinked_error

softpartiallinked_error

```

```{r, fig.width=7, fig.height=7}

probs_windows %>%
  dplyr::filter(truth2=="SoftPartial-linked") %>%
  dplyr::mutate(truth_window = paste(truth2, window, sep="_")) %>%
  dplyr::select(truth_window, pred, accuracy) %>%
  dplyr::group_by(pred, truth_window) %>%
  skimr::skim() %>%
  as.data.frame() %>%
  dplyr::mutate(Classification = case_when(
    pred=="SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>%
  ggplot2::ggplot(aes(x=pred, y=numeric.mean, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  geom_errorbar(aes(ymin = numeric.mean - numeric.sd, ymax = numeric.mean + numeric.sd), width = 0.2) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(-0.03,1)) +
  facet_wrap(~truth_window) +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "SoftPartial-linked",
       x = "Prediction",
       y = "Proportion",
       fill = "")

```

```{r}

tbl_list = list()

for (i in 1:5) {
  tbl <- probs_windows %>%
    dplyr::filter(truth2 == "SoftPartial-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "SoftPartial-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    dplyr::arrange(desc(median))
  tbl_list[[i]] <- tbl
}
tbl_list

```

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

# Initialize a list to store the plot objects
softpartiallinked_error_list <- list()

# Initialise list for distince
dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")

# Loop through the window values and create the plots
for (i in 1:5) {
  plot <- probs_windows %>%
    dplyr::filter(truth2 == "SoftPartial-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "SoftPartial-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
    geom_bar(stat = "identity", position = "identity") +
    geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
    scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
    scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
    theme_classic() +
    theme(
      legend.position = "bottom",
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      strip.text = element_text(size = 8)
    ) +
    guides(fill = guide_legend(reverse = TRUE)) +
    labs(
      title = paste("SoftPartial-linked", dist_list[i], "from target", sep=" "),
      x = "Prediction",
      y = "Proportion (median +/- IQR)",
      fill = ""
    )

  # Add the plot to the list
  softpartiallinked_error_list[[i]] <- plot
}



# # Initialize a list to store the plot objects
# softpartiallinked_error_list_noExp <- list()
# 
# # Initialise list for distince
# dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")
# 
# # Loop through the window values and create the plots
# for (i in 1:5) {
#   plot <- probs_windows_noExp %>%
#     dplyr::filter(truth2 == "SoftPartial-linked") %>%
#     dplyr::filter(window == i) %>%
#     dplyr::select(pred, accuracy) %>%
#     dplyr::group_by(pred) %>%
#     dplyr::summarise(median = median(accuracy),
#                      iqr = IQR(accuracy),
#                      lowerbar = median - iqr,
#                      upperbar = median + iqr) %>%
#     dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
#                   upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
#     dplyr::mutate(Classification = dplyr::case_when(
#       pred == "SoftPartial-linked" ~ "True positive",
#       TRUE ~ "False positive"
#     )) %>%
#     ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
#     geom_bar(stat = "identity", position = "identity") +
#     geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#     scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
#     scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
#     theme_classic() +
#     theme(
#       legend.position = "bottom",
#       axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
#       strip.text = element_text(size = 8)
#     ) +
#     guides(fill = guide_legend(reverse = TRUE)) +
#     labs(
#       title = paste("SoftPartial-linked", dist_list[i], "from target", sep=" "),
#       x = "Prediction",
#       y = "Proportion (median +/- IQR)",
#       fill = ""
#     )
# 
#   # Add the plot to the list
#   softpartiallinked_error_list_noExp[[i]] <- plot
# }

```

```{r, fig.width=4, fig.height=4}

# Access the individual plots using plot_list[[1]], plot_list[[2]], etc.
softpartiallinked_error_list

```

```{r}

probs %>% 
  dplyr::filter(truth2=="SoftPartial-linked") %>% 
  # dplyr::filter(pred!="SoftPartial-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

```{r}

# probs %>% 
#   dplyr::filter(truth2=="SoftPartial-linked") %>% 
#   # dplyr::filter(pred!="SoftPartial-linked") %>% 
#   dplyr::mutate(train_test = paste(train, test, sep="_")) %>% 
#   dplyr::filter(train_test=="constNe_expansionNe" | 
#                   train_test=="expansionNe_expansionNe" | 
#                   train_test=="longShallow_expansionNe" | 
#                   train_test=="shortSevere_expansionNe" | 
#                   train_test=="longShallow_constNe" | 
#                   train_test=="longShallow_longShallow" | 
#                   train_test=="longShallow_shortSevere") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   skimr::skim() %>% 
#   as.data.frame() %>% 
#   arrange(desc(numeric.mean))

```

```{r}

probs %>% 
  dplyr::filter(truth2=="SoftPartial-linked") %>% 
  # dplyr::filter(pred!="SoftPartial-linked") %>% 
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>% 
  dplyr::filter(train_test!="constNe_expansionNe" & 
                  train_test!="expansionNe_expansionNe" & 
                  train_test!="longShallow_expansionNe" & 
                  train_test!="shortSevere_expansionNe" & 
                  train_test!="longShallow_constNe" & 
                  train_test!="longShallow_longShallow" & 
                  train_test!="longShallow_shortSevere") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

# ######################## Figure 4

## Neutral

```{r, fig.width=7, fig.height=7}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialNeutral.col <- RColorBrewer::brewer.pal(9,"Purples")
my.Neutral.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialNeutral.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>% 
  dplyr::filter(truth2=="Neutral") %>% 
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive", 
    pred!=truth2 ~ "False positive"
  )) %>% 
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>% 
  ggplot2::ggplot(aes(x=pred, y=accuracy, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
  scale_fill_manual(values=c(my.soft.col[6], my.Neutral.col[6])) + 
  facet_wrap(~test) + 
  theme_classic() + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Simulated Neutral", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "")

# save pdf

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_4_confusionMatrix_analysis_SCOTT_NEUTRAL.pdf', width=7, height=9, unit='in', dpi=300)

# save png

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_4_confusionMatrix_analysis_SCOTT_NEUTRAL.png', width=7, height=9, unit='in', dpi=300)

# save svg

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_4_confusionMatrix_analysis_SCOTT_NEUTRAL.svg', width=7, height=9, unit='in', dpi=300)

```

```{powershell, scp}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Figure_4_confusionMatrix_analysis_SCOTT_NEUTRAL.* ./

```

```{r, fig.width=7, fig.height=7}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialNeutral.col <- RColorBrewer::brewer.pal(9,"Purples")
my.Neutral.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialNeutral.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>% 
  dplyr::filter(truth2=="Neutral") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Neutral" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
  scale_fill_manual(values=c(my.soft.col[6], my.Neutral.col[6])) + 
  theme_classic() + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Neutral", 
       x = "Prediction", 
       y = "Proportion (median +/- interquartile range)", 
       fill = "")

```

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialNeutral.col <- RColorBrewer::brewer.pal(9,"Purples")
my.Neutral.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialNeutral.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>% 
  dplyr::filter(truth2=="Neutral") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Neutral" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
  scale_fill_manual(values=c(my.soft.col[6], my.Neutral.col[6])) + 
  theme_classic() + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Neutral", 
       x = "Prediction", 
       y = "Proportion (median +/- IQR)", 
       fill = "") -> 
neutral_error

neutral_error



# probs_noExp %>% 
#   dplyr::filter(truth2=="Neutral") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   dplyr::summarise(median = median(accuracy), 
#                    iqr = IQR(accuracy), 
#                    lowerbar = median - iqr, 
#                    upperbar = median + iqr) %>% 
#   dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar), 
#                 upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>% 
#   dplyr::mutate(Classification = dplyr::case_when(
#     pred=="Neutral" ~ "True positive", 
#     TRUE ~ "False positive")) %>% 
#   ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) + 
#   geom_bar(stat="identity", position="identity") + 
#   geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
#   scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
#   scale_fill_manual(values=c(my.soft.col[6], my.Neutral.col[6])) + 
#   theme_classic() + 
#   theme(legend.position="bottom", 
#         axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
#         strip.text = element_text(size = 8)) + 
#   guides(fill = guide_legend(reverse = TRUE)) + 
#   labs(title = "Neutral", 
#        x = "Prediction", 
#        y = "Proportion (median +/- IQR)", 
#        fill = "") -> 
# neutral_error_noExp
# 
# neutral_error_noExp

```

```{r}

## test constNe

probs %>% 
  dplyr::filter(test=="expansionNe/constNe" | 
                  test=="expansionNe_lci/constNe" | 
                  test=="expansionNe_uci/constNe") %>% 
  dplyr::filter(truth2=="Neutral") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(mean = mean(accuracy), 
                   median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Neutral" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

## train expansionNe_uci

probs %>% 
  dplyr::filter(test=="expansionNe_uci/constNe" | 
                  test=="expansionNe_uci/expansionNe" | 
                  test=="expansionNe_uci/expansionNe_lci") %>% 
  dplyr::filter(truth2=="Neutral") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(mean = mean(accuracy), 
                   median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Neutral" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

```


## Stats

```{r}

## Sweeps

probs %>% 
  dplyr::filter(truth2=="Neutral") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Neutral" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

# probs_noExp %>% 
#   dplyr::filter(truth2=="Neutral") %>% 
#   dplyr::select(pred, accuracy) %>% 
#   dplyr::group_by(pred) %>% 
#   dplyr::summarise(median = median(accuracy), 
#                    iqr = IQR(accuracy), 
#                    lowerbar = median - iqr, 
#                    upperbar = median + iqr) %>% 
#   dplyr::mutate(Classification = dplyr::case_when(
#     pred=="Neutral" ~ "True positive", 
#     TRUE ~ "False positive")) %>% 
#   dplyr::arrange(desc(median))

## Hard/HardPartial-Linked

probs_windows %>%
  dplyr::filter(truth2 == "HardPartial-linked") %>%
  dplyr::select(pred, window, accuracy) %>%
  dplyr::group_by(pred, window) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "HardPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>% 
  as.data.frame()

probs_windows_noExp %>%
  dplyr::filter(truth2 == "HardPartial-linked") %>%
  dplyr::select(pred, window, accuracy) %>%
  dplyr::group_by(pred, window) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "HardPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>% 
  as.data.frame()


probs_windows %>%
  dplyr::filter(truth2 == "Hard-linked") %>%
  dplyr::filter(window == 1) %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "Hard-linked" ~ "True positive",
    TRUE ~ "False positive"
  ))

probs_windows %>%
  dplyr::filter(truth2 == "Hard-linked") %>%
  dplyr::filter(window == 5) %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "Hard-linked" ~ "True positive",
    TRUE ~ "False positive"
  ))

## Soft/SoftPartial-Linked

probs_windows %>%
  dplyr::filter(truth2 == "Soft-linked") %>%
  dplyr::select(pred, window, accuracy) %>%
  dplyr::group_by(pred, window) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "Soft-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>% 
  as.data.frame()

probs_windows %>%
  dplyr::filter(truth2 == "SoftPartial-linked") %>%
  dplyr::select(pred, window, accuracy) %>%
  dplyr::group_by(pred, window) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>% 
  as.data.frame()

probs_windows_noExp %>%
  dplyr::filter(truth2 == "SoftPartial-linked") %>%
  dplyr::select(pred, window, accuracy) %>%
  dplyr::group_by(pred, window) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>% 
  as.data.frame()

probs_windows %>%
  dplyr::filter(truth2 == "SoftPartial-linked") %>%
  dplyr::filter(window == 1) %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  ))

probs_windows %>%
  dplyr::filter(truth2 == "SoftPartial-linked") %>%
  dplyr::filter(window == 3) %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  ))

probs_windows %>%
  dplyr::filter(truth2 == "SoftPartial-linked") %>%
  dplyr::filter(window == 5) %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  ))

```

## All error bar

```{r, fig.width=7, fig.height=9}

# Extract the legend from one plot
legend_plot <- hard_error + 
  theme(legend.position = "bottom", 
        legend.text = element_text(size=6), 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust = 0.5)) + 
  labs(title = "Hard")

# Remove the legend from the other plots
hard_error_x <- hard_error + 
  theme(legend.position = "none", 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard")
soft_error_x <- soft_error + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft")
hardpartial_error_x <- hardpartial_error + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial")
softpartial_error_x <- softpartial_error + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial")
neutral_error_x <- neutral_error + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Neutral")

hardlinked_error_1 <- hardlinked_error_list[[1]] + 
  theme(legend.position = "none", 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard-linked 5kb")
hardlinked_error_2 <- hardlinked_error_list[[2]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard-linked 10kb")
hardlinked_error_3 <- hardlinked_error_list[[3]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard-linked 15kb")
hardlinked_error_4 <- hardlinked_error_list[[4]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard-linked 20kb")
hardlinked_error_5 <- hardlinked_error_list[[5]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard-linked 25kb")

softlinked_error_1 <- softlinked_error_list[[1]] + 
  theme(legend.position = "none", 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft-linked 5kb")
softlinked_error_2 <- softlinked_error_list[[2]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft-linked 10kb")
softlinked_error_3 <- softlinked_error_list[[3]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft-linked 15kb")
softlinked_error_4 <- softlinked_error_list[[4]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft-linked 20kb")
softlinked_error_5 <- softlinked_error_list[[5]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft-linked 25kb")

hardpartiallinked_error_1 <- hardpartiallinked_error_list[[1]] + 
  theme(legend.position = "none", 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial-linked 5kb")
hardpartiallinked_error_2 <- hardpartiallinked_error_list[[2]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial-linked 10kb")
hardpartiallinked_error_3 <- hardpartiallinked_error_list[[3]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial-linked 15kb")
hardpartiallinked_error_4 <- hardpartiallinked_error_list[[4]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial-linked 20kb")
hardpartiallinked_error_5 <- hardpartiallinked_error_list[[5]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        axis.text.x = element_blank(), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial-linked 25kb")

softpartiallinked_error_1 <- softpartiallinked_error_list[[1]] + 
  theme(legend.position = "none", 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8),  
        axis.title.x = element_text(size=8), 
        axis.text.x = element_text(size=6),
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial-linked 5kb")
softpartiallinked_error_2 <- softpartiallinked_error_list[[2]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),   
        axis.title.x = element_text(size=8), 
        axis.text.x = element_text(size=6),
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial-linked 10kb")
softpartiallinked_error_3 <- softpartiallinked_error_list[[3]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),   
        axis.title.x = element_text(size=8), 
        axis.text.x = element_text(size=6),
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial-linked 15kb")
softpartiallinked_error_4 <- softpartiallinked_error_list[[4]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),   
        axis.title.x = element_text(size=8), 
        axis.text.x = element_text(size=6),
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial-linked 20kb")
softpartiallinked_error_5 <- softpartiallinked_error_list[[5]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),   
        axis.title.x = element_text(size=8), 
        axis.text.x = element_text(size=6),
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial-linked 25kb")

ggpubr::ggarrange(legend_plot, 
                  soft_error_x, 
                  hardpartial_error_x, 
                  softpartial_error_x, 
                  neutral_error_x, 
                  hardlinked_error_1, 
                  hardlinked_error_2, 
                  hardlinked_error_3, 
                  hardlinked_error_4, 
                  hardlinked_error_5, 
                  softlinked_error_1, 
                  softlinked_error_2, 
                  softlinked_error_3, 
                  softlinked_error_4, 
                  softlinked_error_5, 
                  hardpartiallinked_error_1, 
                  hardpartiallinked_error_2, 
                  hardpartiallinked_error_3, 
                  hardpartiallinked_error_4, 
                  hardpartiallinked_error_5, 
                  softpartiallinked_error_1, 
                  softpartiallinked_error_2, 
                  softpartiallinked_error_3, 
                  softpartiallinked_error_4, 
                  softpartiallinked_error_5, 
                  ncol=5, 
                  nrow=5, 
                  labels="AUTO", 
                  font.label = list(size = 7, face = "bold"),
                  common.legend = TRUE, 
                  legend="bottom", 
                  widths=c(1, 0.75, 0.75, 0.75, 0.75), 
                  heights=c(0.65, 0.65, 0.65, 0.65, 1))

# save pdf

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_X_confusionMatrix_analysis_SCOTT_3.pdf', width=7, height=9, unit='in', dpi=300)

# save png

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_X_confusionMatrix_analysis_SCOTT_3.png', width=7, height=9, unit='in', dpi=300)

```

```{powershell, scp}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Figure_X_confusionMatrix_analysis_SCOTT_3.p* ./

```

## All error bar no expansionNe

```{r, fig.width=7, fig.height=9}

# # Extract the legend from one plot
# legend_plot <- hard_error_noExp + 
#   theme(legend.position = "bottom", 
#         legend.text = element_text(size=6), 
#         axis.title.y = element_text(size=8), 
#         axis.text.y = element_text(size=8), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust = 0.5)) + 
#   labs(title = "Hard")
# 
# # Remove the legend from the other plots
# hard_error_x <- hard_error_noExp + 
#   theme(legend.position = "none", 
#         axis.title.y = element_text(size=8), 
#         axis.text.y = element_text(size=8), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Hard")
# soft_error_x <- soft_error_noExp + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Soft")
# hardpartial_error_x <- hardpartial_error_noExp + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "HardPartial")
# softpartial_error_x <- softpartial_error_noExp + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "SoftPartial")
# neutral_error_x <- neutral_error_noExp + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Neutral")
# 
# hardlinked_error_1 <- hardlinked_error_list_noExp[[1]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_text(size=8), 
#         axis.text.y = element_text(size=8), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Hard-linked 5kb")
# hardlinked_error_2 <- hardlinked_error_list_noExp[[2]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Hard-linked 10kb")
# hardlinked_error_3 <- hardlinked_error_list_noExp[[3]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Hard-linked 15kb")
# hardlinked_error_4 <- hardlinked_error_list_noExp[[4]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Hard-linked 20kb")
# hardlinked_error_5 <- hardlinked_error_list_noExp[[5]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Hard-linked 25kb")
# 
# softlinked_error_1 <- softlinked_error_list_noExp[[1]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_text(size=8), 
#         axis.text.y = element_text(size=8), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Soft-linked 5kb")
# softlinked_error_2 <- softlinked_error_list_noExp[[2]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Soft-linked 10kb")
# softlinked_error_3 <- softlinked_error_list_noExp[[3]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Soft-linked 15kb")
# softlinked_error_4 <- softlinked_error_list_noExp[[4]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Soft-linked 20kb")
# softlinked_error_5 <- softlinked_error_list_noExp[[5]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "Soft-linked 25kb")
# 
# hardpartiallinked_error_1 <- hardpartiallinked_error_list_noExp[[1]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_text(size=8), 
#         axis.text.y = element_text(size=8), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "HardPartial-linked 5kb")
# hardpartiallinked_error_2 <- hardpartiallinked_error_list_noExp[[2]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "HardPartial-linked 10kb")
# hardpartiallinked_error_3 <- hardpartiallinked_error_list_noExp[[3]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "HardPartial-linked 15kb")
# hardpartiallinked_error_4 <- hardpartiallinked_error_list_noExp[[4]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "HardPartial-linked 20kb")
# hardpartiallinked_error_5 <- hardpartiallinked_error_list_noExp[[5]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(), 
#         axis.title.x = element_blank(), 
#         axis.text.x = element_blank(), 
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "HardPartial-linked 25kb")
# 
# softpartiallinked_error_1 <- softpartiallinked_error_list_noExp[[1]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_text(size=8), 
#         axis.text.y = element_text(size=8),  
#         axis.title.x = element_text(size=8), 
#         axis.text.x = element_text(size=6),
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "SoftPartial-linked 5kb")
# softpartiallinked_error_2 <- softpartiallinked_error_list_noExp[[2]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(),   
#         axis.title.x = element_text(size=8), 
#         axis.text.x = element_text(size=6),
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "SoftPartial-linked 10kb")
# softpartiallinked_error_3 <- softpartiallinked_error_list_noExp[[3]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(),   
#         axis.title.x = element_text(size=8), 
#         axis.text.x = element_text(size=6),
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "SoftPartial-linked 15kb")
# softpartiallinked_error_4 <- softpartiallinked_error_list_noExp[[4]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(),   
#         axis.title.x = element_text(size=8), 
#         axis.text.x = element_text(size=6),
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "SoftPartial-linked 20kb")
# softpartiallinked_error_5 <- softpartiallinked_error_list_noExp[[5]] + 
#   theme(legend.position = "none", 
#         axis.title.y = element_blank(), 
#         axis.text.y = element_blank(), 
#         axis.ticks.y = element_blank(),   
#         axis.title.x = element_text(size=8), 
#         axis.text.x = element_text(size=6),
#         plot.title = element_text(size = 7, hjust=0.5)) + 
#   labs(title = "SoftPartial-linked 25kb")
# 
# ggpubr::ggarrange(legend_plot, 
#                   soft_error_x, 
#                   hardpartial_error_x, 
#                   softpartial_error_x, 
#                   neutral_error_x, 
#                   hardlinked_error_1, 
#                   hardlinked_error_2, 
#                   hardlinked_error_3, 
#                   hardlinked_error_4, 
#                   hardlinked_error_5, 
#                   softlinked_error_1, 
#                   softlinked_error_2, 
#                   softlinked_error_3, 
#                   softlinked_error_4, 
#                   softlinked_error_5, 
#                   hardpartiallinked_error_1, 
#                   hardpartiallinked_error_2, 
#                   hardpartiallinked_error_3, 
#                   hardpartiallinked_error_4, 
#                   hardpartiallinked_error_5, 
#                   softpartiallinked_error_1, 
#                   softpartiallinked_error_2, 
#                   softpartiallinked_error_3, 
#                   softpartiallinked_error_4, 
#                   softpartiallinked_error_5, 
#                   ncol=5, 
#                   nrow=5, 
#                   labels="AUTO", 
#                   font.label = list(size = 7, face = "bold"),
#                   common.legend = TRUE, 
#                   legend="bottom", 
#                   widths=c(1, 0.75, 0.75, 0.75, 0.75), 
#                   heights=c(0.65, 0.65, 0.65, 0.65, 1))
# 
# # save pdf
# 
# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")
# 
# ggsave('Figure_X_confusionMatrix_analysis_SCOTT_1.pdf', width=7, height=9, unit='in', dpi=300)
# 
# # save png
# 
# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")
# 
# ggsave('Figure_X_confusionMatrix_analysis_SCOTT_1.png', width=7, height=9, unit='in', dpi=300)

```

```{powershell, scp}

# cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission
# 
# scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Figure_X_confusionMatrix_analysis_SCOTT_1.p* ./

```

# ######################## Figure 3 Pie and Bar

```{r}

library(data.table)
library(RColorBrewer)
library(tidyverse)
#library(ggplot2)
#library(dplyr)
#library(tidyr)
library(cowplot)

# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe_intergenic")

beds <- list.files(pattern = 'ScA8VGg_.*.bed$')

aa <- data.table()

for(ff in beds){
    tmp <- fread(ff, header = FALSE, sep = "\t")
    aa <- rbind(aa, tmp)
}


a1 <- aa %>% 
  extract('V4','State','(.*)_ScA8VGg.*') %>% 
  mutate(V1=gsub('chr','', V1))


a1_df <- data.table(table(a1$State))
# hard_num <- aa_df[V1=="hard",N]
# soft_num <- aa_df[V1=="soft",N]
# hard_pct <- hard_num/(hard_num + soft_num)
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

aplot[V1 %in% c('HardPartial','HardPartial-linked'),lbl2:=lbl]


# PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
#     geom_bar(width=1, stat="identity", color="white") +
#     coord_polar("y", start=0) +
#     geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
#                   y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), label=lbl2), 
#               color="black", size=3) +
#     scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
#                                my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
#                                "grey60")) +
#     theme_void() + theme(legend.position = "none")

PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
  geom_bar(width=1, stat="identity", color="white") +
  coord_polar("y", start=0) +
  geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
                y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), 
                label=lbl2), 
            color="black", size=3) +
  scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                             my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                             "grey60")) +
  theme_void() + theme(legend.position = "none")



a1 <- data.table(a1)

a1[,chr:="xxxx"][V1=="ScA8VGg_76",chr:="3R"][V1=="ScA8VGg_718",
                                        chr:="2L"][V1=="ScA8VGg_542",chr:="2L"][V1=="ScA8VGg_594",
                                                                                chr:="X"][V1=="ScA8VGg_628",chr:="3L"][V1=="ScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$State)) -> a3
a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))

# ggsave("diploSHIC_proprotion_figure1.pdf",PP, width = 8, height = 4, dpi = 300, units = "in")

# save pdf

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_2_intergenic.pdf', width=8, height=4, unit='in', dpi=300)

# save png

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_2_intergenic.png', width=8, height=4, unit='in', dpi=300)


# chisquare test
library(rcompanion)

## Hard vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(1)], rowSums(bc[,c(2,4:9)]))
colnames(b2) <- c('hard','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Hard-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(2)], rowSums(bc[,c(1,3:9)]))
colnames(b2) <- c('hard_linked','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Soft vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(6)], rowSums(bc[,c(1:5,7:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Soft-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(7)], rowSums(bc[,c(1:6,8:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## HardPartial vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(3)], rowSums(bc[,c(1:2,4:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## HardPartial-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(4)], rowSums(bc[,c(1:3,5:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## SoftPartial vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(8)], rowSums(bc[,c(1:7,9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## SoftPartial-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(8)], rowSums(bc[,c(1:8)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

```

## expansionNe_intergenic

```{bash}

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical

```

```{r}

library(tidyverse)
library(data.table)
library(RColorBrewer)
library(cowplot)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})



# a1 <- pred %>% 
#   extract('V4','State','(.*)_ScA8VGg.*') %>% 
#   mutate(V1=gsub('chr','', V1))


# a1_df <- data.table(table(a1$State))
a1_df <- data.table(table(ensemble$pred_ensemble))
# hard_num <- aa_df[V1=="hard",N]
# soft_num <- aa_df[V1=="soft",N]
# hard_pct <- hard_num/(hard_num + soft_num)
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

# aplot[V1 %in% c('HardPartial', 'HardPartial-linked'), lbl2:=lbl]
# aplot[V1 %in% c('HardPartial', 'HardPartial-linked', 'Hard', 'SoftPartial'), lbl2:=lbl]

# PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
#     geom_bar(width=1, stat="identity", color="white") +
#     coord_polar("y", start=0) +
#     geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
#                   y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), label=lbl2), 
#               color="black", size=3) +
#     scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
#                                my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
#                                "grey60")) +
#     theme_void() + theme(legend.position = "none")

# PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
#   geom_bar(width=1, stat="identity", color="white") +
#   coord_polar("y", start=0) +
#   geom_text(aes(x=c(1.2,1.2,1.65,1.2,1.2,1.8,1.7,1.2,1.55),
#                 y=lab_ypos + c(0,0,2.5,0,0,-2.3,-1.5,0,0),
#                 label=lbl2),
#             color="black", size=3) +
#   scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
#                              my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
#                              "grey60")) +
#   theme_void() + theme(legend.position = "none")

PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
  geom_bar(width=1, stat="identity", color="white") +
  coord_polar("y", start=0) +
  # geom_text(aes(x=c(1.2,1.2,1.65,1.2,1.2,1.8,1.7,1.2,1.55),
  #               y=lab_ypos + c(0,0,2.5,0,0,-2.3,-1.5,0,0),
  #               label=lbl2),
  #           color="black", size=3) +
  scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                             my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                             "grey60")) +
  theme_void() + 
  theme(legend.position = "none")



a1 <- data.table(ensemble)

a1[,chr:="xxxx"][contig=="chrScA8VGg_76",chr:="3R"][contig=="chrScA8VGg_718",chr:="2L"][contig=="chrScA8VGg_542",chr:="2L"][contig=="chrScA8VGg_594",chr:="X"][contig=="chrScA8VGg_628",chr:="3L"][contig=="chrScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$pred_ensemble)) -> a3

# a1 <- data.table(a1_df)
# 
# a1[,chr:="xxxx"][V1=="ScA8VGg_76",chr:="3R"][V1=="ScA8VGg_718",
#                                         chr:="2L"][V1=="ScA8VGg_542",chr:="2L"][V1=="ScA8VGg_594",
#                                                                                 chr:="X"][V1=="ScA8VGg_628",chr:="3L"][V1=="ScA8VGg_785",chr:="2R"]
# a1[chr!="xxxx"] -> a2
# data.frame(table(a2$chr, a2$State)) -> a3
a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))

# ggsave("diploSHIC_proprotion_figure1.pdf",PP, width = 8, height = 4, dpi = 300, units = "in")

# save pdf

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_3_intergenic.pdf', width=8, height=4, unit='in', dpi=300)

# save png

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_3_intergenic.png', width=8, height=4, unit='in', dpi=300)

# save svg

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave("Figure_3_pie_and_bar_SCOTT_3_intergenic.svg", device = "svg", width=8, height=4, unit='in', dpi=300)


# chisquare test
library(rcompanion)

## Hard vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(1)], rowSums(bc[,c(2,4:9)]))
colnames(b2) <- c('hard','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Hard-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(2)], rowSums(bc[,c(1,3:9)]))
colnames(b2) <- c('hard_linked','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Soft vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(6)], rowSums(bc[,c(1:5,7:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Soft-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(7)], rowSums(bc[,c(1:6,8:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## HardPartial vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(3)], rowSums(bc[,c(1:2,4:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## HardPartial-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(4)], rowSums(bc[,c(1:3,5:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## SoftPartial vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(8)], rowSums(bc[,c(1:7,9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## SoftPartial-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(8)], rowSums(bc[,c(1:8)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

```

## constNe

```{r}

library(tidyverse)
library(data.table)
library(RColorBrewer)
# library(ggplot2)
# library(dplyr)
# library(tidyr)
library(cowplot)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/pred_empirical")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

# ensemble <- prob_pred %>% 
#   group_by(contig, start, end) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob)

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})



# a1 <- pred %>% 
#   extract('V4','State','(.*)_ScA8VGg.*') %>% 
#   mutate(V1=gsub('chr','', V1))


# a1_df <- data.table(table(a1$State))
a1_df <- data.table(table(ensemble$pred_ensemble))
# hard_num <- aa_df[V1=="hard",N]
# soft_num <- aa_df[V1=="soft",N]
# hard_pct <- hard_num/(hard_num + soft_num)
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

aplot[V1 %in% c('HardPartial','HardPartial-linked'),lbl2:=lbl]


# PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
#     geom_bar(width=1, stat="identity", color="white") +
#     coord_polar("y", start=0) +
#     geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
#                   y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), label=lbl2), 
#               color="black", size=3) +
#     scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
#                                my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
#                                "grey60")) +
#     theme_void() + theme(legend.position = "none")

PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
  geom_bar(width=1, stat="identity", color="white") +
  coord_polar("y", start=0) +
  geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
                y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), 
                label=lbl2), 
            color="black", size=3) +
  scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                             my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                             "grey60")) +
  theme_void() + theme(legend.position = "none")


a1 <- data.table(ensemble)

a1[,chr:="xxxx"][contig=="chrScA8VGg_76",chr:="3R"][contig=="chrScA8VGg_718",chr:="2L"][contig=="chrScA8VGg_542",chr:="2L"][contig=="chrScA8VGg_594",chr:="X"][contig=="chrScA8VGg_628",chr:="3L"][contig=="chrScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$pred_ensemble)) -> a3

# a1 <- data.table(a1_df)
# 
# a1[,chr:="xxxx"][V1=="ScA8VGg_76",chr:="3R"][V1=="ScA8VGg_718",
#                                         chr:="2L"][V1=="ScA8VGg_542",chr:="2L"][V1=="ScA8VGg_594",
#                                                                                 chr:="X"][V1=="ScA8VGg_628",chr:="3L"][V1=="ScA8VGg_785",chr:="2R"]
# a1[chr!="xxxx"] -> a2
# data.frame(table(a2$chr, a2$State)) -> a3
a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))

# ggsave("diploSHIC_proprotion_figure1.pdf",PP, width = 8, height = 4, dpi = 300, units = "in")

# save pdf

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_1_intergenic_constNe.pdf', width=8, height=4, unit='in', dpi=300)

# save png

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_1_intergenic_constNe.png', width=8, height=4, unit='in', dpi=300)


# chisquare test
library(rcompanion)

## Hard vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(1)], rowSums(bc[,c(2,4:9)]))
colnames(b2) <- c('hard','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Hard-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(2)], rowSums(bc[,c(1,3:9)]))
colnames(b2) <- c('hard_linked','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Soft vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(6)], rowSums(bc[,c(1:5,7:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Soft-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(7)], rowSums(bc[,c(1:6,8:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## HardPartial vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(3)], rowSums(bc[,c(1:2,4:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## HardPartial-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(4)], rowSums(bc[,c(1:3,5:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## SoftPartial vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(8)], rowSums(bc[,c(1:7,9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## SoftPartial-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(8)], rowSums(bc[,c(1:8)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

```

## expansionNe_intergenic_lci

```{r}

library(tidyverse)
library(data.table)
library(RColorBrewer)
# library(ggplot2)
# library(dplyr)
# library(tidyr)
library(cowplot)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/pred_empirical")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

# ensemble <- prob_pred %>% 
#   group_by(contig, start, end) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob)

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})



# a1 <- pred %>% 
#   extract('V4','State','(.*)_ScA8VGg.*') %>% 
#   mutate(V1=gsub('chr','', V1))


# a1_df <- data.table(table(a1$State))
a1_df <- data.table(table(ensemble$pred_ensemble))
# hard_num <- aa_df[V1=="hard",N]
# soft_num <- aa_df[V1=="soft",N]
# hard_pct <- hard_num/(hard_num + soft_num)
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

aplot[V1 %in% c('HardPartial','HardPartial-linked'),lbl2:=lbl]


# PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
#     geom_bar(width=1, stat="identity", color="white") +
#     coord_polar("y", start=0) +
#     geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
#                   y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), label=lbl2), 
#               color="black", size=3) +
#     scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
#                                my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
#                                "grey60")) +
#     theme_void() + theme(legend.position = "none")

PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
  geom_bar(width=1, stat="identity", color="white") +
  coord_polar("y", start=0) +
  geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
                y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), 
                label=lbl2), 
            color="black", size=3) +
  scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                             my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                             "grey60")) +
  theme_void() + theme(legend.position = "none")


a1 <- data.table(ensemble)

a1[,chr:="xxxx"][contig=="chrScA8VGg_76",chr:="3R"][contig=="chrScA8VGg_718",chr:="2L"][contig=="chrScA8VGg_542",chr:="2L"][contig=="chrScA8VGg_594",chr:="X"][contig=="chrScA8VGg_628",chr:="3L"][contig=="chrScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$pred_ensemble)) -> a3

# a1 <- data.table(a1_df)
# 
# a1[,chr:="xxxx"][V1=="ScA8VGg_76",chr:="3R"][V1=="ScA8VGg_718",
#                                         chr:="2L"][V1=="ScA8VGg_542",chr:="2L"][V1=="ScA8VGg_594",
#                                                                                 chr:="X"][V1=="ScA8VGg_628",chr:="3L"][V1=="ScA8VGg_785",chr:="2R"]
# a1[chr!="xxxx"] -> a2
# data.frame(table(a2$chr, a2$State)) -> a3
a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))

# ggsave("diploSHIC_proprotion_figure1.pdf",PP, width = 8, height = 4, dpi = 300, units = "in")

# save pdf

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_1_intergenic_lci.pdf', width=8, height=4, unit='in', dpi=300)

# save png

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_1_intergenic_lci.png', width=8, height=4, unit='in', dpi=300)


# chisquare test
library(rcompanion)

## Hard vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(1)], rowSums(bc[,c(2,4:9)]))
colnames(b2) <- c('hard','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Hard-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(2)], rowSums(bc[,c(1,3:9)]))
colnames(b2) <- c('hard_linked','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Soft vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(6)], rowSums(bc[,c(1:5,7:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Soft-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(7)], rowSums(bc[,c(1:6,8:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## HardPartial vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(3)], rowSums(bc[,c(1:2,4:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## HardPartial-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(4)], rowSums(bc[,c(1:3,5:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## SoftPartial vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(8)], rowSums(bc[,c(1:7,9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## SoftPartial-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(8)], rowSums(bc[,c(1:8)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

```

## expansionNe_intergenic_uci

```{r}

library(tidyverse)
library(data.table)
library(RColorBrewer)
# library(ggplot2)
# library(dplyr)
# library(tidyr)
library(cowplot)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/pred_empirical")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

# ensemble <- prob_pred %>% 
#   group_by(contig, start, end) %>% 
#   summarise(Neutral=sum(Neutral), 
#             Hard=sum(Hard), 
#             `Hard-linked`=sum(`Hard-linked`), 
#             Soft=sum(Soft), 
#             `Soft-linked`=sum(`Soft-linked`), 
#            HardPartial=sum(HardPartial), 
#             `HardPartial-linked`=sum(`HardPartial-linked`), 
#            SoftPartial=sum(SoftPartial), 
#             `SoftPartial-linked`=sum(`SoftPartial-linked`)) %>% 
#   pivot_longer(Neutral:`SoftPartial-linked`, names_to="class", values_to="prob") %>% 
#   mutate(pred_ensemble = class[prob==max(prob)]) %>% 
#   pivot_wider(names_from=class, values_from=prob)

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})



# a1 <- pred %>% 
#   extract('V4','State','(.*)_ScA8VGg.*') %>% 
#   mutate(V1=gsub('chr','', V1))


# a1_df <- data.table(table(a1$State))
a1_df <- data.table(table(ensemble$pred_ensemble))
# hard_num <- aa_df[V1=="hard",N]
# soft_num <- aa_df[V1=="soft",N]
# hard_pct <- hard_num/(hard_num + soft_num)
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

aplot[V1 %in% c('HardPartial','HardPartial-linked'),lbl2:=lbl]


# PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
#     geom_bar(width=1, stat="identity", color="white") +
#     coord_polar("y", start=0) +
#     geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
#                   y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), label=lbl2), 
#               color="black", size=3) +
#     scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
#                                my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
#                                "grey60")) +
#     theme_void() + theme(legend.position = "none")

PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
  geom_bar(width=1, stat="identity", color="white") +
  coord_polar("y", start=0) +
  geom_text(aes(x=c(1.2,1.2,1.2,1.2,1.2,1.8,1.7,1.2,1.4), 
                y=lab_ypos + c(0,0,0,0,0,-5,2,0,0), 
                label=lbl2), 
            color="black", size=3) +
  scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                             my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                             "grey60")) +
  theme_void() + theme(legend.position = "none")


a1 <- data.table(ensemble)

a1[,chr:="xxxx"][contig=="chrScA8VGg_76",chr:="3R"][contig=="chrScA8VGg_718",chr:="2L"][contig=="chrScA8VGg_542",chr:="2L"][contig=="chrScA8VGg_594",chr:="X"][contig=="chrScA8VGg_628",chr:="3L"][contig=="chrScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$pred_ensemble)) -> a3

# a1 <- data.table(a1_df)
# 
# a1[,chr:="xxxx"][V1=="ScA8VGg_76",chr:="3R"][V1=="ScA8VGg_718",
#                                         chr:="2L"][V1=="ScA8VGg_542",chr:="2L"][V1=="ScA8VGg_594",
#                                                                                 chr:="X"][V1=="ScA8VGg_628",chr:="3L"][V1=="ScA8VGg_785",chr:="2R"]
# a1[chr!="xxxx"] -> a2
# data.frame(table(a2$chr, a2$State)) -> a3
a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))

# ggsave("diploSHIC_proprotion_figure1.pdf",PP, width = 8, height = 4, dpi = 300, units = "in")

# save pdf

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_1_intergenic_uci.pdf', width=8, height=4, unit='in', dpi=300)

# save png

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_1_intergenic_uci.png', width=8, height=4, unit='in', dpi=300)


# chisquare test
library(rcompanion)

## Hard vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(1)], rowSums(bc[,c(2,4:9)]))
colnames(b2) <- c('hard','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Hard-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(2)], rowSums(bc[,c(1,3:9)]))
colnames(b2) <- c('hard_linked','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Soft vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(6)], rowSums(bc[,c(1:5,7:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## Soft-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(7)], rowSums(bc[,c(1:6,8:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## HardPartial vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(3)], rowSums(bc[,c(1:2,4:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## HardPartial-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(4)], rowSums(bc[,c(1:3,5:9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## SoftPartial vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(8)], rowSums(bc[,c(1:7,9)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

## SoftPartial-linked vs others

bc <- table(a2$chr, a2$State)
bc

b2 <- cbind(bc[,c(8)], rowSums(bc[,c(1:8)]))
colnames(b2) <- c('soft','others')
rownames(b2) <- c('2L','2R','3L','3R','X')
b2

chisq.test(b2)

pairwiseNominalIndependence(b2, fisher = F, gtest = F, chisq = T, method = "fdr")

apply(b2,1,prop.table)

```

```{powershell, scp}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Figure_3_pie_and_bar_SCOTT_* ./

```



# ######################## Figure 4 snpEff/snpSift

* MOVED FROM ~/Programs/snpEff to /scratch/project_mnt/S0032/partialSHIC_DsGRP/snpEff

## Bunya

### Install

```{shell}

# Bunya

cd /home/uqsalle3/Programs
  
wget https://snpeff.blob.core.windows.net/versions/snpEff_latest_core.zip

unzip snpEff_latest_core.zip

rm snpEff_latest_core.zip

# Dell 2

cd /home/uqsalle3/shared/Programs
  
wget https://snpeff.blob.core.windows.net/versions/snpEff_latest_core.zip

unzip snpEff_latest_core.zip

rm snpEff_latest_core.zip

```

### Build database for  scf3150

#### Step 1 Configure snpEff.config

Configure a new genome in SnpEff's config file snpEff.config.

a) Add genome entry to snpEff's configuration
b) If the genome uses a non-standard codon table: Add codon table parameter

```{shell}

vim snpEff.config

# # Drosophila serrata genome, version pbdagcon_relaxed_quiver scf3150
# Dserrata_PacBio_scf3150.genome : Dserrata_PacBio_scf3150

# # change data.dir
# data.dir = /home/uqsalle3/Programs/snpEff/data/

```

#### Step 2 Build database

Build using gene annotations and reference sequences

Option 1: Building a database from GTF files (recommended for large genomes)
Option 2: Building a database from GenBank files (recommended for small genomes)
Option 3: Building a database from GFF files
Option 4: Building a database from RefSeq table from UCSC

To build a database, SnpEff needs:

The reference genome sequence: This is the sequence of all chromosomes in the genome, typically in a FASTA file

Gene annotations files: This is the information on where the genes, transcripts and exons are in the genome. Typically, from files in GTF, GeneBank, GFF, or RefSeq formats

Sequences of CDS or Proteins from the genome. These are used to check that SnpEff's database is consistent and doesn't have errors (see sections Checking CDS sequences and Checking Protein sequences)

```{shell}

# Create directory for this new genome
cd /home/uqsalle3/Programs/snpEff
mkdir -p ./data
cd /home/uqsalle3/Programs/snpEff/data
mkdir -p Dserrata_PacBio_scf3150
cd Dserrata_PacBio_scf3150

# Get annotation files GTF
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/GCF_002093755.1_Dser1.0_genomic_OGcontigs.gtf ./
mv GCF_002093755.1_Dser1.0_genomic_OGcontigs.gtf genes.gtf
# remove all but scf3150 via vim
# .,$d

# # Get annotation files GFF
# rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/GCF_002093755.1_Dser1.0_genomic_OGcontigs_sorted.gff ./
# mv GCF_002093755.1_Dser1.0_genomic_OGcontigs_sorted.gff genes.gff

# Get the genome reference sequence file
cd /home/uqsalle3/Programs/snpEff/data/Dserrata_PacBio_scf3150
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/pbdagcon_relaxed_quiver.fasta ./
mv pbdagcon_relaxed_quiver.fasta sequences.fa
# remove all but scf3150 via vim
# .,$d

# Get the CDS sequence file
cd /home/uqsalle3/Programs/snpEff/data/Dserrata_PacBio_scf3150
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/GCF_002093755.1_Dser1.0_rna_from_genomic.fna ./
mv GCF_002093755.1_Dser1.0_rna_from_genomic.fna cds.fa
# sed -i 's/>XP_/>cds-XP_/g' protein.fa

# Get the protein sequence file
cd /home/uqsalle3/Programs/snpEff/data/Dserrata_PacBio_scf3150
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/GCF_002093755.1_Dser1.0_protein.faa.gz ./
gzip -d GCF_002093755.1_Dser1.0_protein.faa.gz
mv GCF_002093755.1_Dser1.0_protein.faa protein.fa
# sed -i 's/>XP_/>cds-XP_/g' protein.fa

# Create database
salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=16G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l

module load java

cd /home/uqsalle3/Programs/snpEff

# # GTF
java -jar snpEff.jar build -gtf22 -v Dserrata_PacBio_scf3150 2>&1 | tee Dserrata_PacBio_scf3150.build



#GFF
# java -jar snpEff.jar build -gff3 -d Dserrata_PacBio_scf3150 2>&1 | tee Dserrata_PacBio_scf3150.build

```

### Build database for PacBio

#### Step 1 Configure snpEff.config

Configure a new genome in SnpEff's config file snpEff.config.

a) Add genome entry to snpEff's configuration
b) If the genome uses a non-standard codon table: Add codon table parameter

```{shell}

vim snpEff.config

# # Drosophila serrata genome, version pbdagcon_relaxed_quiver
# Dserrata_PacBio.genome : Dserrata_PacBio

# # change data.dir
# data.dir = /home/uqsalle3/Programs/snpEff/data/

```

#### Step 2 Build database

Build using gene annotations and reference sequences

Option 1: Building a database from GTF files (recommended for large genomes)
Option 2: Building a database from GenBank files (recommended for small genomes)
Option 3: Building a database from GFF files
Option 4: Building a database from RefSeq table from UCSC

To build a database, SnpEff needs:

The reference genome sequence: This is the sequence of all chromosomes in the genome, typically in a FASTA file

Gene annotations files: This is the information on where the genes, transcripts and exons are in the genome. Typically, from files in GTF, GeneBank, GFF, or RefSeq formats

Sequences of CDS or Proteins from the genome. These are used to check that SnpEff's database is consistent and doesn't have errors (see sections Checking CDS sequences and Checking Protein sequences)

```{shell}

# Create directory for this new genome
cd /home/uqsalle3/Programs/snpEff
mkdir -p ./data
cd /home/uqsalle3/Programs/snpEff/data
mkdir -p Dserrata_PacBio
cd Dserrata_PacBio

# Get annotation files GTF
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/GCF_002093755.1_Dser1.0_genomic_OGcontigs.gtf ./
mv GCF_002093755.1_Dser1.0_genomic_OGcontigs.gtf genes.gtf

# # Get annotation files GFF
# rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/GCF_002093755.1_Dser1.0_genomic_OGcontigs_sorted.gff ./
# mv GCF_002093755.1_Dser1.0_genomic_OGcontigs_sorted.gff genes.gff

# Get the genome reference sequence file
cd /home/uqsalle3/Programs/snpEff/data/Dserrata_PacBio
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/pbdagcon_relaxed_quiver.fasta ./
mv pbdagcon_relaxed_quiver.fasta sequences.fa

# Get the protein sequence file
cd /home/uqsalle3/Programs/snpEff/data/Dserrata_PacBio
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/GCF_002093755.1_Dser1.0_protein.faa.gz ./
gzip -d GCF_002093755.1_Dser1.0_protein.faa.gz
mv GCF_002093755.1_Dser1.0_protein.faa protein.fa
# sed -i 's/>XP_/>cds-XP_/g' protein.fa

# Create database
salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=16G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l

module load java

cd /home/uqsalle3/Programs/snpEff

# # GTF
# java -jar snpEff.jar build -gtf22 -v Dserrata_PacBio

#GFF
java -jar snpEff.jar build -gff3 -d Dserrata_PacBio 2>&1 | tee Dserrata_PacBio.build

```

### Build database for HiC

#### Step 1 Configure snpEff.config

Configure a new genome in SnpEff's config file snpEff.config.

a) Add genome entry to snpEff's configuration
b) If the genome uses a non-standard codon table: Add codon table parameter

```{shell}

vim snpEff.config

# Drosophila serrata genome, version drosophila_06Jul2018_A8VGg
Dserrata_HiC.genome : Dserrata_HiC

# # change data.dir at the top of the file
# data.dir = /home/uqsalle3/shared/Programs/snpEff/data/

```

#### Step 2 Build database

Build using gene annotations and reference sequences

Option 1: Building a database from GTF files (recommended for large genomes)
Option 2: Building a database from GenBank files (recommended for small genomes)
Option 3: Building a database from GFF files
Option 4: Building a database from RefSeq table from UCSC

To build a database, SnpEff needs:

The reference genome sequence: This is the sequence of all chromosomes in the genome, typically in a FASTA file

Gene annotations files: This is the information on where the genes, transcripts and exons are in the genome. Typically, from files in GTF, GeneBank, GFF, or RefSeq formats

Sequences of CDS or Proteins from the genome. These are used to check that SnpEff's database is consistent and doesn't have errors (see sections Checking CDS sequences and Checking Protein sequences)

```{shell}

# Bunya

salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=16G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l

# Create directory for this new genome
cd /home/uqsalle3/Programs/snpEff
mkdir -p ./data
cd /home/uqsalle3/Programs/snpEff/data
mkdir -p Dserrata_HiC
cd Dserrata_HiC

# Get annotation files GTF
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome_2/annotation_transfer/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver.gtf ./
mv GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver.gtf genes.gtf

# Get annotation files GFF
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome_2/annotation_transfer/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gff ./
mv GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gff genes.gff

# Get the genome reference sequence file
cd /home/uqsalle3/Programs/snpEff/data/Dserrata_HiC
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome_2/dovetail_files/drosophila_06Jul2018_A8VGg.fasta ./
mv drosophila_06Jul2018_A8VGg.fasta sequences.fa

# Get the CDS sequence file
cd /home/uqsalle3/Programs/snpEff/data/Dserrata_HiC
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/GCF_002093755.1_Dser1.0_rna_from_genomic.fna ./
mv GCF_002093755.1_Dser1.0_rna_from_genomic.fna cds.fa
sed 's/.*\[transcript_id=/>/' cds.fa > temp1.fa
sed 's/\].*//' temp1.fa > cds.fa
rm temp1.fa

# Get the protein sequence file
cd /home/uqsalle3/Programs/snpEff/data/Dserrata_HiC
rsync -ahPv /QRISdata/Q1013/sci-data05/Reference_genome/GCF_002093755.1_Dser1.0_protein.faa.gz ./
gzip -d GCF_002093755.1_Dser1.0_protein.faa.gz
mv GCF_002093755.1_Dser1.0_protein.faa protein.fa

awk -F '\t' '{ split($9, a, "; "); protein_id=""; transcript_id=""; for (i in a) { split(a[i], b, " "); gsub(/"/, "", b[2]); if (b[1]=="protein_id") protein_id=b[2]; else if (b[1]=="transcript_id") transcript_id=b[2]; } if (protein_id!="" && transcript_id!="") print protein_id, transcript_id }' genes.gtf | uniq > protein2transcript_key.txt

module load python
python replace_protein_ids.py protein.fa protein2transcript_key.txt protein2.fa
mv protein2.fa protein.fa

# Create database

module load java

cd /home/uqsalle3/Programs/snpEff

# # GTF
java -jar snpEff.jar build -gtf22 -v -noCheckProtein Dserrata_HiC 2>&1 | tee Dserrata_HiC.build

# #GFF
# java -jar snpEff.jar build -gff3 -v Dserrata_HiC 2>&1 | tee Dserrata_HiC.build



# Dell 2

rsync -ahPv uqsalle3@bunya.rcc.uq.edu.au:/home/uqsalle3/Programs/snpEff ./
  
# # change data.dir at the top of the config file
# data.dir = /home/shared/Programs/snpEff/data/

```

#### Step 3 Annotate VCF

```{shell, annotate vcf Bunya}

salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l

module load java



# Get VCF JGIL
mkdir -p /home/uqsalle3/Programs/snpEff/data/vcf
cd /home/uqsalle3/Programs/snpEff/data/vcf
rsync -ahPv /QRISdata/Q1013/sci-data05/SIFT/DsGRP_DS100_JGIL_mapQ20/jgil_vcf_files_liftOver_HiC/JGIL_mapQ20_liftOver_HiC_RefMajorMinor.vcf.gz ./
gzip -d JGIL_mapQ20_liftOver_HiC_RefMajorMinor.vcf.gz
cut -f 1,2,3,4,5,6,7,8 JGIL_mapQ20_liftOver_HiC_RefMajorMinor.vcf > min.vcf

# Annotate
cd /home/uqsalle3/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/uqsalle3/Programs/snpEff/data/vcf/min.vcf > ./results/JGIL_mapQ20_liftOver_HiC_RefMajorMinor.ann.vcf

grep 'MODIFIER' ./results/JGIL_mapQ20_liftOver_HiC_RefMajorMinor.ann.vcf | wc -l
grep 'LOW' ./results/JGIL_mapQ20_liftOver_HiC_RefMajorMinor.ann.vcf | wc -l
grep 'MODERATE' ./results/JGIL_mapQ20_liftOver_HiC_RefMajorMinor.ann.vcf | wc -l
grep 'HIGH' ./results/JGIL_mapQ20_liftOver_HiC_RefMajorMinor.ann.vcf | wc -l



# Get VCF GATK 542

## rsync from Dell 2
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/
rsync -ahPv ./GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf uqsalle3@bunya.rcc.uq.edu.au:/home/uqsalle3/Programs/snpEff/data/vcf/

## back to Bunya
mkdir -p /home/uqsalle3/Programs/snpEff/data/vcf
cd /home/uqsalle3/Programs/snpEff/data/vcf
## convert to original scaffold names
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_542.vcf

## interactive session
salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l

module load java

# Annotate
cd /home/uqsalle3/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/uqsalle3/Programs/snpEff/data/vcf/min_542.vcf > ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l



# Get VCF GATK 594

## rsync from Dell 2
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/
rsync -ahPv ./GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf uqsalle3@bunya.rcc.uq.edu.au:/home/uqsalle3/Programs/snpEff/data/vcf/

## back to Bunya
mkdir -p /home/uqsalle3/Programs/snpEff/data/vcf
cd /home/uqsalle3/Programs/snpEff/data/vcf
## convert to original scaffold names
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_594.vcf

## interactive session
salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l

module load java

# Annotate
cd /home/uqsalle3/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/uqsalle3/Programs/snpEff/data/vcf/min_594.vcf > ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l



# Get VCF GATK 628

## rsync from Dell 2
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/
rsync -ahPv ./GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf uqsalle3@bunya.rcc.uq.edu.au:/home/uqsalle3/Programs/snpEff/data/vcf/

## back to Bunya
mkdir -p /home/uqsalle3/Programs/snpEff/data/vcf
cd /home/uqsalle3/Programs/snpEff/data/vcf
## convert to original scaffold names
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_628.vcf

# ## interactive session
# salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l
# 
# module load java

# Annotate
cd /home/uqsalle3/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/uqsalle3/Programs/snpEff/data/vcf/min_628.vcf > ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l



# Get VCF GATK 718

## rsync from Dell 2
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/
rsync -ahPv ./GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf uqsalle3@bunya.rcc.uq.edu.au:/home/uqsalle3/Programs/snpEff/data/vcf/

## back to Bunya
mkdir -p /home/uqsalle3/Programs/snpEff/data/vcf
cd /home/uqsalle3/Programs/snpEff/data/vcf
## convert to original scaffold names
# ScA8VGg_1;HRSCAF=2
# ScA8VGg_1_HRSCAF_2
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_718.vcf

## interactive session
salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l

module load java

# Annotate
cd /home/uqsalle3/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/uqsalle3/Programs/snpEff/data/vcf/min_718.vcf > ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l



# Get VCF GATK 76

## rsync from Dell 2
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/
rsync -ahPv ./GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf uqsalle3@bunya.rcc.uq.edu.au:/home/uqsalle3/Programs/snpEff/data/vcf/

## back to Bunya
mkdir -p /home/uqsalle3/Programs/snpEff/data/vcf
cd /home/uqsalle3/Programs/snpEff/data/vcf
## convert to original scaffold names
# ScA8VGg_1;HRSCAF=2
# ScA8VGg_1_HRSCAF_2
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_76.vcf

# ## interactive session
# salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l
# 
# module load java

# Annotate
cd /home/uqsalle3/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/uqsalle3/Programs/snpEff/data/vcf/min_76.vcf > ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l



# Get VCF GATK 785

## rsync from Dell 2
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/
rsync -ahPv ./GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf uqsalle3@bunya.rcc.uq.edu.au:/home/uqsalle3/Programs/snpEff/data/vcf/

## back to Bunya
mkdir -p /home/uqsalle3/Programs/snpEff/data/vcf
cd /home/uqsalle3/Programs/snpEff/data/vcf
## convert to original scaffold names
# ScA8VGg_1;HRSCAF=2
# ScA8VGg_1_HRSCAF_2
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_785.vcf

# ## interactive session
# salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l
# 
# module load java

# Annotate
cd /home/uqsalle3/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/uqsalle3/Programs/snpEff/data/vcf/min_785.vcf > ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l

```

```{shell, annotate vcf Dell 2}

# Get VCF JGIL
mkdir -p /home/shared/Programs/snpEff/data/vcf
cd /home/shared/Programs/snpEff/data/vcf
rsync -ahPv /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_*_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf ./

# Annotate

# 542

## convert to original scaffold names
cd /home/shared/Programs/snpEff/data/vcf
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf > min_542.vcf

# Annotate
cd /home/shared/Programs/snpEff

/home/shared/Programs/jdk-20.0.2/bin/java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/shared/Programs/snpEff/data/vcf/min_542.vcf > ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l



# 594

## convert to original scaffold names
cd /home/shared/Programs/snpEff/data/vcf
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf > min_594.vcf

# Annotate
cd /home/shared/Programs/snpEff

/home/shared/Programs/jdk-20.0.2/bin/java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/shared/Programs/snpEff/data/vcf/min_594.vcf > ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l



# 628

## convert to original scaffold names
cd /home/shared/Programs/snpEff/data/vcf
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf > min_628.vcf

# Annotate

cd /home/shared/Programs/snpEff
/home/shared/Programs/jdk-20.0.2/bin/java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/shared/Programs/snpEff/data/vcf/min_628.vcf > ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l



# 718

## convert to original scaffold names
cd /home/shared/Programs/snpEff/data/vcf
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf > min_718.vcf

# Annotate
cd /home/shared/Programs/snpEff

/home/shared/Programs/jdk-20.0.2/bin/java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/shared/Programs/snpEff/data/vcf/min_718.vcf > ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l



# 76

## convert to original scaffold names
cd /home/shared/Programs/snpEff/data/vcf
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf > min_76.vcf

# Annotate
cd /home/shared/Programs/snpEff
/home/shared/Programs/jdk-20.0.2/bin/java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/shared/Programs/snpEff/data/vcf/min_76.vcf > ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l



# 785

## convert to original scaffold names
cd /home/shared/Programs/snpEff/data/vcf
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf > min_785.vcf

# Annotate
cd /home/shared/Programs/snpEff
/home/shared/Programs/jdk-20.0.2/bin/java -Xmx8g -jar snpEff.jar Dserrata_HiC /home/shared/Programs/snpEff/data/vcf/min_785.vcf > ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf | wc -l

```


#### Step 4 parse

```{shell, parse Bunya}

salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=1 --mem=8G --job-name=SIFT_JGIL_mapQ20 --time=01:00:00 --partition=general --account=a_chenoweth srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l

module load java

cd /home/uqsalle3/Programs/snpEff/results

cat JGIL_mapQ20_liftOver_HiC_RefMajorMinor.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > JGIL_mapQ20_liftOver_HiC_RefMajorMinor.ann.tsv

# 542 

cat GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

# 594 

cat GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

# 628 

cat GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

# 718

cat GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

# 76

cat GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

# 785

cat GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

```

```{shell, parse Dell 2}

cd /home/shared/Programs/snpEff/results

# 542 

cat GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 /home/shared/Programs/jdk-20.0.2/bin/java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.ann.tsv

# 594 

cat GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 /home/shared/Programs/jdk-20.0.2/bin/java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.ann.tsv

# 628 

cat GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 /home/shared/Programs/jdk-20.0.2/bin/java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.ann.tsv

# 718

cat GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 /home/shared/Programs/jdk-20.0.2/bin/java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.ann.tsv

# 76

cat GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 /home/shared/Programs/jdk-20.0.2/bin/java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.ann.tsv

# 785

cat GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 /home/shared/Programs/jdk-20.0.2/bin/java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.ann.tsv

```

#### Step 5 analyse on Dell 2

module load anaconda3
source ~/conda-init
conda activate r_env

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff

```{shell}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff

rsync -ahPv uqsalle3@bunya.rcc.uq.edu.au:/home/uqsalle3/Programs/snpEff/results/GenotypeGVCFs_*.tsv ./

```


##### EDA

```{r}

library(tidyverse)
library(data.table)

# 100% genotyped (the same SNPs as used for partialS/HIC)
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")

ffs <- list.files(pattern = '*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv', recursive = TRUE)

snpEff100 <- data.table()

for(ff in ffs){
    # tmp <- fread(ff, header = TRUE, sep = '\t')
    tmp <- read_tsv(ff)
    snpEff100 <- rbind(snpEff100, tmp)
}



# good SNP calls and homozygous alternate
setwd("/home/shared/Programs/snpEff/results")

ffs <- list.files(pattern = '*_Default_allSites_hardFilteredBiallelic_goodCalls_recode_biallelicNoRef_MonomorphicAlt.ann.tsv', recursive = TRUE)

snpEffMax <- data.table()

for(ff in ffs){
    # tmp <- fread(ff, header = TRUE, sep = '\t')
    tmp <- read_tsv(ff)
    snpEffMax <- rbind(snpEffMax, tmp)
}

```

##### Keep most severe

```{r, 100% biallelic}

snpEff100 %>% 
  filter(`ANN[*].IMPACT`=="HIGH") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_HIGH = TRUE) %>% 
  distinct() -> 
high

snpEff100 %>% 
  filter(`ANN[*].IMPACT`=="MODERATE") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_MODERATE = TRUE) %>% 
  distinct() -> 
moderate

snpEff100 %>% 
  filter(`ANN[*].IMPACT`=="LOW") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_LOW = TRUE) %>% 
  distinct() -> 
low

snpEff100 %>% 
  filter(`ANN[*].IMPACT`=="MODIFIER") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_MODIFIER = TRUE) %>% 
  distinct() -> 
modifier

moderate %>% 
  anti_join(high) -> 
moderate2

low %>% 
  anti_join(moderate) %>% 
  anti_join(high) -> 
low2

modifier %>% 
  anti_join(high) %>% 
  anti_join(moderate) %>% 
  anti_join(low) -> 
modifier2

high %>% 
  bind_rows(moderate2) %>% 
  bind_rows(low2) %>% 
  bind_rows(modifier2) -> 
snpEff100_severe

snpEff100_severe %>% 
  pivot_longer(IMPACT_HIGH:IMPACT_MODIFIER, names_to="IMPACT", values_to="value") %>% 
  filter(value==TRUE) %>% 
  mutate(IMPACT = str_replace(IMPACT, "IMPACT_", "")) %>% 
  select(-value) %>% 
  arrange(CHROM, POS, IMPACT) -> 
snpEff100_severe

# write_tsv(snpEff100_severe, file="GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")

```

```{r, good calls homozygous alternate}

snpEffMax %>% 
  filter(`ANN[*].IMPACT`=="HIGH") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_HIGH = TRUE) %>% 
  distinct() -> 
high

snpEffMax %>% 
  filter(`ANN[*].IMPACT`=="MODERATE") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_MODERATE = TRUE) %>% 
  distinct() -> 
moderate

snpEffMax %>% 
  filter(`ANN[*].IMPACT`=="LOW") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_LOW = TRUE) %>% 
  distinct() -> 
low

snpEffMax %>% 
  filter(`ANN[*].IMPACT`=="MODIFIER") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_MODIFIER = TRUE) %>% 
  distinct() -> 
modifier

moderate %>% 
  anti_join(high) -> 
moderate2

low %>% 
  anti_join(moderate) %>% 
  anti_join(high) -> 
low2

modifier %>% 
  anti_join(high) %>% 
  anti_join(moderate) %>% 
  anti_join(low) -> 
modifier2

high %>% 
  bind_rows(moderate2) %>% 
  bind_rows(low2) %>% 
  bind_rows(modifier2) -> 
snpEffMax_severe

snpEffMax_severe %>% 
  pivot_longer(IMPACT_HIGH:IMPACT_MODIFIER, names_to="IMPACT", values_to="value") %>% 
  filter(value==TRUE) %>% 
  mutate(IMPACT = str_replace(IMPACT, "IMPACT_", "")) %>% 
  select(-value) %>% 
  arrange(CHROM, POS, IMPACT) -> 
snpEffMax_severe

# write_tsv(snpEff100_severe, file="GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")

```

```{r}

snpEff100_severe
snpEffMax_severe
rm(snpEff100, snpEffMax, high, moderate, moderate2, low, low2, modifier, modifier2)

```

##### Convert windows to peaks

* written by Yiguan, rewritten by me.
* I added ensemble stuff

```{r}

library(data.table)
library(dplyr)
library(stringr)
library(parallel)

# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical")

beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})



contigNames <- c("chrScA8VGg_542", "chrScA8VGg_594", "chrScA8VGg_628", "chrScA8VGg_718", "chrScA8VGg_76", "chrScA8VGg_785")

for (contigName in contigNames) {
  cat("Merging windows for", contigName, "\n")
  tempData <- ensemble %>% filter(contig==contigName)
  tempData$merged <- 1
  m <- 1
  for (i in 2:nrow(tempData)) {
    if (tempData$start[i]==tempData$end[i-1] & tempData$pred_ensemble[i]==tempData$pred_ensemble[i-1]) {
      tempData$merged[i] <- m
    } else {
      m <- m+1
      tempData$merged[i] <- m
    }
  }
  assign(contigName, tempData)
}

ensemble <- bind_rows(chrScA8VGg_542, chrScA8VGg_594, chrScA8VGg_628, chrScA8VGg_718, chrScA8VGg_76, chrScA8VGg_785)

readr::write_tsv(ensemble, "ensemble_predictions_merged_windows.tsv")

# preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")
# 
# class2peaks <- function(dat){
#   r <- rle(dat$State)
#   rr <- data.frame("len" = r$lengths, "val" = r$values)
#   rr$endWin <- cumsum(rr$len)
#   rr$startWin <- lag(rr$endWin,1,0) + 1
#   rr$start_pos <- dat[rr$startWin, V2]
#   rr$end_pos <- dat[rr$endWin, V3]
#   return(rr)
# }
# 
# c542 <- ensemble %>% filter(contig=="chrScA8VGg_542")
# c542$lag <- lag(c542$end, default=0)
# 
# ensemble %>%
#   # Arrange the data by contig and start
#   arrange(contig, start) %>%
#   # Create a new column grouping based on continuos stretches of a chromosome
#   mutate(contigRun = cumsum(lag(end)-start == 0))
# 
# main <- function(x) {
#     aa <- fread(x, header = FALSE, sep = "\t") %>%
#         tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
#         mutate(V1=gsub('chr','', V1))
#     aa <- aa[,1:4]
#     aa$t1 <- lag(aa$V2,1)
#     aa$t2 <- aa$V2 - aa$t1
#     # count gaps between wins
#     aa$wingap <- aa$t2/5000
#     aa$wingap[1] <- 1
#     aa$win_seq <- 0
#     win_s = 1
#     # gap larger than 2*5000kbp were considered separately
#     for(i in 1:nrow(aa)){
#         if(aa[i,wingap]<=2){
#             aa$win_seq[i] <- win_s
#             }
#         else{
#             win_s <- win_s + 1
#             aa$win_seq[i] <- win_s
#         }
#     }
#     # for each segmentation, run class2peaks
#     merge_return <- data.frame()
#     for(j in unique(aa$win_seq)){
#         sub_aa <- aa[win_seq==j,]
#         tmp <- class2peaks(sub_aa)
#         tmp$scf <- str_split(x,"\\.")[[1]][1]
#         tmp <- tmp[,c("scf","len","val","start_pos","end_pos")]
#         tmp$win_seq <- j
#         merge_return <- rbind(merge_return,tmp)
#     }
#     fwrite(merge_return, paste0(x,".peak"), col.names = T, row.names = F, sep = "\t", quote = F)
# }
# 
# mclapply(preFiles, main, mc.cores = 6)

```

```{r}

# # setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")
# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe_intergenic")
# 
# for (scfid in c(542,594,628,718,76,785)) {
#   peakFile <- paste0("ScA8VGg_",scfid,".train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed.peak")
#   tempData <- fread(peakFile)
#   if (scfid == 542) { predPeaks = tempData }
#   else { predPeaks = rbind(predPeaks, tempData) }
# }
# 
# predPeaks

```

##### Add impact to merged windows

```{r, 100% biallelic}

# setwd("C:\\Users\\scott\\OneDrive - The University of Queensland\\Documents\\Stephen_Chenoweth\\Yiguan\\partialSHIC\\snpEff100")
# 
# snpEff100_severe <- fread("JGIL_mapQ20_liftOver_HiC_RefMajorMinor.ann.tsv")

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")

snpEff100_severe <- readr::read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")

snpEff100_severe <- snpEff100_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))
setDT(snpEff100_severe)
setkey(snpEff100_severe, CHROM)

predPeaks <- ensemble
predPeaks <- predPeaks %>% mutate(contig = str_replace(contig, "chr", "")) %>% rename(CHROM = contig)

predPeaks$HIGH <- NA
predPeaks$MODERATE <- NA
predPeaks$LOW <- NA
predPeaks$MODIFIER <- NA

for (i in 1:nrow(predPeaks)) {
  cat(i, "\n")
  snpEff100_severe[predPeaks$CHROM[i], 
              .(CHROM, POS, REF, ALT, IMPACT), 
              nomatch = 0,
              on = "CHROM"][POS >= predPeaks$start[i] & POS <= predPeaks$end[i]] %>% 
    group_by(IMPACT) %>% 
    summarise(n=n()) %>% 
    tidyr::pivot_wider(names_from=IMPACT, values_from=n) -> 
  tempData
  if ("HIGH" %in% colnames(tempData)) { predPeaks$HIGH[i] <- tempData$HIGH }
  if ("MODERATE" %in% colnames(tempData)) { predPeaks$MODERATE[i] <- tempData$MODERATE }
  if ("LOW" %in% colnames(tempData)) { predPeaks$LOW[i] <- tempData$LOW }
  if ("MODIFIER" %in% colnames(tempData)) { predPeaks$MODIFIER[i] <- tempData$MODIFIER }
}

predPeaks100 <- predPeaks

predPeaks100 %>% 
  mutate(length=end-start) -> 
predPeaks100

tot <- sum(predPeaks100$length)

predPeaks100
tot

# write_tsv(predPeaks, file="predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")
write_tsv(predPeaks100, file="predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

```

```{r, good calls homozygous alternate}

snpEff100_severe <- snpEff100_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))
setDT(snpEff100_severe)
setkey(snpEff100_severe, CHROM)

predPeaks$HIGH <- NA
predPeaks$MODERATE <- NA
predPeaks$LOW <- NA
predPeaks$MODIFIER <- NA

for (i in 1:nrow(predPeaks)) {
  cat(i, "\n")
  snpEff100_severe[predPeaks$scf[i], 
              .(CHROM, POS, REF, ALT, IMPACT), 
              nomatch = 0,
              on = "CHROM"][POS >= predPeaks$start_pos[i] & POS <= predPeaks$end_pos[i]] %>% 
    group_by(IMPACT) %>% 
    summarise(n=n()) %>% 
    pivot_wider(names_from=IMPACT, values_from=n) -> 
  tempData
  if ("HIGH" %in% colnames(tempData)) { predPeaks$HIGH[i] <- tempData$HIGH }
  if ("MODERATE" %in% colnames(tempData)) { predPeaks$MODERATE[i] <- tempData$MODERATE }
  if ("LOW" %in% colnames(tempData)) { predPeaks$LOW[i] <- tempData$LOW }
  if ("MODIFIER" %in% colnames(tempData)) { predPeaks$MODIFIER[i] <- tempData$MODIFIER }
}

predPeaks100 <- predPeaks

predPeaks100 %>% 
  mutate(length=end_pos-start_pos) -> 
predPeaks100

tot <- sum(predPeaks100$length)

predPeaks100
tot

# setwd("C:\\Users\\scott\\OneDrive - The University of Queensland\\Documents\\Stephen_Chenoweth\\Yiguan\\partialSHIC\\snpEffMax")
# 
# snpEffMax_severe <- fread("JGIL_mapQ20_liftOver_HiC_RefMajorMinor.ann.tsv")

snpEffMax_severe <- snpEffMax_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))
setDT(snpEffMax_severe)
setkey(snpEffMax_severe, CHROM)

predPeaks$HIGH <- NA
predPeaks$MODERATE <- NA
predPeaks$LOW <- NA
predPeaks$MODIFIER <- NA

for (i in 1:nrow(predPeaks)) {
  cat(i, "\n")
  snpEffMax_severe[predPeaks$scf[i], 
              .(CHROM, POS, REF, ALT, IMPACT), 
              nomatch = 0,
              on = "CHROM"][POS >= predPeaks$start_pos[i] & POS <= predPeaks$end_pos[i]] %>% 
    group_by(IMPACT) %>% 
    summarise(n=n()) %>% 
    pivot_wider(names_from=IMPACT, values_from=n) -> 
  tempData
  if ("HIGH" %in% colnames(tempData)) { predPeaks$HIGH[i] <- tempData$HIGH }
  if ("MODERATE" %in% colnames(tempData)) { predPeaks$MODERATE[i] <- tempData$MODERATE }
  if ("LOW" %in% colnames(tempData)) { predPeaks$LOW[i] <- tempData$LOW }
  if ("MODIFIER" %in% colnames(tempData)) { predPeaks$MODIFIER[i] <- tempData$MODIFIER }
}

predPeaksMax <- predPeaks

predPeaksMax %>% 
  mutate(length=end_pos-start_pos) -> 
predPeaksMax

tot <- sum(predPeaksMax$length)

predPeaksMax
tot

# write_tsv(predPeaks, file="predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_MaxprecentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

```

##### Bar plot

```{r}

# predPeaks100 %>%
#   group_by(pred_ensemble) %>%
#   summarise(basePairs=sum(length, na.rm=TRUE),
#             high=sum(HIGH, na.rm=TRUE),
#             moderate=sum(MODERATE, na.rm=TRUE),
#             low=sum(LOW, na.rm=TRUE),
#             modifier=sum(MODIFIER, na.rm=TRUE),
#             highCPM=sum(HIGH, na.rm=TRUE)/(basePairs/1000000),
#             moderateCPM=sum(MODERATE, na.rm=TRUE)/(basePairs/1000000),
#             lowCPM=sum(LOW, na.rm=TRUE)/(basePairs/1000000),
#             modifierCPM=sum(MODIFIER, na.rm=TRUE)/(basePairs/1000000)) ->
# tempData
# tempData
# 
# tempData %>%
#     select(-basePairs, -high, -moderate, -low, -modifier) %>%
#     pivot_longer(
#       highCPM:modifierCPM,
#       names_to="IMPACT",
#       values_to="CPM"
#     ) %>%
#     rename(Class=val) %>%
#     mutate(IMPACT = factor(IMPACT, levels=c("highCPM", "moderateCPM", "lowCPM", "modifierCPM"))) %>%
#     ggplot(aes(x=Class, y=CPM, fill=Class)) +
#     geom_bar(stat="identity", position=position_dodge()) +
#     facet_wrap(~IMPACT, scale="free_y") +
#     theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

##### Permutation

```{r, test on Dell 2 OLD}

library(tidyverse)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe_intergenic")

# arrange predPeaks by peak length
predPeaks <- predPeaks %>% mutate(length=end_pos-start_pos) %>% arrange(desc(length))

## Expand predPeaks to 5kb resolution

predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length, win_seq)
write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
write.table(predictions, "predictions_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

cmd <- paste0("bedtools intersect -a predictions_temp.tsv -b predPeaks_temp.tsv -wo > predPeaks_5kb.tsv")
system(cmd)

# Number of permutations
# n_permutations <- 1000
n_permutations <- 10

# get min start_pos
min_start <- predPeaks %>% 
  group_by(scf) %>% 
  summarise(min = min(start_pos))

# get max end_pos
max_end <- predPeaks %>% 
  group_by(scf) %>% 
  summarise(max = max(end_pos))

# get potential sub-windows (i.e. ignoring failures due to masking and/or 0 SNPs)
subwins <- min_start %>% 
  left_join(max_end) %>% 
  mutate(span = max - min) %>% 
  mutate(subwins = 1+(span/5000))

# create scf and start_pos to sample from

scf_start <- data.frame(
  scf = c(rep("ScA8VGg_542", 4183), 
          rep("ScA8VGg_594", 3927), 
          rep("ScA8VGg_628", 6204), 
          rep("ScA8VGg_718", 1654), 
          rep("ScA8VGg_76", 7412), 
          rep("ScA8VGg_785", 4908)), 
  start_pos = c(seq(from=82500, to=20992500, by=5000), 
                seq(from=42500, to=19672500, by=5000), 
                seq(from=207500, to=31222500, by=5000), 
                seq(from=222500, to=8487500, by=5000), 
                seq(from=22500, to=37077500, by=5000), 
                seq(from=22500, to=24557500, by=5000))
)

# create list for permutations

library(foreach)
library(doParallel)

# Number of cores to use for parallel processing
num_cores <- 38  # You can change this to the number of cores you want to use

# Register parallel backend
registerDoParallel(cores = num_cores)
getDoParWorkers()

# Create a list to store the results
permutations <- foreach(i = 1:n_permutations, .combine = "list") %dopar% {
  # cat("Permutation ", i, " running...\n")
  start_time <- Sys.time()
  shuffled_data <- predPeaks
  scf_start_temp <- scf_start
  for (j in 1:nrow(shuffled_data)) {
    cat("Permutation ", i, ", Iteration ", j, "\n")
    peak <- shuffled_data[j,]
    selected_scf_start <- scf_start_temp[sample(nrow(scf_start_temp), 1), ]
    max_attempts <- 28288
    while(selected_scf_start$start_pos + peak$length > max_end$max[max_end$scf == selected_scf_start$scf] && max_attempts > 0) {
    # while(selected_scf_start$start_pos + peak$length > max_end$max[max_end$scf == selected_scf_start$scf]) {
      # Randomly select a row index from the data frame
      selected_scf_start_index <- sample(nrow(scf_start_temp), 1)
      # Get the selected row
      selected_scf_start <- scf_start_temp[selected_scf_start_index, ]
      # Decrement attempts counter
      max_attempts <- max_attempts - 1  
    }
    # Break if max_attempts
    if (max_attempts == 0) {
      shuffled_data <- NULL
      break
    }
    # Remove the selected region from the pool
    region <- scf_start_temp %>% 
      filter(scf==selected_scf_start$scf) %>% 
      filter(start_pos >= selected_scf_start$start_pos & start_pos <= selected_scf_start$start_pos+peak$length)
    scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
    # add region to permutation
    shuffled_data$perm[j] <- i
    shuffled_data$scf_perm[j] <- selected_scf_start$scf
    shuffled_data$start_pos_perm[j] <- selected_scf_start$start_pos
    shuffled_data$end_pos_perm[j] <- selected_scf_start$start_pos+peak$length
    # get impact of region
    tempData <- snpEff_severe %>%
      filter(CHROM == selected_scf_start$scf,
             POS >= selected_scf_start$start_pos & POS <= selected_scf_start$start_pos + peak$length) %>%
      group_by(IMPACT) %>%
      summarise(n = n()) %>%
      pivot_wider(names_from = IMPACT, values_from = n, values_fill = 0)
    if ("HIGH" %in% colnames(tempData)) { shuffled_data$HIGH_perm[j] <- tempData$HIGH } else { shuffled_data$HIGH_perm[j] <- NA }
    if ("MODERATE" %in% colnames(tempData)) { shuffled_data$MODERATE_perm[j] <- tempData$MODERATE } else { shuffled_data$MODERATE_perm[j] <- NA }
    if ("LOW" %in% colnames(tempData)) { shuffled_data$LOW_perm[j] <- tempData$LOW } else { shuffled_data$LOW_perm[j] <- NA }
    if ("MODIFIER" %in% colnames(tempData)) { shuffled_data$MODIFIER_perm[j] <- tempData$MODIFIER } else { shuffled_data$MODIFIER_perm[j] <- NA }
  }
  if (!is.null(shuffled_data)) {
    write_tsv(shuffled_data, file="predPeaks_permutation_1_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")
  }
  end_time <- Sys.time()
  cat(start_time, "\n")
  cat(end_time, "\n")
}

# Stop the parallel backend
stopImplicitCluster()

```

```{r, test on Dell 2 2nd attempt}

library(tidyverse)
# library(dplyr)
# library(tidyr)
# library(readr)
# library(rlang)
# library(magrittr)
# library(ggplot2)
library(qvalue)
library(data.table)
library(foreach)
library(doSNOW)

## Import data

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")

snpEff_severe <- read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")
snpEff_severe <- snpEff_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))

# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")
# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe_intergenic")

# predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")
# predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")
predPeaks_5kb <- read_tsv(
  "predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv"
  )

# check run lengths

predPeaks_5kb$run <- 1
g <- 1

for (i in 2:nrow(predPeaks_5kb)) {
  if (predPeaks_5kb$start[i] == predPeaks_5kb$end[i-1]) {
    predPeaks_5kb$run[i] <- g
  } else {
    g <- g+1
    predPeaks_5kb$run[i] <-g
  }
}

runLength <- as.data.frame(rle(predPeaks_5kb$run)$lengths) %>%
  rename(rle = `rle(predPeaks_5kb$run)$lengths`) %>%
  mutate(runLength = rle*5000)

runLength %>%
  group_by(runLength) %>%
  tally() %>%
  arrange(desc(runLength)) %>%
  print(n=136)



predPeaks <- predPeaks_5kb %>% 
  group_by(CHROM, merged, pred_ensemble) %>% 
  summarise(start = min(start), 
            end = max(end), 
            HIGH = sum(HIGH, na.rm=TRUE), 
            MODERATE = sum(MODERATE, na.rm=TRUE), 
            LOW = sum(LOW, na.rm=TRUE), 
            MODIFIER = sum(MODIFIER, na.rm=TRUE)) %>% 
  ungroup() %>% 
  mutate(length = end - start)

# GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
#   set_names(c("fbgn", "geneName", "go", "process"))
# 
# GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>% 
#   set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))
# 
# go_info <- read_tsv("go_info.tsv") %>% rename(go = go_id)

# preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")
# 
# predictions <- data.table()
# 
# for(ff in preFiles){
#     tmp <- fread(ff, header = FALSE, sep = "\t") %>%
#         tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
#         mutate(V1=gsub('chr','', V1))
#     predictions <- rbind(predictions, tmp)
# }
# 
# predictions <- predictions %>%
#   set_names(c("scf", "start_pos", "end_pos", "val", "V5", "V6", "V7", "V8", "V9"))
# 
# ## Expand predPeaks to 5kb resolution
# 
# predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length, win_seq)
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(predictions, "predictions_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# cmd <- paste0("bedtools intersect -a predictions_temp.tsv -b predPeaks_temp.tsv -wo > predPeaks_5kb.tsv")
# system(cmd)
# 
# predPeaks_5kb <- read_tsv("predPeaks_5kb.tsv", col_names=FALSE)
# 
# colnames(predPeaks_5kb) <- c("scf", "start_pos", "end_pos", "val", "X5", "X6", 
#                             "X7", "X8", "X9", "scf_peak", "start_pos_peak", 
#                             "end_pos_peak", "X13", "len", "length", "win_seq", 
#                             "intersect")
# 
# predPeaks_5kb <- predPeaks_5kb %>% select(-X5:-X9, -X13)

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))
# predPeaks_Hard <- predPeaks %>% filter(val=="Hard") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER) %>% arrange(desc(length))

# scf_start <- predPeaks_5kb %>% select(scf, start_pos, end_pos, win_seq)
scf_start <- predPeaks_5kb %>% select(CHROM, start, end, merged, run)

# peak_start_end <- predPeaks_5kb %>% group_by(scf, win_seq) %>% summarise(start_pos_peak = min(start_pos_peak), end_pos_peak = max(end_pos_peak))
peak_start_end <- scf_start %>% group_by(CHROM, run) %>% summarise(start = min(start), end = max(end))



num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

# permutations <- foreach(j = 1:10, .combine = "rbind", .packages = c("dplyr", "tidyr", "readr")) %dopar% {
#   start_time <- Sys.time()
#   shuffled_data <- predPeaks
#   scf_start_temp <- scf_start
#   for (i in 1:nrow(shuffled_data)) {
#     # Get peak
#     peak <- shuffled_data[i,]
#     # Move peak to a random genomic location
#     selected_scf_start <- sample_n(scf_start_temp, 1)
#     peak_end <- selected_scf_start %>% left_join(peak_start_end, by = c("scf", "win_seq"))
#     end_not_available <- ((selected_scf_start$start_pos + peak$length) %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#     # Make sure the new genomic location is valid
#     # Check if the new genomic location is within a prediction peak or the new end_pos is still available
#     max_attempts <- 10000
#     while(((selected_scf_start$start_pos + peak$length) > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
#       # Move peak to a random genomic location
#       selected_scf_start <- sample_n(scf_start_temp, 1)
#       peak_end <- selected_scf_start %>% left_join(peak_start_end, by = c("scf", "win_seq"))
#       end_not_available <- ((selected_scf_start$start_pos + peak$length) %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#       # Decrement attempts counter
#       max_attempts <- max_attempts - 1
#     }
#     # Break if max_attempts
#     if (max_attempts == 0) {
#       cat("Failed to find valid random location.\n")
#       shuffled_data <- NULL
#       break
#     }
#     # Remove the randomly selected location from the pool
#     region <- scf_start_temp %>%
#       filter(scf==selected_scf_start$scf) %>%
#       filter(start_pos >= selected_scf_start$start_pos & end_pos <= selected_scf_start$start_pos + peak$length)
#     scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
#     # Add random location to predPeaks
#     shuffled_data$perm[i] <- j
#     shuffled_data$scf_perm[i] <- selected_scf_start$scf
#     shuffled_data$start_pos_perm[i] <- selected_scf_start$start_pos
#     shuffled_data$end_pos_perm[i] <- selected_scf_start$start_pos+peak$length
#     # get impact of region
#     tempData <- snpEff_severe %>%
#       filter(CHROM == selected_scf_start$scf,
#              POS >= selected_scf_start$start_pos & POS <= (selected_scf_start$start_pos + peak$length)) %>%
#       group_by(IMPACT) %>%
#       summarise(n = n()) %>%
#       pivot_wider(names_from = IMPACT, values_from = n, values_fill = 0)
#     if ("HIGH" %in% colnames(tempData)) { shuffled_data$HIGH_perm[i] <- tempData$HIGH } else { shuffled_data$HIGH_perm[i] <- NA }
#     if ("MODERATE" %in% colnames(tempData)) { shuffled_data$MODERATE_perm[i] <- tempData$MODERATE } else { shuffled_data$MODERATE_perm[i] <- NA }
#     if ("LOW" %in% colnames(tempData)) { shuffled_data$LOW_perm[i] <- tempData$LOW } else { shuffled_data$LOW_perm[i] <- NA }
#     if ("MODIFIER" %in% colnames(tempData)) { shuffled_data$MODIFIER_perm[i] <- tempData$MODIFIER } else { shuffled_data$MODIFIER_perm[i] <- NA }
#   }
#   if (!is.null(shuffled_data)) {
#     write_tsv(shuffled_data, file="predPeaks_permutation_1_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")
#   }
#   end_time <- Sys.time()
#   cat(start_time, "\n")
#   cat(end_time, "\n")
# }

permutations <- foreach(j = 1:10, .combine = "rbind", .packages = c("dplyr", "tidyr", "readr")) %dopar% {
  start_time <- Sys.time()
  shuffled_data <- predPeaks
  scf_start_temp <- scf_start
  peak_start_end_temp <- peak_start_end
  for (i in 1:nrow(shuffled_data)) {
    # Get peak
    peak <- shuffled_data[i,]
    # Move peak to a random genomic location
    selected_scf_start <- sample_n(scf_start_temp, 1)
    # peak_end <- selected_scf_start %>% left_join(peak_start_end, by = c("CHROM", "start"))
    peak_end <- peak_start_end_temp %>% filter(CHROM==selected_scf_start$CHROM & start <= selected_scf_start$start & end >= selected_scf_start$start + peak$length)
    # end_not_available <- ((selected_scf_start$start + peak$length) %in% scf_start_temp$end[scf_start_temp$CHROM==selected_scf_start$CHROM])==FALSE
    end_not_available <- nrow(peak_end) < 1
    # Make sure the new genomic location is valid
    # Check if the new genomic location is within a prediction peak or the new end_pos is still available
    scf_start_temp2 <- scf_start_temp
    int_max_attempts <- nrow(scf_start_temp2)
    max_attempts <- nrow(scf_start_temp2)
    # while(((selected_scf_start$start + peak$length) > peak_end$end || end_not_available) && max_attempts > 0) {
    while(end_not_available && max_attempts > 0) {
      # Move peak to a random genomic location
      selected_scf_start <- sample_n(scf_start_temp2, 1)
      # peak_end <- selected_scf_start %>% left_join(peak_start_end, by = c("CHROM", "merged"))
      peak_end <- peak_start_end_temp %>% filter(CHROM==selected_scf_start$CHROM & start <= selected_scf_start$start & end >= selected_scf_start$start + peak$length)
      # end_not_available <- ((selected_scf_start$start + peak$length) %in% scf_start_temp2$end[scf_start_temp2$CHROM==selected_scf_start$CHROM])==FALSE
      end_not_available <- nrow(peak_end) < 1
      # Decrement attempts counter
      max_attempts <- max_attempts - 1
      scf_start_temp2 <- scf_start_temp2 %>% anti_join(selected_scf_start, by = c("CHROM", "start"))
    }
    # Break if max_attempts
    if (max_attempts == 0) {
      cat("Failed to find valid random location.\n")
      shuffled_data <- NULL
      break
    }
    cat("perm", j, "row", i, "location found after", int_max_attempts-max_attempts, "attempts\n")
    # Remove the randomly selected location from the pool
    region <- scf_start_temp %>%
      filter(CHROM==selected_scf_start$CHROM) %>%
      filter(start >= selected_scf_start$start & end <= selected_scf_start$start + peak$length)
    scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("CHROM", "start"))
    
    peak_start_end_temp <- scf_start_temp
    peak_start_end_temp$run <- 1
    g <- 1
    
    for (k in 2:nrow(peak_start_end_temp)) {
      if (peak_start_end_temp$start[k] == peak_start_end_temp$end[k-1]) {
        peak_start_end_temp$run[k] <- g
      } else {
      g <- g+1
      peak_start_end_temp$run[k] <-g
      }
    }
    
    peak_start_end_temp <- peak_start_end_temp %>% group_by(CHROM, run) %>% summarise(start = min(start), end = max(end)) %>% ungroup()
    
    # Add random location to shuffled_data
    shuffled_data$perm[i] <- j
    shuffled_data$scf_perm[i] <- selected_scf_start$CHROM
    shuffled_data$start_pos_perm[i] <- selected_scf_start$start
    shuffled_data$end_pos_perm[i] <- selected_scf_start$start+peak$length
    # get impact of region
    tempData <- snpEff_severe %>%
      filter(CHROM == selected_scf_start$CHROM,
             POS >= selected_scf_start$start & POS <= (selected_scf_start$start + peak$length)) %>%
      group_by(IMPACT) %>%
      summarise(n = n()) %>%
      pivot_wider(names_from = IMPACT, values_from = n, values_fill = 0)
    if ("HIGH" %in% colnames(tempData)) { shuffled_data$HIGH_perm[i] <- tempData$HIGH } else { shuffled_data$HIGH_perm[i] <- NA }
    if ("MODERATE" %in% colnames(tempData)) { shuffled_data$MODERATE_perm[i] <- tempData$MODERATE } else { shuffled_data$MODERATE_perm[i] <- NA }
    if ("LOW" %in% colnames(tempData)) { shuffled_data$LOW_perm[i] <- tempData$LOW } else { shuffled_data$LOW_perm[i] <- NA }
    if ("MODIFIER" %in% colnames(tempData)) { shuffled_data$MODIFIER_perm[i] <- tempData$MODIFIER } else { shuffled_data$MODIFIER_perm[i] <- NA }
  }
  if (!is.null(shuffled_data)) {
    write_tsv(shuffled_data, file="predPeaks_permutation_1_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")
  }
  end_time <- Sys.time()
  cat(start_time, "\n")
  cat(end_time, "\n")
}

stopCluster(cl)



# Save file
fileName <- "predPeaks_permutation_Hard_100000_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv"
write_tsv(permutations, file=fileName)

permutations <- fread("predPeaks_permutation_Hard_100000_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

```

```{shell, transfer needed files to Bunya}

# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff
# 
# rsync\
#  -ahPv\
#  GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv\
#  uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations/
# 
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe
# 
# rsync\
#  -ahPv\
#  predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv\
#  uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations/
# 
# rsync\
#  -ahPv\
#  predPeaks_5kb.tsv\
#  uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations/



# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff
# 
# rsync\
#  -ahPv\
#  GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv\
#  uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations_intergenic/
# 
# cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe_intergenic
# 
# rsync\
#  -ahPv\
#  predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv\
#  uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations_intergenic/
# 
# rsync\
#  -ahPv\
#  predPeaks_5kb.tsv\
#  uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations_intergenic/



cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff

rsync\
 -ahPv\
 GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations_intergenic_2/

rsync\
 -ahPv\
 predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations_intergenic_2/

```

```{r, R template for Bunya}

library(tidyverse)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

# Import data
snpEff_severe <- read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")
snpEff_severe <- snpEff_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

# Arrange predPeaks by peak length
predPeaks <- predPeaks %>% arrange(desc(length))

# Permutation number
permutation <- [PERM]

# Get min start_pos
min_start <- predPeaks %>% 
  group_by(scf) %>% 
  summarise(min = min(start_pos))

# Get max end_pos
max_end <- predPeaks %>% 
  group_by(scf) %>% 
  summarise(max = max(end_pos))

# Get potential sub-windows (i.e. ignoring failures due to masking and/or 0 SNPs)
subwins <- min_start %>% 
  left_join(max_end) %>% 
  mutate(span = max - min) %>% 
  mutate(subwins = 1+(span/5000))

# Create scf and start_pos to sample from

scf_start <- data.frame(
  scf = c(rep("ScA8VGg_542", 4183), 
          rep("ScA8VGg_594", 3927), 
          rep("ScA8VGg_628", 6204), 
          rep("ScA8VGg_718", 1654), 
          rep("ScA8VGg_76", 7412), 
          rep("ScA8VGg_785", 4908)), 
  start_pos = c(seq(from=82500, to=20992500, by=5000), 
                seq(from=42500, to=19672500, by=5000), 
                seq(from=207500, to=31222500, by=5000), 
                seq(from=222500, to=8487500, by=5000), 
                seq(from=22500, to=37077500, by=5000), 
                seq(from=22500, to=24557500, by=5000))
)

shuffled_data <- predPeaks
scf_start_temp <- scf_start

for (i in 1:nrow(shuffled_data)) {
  start_time <- Sys.time()
  cat("Permutation [PERM], Iteration ", i, "\n")
  # Get peak
  peak <- shuffled_data[i,]
  # Move peak to a random genomic location
  selected_scf_start <- scf_start_temp[sample(nrow(scf_start_temp), 1), ]
  # Make sure the new genomic location is valid
  max_attempts <- 28288
  while(selected_scf_start$start_pos + peak$length > max_end$max[max_end$scf == selected_scf_start$scf] && max_attempts > 0) {
    # Randomly select a row index from the data frame
    selected_scf_start_index <- sample(nrow(scf_start_temp), 1)
    # Get the selected row
    selected_scf_start <- scf_start_temp[selected_scf_start_index, ]
    # Decrement attempts counter
    max_attempts <- max_attempts - 1  
  }
  # Break if max_attempts
  if (max_attempts == 0) {
    cat("Failed to find valid random location.\n")
    shuffled_data <- NULL
    break
  }
  # Remove the randomly selected location from the pool
  region <- scf_start_temp %>% 
    filter(scf==selected_scf_start$scf) %>% 
    filter(start_pos >= selected_scf_start$start_pos & start_pos <= selected_scf_start$start_pos+peak$length)
  scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
  # Add random location to predPeaks
  shuffled_data$perm[i] <- [PERM]
  shuffled_data$scf_perm[i] <- selected_scf_start$scf
  shuffled_data$start_pos_perm[i] <- selected_scf_start$start_pos
  shuffled_data$end_pos_perm[i] <- selected_scf_start$start_pos+peak$length
  # Get impact of the random location
  tempData <- snpEff_severe %>%
    filter(CHROM == selected_scf_start$scf,
           POS >= selected_scf_start$start_pos & POS <= selected_scf_start$start_pos + peak$length) %>%
    group_by(IMPACT) %>%
    summarise(n = n()) %>%
    pivot_wider(names_from = IMPACT, values_from = n, values_fill = 0)
  if ("HIGH" %in% colnames(tempData)) { shuffled_data$HIGH_perm[i] <- tempData$HIGH } else { shuffled_data$HIGH_perm[i] <- NA }
  if ("MODERATE" %in% colnames(tempData)) { shuffled_data$MODERATE_perm[i] <- tempData$MODERATE } else { shuffled_data$MODERATE_perm[i] <- NA }
  if ("LOW" %in% colnames(tempData)) { shuffled_data$LOW_perm[i] <- tempData$LOW } else { shuffled_data$LOW_perm[i] <- NA }
  if ("MODIFIER" %in% colnames(tempData)) { shuffled_data$MODIFIER_perm[i] <- tempData$MODIFIER } else { shuffled_data$MODIFIER_perm[i] <- NA }
}

# Save file
if (!is.null(shuffled_data)) {
  fileName <- "predPeaks_permutation_[PERM]_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv"
  write_tsv(shuffled_data, file=fileName)
}

# Run time
end_time <- Sys.time()
print(start_time, "\n")
print(end_time, "\n")

# Quit
quit(save="no")

```

```{r, R template for Bunya 2nd attempt}

library(tidyverse)

setwd("/scratch/project/chenoase/partialSHIC_DsGRP/permutations/")

# Import data
snpEff_severe <- read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")
snpEff_severe <- snpEff_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")
predPeaks_5kb <- read_tsv("predPeaks_5kb.tsv", col_names=FALSE)

colnames(predPeaks_5kb) <- c("scf", "start_pos", "end_pos", "val", "X5", "X6", 
                            "X7", "X8", "X9", "scf_peak", "start_pos_peak", 
                            "end_pos_peak", "X13", "len", "length", "win_seq", 
                            "intersect")

predPeaks_5kb <- predPeaks_5kb %>% select(-X5:-X9, -X13)

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))
# predPeaks_Hard <- predPeaks %>% filter(val=="Hard") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER) %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(scf, start_pos, end_pos, win_seq)

peak_start_end <- predPeaks_5kb %>% group_by(scf, win_seq) %>% summarise(start_pos_peak = min(start_pos_peak), end_pos_peak = max(end_pos_peak))

# Permutation number
permutation <- [PERM]

shuffled_data <- predPeaks
scf_start_temp <- scf_start

for (i in 1:nrow(shuffled_data)) {
  start_time <- Sys.time()
  cat("Permutation [PERM], Iteration ", i, "\n")
  # Get peak
  peak <- shuffled_data[i,]
  # Move peak to a random genomic location
  selected_scf_start <- sample_n(scf_start_temp, 1)
  peak_end <- selected_scf_start %>% left_join(peak_start_end, by = c("scf", "win_seq"))
  end_not_available <- ((selected_scf_start$start_pos + peak$length) %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
  # Make sure the new genomic location is valid
  # Check if the new genomic location is within a prediction peak or the new end_pos is still available
  max_attempts <- 8709
  while(((selected_scf_start$start_pos + peak$length) > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
    # Move peak to a random genomic location
    selected_scf_start <- sample_n(scf_start_temp, 1)
    peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
    end_not_available <- ((selected_scf_start$start_pos + peak$length) %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
    # Decrement attempts counter
    max_attempts <- max_attempts - 1
  }
  # Break if max_attempts
  if (max_attempts == 0) {
    cat("Failed to find valid random location.\n")
    shuffled_data <- NULL
    break
  }
  # Remove the randomly selected location from the pool
  region <- scf_start_temp %>%
    filter(scf==selected_scf_start$scf) %>%
    filter(start_pos >= selected_scf_start$start_pos & end_pos <= selected_scf_start$start_pos + peak$length)
  scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
  # Add random location to predPeaks
  shuffled_data$perm[i] <- [PERM]
  shuffled_data$scf_perm[i] <- selected_scf_start$scf
  shuffled_data$start_pos_perm[i] <- selected_scf_start$start_pos
  shuffled_data$end_pos_perm[i] <- selected_scf_start$start_pos+peak$length
  # get impact of region
  tempData <- snpEff_severe %>%
    filter(CHROM == selected_scf_start$scf,
           POS >= selected_scf_start$start_pos & POS <= (selected_scf_start$start_pos + peak$length)) %>%
    group_by(IMPACT) %>%
    summarise(n = n()) %>%
    pivot_wider(names_from = IMPACT, values_from = n, values_fill = 0)
  if ("HIGH" %in% colnames(tempData)) { shuffled_data$HIGH_perm[i] <- tempData$HIGH } else { shuffled_data$HIGH_perm[i] <- NA }
  if ("MODERATE" %in% colnames(tempData)) { shuffled_data$MODERATE_perm[i] <- tempData$MODERATE } else { shuffled_data$MODERATE_perm[i] <- NA }
  if ("LOW" %in% colnames(tempData)) { shuffled_data$LOW_perm[i] <- tempData$LOW } else { shuffled_data$LOW_perm[i] <- NA }
  if ("MODIFIER" %in% colnames(tempData)) { shuffled_data$MODIFIER_perm[i] <- tempData$MODIFIER } else { shuffled_data$MODIFIER_perm[i] <- NA }
}

# Save file
if (!is.null(shuffled_data)) {
  fileName <- "predPeaks_permutation_[PERM]_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv"
  write_tsv(shuffled_data, file=fileName)
}

# Run time
end_time <- Sys.time()
print(start_time, "\n")
print(end_time, "\n")

# Quit
quit(save="no")

```

```{shell, create R scripts Bunya}

# for p in {1..1000}; do
#   echo "$p"
#   cp permutation_template.txt permutation_${p}.R
#   replace_CMD="s/\[PERM\]/${p}/g"
#   perl -pi -e "$replace_CMD" permutation_${p}.R
# done
# 
# chmod 755 ./*.R



# for p in {1..1000}; do
# for p in {2001..5000}; do
for p in {25001..30000}; do
  echo "$p"
  cp permutation_template_2ndAttempt.txt permutation_${p}_2ndAttempt.R
  replace_CMD="s/\[PERM\]/${p}/g"
  perl -pi -e "$replace_CMD" permutation_${p}_2ndAttempt.R
done

chmod 755 ./*.R

```

```{shell, slurm test for Bunya}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=16G
#SBATCH --job-name=perm_1
#SBATCH --time=1:00:00
#SBATCH --partition=general_beta
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/permutations/slurm_perm_1.output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/permutations/slurm_perm_1.error

module load r
module load anaconda3
source ~/conda-init

cd /scratch/project/chenoase/partialSHIC_DsGRP/permutations/

srun R CMD BATCH permutation_1.R

```

```{shell, slurm template for Bunya}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=16G
#SBATCH --job-name=perm_[PERM]
#SBATCH --time=1:00:00
#SBATCH --partition=general_beta
#SBATCH --account=a_chenoweth
#SBATCH -o /scratch/project/chenoase/partialSHIC_DsGRP/permutations/slurm_perm_[PERM].output
#SBATCH -e /scratch/project/chenoase/partialSHIC_DsGRP/permutations/slurm_perm_[PERM].error

module load r
module load anaconda3
source ~/conda-init

cd /scratch/project/chenoase/partialSHIC_DsGRP/permutations/

srun R CMD BATCH permutation_[PERM].R

```

```{shell, create slurm scripts Bunya}

# for p in {1..1000}; do
#   echo "$p"
#   cp permutation_template_slurm.txt permutation_${p}.slurm.sh
#   replace_CMD="s/\[PERM\]/${p}/g"
#   perl -pi -e "$replace_CMD" permutation_${p}.slurm.sh
# done
# 
# chmod 755 ./*.slurm.sh
# 
# for p in {1..1000}; do
#   echo "permutation_${p}.slurm.sh submitted"
#   sbatch permutation_${p}.slurm.sh
#   sleep 0.5
# done
# 
# # rerun
# 
# while read p; do
#   echo "$p"
#   cp permutation_template_slurm.txt permutation_${p}.slurm.sh
#   replace_CMD="s/\[PERM\]/${p}/g"
#   perl -pi -e "$replace_CMD" permutation_${p}.slurm.sh
# done < rerun1.txt
# 
# chmod 755 ./*.slurm.sh
# 
# while read p; do
#   echo "permutation_${p}.slurm.sh submitted"
#   sbatch permutation_${p}.slurm.sh
#   sleep 0.5
# done < rerun1.txt



for p in {25001..30000}; do
  echo "$p"
  cp permutation_template_slurm_2ndAttempt.txt permutation_${p}_2ndAttempt.slurm.sh
  replace_CMD="s/\[PERM\]/${p}/g"
  perl -pi -e "$replace_CMD" permutation_${p}_2ndAttempt.slurm.sh
done

chmod 755 ./*_2ndAttempt.slurm.sh

for p in {25001..30000}; do
  echo "permutation_${p}_2ndAttempt.slurm.sh submitted"
  sbatch permutation_${p}_2ndAttempt.slurm.sh
  sleep 0.05
done

# rerun

## look for missing files

echo predPeaks_permutation_{1..30000}_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv | xargs ls | grep 'error'

## look for files with not enough rows

# wc predPeaks_permutation_*_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv | grep -v -P '7624'
find . -name "predPeaks_permutation_*_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv" -print0 | xargs -0 wc | grep -v -P '7624'


find ./ -maxdepth 1 -type f -name '*_2ndAttempt.tsv' -exec wc {} \; | sort -V | grep -v '7751'

while read p; do
  echo "$p"
  cp permutation_template_slurm_2ndAttempt.txt permutation_${p}_2ndAttempt.slurm.sh
  replace_CMD="s/\[PERM\]/${p}/g"
  perl -pi -e "$replace_CMD" permutation_${p}_2ndAttempt.slurm.sh
done < rerun2.txt

chmod 755 ./*.slurm.sh

while read p; do
  echo "permutation_${p}_2ndAttempt.slurm.sh submitted"
  sbatch permutation_${p}_2ndAttempt.slurm.sh
  sleep 0.5
done < rerun2.txt

```

```{shell, copy permutations to Dell 2}

# cd /home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/permutations
# 
# rsync\
#  -ahPv\
#  uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations/predPeaks_permutation_*\
#  ./
# 
# 
# 
# cd /home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe_intergenic/permutations
# 
# rsync\
#  -ahPv\
#  uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations_intergenic/predPeaks_permutation_*\
#  ./



cd /home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_SIFT

rsync\
 -ahPv\
 uqsalle3@bunya.rcc.uq.edu.au:/scratch/project/chenoase/partialSHIC_DsGRP/permutations_intergenic_2/predPeaks_permutation_*\
 ./

```

##### Permutation test

```{bash}

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

cd /home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_SIFT

```


```{r, permutations test}

library(tidyverse)
library(data.table)

# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")
# predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

#setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe_intergenic")
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

# setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe_intergenic/permutations")
setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_SIFT")

ffs <- list.files(pattern = '*_2ndAttempt.tsv', recursive = TRUE)

permutations <- rbindlist(lapply(ffs, fread))

setorder(permutations, perm, CHROM, start)



# Observed

permutations %>% 
  filter(perm==1) %>% 
  group_by(pred_ensemble) %>% 
  summarise(basePairs=sum(length, na.rm=TRUE), 
            high=sum(HIGH, na.rm=TRUE), 
            moderate=sum(MODERATE, na.rm=TRUE), 
            low=sum(LOW, na.rm=TRUE), 
            modifier=sum(MODIFIER, na.rm=TRUE)) %>% 
  ungroup() %>% 
  mutate(high_prop = high/(high+moderate+low+modifier), 
         moderate_prop = moderate/(high+moderate+low+modifier), 
         low_prop = low/(high+moderate+low+modifier), 
         modifier_prop = modifier/(high+moderate+low+modifier))

# predPeaks %>% 
#   group_by(val) %>%
#   summarise(basePairs=sum(length, na.rm=TRUE),
#             high=sum(HIGH, na.rm=TRUE),
#             moderate=sum(MODERATE, na.rm=TRUE),
#             low=sum(LOW, na.rm=TRUE),
#             modifier=sum(MODIFIER, na.rm=TRUE)) %>%
#   ungroup() %>%
#   mutate(high_prop = high/(high+moderate+low+modifier),
#          moderate_prop = moderate/(high+moderate+low+modifier),
#          low_prop = low/(high+moderate+low+modifier),
#          modifier_prop = modifier/(high+moderate+low+modifier),
#          high_genome = high/basePairs,
#          moderate_genome = moderate/basePairs,
#          low_genome = low/basePairs,
#          modifier_genome = modifier/basePairs)

# predPeaksMax %>% 
#   group_by(val) %>% 
#   summarise(basePairs=sum(length, na.rm=TRUE), 
#             high=sum(HIGH, na.rm=TRUE), 
#             moderate=sum(MODERATE, na.rm=TRUE), 
#             low=sum(LOW, na.rm=TRUE), 
#             modifier=sum(MODIFIER, na.rm=TRUE)) %>% 
#   ungroup() %>% 
#   mutate(high_prop = high/(high+moderate+low+modifier), 
#          moderate_prop = moderate/(high+moderate+low+modifier), 
#          low_prop = low/(high+moderate+low+modifier), 
#          modifier_prop = modifier/(high+moderate+low+modifier), 
#          high_genome = high/basePairs, 
#          moderate_genome = moderate/basePairs, 
#          low_genome = low/basePairs, 
#          modifier_genome = modifier/basePairs)

# # log-odds ratio
# 
# logOdds <- permutations %>% 
#   group_by(val, perm) %>% 
#   summarise(basePairs=sum(length, na.rm=TRUE), 
#             high=sum(HIGH, na.rm=TRUE), 
#             moderate=sum(MODERATE, na.rm=TRUE), 
#             low=sum(LOW, na.rm=TRUE), 
#             modifier=sum(MODIFIER, na.rm=TRUE), 
#             high_perm=sum(HIGH_perm, na.rm=TRUE), 
#             moderate_perm=sum(MODERATE_perm, na.rm=TRUE), 
#             low_perm=sum(LOW_perm, na.rm=TRUE), 
#             modifier_perm=sum(MODIFIER_perm, na.rm=TRUE), 
#             high_lodds=log2(high/high_perm), 
#             moderate_lodds=log2(moderate/moderate_perm), 
#             low_lodds=log2(low/low_perm),
#             modifier_lodds=log2(modifier/modifier_perm)) %>% 
#   select(val, perm, high_lodds:modifier_lodds) %>% 
#   ungroup() %>% 
#   group_by(val) %>% 
#   summarise(high_lci = quantile(high_lodds, 0.025), 
#             high_lodds_mean=mean(high_lodds), 
#             high_uci = quantile(high_lodds, 0.975), 
#             moderate_lci = quantile(moderate_lodds, 0.025), 
#             moderate_lodds_mean=mean(moderate_lodds), 
#             moderate_uci = quantile(moderate_lodds, 0.975), 
#             low_lci = quantile(low_lodds, 0.025), 
#             low_lodds_mean=mean(low_lodds), 
#             low_uci = quantile(low_lodds, 0.975), 
#             modifier_lci = quantile(modifier_lodds, 0.025), 
#             modifier_lodds_mean=mean(modifier_lodds), 
#             modifier_uci = quantile(modifier_lodds, 0.975))
# 
# # dot chart
# 
# logOdds_avg <- logOdds %>% 
#   select(val, high_lodds_mean, moderate_lodds_mean, low_lodds_mean, modifier_lodds_mean) %>% 
#   set_names(c("genome_class", "HIGH", "MODERATE", "LOW", "MODIFIER")) %>% 
#   pivot_longer(HIGH:MODIFIER, names_to="impact", values_to="avg") %>% 
#   as.data.frame()
# 
# logOdds_avg$impact <- factor(logOdds_avg$impact, c("HIGH","MODERATE","LOW","MODIFIER"))
# logOdds_avg$genome_class <- factor(logOdds_avg$genome_class, c(
#   "Hard", "Hard-linked", "HardPartial", "HardPartial-linked",
#   "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked",
#   "Neutral"))
# logOdds_avg <- logOdds_avg[order(logOdds_avg$genome_class,logOdds_avg$impact, decreasing = T),]
# 
# pdf("Figure_4_sweepImpact_dotplot_9states_SCOTT_snpEff_log2_version_2.pdf", width=6, height = 8)
# 
# dotchart(logOdds_avg$avg,
#          labels = logOdds_avg$impact,
#          groups = logOdds_avg$genome_class, 
#          xlab = "SNP enrichment log2 fold",
#          cex= 0.7, 
#          xlim = c(-2.5, 1))
# abline(v=0, col="red")
# 
# # add 95%CI line
# 
# logOdds_lci <- logOdds %>% 
#   select(val, high_lci, moderate_lci, low_lci, modifier_lci) %>% 
#   set_names(c("genome_class", "HIGH", "MODERATE", "LOW", "MODIFIER")) %>% 
#   pivot_longer(HIGH:MODIFIER, names_to="impact", values_to="lci") %>% 
#   as.data.frame()
# 
# logOdds_lci$impact <- factor(logOdds_lci$impact, c("HIGH","MODERATE","LOW","MODIFIER"))
# logOdds_lci$genome_class <- factor(logOdds_lci$genome_class, c(
#   "Hard", "Hard-linked", "HardPartial", "HardPartial-linked",
#   "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked",
#   "Neutral"))
# logOdds_lci <- logOdds_lci[order(logOdds_lci$genome_class,logOdds_lci$impact, decreasing = T),]
# 
# logOdds_uci <- logOdds %>% 
#   select(val, high_uci, moderate_uci, low_uci, modifier_uci) %>% 
#   set_names(c("genome_class", "HIGH", "MODERATE", "LOW", "MODIFIER")) %>% 
#   pivot_longer(HIGH:MODIFIER, names_to="impact", values_to="uci") %>% 
#   as.data.frame()
# 
# logOdds_uci$impact <- factor(logOdds_uci$impact, c("HIGH","MODERATE","LOW","MODIFIER"))
# logOdds_uci$genome_class <- factor(logOdds_uci$genome_class, c(
#   "Hard", "Hard-linked", "HardPartial", "HardPartial-linked",
#   "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked",
#   "Neutral"))
# logOdds_uci <- logOdds_uci[order(logOdds_uci$genome_class,logOdds_uci$impact, decreasing = T),]
# 
# j = 1
# for(i in 1:nrow(logOdds_lci)){
#     if(i%%4!=0){
#         if(logOdds_lci$lci[i]!=0){segments(logOdds_lci$lci[i],j,logOdds_uci$uci[i],j)}
#         j=j+1
#     }else{
#         if(logOdds_lci$lci[i]!=0){segments(logOdds_lci$lci[i],j,logOdds_uci$uci[i],j)}
#         j=j+3
#     }
# }
# 
# dev.off()



# log-odds ratio of proportion (i.e. relative to the number of SNPs)

# logOdds <- permutations %>% 
#   group_by(val, perm) %>% 
#   summarise(basePairs=sum(length, na.rm=TRUE), 
#             high=sum(HIGH, na.rm=TRUE), 
#             moderate=sum(MODERATE, na.rm=TRUE), 
#             low=sum(LOW, na.rm=TRUE), 
#             modifier=sum(MODIFIER, na.rm=TRUE), 
#             high_perm=sum(HIGH_perm, na.rm=TRUE), 
#             moderate_perm=sum(MODERATE_perm, na.rm=TRUE), 
#             low_perm=sum(LOW_perm, na.rm=TRUE), 
#             modifier_perm=sum(MODIFIER_perm, na.rm=TRUE)) %>% 
#   ungroup() %>% 
#   mutate(high_prop = high/(high+moderate+low+modifier), 
#          moderate_prop = moderate/(high+moderate+low+modifier), 
#          low_prop = low/(high+moderate+low+modifier), 
#          modifier_prop = modifier/(high+moderate+low+modifier), 
#          high_prop_perm = high_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
#          moderate_prop_perm = moderate_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
#          low_prop_perm = low_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
#          modifier_prop_perm = modifier_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
#          high_lodds = log2(high_prop/high_prop_perm), 
#          moderate_lodds = log2(moderate_prop/moderate_prop_perm), 
#          low_lodds = log2(low_prop/low_prop_perm), 
#          modifier_lodds = log2(modifier_prop/modifier_prop_perm)) %>% 
#   select(val, perm, high_lodds:modifier_lodds) %>% 
#   ungroup() %>% 
#   group_by(val) %>% 
#   summarise(high_lci = quantile(high_lodds, 0.025, na.rm=TRUE), 
#             high_lodds_mean=mean(high_lodds, na.rm=TRUE), 
#             high_uci = quantile(high_lodds, 0.975, na.rm=TRUE), 
#             moderate_lci = quantile(moderate_lodds, 0.025, na.rm=TRUE), 
#             moderate_lodds_mean=mean(moderate_lodds, na.rm=TRUE), 
#             moderate_uci = quantile(moderate_lodds, 0.975, na.rm=TRUE), 
#             low_lci = quantile(low_lodds, 0.025, na.rm=TRUE), 
#             low_lodds_mean=mean(low_lodds, na.rm=TRUE), 
#             low_uci = quantile(low_lodds, 0.975, na.rm=TRUE), 
#             modifier_lci = quantile(modifier_lodds, 0.025, na.rm=TRUE), 
#             modifier_lodds_mean=mean(modifier_lodds, na.rm=TRUE), 
#             modifier_uci = quantile(modifier_lodds, 0.975, na.rm=TRUE))

logOdds <- permutations %>% 
  mutate(HIGH = ifelse(is.na(HIGH), 1, HIGH+1), 
         MODERATE = ifelse(is.na(MODERATE), 1, MODERATE+1), 
         LOW = ifelse(is.na(LOW), 1, LOW+1), 
         MODIFIER = ifelse(is.na(MODIFIER), 1, MODIFIER+1), 
         HIGH_perm = ifelse(is.na(HIGH_perm), 1, HIGH_perm+1), 
         MODERATE_perm = ifelse(is.na(MODERATE_perm), 1, MODERATE_perm+1), 
         LOW_perm = ifelse(is.na(LOW_perm), 1, LOW_perm+1), 
         MODIFIER_perm = ifelse(is.na(MODIFIER_perm), 1, MODIFIER_perm+1)) %>% 
  group_by(pred_ensemble, perm) %>% 
  summarise(basePairs=sum(length), 
            high=sum(HIGH), 
            moderate=sum(MODERATE), 
            low=sum(LOW), 
            modifier=sum(MODIFIER), 
            high_perm=sum(HIGH_perm), 
            moderate_perm=sum(MODERATE_perm), 
            low_perm=sum(LOW_perm), 
            modifier_perm=sum(MODIFIER_perm)) %>% 
  ungroup() %>% 
  mutate(high_prop = high/(high+moderate+low+modifier), 
         moderate_prop = moderate/(high+moderate+low+modifier), 
         low_prop = low/(high+moderate+low+modifier), 
         modifier_prop = modifier/(high+moderate+low+modifier), 
         high_prop_perm = high_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
         moderate_prop_perm = moderate_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
         low_prop_perm = low_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
         modifier_prop_perm = modifier_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
         high_lodds = log2(high_prop/high_prop_perm), 
         moderate_lodds = log2(moderate_prop/moderate_prop_perm), 
         low_lodds = log2(low_prop/low_prop_perm), 
         modifier_lodds = log2(modifier_prop/modifier_prop_perm)) %>% 
  select(pred_ensemble, perm, high_lodds:modifier_lodds) %>% 
  ungroup() %>% 
  group_by(pred_ensemble) %>% 
  summarise(high_lci = quantile(high_lodds, 0.025), 
            high_lodds_mean=mean(high_lodds), 
            high_uci = quantile(high_lodds, 0.975), 
            moderate_lci = quantile(moderate_lodds, 0.025), 
            moderate_lodds_mean=mean(moderate_lodds), 
            moderate_uci = quantile(moderate_lodds, 0.975), 
            low_lci = quantile(low_lodds, 0.025), 
            low_lodds_mean=mean(low_lodds), 
            low_uci = quantile(low_lodds, 0.975), 
            modifier_lci = quantile(modifier_lodds, 0.025), 
            modifier_lodds_mean=mean(modifier_lodds), 
            modifier_uci = quantile(modifier_lodds, 0.975))

# dot chart

logOdds_avg <- logOdds %>% 
  select(pred_ensemble, high_lodds_mean, moderate_lodds_mean, low_lodds_mean, modifier_lodds_mean) %>% 
  set_names(c("genome_class", "HIGH", "MODERATE", "LOW", "MODIFIER")) %>% 
  pivot_longer(HIGH:MODIFIER, names_to="impact", values_to="avg") %>% 
  as.data.frame()

logOdds_avg$impact <- factor(logOdds_avg$impact, c("HIGH","MODERATE","LOW","MODIFIER"))
logOdds_avg$genome_class <- factor(logOdds_avg$genome_class, c(
  "Hard", "Hard-linked", "HardPartial", "HardPartial-linked",
  "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked",
  "Neutral"))
logOdds_avg <- logOdds_avg[order(logOdds_avg$genome_class,logOdds_avg$impact, decreasing = T),]

# pdf("Figure_4_sweepImpact_dotplot_9states_SCOTT_snpEff_log2_proportion.pdf", width=6, height = 8)
# png("Figure_4_sweepImpact_dotplot_9states_SCOTT_snpEff_log2_proportion.png", width=6, height = 8, units="in", res=300)
# pdf("Figure_4_sweepImpact_dotplot_9states_SCOTT_snpEff_log2_proportion_intergenic_3.pdf", width=6, height = 8)
# png("Figure_4_sweepImpact_dotplot_9states_SCOTT_snpEff_log2_proportion_intergenic_3.png", width=6, height = 8, units="in", res=300)

dotchart(logOdds_avg$avg,
         labels = logOdds_avg$impact,
         groups = logOdds_avg$genome_class, 
         xlab = "SNP enrichment log2 fold",
         cex= 0.7, 
         xlim = c(-2, 2))
abline(v=0, col="red")

# add 95%CI line

logOdds_lci <- logOdds %>% 
  select(pred_ensemble, high_lci, moderate_lci, low_lci, modifier_lci) %>% 
  set_names(c("genome_class", "HIGH", "MODERATE", "LOW", "MODIFIER")) %>% 
  pivot_longer(HIGH:MODIFIER, names_to="impact", values_to="lci") %>% 
  as.data.frame()

logOdds_lci$impact <- factor(logOdds_lci$impact, c("HIGH","MODERATE","LOW","MODIFIER"))
logOdds_lci$genome_class <- factor(logOdds_lci$genome_class, c(
  "Hard", "Hard-linked", "HardPartial", "HardPartial-linked",
  "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked",
  "Neutral"))
logOdds_lci <- logOdds_lci[order(logOdds_lci$genome_class,logOdds_lci$impact, decreasing = T),]

logOdds_uci <- logOdds %>% 
  select(pred_ensemble, high_uci, moderate_uci, low_uci, modifier_uci) %>% 
  set_names(c("genome_class", "HIGH", "MODERATE", "LOW", "MODIFIER")) %>% 
  pivot_longer(HIGH:MODIFIER, names_to="impact", values_to="uci") %>% 
  as.data.frame()

logOdds_uci$impact <- factor(logOdds_uci$impact, c("HIGH","MODERATE","LOW","MODIFIER"))
logOdds_uci$genome_class <- factor(logOdds_uci$genome_class, c(
  "Hard", "Hard-linked", "HardPartial", "HardPartial-linked",
  "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked",
  "Neutral"))
logOdds_uci <- logOdds_uci[order(logOdds_uci$genome_class,logOdds_uci$impact, decreasing = T),]

j = 1
for(i in 1:nrow(logOdds_lci)){
    if(i%%4!=0){
        if(logOdds_lci$lci[i]!=0){segments(logOdds_lci$lci[i],j,logOdds_uci$uci[i],j)}
        j=j+1
    }else{
        if(logOdds_lci$lci[i]!=0){segments(logOdds_lci$lci[i],j,logOdds_uci$uci[i],j)}
        j=j+3
    }
}

dev.off()

```

```{r, stats}

temp1 <- permutations %>% 
  filter(perm==1) %>% 
  select(scf:length) %>% 
  pivot_longer(HIGH:MODIFIER, names_to="impact", values_to="value")

total <- sum(temp1$value, na.rm=TRUE)
total

temp1 %>% 
  group_by(impact) %>% 
  summarise(proportion = sum(value, na.rm=TRUE)/total)

temp2 <- temp1 %>% 
  group_by(val, impact) %>% 
  summarise(count = sum(value, na.rm=TRUE)) %>% 
  pivot_wider(names_from=val, values_from=count)
temp2

chisq.test(temp2)

```


```{powershell, transfer}

# scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/permutations/*.pdf C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission\
# 
# scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/permutations/*.png C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission\
# 
# 
# 
# scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe_intergenic/permutations/*.pdf C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission\
# 
# scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe_intergenic/permutations/*.png C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission\



scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_SIFT/Figure_4_sweepImpact_dotplot_9states_SCOTT_snpEff_log2_proportion_intergenic_3.pdf C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission\

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_SIFT/Figure_4_sweepImpact_dotplot_9states_SCOTT_snpEff_log2_proportion_intergenic_3.png C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission\

```

# ######################## Empirical stats

```{r}

library(tidyverse)
library(data.table)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

tally_5kb <- predPeaks %>% 
  group_by(pred_ensemble) %>% 
  tally() %>% 
  mutate(percentage = n/21501)

tally_merged <- predPeaks %>% 
  group_by(CHROM, pred_ensemble, merged) %>% 
  summarise(start = min(start), end = max(end)) %>% 
  ungroup() %>% 
  arrange(CHROM, start, end) %>% 
  mutate(length = end - start) %>% 
  group_by(pred_ensemble) %>% 
  tally() %>% 
  rename(n_merged = n)

tally_5kb %>% 
  select(-percentage) %>% 
  left_join(tally_merged) %>% 
  mutate(reduction = 1 - n_merged/n) %>% 
  arrange(desc(reduction))

predPeaks %>% 
  group_by(CHROM, pred_ensemble, merged) %>% 
  summarise(start = min(start), end = max(end)) %>% 
  ungroup() %>% 
  arrange(CHROM, start, end) %>% 
  mutate(length = end - start) %>% 
  group_by(pred_ensemble) %>% 
  summarise(length = mean(length))

# percent genome impacted directly

(451+2286+67+221)/21501

# precent genome indirectly

(13588+2919+340+798)/21501

# soft vs hard

1818/336

# complete vs partial

(1818+336)/(221+67)

sum(predPeaks$HIGH, na.rm=TRUE)
sum(predPeaks$MODERATE, na.rm=TRUE)
sum(predPeaks$LOW, na.rm=TRUE)
sum(predPeaks$MODIFIER, na.rm=TRUE)


```


# ######################## GO term enrichment

```{bash}

module load bedtools

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

cd /home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class

```

## Permutation

### Bunya

```{bash, on bunya}

while read p; do
  echo "$p"
  cp permutation_SOFTPARTIAL_template.R permutation_SOFTPARTIAL_${p}.R
  replace_CMD1="s/\[PERMS\]/${p}/g"
  perl -pi -e "$replace_CMD1" permutation_SOFTPARTIAL_${p}.R
  replace_CMD2="s/\[REGION\]/${p}/g"
  perl -pi -e "$replace_CMD2" permutation_SOFTPARTIAL_${p}.R
done < regions_list.txt



while read p; do
  echo "$p"
  cp permutation_SOFTPARTIAL_template.slurm.sh permutation_SOFTPARTIAL_${p}.slurm.sh
  replace_CMD="s/\[REGION\]/${p}/g"
  perl -pi -e "$replace_CMD" permutation_SOFTPARTIAL_${p}.slurm.sh
done < regions_list.txt



find ./ -maxdepth 1 -type f -name 'permutation_SOFTPARTIAL_*.slurm.sh' | grep -v template | sed 's/\.\///' | sort -V > list_permutation_files.txt

while read p; do
  echo "${p} submitted"
  sbatch ${p}
  sleep 0.05
done < list_permutation_files.txt

```


```{r, fast-ish code}

library(tidyverse)
library(foreach)
library(doSNOW)
library(data.table)

## Import and wrangle data

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/snpEff/")
snpEff_severe <- read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")
snpEff_severe <- snpEff_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")
predPeaks_5kb <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

predPeaks <- predPeaks_5kb %>%
  group_by(CHROM, merged, pred_ensemble) %>%
  summarise(start = min(start),
            end = max(end),
            HIGH = sum(HIGH, na.rm=TRUE),
            MODERATE = sum(MODERATE, na.rm=TRUE),
            LOW = sum(LOW, na.rm=TRUE),
            MODIFIER = sum(MODIFIER, na.rm=TRUE)) %>%
  ungroup() %>%
  mutate(length = end - start)

predPeaks_5kb$run <- 1
g <- 1

for (i in 2:nrow(predPeaks_5kb)) {
  if (predPeaks_5kb$start[i] == predPeaks_5kb$end[i-1]) {
    predPeaks_5kb$run[i] <- g
  } else {
    g <- g+1
    predPeaks_5kb$run[i] <-g
  }
}

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(CHROM, start, end, merged, run)

peak_start_end <- scf_start %>% group_by(CHROM, run) %>% summarise(start = min(start), end = max(end))


# shuffled_data <- predPeaks %>% filter(pred_ensemble=="Hard") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER)
# scf_start_temp <- scf_start
# peak_start_end_temp <- peak_start_end

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")



# Hard

shuffled_data <- predPeaks %>% filter(pred_ensemble=="Hard") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER)
shuffled_data <- shuffled_data %>% arrange(desc(length))
scf_start_temp <- scf_start
peak_start_end_temp <- peak_start_end

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

permutations <- foreach(j = 1:100000, .combine = "rbind", .packages = c("dplyr")) %dopar% {
  
  # Randomly select loci
  selected_scf_start <- sample_n(scf_start_temp, nrow(shuffled_data))
  shuffled_data <- shuffled_data %>% 
    mutate(CHROM_perm = selected_scf_start$CHROM, 
           start_perm = selected_scf_start$start, 
           end_perm = start_perm + length)
  
  # Check if end of new loci's region is available
  shuffled_data$end_not_available <- NA
  for (i in 1:nrow(shuffled_data)) {
    peak_end <- peak_start_end_temp %>% 
      filter(CHROM==shuffled_data$CHROM_perm[i] & start <= shuffled_data$start_perm[i] & end >= shuffled_data$end_perm[i])
    shuffled_data$end_not_available[i] <- nrow(peak_end) < 1
  }
  
  # Check if any randomly selected loci overlap
  shuffled_data <- shuffled_data %>%
    group_by(CHROM_perm) %>%
    arrange(CHROM_perm, start_perm, end_perm) %>%
    mutate(
      overlap_next_start = lag(end_perm) >= start_perm,
      overlap_prev_end = lead(start_perm) <= end_perm,
      overlap = (overlap_next_start | overlap_prev_end)
    ) %>%
    ungroup()
  
  # How many permutation failed?
  failed_permutations_n <- shuffled_data %>% 
    filter(end_not_available | overlap) %>% 
    nrow()
  
  # Which permutation failed?
  failed_permutations <- shuffled_data %>% 
    filter(end_not_available | overlap)
  
  # cat("Failed permutations on attempt 1:", failed_permutations_n, "\n")
  
  # If no permutations failed, print
  if (failed_permutations_n < 1) { 
    shuffled_data$perm <- j
    shuffled_data
  }
  
  # If some permutations failed, try and fix them
  
  attempt <- 2
  
  while(failed_permutations_n > 0 & attempt <= 100) {
    
    # Remove used loci from the pool of options
    # scf_start_temp2 <- scf_start_temp %>% anti_join(selected_scf_start)
    scf_start_temp2 <- scf_start_temp %>% anti_join(shuffled_data, by=c("CHROM", "start", "end"))
    
    # Randomly select loci
    selected_scf_start2 <- sample_n(scf_start_temp2, nrow(failed_permutations))
    failed_permutations <- failed_permutations %>% 
      mutate(CHROM_perm = selected_scf_start2$CHROM, 
             start_perm = selected_scf_start2$start, 
             end_perm = start_perm + length)
    
    # Check if end of new loci's region is available
    for (i in 1:nrow(failed_permutations)) {
      peak_end <- peak_start_end_temp %>% 
        filter(CHROM==failed_permutations$CHROM_perm[i] & start <= failed_permutations$start_perm[i] & end >= failed_permutations$end_perm[i])
      failed_permutations$end_not_available[i] <- nrow(peak_end) < 1
      
    }
    
    # Check if any randomly selected loci overlap
    failed_permutations_OG <- shuffled_data %>% filter(end_not_available | overlap)
    shuffled_data2 <- shuffled_data %>% anti_join(failed_permutations_OG)
    shuffled_data2 <- bind_rows(shuffled_data2, failed_permutations)
    shuffled_data2 <- shuffled_data2 %>%
      group_by(CHROM_perm) %>%
      arrange(CHROM_perm, start_perm, end_perm) %>%
      mutate(
        overlap_next_start = lag(end_perm) >= start_perm,
        overlap_prev_end = lead(start_perm) <= end_perm,
        overlap = (overlap_next_start | overlap_prev_end)
      ) %>%
      ungroup()
    
    # How many permutation failed?
    failed_permutations_n <- shuffled_data2 %>% 
      filter(end_not_available | overlap) %>% 
      nrow()
    
    # Which permutation failed?
    failed_permutations <- shuffled_data2 %>% 
      filter(end_not_available | overlap)
    
    # cat("Failed permutations on attempt", attempt, ":", failed_permutations_n, "\n")
    
    # Overwrite shuffled_data
    
    shuffled_data <- shuffled_data2
    
    attempt <- attempt+1
    
  }
  
  if (attempt <= 100) {
    shuffled_data$perm <- j
    shuffled_data
  }
  
}

stopCluster(cl)

# Save file

(predPeaks %>% filter(pred_ensemble=="Hard") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER) %>% nrow())*100000 == nrow(permutations)

fileName <- "predPeaks_permutation_HARD_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv"
fwrite(permutations, file=fileName, sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)



# Soft FAILED

# shuffled_data <- predPeaks %>% filter(pred_ensemble=="Soft") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER)
# shuffled_data <- shuffled_data %>% arrange(desc(length))
scf_start_temp <- scf_start
peak_start_end_temp <- peak_start_end

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

permutations <- foreach(j = 1:100000, .combine = "rbind", .packages = c("dplyr")) %dopar% {
  
  shuffled_data <- predPeaks %>% filter(pred_ensemble=="Soft") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER)
  shuffled_data <- shuffled_data %>% arrange(desc(length))
  
  # Randomly select loci
  selected_scf_start <- sample_n(scf_start_temp, nrow(shuffled_data))
  shuffled_data2 <- shuffled_data %>% 
    mutate(CHROM_perm = selected_scf_start$CHROM, 
           start_perm = selected_scf_start$start, 
           end_perm = start_perm + length)
  
  # Check if end of new loci's region is available
  shuffled_data2$end_not_available <- NA
  for (i in 1:nrow(shuffled_data2)) {
    peak_end <- peak_start_end_temp %>% 
      filter(CHROM==shuffled_data2$CHROM_perm[i] & start <= shuffled_data2$start_perm[i] & end >= shuffled_data2$end_perm[i])
    shuffled_data2$end_not_available[i] <- nrow(peak_end) < 1
  }
  
  # Check if any randomly selected loci overlap
  shuffled_data2 <- shuffled_data2 %>%
    group_by(CHROM_perm) %>%
    arrange(CHROM_perm, start_perm, end_perm) %>%
    mutate(
      overlap_next_start = lag(end_perm) >= start_perm,
      overlap_prev_end = lead(start_perm) <= end_perm,
      overlap = (overlap_next_start | overlap_prev_end)
    ) %>%
    ungroup()
  
  # How many permutation failed?
  failed_permutations_n <- shuffled_data2 %>% 
    filter(end_not_available | overlap) %>% 
    nrow()
  
  # Which permutation failed?
  failed_permutations <- shuffled_data2 %>% 
    filter(end_not_available | overlap)
  
  # cat("Failed permutations on attempt 1:", failed_permutations_n, "\n")
  
  # If no permutations failed, print and end iteration
  if (failed_permutations_n < 1) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }
  
  # If some permutations failed, try and fix them
  
  attempt <- 2
  
  while(failed_permutations_n > 0 & attempt <= 100) {
  # while(failed_permutations_n > 0) {
    
    # Remove used loci from the pool of options
    # scf_start_temp2 <- scf_start_temp %>% anti_join(selected_scf_start)
    scf_start_temp2 <- scf_start_temp %>% anti_join(shuffled_data2, by=c("CHROM", "start", "end"))
    
    # Randomly select loci
    selected_scf_start2 <- sample_n(scf_start_temp2, nrow(failed_permutations))
    failed_permutations <- failed_permutations %>% 
      mutate(CHROM_perm = selected_scf_start2$CHROM, 
             start_perm = selected_scf_start2$start, 
             end_perm = start_perm + length)
    
    # Check if end of new loci's region is available
    for (i in 1:nrow(failed_permutations)) {
      peak_end <- peak_start_end_temp %>% 
        filter(CHROM==failed_permutations$CHROM_perm[i] & start <= failed_permutations$start_perm[i] & end >= failed_permutations$end_perm[i])
      failed_permutations$end_not_available[i] <- nrow(peak_end) < 1
      
    }
    
    # Check if any randomly selected loci overlap
    failed_permutations_OG <- shuffled_data2 %>% filter(end_not_available | overlap)
    shuffled_data3 <- shuffled_data2 %>% anti_join(failed_permutations_OG)
    shuffled_data3 <- bind_rows(shuffled_data3, failed_permutations)
    shuffled_data3 <- shuffled_data3 %>%
      group_by(CHROM_perm) %>%
      arrange(CHROM_perm, start_perm, end_perm) %>%
      mutate(
        overlap_next_start = lag(end_perm) >= start_perm,
        overlap_prev_end = lead(start_perm) <= end_perm,
        overlap = (overlap_next_start | overlap_prev_end)
      ) %>%
      ungroup()
    
    # How many permutation failed?
    failed_permutations_n <- shuffled_data3 %>% 
      filter(end_not_available | overlap) %>% 
      nrow()
    
    # Which permutation failed?
    failed_permutations <- shuffled_data3 %>% 
      filter(end_not_available | overlap)
    
    # cat("Failed permutations on attempt", attempt, ":", failed_permutations_n, "\n")
    
    # Overwrite shuffled_data
    
    shuffled_data2 <- shuffled_data3
    
    attempt <- attempt+1
    
  }
  
  if (attempt >=3 & attempt <= 100) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }
  
  shuffled_data_final
  
}

stopCluster(cl)

# Save file

(predPeaks %>% filter(pred_ensemble=="Hard") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER) %>% nrow())*100000 == nrow(permutations)

fileName <- "predPeaks_permutation_HARD_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv"
fwrite(permutations, file=fileName, sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)



# HardPartial

# shuffled_data <- predPeaks %>% filter(pred_ensemble=="HardPartial") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER)
# shuffled_data <- shuffled_data %>% arrange(desc(length))
scf_start_temp <- scf_start
peak_start_end_temp <- peak_start_end

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

permutations <- foreach(j = 1:100000, .combine = "rbind", .packages = c("dplyr")) %dopar% {
  
  shuffled_data <- predPeaks %>% filter(pred_ensemble=="HardPartial") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER)
  shuffled_data <- shuffled_data %>% arrange(desc(length))
  
  # Randomly select loci
  selected_scf_start <- sample_n(scf_start_temp, nrow(shuffled_data))
  shuffled_data2 <- shuffled_data %>% 
    mutate(CHROM_perm = selected_scf_start$CHROM, 
           start_perm = selected_scf_start$start, 
           end_perm = start_perm + length)
  
  # Check if end of new loci's region is available
  shuffled_data2$end_not_available <- NA
  for (i in 1:nrow(shuffled_data2)) {
    peak_end <- peak_start_end_temp %>% 
      filter(CHROM==shuffled_data2$CHROM_perm[i] & start <= shuffled_data2$start_perm[i] & end >= shuffled_data2$end_perm[i])
    shuffled_data2$end_not_available[i] <- nrow(peak_end) < 1
  }
  
  # Check if any randomly selected loci overlap
  shuffled_data2 <- shuffled_data2 %>%
    group_by(CHROM_perm) %>%
    arrange(CHROM_perm, start_perm, end_perm) %>%
    mutate(
      overlap_next_start = lag(end_perm) >= start_perm,
      overlap_prev_end = lead(start_perm) <= end_perm,
      overlap = (overlap_next_start | overlap_prev_end)
    ) %>%
    ungroup()
  
  # How many permutation failed?
  failed_permutations_n <- shuffled_data2 %>% 
    filter(end_not_available | overlap) %>% 
    nrow()
  
  # Which permutation failed?
  failed_permutations <- shuffled_data2 %>% 
    filter(end_not_available | overlap)
  
  # cat("Failed permutations on attempt 1:", failed_permutations_n, "\n")
  
  # If no permutations failed, print and end iteration
  if (failed_permutations_n < 1) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }
  
  # If some permutations failed, try and fix them
  
  attempt <- 2
  
  while(failed_permutations_n > 0 & attempt <= 100) {
  # while(failed_permutations_n > 0) {
    
    # Remove used loci from the pool of options
    # scf_start_temp2 <- scf_start_temp %>% anti_join(selected_scf_start)
    scf_start_temp2 <- scf_start_temp %>% anti_join(shuffled_data2, by=c("CHROM", "start", "end"))
    
    # Randomly select loci
    selected_scf_start2 <- sample_n(scf_start_temp2, nrow(failed_permutations))
    failed_permutations <- failed_permutations %>% 
      mutate(CHROM_perm = selected_scf_start2$CHROM, 
             start_perm = selected_scf_start2$start, 
             end_perm = start_perm + length)
    
    # Check if end of new loci's region is available
    for (i in 1:nrow(failed_permutations)) {
      peak_end <- peak_start_end_temp %>% 
        filter(CHROM==failed_permutations$CHROM_perm[i] & start <= failed_permutations$start_perm[i] & end >= failed_permutations$end_perm[i])
      failed_permutations$end_not_available[i] <- nrow(peak_end) < 1
      
    }
    
    # Check if any randomly selected loci overlap
    failed_permutations_OG <- shuffled_data2 %>% filter(end_not_available | overlap)
    shuffled_data3 <- shuffled_data2 %>% anti_join(failed_permutations_OG)
    shuffled_data3 <- bind_rows(shuffled_data3, failed_permutations)
    shuffled_data3 <- shuffled_data3 %>%
      group_by(CHROM_perm) %>%
      arrange(CHROM_perm, start_perm, end_perm) %>%
      mutate(
        overlap_next_start = lag(end_perm) >= start_perm,
        overlap_prev_end = lead(start_perm) <= end_perm,
        overlap = (overlap_next_start | overlap_prev_end)
      ) %>%
      ungroup()
    
    # How many permutation failed?
    failed_permutations_n <- shuffled_data3 %>% 
      filter(end_not_available | overlap) %>% 
      nrow()
    
    # Which permutation failed?
    failed_permutations <- shuffled_data3 %>% 
      filter(end_not_available | overlap)
    
    # cat("Failed permutations on attempt", attempt, ":", failed_permutations_n, "\n")
    
    # Overwrite shuffled_data
    
    shuffled_data2 <- shuffled_data3
    
    attempt <- attempt+1
    
  }
  
  if (attempt >=3 & attempt <= 100) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }
  
  shuffled_data_final
  
}

stopCluster(cl)

# Save file

(predPeaks %>% filter(pred_ensemble=="Hard") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER) %>% nrow())*100000 == nrow(permutations)

fileName <- "predPeaks_permutation_HARDPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv"
fwrite(permutations, file=fileName, sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)




```

```{r, HardPartial}

library(tidyverse)
library(data.table)
library(qvalue)

## Import data

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")
permutations <- read_tsv("predPeaks_permutation_HARDPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv")

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>% 
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv") %>% rename(go = go_id)

## Join GO terms for predictions peaks

predPeaks_temp <- predPeaks %>% 
  dplyr::select(CHROM, start, end, pred_ensemble, length, merged)

write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "merged", "intersect")
predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

temp <- predPeaksGO %>% 
  filter(val=="SoftPartial") %>%
  select(val, chrom, merged, fbgn) %>% 
  distinct() %>% 
  group_by(val, chrom, merged) %>% 
  tally() %>% 
  ungroup()

summary(temp$n)

temp <- predPeaksGO %>% 
  filter(val=="Hard" | val=="HardPartial" | val=="Soft" | val=="SoftPartial") %>%
  select(chrom, merged, fbgn) %>% 
  distinct() %>% 
  group_by(chrom, merged) %>% 
  tally() %>% 
  ungroup()

summary(temp$n)

## Join GO terms to permutations

permutations_temp <- permutations %>% 
  dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length)

write.table(permutations_temp, "permutations_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp.tsv -wo > permutations_GO_merged.tsv")
system(cmd)

permutationsGO_HardPartial <- fread("permutations_GO_merged.tsv", header=FALSE) 
colnames(permutationsGO_HardPartial) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")

permutationsGO_HardPartial <- permutationsGO_HardPartial %>% 
    left_join(GO_geneName, relationship = "many-to-many") %>% 
    arrange(scf_perm, start_pos_perm, end_pos_perm)



## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_HardPartial %>% 
  group_by(perm, val, go, process) %>% 
  summarise(intersect_perm = sum(intersect)) %>% 
  ungroup()



# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))

### Check that intersect is never greater than one

permutationsGO_summary %>% 
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
  filter(is.na(intersect_empirical)==FALSE) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_Hard) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
  arrange(desc(intersect_proportion_perm))

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/100000,
            p_proportion = sum(test_proportion)/100000)

# Biological process (HardPartial)

## Intersect

permTest_intersect_HardPartial <- permTest_intersect %>% 
  filter(val=="HardPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_HardPartial %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("HardPartial")

permTest_intersect_HardPartial$fdr <- p.adjust(permTest_intersect_HardPartial$p_proportion, method="fdr")
# permTest_intersect_HardPartial$q <- qvalue(permTest_intersect_HardPartial$p_proportion)$qvalues

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="HardPartial") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="HardPartial") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(fdr <= 0.05) %>% 
  print(n=70)

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(go, fdr, name_1006) %>% 
  filter(fdr <= 0.05) %>% 
  rename(p_adjusted = fdr, go_name = name_1006) %>% 
  print(n=70)

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p_proportion))) + 
  geom_point()

```

### Soft

* Soft permutations via permutation_SOFT.R

```{r, Soft}

library(tidyverse)
library(data.table)
library(qvalue)
# library(foreach)
# library(doSNOW)

## Import data

# # already used
# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")
# predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")

# # already used
# permutations <- read_tsv(
#   "predPeaks_permutation_SOFT_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv"
#   )
# permutations <- read_tsv(
#   "predPeaks_permutation_SOFT_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_100-200k.tsv"
#   )
# permutations <- read_tsv(
#   "predPeaks_permutation_SOFT_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_200-500k.tsv"
#   )

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

# # already used
# GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>% 
#   set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv") %>% rename(go = go_id)

## Join GO terms for predictions peaks

# # already done
# predPeaks_temp <- predPeaks %>% 
#   dplyr::select(CHROM, start, end, pred_ensemble, length)
# 
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
# system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "merged", "intersect")

predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)


# # already done
# ## Join GO terms to permutations
# 
# permutations_temp <- permutations %>% 
#   dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length) %>% 
#   dplyr::filter(perm <= 300000)
# fwrite(permutations_temp, "permutations_temp_200-300k.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# permutations_temp <- permutations %>% 
#   dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length) %>% 
#   dplyr::filter(perm > 300000 & perm <= 400000)
# fwrite(permutations_temp, "permutations_temp_300-400k.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# permutations_temp <- permutations %>% 
#   dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length) %>% 
#   dplyr::filter(perm > 400000 & perm <= 500000)
# fwrite(permutations_temp, "permutations_temp_400-500k.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# nohup bedtools intersect -a GO.bed -b permutations_temp_200-300k.tsv -wo > permutations_GO_merged_200-300k.tsv &
# nohup bedtools intersect -a GO.bed -b permutations_temp_300-400k.tsv -wo > permutations_GO_merged_300-400k.tsv &
# nohup bedtools intersect -a GO.bed -b permutations_temp_400-500k.tsv -wo > permutations_GO_merged_400-500k.tsv &
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp.tsv -wo > permutations_GO_merged.tsv")
# system(cmd)
# 
# # permutationsGO_Soft <- fread("permutations_GO_merged_200-300k.tsv", header=FALSE)
# permutationsGO_Soft <- fread("permutations_GO_merged_400-500k.tsv", header=FALSE)
# colnames(permutationsGO_Soft) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")
# 
# setDT(permutationsGO_Soft)
# setDT(GO_geneName)
# 
# # for (i in 1:100000) {
# # for (i in 100001:200000) {
# # for (i in 200001:300000) {
# # for (i in 300001:400000) {
# for (i in 400001:500000) {
#   cat(i, "\n")
#   temp <- permutationsGO_Soft[perm == i][GO_geneName, on = .(fbgn), nomatch = 0]
#   setorder(temp, scf_perm, start_pos_perm, end_pos_perm)
#   fileName <- paste("permutations_GO_merged_SOFT_", i, ".csv", sep="")
#   fwrite(temp, fileName, row.names = FALSE, quote = FALSE, col.names = TRUE)
# }
# 
# rm(permutationsGO_Soft)
# gc()



## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

# # already done
# ## Permutation summary
# 
# # permutationsGO_summary <- permutationsGO_Soft %>% 
# #   group_by(perm, val, go, process) %>% 
# #   summarise(intersect_perm = sum(intersect)) %>% 
# #   ungroup()
# 
# # ffs <- list.files(pattern = '*.csv', recursive = TRUE)
# all_csv_files <- list.files(pattern = '\\.csv$', recursive = TRUE)
# ffs <- all_csv_files[!grepl('_summary\\.csv$', all_csv_files)]
# length(ffs)
# 
# count <- 1
# 
# for (ff in ffs) {
#   
#   cat(count, "Processing", ff, "\n")
#   temp <- fread(ff)
#   temp_summary <- temp %>% 
#     group_by(perm, val, go, process) %>% 
#     summarise(intersect_perm = sum(intersect)) %>% 
#     ungroup()
#   fileName <- str_replace(ff, '.csv', '_summary.csv')
#   fwrite(temp_summary, file=fileName, row.names=FALSE, col.names=TRUE, quote=FALSE)
#   count <- count + 1
#   
# }
# 
# ffs <- list.files(pattern = '*_summary.csv', recursive = TRUE)
# length(ffs)
# permutationsGO_summary <- rbindlist(lapply(ffs, fread))
# 
# fwrite(permutationsGO_summary, 
#        file="permutations_GO_merged_SOFT_summary_1-500k.csv", 
#        row.names=FALSE, 
#        col.names=TRUE, 
#        quote=FALSE, 
#        sep=",")
# 
permutationsGO_summary <- fread("permutations_GO_merged_SOFT_summary_1-500k.csv.gz", sep=",")
# 
# # Permutation test
# 
# ## Intersect
# 
intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# # intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))
# 
# ### Check that intersect is never greater than one
# 
# permutationsGO_summary %>% 
#   left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
#   filter(is.na(intersect_empirical)==FALSE) %>% 
#   left_join(intersect_genome) %>% 
#   # left_join(intersect_Hard) %>% 
#   mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
#   mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
#   select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
#   arrange(desc(intersect_proportion_perm))
# 
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm <= 50000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 50000 & perm <= 100000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 100000 & perm <= 150000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 150000 & perm <= 200000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 200000 & perm <= 250000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 250000 & perm <= 300000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 300000 & perm <= 350000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 350000 & perm <= 400000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 400000 & perm <= 450000)
# permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 450000 & perm <= 500000)
# 
# permTest_intersect <- permutationsGO_summary_subset %>%
#   left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
#   filter(is.na(intersect_empirical)==FALSE) %>%
#   left_join(intersect_genome) %>%
#   mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
#   mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
#   mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
#   mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
#   mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
#   mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
#   group_by(val, go, process) # %>%
#   # summarise(p_intersect = sum(test_intersect)/500000,
#   #           p_proportion = sum(test_proportion)/500000)
# 
# fwrite(permTest_intersect, 
#        file="permTest_intersect_Soft_450-500k.csv", 
#        row.names=FALSE, 
#        col.names=TRUE, 
#        quote=FALSE, 
#        sep=",")
# 
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Soft_1-50k.csv > permTest_intersect_Soft_1-50k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Soft_50-100k.csv > permTest_intersect_Soft_50-100k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Soft_100-150k.csv > permTest_intersect_Soft_100-150k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Soft_150-200k.csv > permTest_intersect_Soft_150-200k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Soft_200-250k.csv > permTest_intersect_Soft_200-250k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Soft_250-300k.csv > permTest_intersect_Soft_250-300k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Soft_300-350k.csv > permTest_intersect_Soft_300-350k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Soft_350-400k.csv > permTest_intersect_Soft_350-400k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Soft_400-450k.csv > permTest_intersect_Soft_400-450k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Soft_450-500k.csv > permTest_intersect_Soft_450-500k_minimumInfor.csv

ffs <- list.files(pattern = '.*_minimumInfor.csv', recursive = TRUE)
length(ffs)
permTest_intersect <- rbindlist(lapply(ffs, fread))

permTest_intersect <- permTest_intersect %>% 
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/500000)

# Biological process (Soft)

## Intersect

permTest_intersect_Soft <- permTest_intersect %>% 
  filter(val=="Soft") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_Soft %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("Soft")

permTest_intersect_Soft$fdr <- p.adjust(permTest_intersect_Soft$p_intersect, method="fdr")
permTest_intersect_Soft$q <- qvalue(permTest_intersect_Soft$p_intersect)$qvalues

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="Soft") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="Soft") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(q <= 0.05) %>% 
  print(n=40)

results <- permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean)

write_tsv(results, file="go_soft.tsv")

```

```{bash, compress and store files}

find ./ -maxdepth 1 -type f -regex './permutations_GO_merged_SOFT_[0-9]+.csv' > filelist.txt
tar -czvf permutations_GO_merged_SOFT_files.tar.gz -T filelist.txt
find ./ -maxdepth 1 -type f -regex './permutations_GO_merged_SOFT_[0-9]+.csv' -exec rm -v {} \;

find ./ -maxdepth 1 -type f -name 'permutations_GO_merged_SOFT_*_summary.csv' > filelist.txt
tar -czvf permutations_GO_merged_SOFT_summary_files.tar.gz -T filelist.txt
find ./ -maxdepth 1 -type f -name 'permutations_GO_merged_SOFT_*_summary.csv' -exec rm -v {} \;

```

```{shell}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission
scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class/go_hard.tsv ./

```

### Hard

* Hard permutations via Bunya

```{r, Hard}

library(tidyverse)
library(data.table)
library(qvalue)
# library(foreach)
# library(doSNOW)

## Import data

# already used
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")

# already used
ffs <- list.files(
  pattern = 'predPeaks_permutation_HARD_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_.*.tsv', 
  recursive = TRUE)
permutations <- rbindlist(lapply(ffs, fread))

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

# already used
GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv") %>% rename(go = go_id)

## Join GO terms for predictions peaks

# # already done
# predPeaks_temp <- predPeaks %>% 
#   dplyr::select(CHROM, start, end, pred_ensemble, length)
# 
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
# system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "intersect")
predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)


# already done
## Join GO terms to permutations

permutations_temp <- permutations %>%
  dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length)
fwrite(permutations_temp, "permutations_temp_HARD_1-500k.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp_HARD_1-500k.tsv -wo > permutations_GO_merged_Hard.tsv")
system(cmd)

permutationsGO_Hard <- fread("permutations_GO_merged_Hard.tsv", header=FALSE)
colnames(permutationsGO_Hard) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")

# NOT ENOUGH RAM
# permutationsGO_Hard <- permutationsGO_Hard %>% 
#     left_join(GO_geneName, relationship = "many-to-many") %>% 
#     arrange(scf_perm, start_pos_perm, end_pos_perm)

rm(permutations)
gc()

setDT(permutationsGO_Hard)
setDT(GO_geneName)

for (i in 1:500000) {
  cat(i, "\n")
  temp <- permutationsGO_Hard[perm == i][GO_geneName, on = .(fbgn), nomatch = 0]
  setorder(temp, scf_perm, start_pos_perm, end_pos_perm)
  fileName <- paste("permutations_GO_merged_HARD_", i, ".csv", sep="")
  fwrite(temp, fileName, row.names = FALSE, quote = FALSE, col.names = TRUE)
}

rm(permutationsGO_Hard)
gc()



## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

# already done
## Permutation summary

# permutationsGO_summary <- permutationsGO_Hard %>%
#   group_by(perm, val, go, process) %>%
#   summarise(intersect_perm = sum(intersect)) %>%
#   ungroup()

# ffs <- list.files(pattern = '*.csv', recursive = TRUE)
all_csv_files <- list.files(pattern = '\\.csv$', recursive = TRUE)
ffs <- all_csv_files[!grepl('_summary\\.csv$', all_csv_files)]
length(ffs)
length(ffs[grepl('_HARD_', ffs)])

count <- 1

for (ff in ffs) {

  # cat(count, "Processing", ff, "\n")
  temp <- fread(ff)
  temp_summary <- temp %>%
    group_by(perm, val, go, process) %>%
    summarise(intersect_perm = sum(intersect)) %>%
    ungroup()
  fileName <- str_replace(ff, '.csv', '_summary.csv')
  fwrite(temp_summary, file=fileName, row.names=FALSE, col.names=TRUE, quote=FALSE)
  count <- count + 1

}

ffs <- list.files(pattern = '*_summary.csv', recursive = TRUE)
length(ffs)
permutationsGO_summary <- rbindlist(lapply(ffs, fread))

fwrite(permutationsGO_summary,
       file="permutations_GO_merged_HARD_summary_1-500k.csv",
       row.names=FALSE,
       col.names=TRUE,
       quote=FALSE,
       sep=",")

permutationsGO_summary <- fread("permutations_GO_merged_SOFT_summary_1-500k.csv", sep=",")
# 
# # Permutation test
# 
# ## Intersect
# 
intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# # intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))
# 
# ### Check that intersect is never greater than one

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/500000,
            p_proportion = sum(test_proportion)/500000)

# Biological process (Hard)

## Intersect

permTest_intersect_Hard <- permTest_intersect %>% 
  filter(val=="Hard") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_Hard %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("Hard")

permTest_intersect_Hard$fdr <- p.adjust(permTest_intersect_Hard$p_intersect, method="fdr")
permTest_intersect_Hard$q <- qvalue(permTest_intersect_Hard$p_intersect)$qvalues

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(q <= 0.1)

results <- permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean)

write_tsv(results, file="go_hard.tsv")

```

```{bash, compress and store files}

find ./ -maxdepth 1 -type f -regex './permutations_GO_merged_HARD_[0-9]+.csv' > filelist.txt
tar -czvf permutations_GO_merged_HARD_files.tar.gz -T filelist.txt
find ./ -maxdepth 1 -type f -regex './permutations_GO_merged_HARD_[0-9]+.csv' -exec rm -v {} \;

find ./ -maxdepth 1 -type f -name 'permutations_GO_merged_HARD_*_summary.csv' > filelist.txt
tar -czvf permutations_GO_merged_HARD_summary_files.tar.gz -T filelist.txt
find ./ -maxdepth 1 -type f -name 'permutations_GO_merged_HARD_*_summary.csv' -exec rm -v {} \;

```

```{shell}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission
scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class/go_hard.tsv ./

```


### HardPartial

* HardPartial permutations via permutation_HARDPARTIAL.R

```{bash, compress and store files}

find ./ -maxdepth 1 -type f -name 'permutation_HARDPARTIAL_*:*.R' > filelist.txt
tar -czvf permutation_HARDPARTIAL_R_files.tar.gz -T filelist.txt
find ./ -maxdepth 1 -type f -name 'permutation_HARDPARTIAL_*:*.R' -exec rm -v {} \;

find ./ -maxdepth 1 -type f -name 'permutation_HARDPARTIAL_*:*.slurm.sh' > filelist.txt
tar -czvf permutation_HARDPARTIAL_slurm_files.tar.gz -T filelist.txt
find ./ -maxdepth 1 -type f -name 'permutation_HARDPARTIAL_*:*.slurm.sh' -exec rm -v {} \;

find ./ -maxdepth 1 -type f -name 'predPeaks_permutation_HARDPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_*:*.tsv' > filelist.txt
tar -czvf permutation_HARDPARTIAL_tsv_files.tar.gz -T filelist.txt
find ./ -maxdepth 1 -type f -name 'predPeaks_permutation_HARDPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_*:*.tsv' -exec rm -v {} \;

```

```{r, Hard}

library(tidyverse)
library(data.table)
library(qvalue)
# library(foreach)
# library(doSNOW)

## Import data

# already used
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")

# already used
ffs <- list.files(
  pattern = 'predPeaks_permutation_HARDPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_.*.tsv', 
  recursive = TRUE)
permutations <- rbindlist(lapply(ffs, fread))

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

# already used
GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv") %>% rename(go = go_id)

## Join GO terms for predictions peaks

# # already done
# predPeaks_temp <- predPeaks %>% 
#   dplyr::select(CHROM, start, end, pred_ensemble, length)
# 
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
# system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "merged", "intersect")
predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)


# already done
## Join GO terms to permutations

permutations_temp <- permutations %>%
  dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length)
fwrite(permutations_temp, "permutations_temp_HARD_1-500k.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp_HARD_1-500k.tsv -wo > permutations_GO_merged_HardPartial.tsv")
system(cmd)

permutationsGO_HardPartial <- fread("permutations_GO_merged_HardPartial.tsv", header=FALSE)
colnames(permutationsGO_HardPartial) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")

permutationsGO_HardPartial <- permutationsGO_HardPartial %>%
    left_join(GO_geneName, relationship = "many-to-many") %>%
    arrange(scf_perm, start_pos_perm, end_pos_perm)

# rm(permutations)
# gc()
# 
# setDT(permutationsGO_Hard)
# setDT(GO_geneName)
# 
# for (i in 1:500000) {
#   cat(i, "\n")
#   temp <- permutationsGO_Hard[perm == i][GO_geneName, on = .(fbgn), nomatch = 0]
#   setorder(temp, scf_perm, start_pos_perm, end_pos_perm)
#   fileName <- paste("permutations_GO_merged_HARD_", i, ".csv", sep="")
#   fwrite(temp, fileName, row.names = FALSE, quote = FALSE, col.names = TRUE)
# }
# 
# rm(permutationsGO_Hard)
# gc()



## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

# already done
## Permutation summary

permutationsGO_summary <- permutationsGO_HardPartial %>%
  group_by(perm, val, go, process) %>%
  summarise(intersect_perm = sum(intersect)) %>%
  ungroup()

# # ffs <- list.files(pattern = '*.csv', recursive = TRUE)
# all_csv_files <- list.files(pattern = '\\.csv$', recursive = TRUE)
# ffs <- all_csv_files[!grepl('_summary\\.csv$', all_csv_files)]
# length(ffs)
# length(ffs[grepl('_HARD_', ffs)])
# 
# count <- 1
# 
# for (ff in ffs) {
# 
#   # cat(count, "Processing", ff, "\n")
#   temp <- fread(ff)
#   temp_summary <- temp %>%
#     group_by(perm, val, go, process) %>%
#     summarise(intersect_perm = sum(intersect)) %>%
#     ungroup()
#   fileName <- str_replace(ff, '.csv', '_summary.csv')
#   fwrite(temp_summary, file=fileName, row.names=FALSE, col.names=TRUE, quote=FALSE)
#   count <- count + 1
# 
# }
# 
# ffs <- list.files(pattern = '*_summary.csv', recursive = TRUE)
# length(ffs)
# permutationsGO_summary <- rbindlist(lapply(ffs, fread))
# 
# fwrite(permutationsGO_summary,
#        file="permutations_GO_merged_HARD_summary_1-500k.csv",
#        row.names=FALSE,
#        col.names=TRUE,
#        quote=FALSE,
#        sep=",")
# 
# permutationsGO_summary <- fread("permutations_GO_merged_SOFT_summary_1-500k.csv", sep=",")
# 
# # Permutation test
# 
# ## Intersect
# 
intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# # intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))
# 
# ### Check that intersect is never greater than one

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/500000,
            p_proportion = sum(test_proportion)/500000)

# Biological process (Hard)

## Intersect

permTest_intersect_Hard <- permTest_intersect %>% 
  filter(val=="HardPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_Hard %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("Hard")

permTest_intersect_Hard$fdr <- p.adjust(permTest_intersect_Hard$p_intersect, method="fdr")
# permTest_intersect_Hard$q <- qvalue(permTest_intersect_Hard$p_intersect, lambda=0)$qvalues
permTest_intersect_Hard$q <- qvalue(c(permTest_intersect_Hard$p_intersect, 0.95, 0.95))$qvalues[1:304]

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(q <= 0.05)

results <- permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean)

write_tsv(results, file="go_hardpartial.tsv")

```

```{shell}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission
scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class/go_hardpartial.tsv ./

```

```{r, Hard OLD}

library(tidyverse)
library(data.table)
library(qvalue)
# library(foreach)
# library(doSNOW)

## Import data

# already used
setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")

# already used
ffs <- list.files(
  pattern = 'predPeaks_permutation_HARD_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_.*.tsv', 
  recursive = TRUE)
permutations <- rbindlist(lapply(ffs, fread))

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

# already used
GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv") %>% rename(go = go_id)

## Join GO terms for predictions peaks

# # already done
# predPeaks_temp <- predPeaks %>% 
#   dplyr::select(CHROM, start, end, pred_ensemble, length)
# 
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
# system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "intersect")
predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)


# already done
## Join GO terms to permutations

permutations_temp <- permutations %>%
  dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length)
fwrite(permutations_temp, "permutations_temp_HARD_1-500k.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp_HARD_1-500k.tsv -wo > permutations_GO_merged_Hard.tsv")
system(cmd)

permutationsGO_Hard <- fread("permutations_GO_merged_Hard.tsv", header=FALSE)
colnames(permutationsGO_Hard) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")

# NOT ENOUGH RAM
# permutationsGO_Hard <- permutationsGO_Hard %>% 
#     left_join(GO_geneName, relationship = "many-to-many") %>% 
#     arrange(scf_perm, start_pos_perm, end_pos_perm)

rm(permutations)
gc()

setDT(permutationsGO_Hard)
setDT(GO_geneName)

for (i in 1:500000) {
  cat(i, "\n")
  temp <- permutationsGO_Hard[perm == i][GO_geneName, on = .(fbgn), nomatch = 0]
  setorder(temp, scf_perm, start_pos_perm, end_pos_perm)
  fileName <- paste("permutations_GO_merged_HARD_", i, ".csv", sep="")
  fwrite(temp, fileName, row.names = FALSE, quote = FALSE, col.names = TRUE)
}

rm(permutationsGO_Hard)
gc()



## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

# already done
## Permutation summary

# permutationsGO_summary <- permutationsGO_Hard %>%
#   group_by(perm, val, go, process) %>%
#   summarise(intersect_perm = sum(intersect)) %>%
#   ungroup()

# ffs <- list.files(pattern = '*.csv', recursive = TRUE)
all_csv_files <- list.files(pattern = '\\.csv$', recursive = TRUE)
ffs <- all_csv_files[!grepl('_summary\\.csv$', all_csv_files)]
length(ffs)
length(ffs[grepl('_HARD_', ffs)])

count <- 1

for (ff in ffs) {

  # cat(count, "Processing", ff, "\n")
  temp <- fread(ff)
  temp_summary <- temp %>%
    group_by(perm, val, go, process) %>%
    summarise(intersect_perm = sum(intersect)) %>%
    ungroup()
  fileName <- str_replace(ff, '.csv', '_summary.csv')
  fwrite(temp_summary, file=fileName, row.names=FALSE, col.names=TRUE, quote=FALSE)
  count <- count + 1

}

ffs <- list.files(pattern = '*_summary.csv', recursive = TRUE)
length(ffs)
permutationsGO_summary <- rbindlist(lapply(ffs, fread))

fwrite(permutationsGO_summary,
       file="permutations_GO_merged_HARD_summary_1-500k.csv",
       row.names=FALSE,
       col.names=TRUE,
       quote=FALSE,
       sep=",")

permutationsGO_summary <- fread("permutations_GO_merged_SOFT_summary_1-500k.csv", sep=",")
# 
# # Permutation test
# 
# ## Intersect
# 
intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# # intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))
# 
# ### Check that intersect is never greater than one
# 
# permutationsGO_summary %>% 
#   left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
#   filter(is.na(intersect_empirical)==FALSE) %>% 
#   left_join(intersect_genome) %>% 
#   # left_join(intersect_Hard) %>% 
#   mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
#   mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
#   select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
#   arrange(desc(intersect_proportion_perm))
# 
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm <= 50000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 50000 & perm <= 100000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 100000 & perm <= 150000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 150000 & perm <= 200000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 200000 & perm <= 250000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 250000 & perm <= 300000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 300000 & perm <= 350000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 350000 & perm <= 400000)
# # permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 400000 & perm <= 450000)
# permutationsGO_summary_subset <- permutationsGO_summary %>% filter(perm > 450000 & perm <= 500000)
# 
# permTest_intersect <- permutationsGO_summary_subset %>%
#   left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
#   filter(is.na(intersect_empirical)==FALSE) %>%
#   left_join(intersect_genome) %>%
#   mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
#   mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
#   mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
#   mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
#   mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
#   mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
#   group_by(val, go, process) # %>%
#   # summarise(p_intersect = sum(test_intersect)/500000,
#   #           p_proportion = sum(test_proportion)/500000)
# 
# fwrite(permTest_intersect, 
#        file="permTest_intersect_Hard_450-500k.csv", 
#        row.names=FALSE, 
#        col.names=TRUE, 
#        quote=FALSE, 
#        sep=",")
# 
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Hard_1-50k.csv > permTest_intersect_Hard_1-50k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Hard_50-100k.csv > permTest_intersect_Hard_50-100k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Hard_100-150k.csv > permTest_intersect_Hard_100-150k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Hard_150-200k.csv > permTest_intersect_Hard_150-200k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Hard_200-250k.csv > permTest_intersect_Hard_200-250k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Hard_250-300k.csv > permTest_intersect_Hard_250-300k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Hard_300-350k.csv > permTest_intersect_Hard_300-350k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Hard_350-400k.csv > permTest_intersect_Hard_350-400k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Hard_400-450k.csv > permTest_intersect_Hard_400-450k_minimumInfor.csv
# cut -d ',' -f 1,2,3,4,11 permTest_intersect_Hard_450-500k.csv > permTest_intersect_Hard_450-500k_minimumInfor.csv

ffs <- list.files(pattern = '.*_minimumInfor.csv', recursive = TRUE)
length(ffs)
permTest_intersect <- rbindlist(lapply(ffs, fread))

permTest_intersect <- permTest_intersect %>% 
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/500000)

# Biological process (Hard)

## Intersect

permTest_intersect_Hard <- permTest_intersect %>% 
  filter(val=="Hard") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_Hard %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("Hard")

permTest_intersect_Hard$fdr <- p.adjust(permTest_intersect_Hard$p_intersect, method="fdr")
permTest_intersect_Hard$q <- qvalue(permTest_intersect_Hard$p_intersect)$qvalues

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(q <= 0.05) %>% 
  print(n=40)

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(go, fdr, name_1006) %>% 
  filter(fdr <= 0.05) %>% 
  rename(p_adjusted = fdr, go_name = name_1006) %>% 
  print(n=70)

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p_proportion))) + 
  geom_point()

```

### SoftPartial

* SoftPartial permutations via permutation_HARD.R

```{bash, compress and store files}

find ./ -maxdepth 1 -type f -name 'permutation_SOFTPARTIAL_*:*.R' > filelist.txt
tar -czvf permutation_SOFTPARTIAL_R_files.tar.gz -T filelist.txt
find ./ -maxdepth 1 -type f -name 'permutation_SOFTPARTIAL_*:*.R' -exec rm -v {} \;

find ./ -maxdepth 1 -type f -name 'permutation_SOFTPARTIAL_*:*.slurm.sh' > filelist.txt
tar -czvf permutation_SOFTPARTIAL_slurm_files.tar.gz -T filelist.txt
find ./ -maxdepth 1 -type f -name 'permutation_SOFTPARTIAL_*:*.slurm.sh' -exec rm -v {} \;

find ./ -maxdepth 1 -type f -name 'predPeaks_permutation_SOFTPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_*:*.tsv' > filelist.txt
tar -czvf permutation_SOFTPARTIAL_tsv_files.tar.gz -T filelist.txt
find ./ -maxdepth 1 -type f -name 'predPeaks_permutation_SOFTPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_*:*.tsv' -exec rm -v {} \;

```

```{r, Hard}

library(tidyverse)
library(data.table)
library(qvalue)
# library(foreach)
# library(doSNOW)

## Import data

# # already used
# setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")
# predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")

# # already used
# ffs <- list.files(
#   pattern = 'predPeaks_permutation_SOFTPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_.*.tsv', 
#   recursive = TRUE)
# permutations <- rbindlist(lapply(ffs, fread))

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

# already used
GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv") %>% rename(go = go_id)

## Join GO terms for predictions peaks

# # already done
# predPeaks_temp <- predPeaks %>% 
#   dplyr::select(CHROM, start, end, pred_ensemble, length)
# 
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
# system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "merged", "intersect")
predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)


# # already done
# ## Join GO terms to permutations
# 
# permutations_temp <- permutations %>%
#   dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length)
# fwrite(permutations_temp, "permutations_temp_SOFTPARTIAL_1-500k.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp_SOFTPARTIAL_1-500k.tsv -wo > permutations_GO_merged_SoftPartial.tsv")
# system(cmd)
# 
# permutationsGO_HardPartial <- fread("permutations_GO_merged_SoftPartial.tsv", header=FALSE)
# colnames(permutationsGO_HardPartial) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")
# 
# permutationsGO_HardPartial <- permutationsGO_HardPartial %>%
#     left_join(GO_geneName, relationship = "many-to-many") %>%
#     arrange(scf_perm, start_pos_perm, end_pos_perm)
# 
# rm(permutations)
# gc()
# 
# setDT(permutationsGO_HardPartial)
# setDT(GO_geneName)
# 
# for (i in 1:500000) {
#   # cat(i, "\n")
#   temp <- permutationsGO_HardPartial[perm == i][GO_geneName, on = .(fbgn), nomatch = 0]
#   setorder(temp, scf_perm, start_pos_perm, end_pos_perm)
#   fileName <- paste("permutations_GO_merged_SOFTPARTIAL_", i, ".csv", sep="")
#   fwrite(temp, fileName, row.names = FALSE, quote = FALSE, col.names = TRUE)
# }
# 
# rm(permutationsGO_HardPartial)
# gc()



## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

# already done
## Permutation summary

# permutationsGO_summary <- permutationsGO_HardPartial %>%
#   group_by(perm, val, go, process) %>%
#   summarise(intersect_perm = sum(intersect)) %>%
#   ungroup()
# 
# # ffs <- list.files(pattern = '*.csv', recursive = TRUE)
# all_csv_files <- list.files(pattern = '\\.csv$', recursive = TRUE)
# ffs <- all_csv_files[!grepl('_summary\\.csv$', all_csv_files)]
# ffs <- ffs[grepl('_SOFTPARTIAL_', ffs)]
# length(ffs)
# length(ffs[grepl('_SOFTPARTIAL_', ffs)])
# 
# count <- 1
# 
# for (ff in ffs) {
# 
#   # cat(count, "Processing", ff, "\n")
#   temp <- fread(ff)
#   temp_summary <- temp %>%
#     group_by(perm, val, go, process) %>%
#     summarise(intersect_perm = sum(intersect)) %>%
#     ungroup()
#   fileName <- str_replace(ff, '.csv', '_summary.csv')
#   fwrite(temp_summary, file=fileName, row.names=FALSE, col.names=TRUE, quote=FALSE)
#   # count <- count + 1
# 
# }
# 
# ffs <- list.files(pattern = '*_summary.csv', recursive = TRUE)
# length(ffs)
# permutationsGO_summary <- rbindlist(lapply(ffs, fread))
# 
# fwrite(permutationsGO_summary,
#        file="permutations_GO_merged_SOFTPARTIAL_summary_1-500k.csv",
#        row.names=FALSE,
#        col.names=TRUE,
#        quote=FALSE,
#        sep=",")

permutationsGO_summary <- fread("permutations_GO_merged_SOFTPARTIAL_summary_1-500k.csv", sep=",")
# 
# # Permutation test
# 
# ## Intersect
# 
intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# # intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))
# 
# ### Check that intersect is never greater than one

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/500000,
            p_proportion = sum(test_proportion)/500000)

# Biological process (Hard)

## Intersect

permTest_intersect_Hard <- permTest_intersect %>% 
  filter(val=="SoftPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_Hard %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("SoftPartial")

permTest_intersect_Hard$fdr <- p.adjust(permTest_intersect_Hard$p_intersect, method="fdr")
# permTest_intersect_Hard$q <- qvalue(permTest_intersect_Hard$p_intersect)$qvalues
# permTest_intersect_Hard$q <- qvalue(permTest_intersect_Hard$p_intersect, lambda=0)$qvalues
permTest_intersect_Hard$q <- qvalue(c(permTest_intersect_Hard$p_intersect, 0.95, 0.95))$qvalues[1:536]

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(q <= 0.3) %>% 
  print(n=208)

results <- permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean)

write_tsv(results, file="go_softpartial.tsv")

```

```{shell}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission
scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class/go_softpartial.tsv ./

```

```{r, OLD}

library(tidyverse)
library(data.table)
library(qvalue)

## Import data

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/snpEff")
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_SIFT")
ffs <- list.files(pattern = '*_2ndAttempt.tsv', recursive = TRUE)
permutations <- rbindlist(lapply(ffs, fread))
setorder(permutations, perm, CHROM, start)

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>% 
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv") %>% rename(go = go_id)



## Join GO terms for predictions peaks

predPeaks_temp <- permutations %>% 
  filter(perm==1) %>% 
  dplyr::select(CHROM, start, end, pred_ensemble, length)

write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "intersect")
predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)



## Join GO terms to permutations

permutations_temp <- permutations %>% 
  dplyr::select(scf_perm, start_pos_perm, end_pos_perm, perm, pred_ensemble, length)

write.table(permutations_temp, "permutations_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp.tsv -wo > permutations_GO_merged.tsv")
system(cmd)

permutationsGO <- fread("permutations_GO_merged.tsv", header=FALSE) 
colnames(permutationsGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")

sweep_classes <- c("Hard", "HardPartial", "Soft", "SoftPartial")

for(sweep_class in sweep_classes) {
  temp <- permutationsGO %>% 
    filter(val==sweep_class) %>% 
    left_join(GO_geneName, relationship = "many-to-many") %>% 
    arrange(scf_perm, start_pos_perm, end_pos_perm)
  fileName <- paste("permutations_GO_merged_", sweep_class, ".csv", sep="")
  fwrite(temp, fileName, row.names=FALSE, quote=FALSE, col.names=TRUE)
}

rm(permutationsGO)
gc()



# Hard


permutationsGO_hard <- fread("permutations_GO_merged_Hard.csv", sep=",", header=TRUE)


## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_hard %>% 
  group_by(perm, val, go, process) %>% 
  summarise(intersect_perm = sum(intersect)) %>% 
  ungroup()



# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))

### Check that intersect is never greater than one

permutationsGO_summary %>% 
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
  filter(is.na(intersect_empirical)==FALSE) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_Hard) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
  arrange(desc(intersect_proportion_perm))

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/20000,
            p_proportion = sum(test_proportion)/20000)



# Biological process (Hard)

## Intersect

permTest_intersect_Hard <- permTest_intersect %>% 
  filter(val=="Hard") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_Hard %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("Hard")

permTest_intersect_Hard$fdr <- p.adjust(permTest_intersect_Hard$p_proportion, method="fdr")
permTest_intersect_Hard$q <- qvalue(permTest_intersect_Hard$p_proportion)$qvalues

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -q, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(fdr <= 0.2) %>% 
  print(n=56)

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(go, fdr, name_1006) %>% 
  filter(fdr <= 0.2) %>% 
  rename(p_adjusted = fdr, go_name = name_1006) %>% 
  head(n=61) %>% 
  as.data.frame()

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p_proportion))) + 
  geom_point()



# Soft


permutationsGO_soft <- fread("permutations_GO_merged_Soft.csv", sep=",", header=TRUE)


## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_soft %>% 
  group_by(perm, val, go, process) %>% 
  summarise(intersect_perm = sum(intersect)) %>% 
  ungroup()



# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))

### Check that intersect is never greater than one

permutationsGO_summary %>% 
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
  filter(is.na(intersect_empirical)==FALSE) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_Hard) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
  arrange(desc(intersect_proportion_perm))

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/20000,
            p_proportion = sum(test_proportion)/20000)

# Biological process (Soft)

## Intersect

permTest_intersect_Soft <- permTest_intersect %>% 
  filter(val=="Soft") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_Soft %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("Soft")

permTest_intersect_Soft$fdr <- p.adjust(permTest_intersect_Soft$p_proportion, method="fdr")
permTest_intersect_Soft$q <- qvalue(permTest_intersect_Soft$p_proportion)$qvalues

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="Soft") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="Soft") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -q, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(fdr <= 0.3) %>% 
  print(n=9)

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(go, fdr, name_1006) %>% 
  filter(fdr <= 0.3) %>% 
  rename(p_adjusted = fdr, go_name = name_1006) %>% 
  head(n=9) %>% 
  as.data.frame()

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p_proportion))) + 
  geom_point()



# HardPartial


permutationsGO_HardPartial <- fread("permutations_GO_merged_HardPartial.csv", sep=",", header=TRUE)


## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_HardPartial %>% 
  group_by(perm, val, go, process) %>% 
  summarise(intersect_perm = sum(intersect)) %>% 
  ungroup()



# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))

### Check that intersect is never greater than one

permutationsGO_summary %>% 
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
  filter(is.na(intersect_empirical)==FALSE) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_Hard) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
  arrange(desc(intersect_proportion_perm))

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/20000,
            p_proportion = sum(test_proportion)/20000)

# Biological process (HardPartial)

## Intersect

permTest_intersect_HardPartial <- permTest_intersect %>% 
  filter(val=="HardPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_HardPartial %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("HardPartial")

permTest_intersect_HardPartial$fdr <- p.adjust(permTest_intersect_HardPartial$p_proportion, method="fdr")
# permTest_intersect_HardPartial$q <- qvalue(permTest_intersect_HardPartial$p_proportion)$qvalues

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="HardPartial") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="HardPartial") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(fdr <= 0.05) %>% 
  print(n=70)

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(go, fdr, name_1006) %>% 
  filter(fdr <= 0.05) %>% 
  rename(p_adjusted = fdr, go_name = name_1006) %>% 
  print(n=70)

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p_proportion))) + 
  geom_point()



# SoftPartial


permutationsGO_SoftPartial <- fread("permutations_GO_merged_SoftPartial.csv", sep=",", header=TRUE)


## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_SoftPartial %>% 
  group_by(perm, val, go, process) %>% 
  summarise(intersect_perm = sum(intersect)) %>% 
  ungroup()



# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))

### Check that intersect is never greater than one

permutationsGO_summary %>% 
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
  filter(is.na(intersect_empirical)==FALSE) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_Hard) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
  arrange(desc(intersect_proportion_perm))

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/20000,
            p_proportion = sum(test_proportion)/20000)

# Biological process

## Intersect

permTest_intersect_SoftPartial <- permTest_intersect %>% 
  filter(val=="SoftPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_SoftPartial %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("SoftPartial")

permTest_intersect_SoftPartial$fdr <- p.adjust(permTest_intersect_SoftPartial$p_proportion, method="fdr")
# permTest_intersect_HardPartial$q <- qvalue(permTest_intersect_HardPartial$p_proportion)$qvalues

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="SoftPartial") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="SoftPartial") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_SoftPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(fdr <= 0.05) %>% 
  print(n=70)

permTest_intersect_SoftPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(go, fdr, name_1006) %>% 
  filter(fdr <= 0.05) %>% 
  rename(p_adjusted = fdr, go_name = name_1006) %>% 
  print(n=70)

permTest_intersect_SoftPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p_proportion))) + 
  geom_point()

```

### Hard

```{r, OLD}

library(tidyverse)
library(qvalue)
library(data.table)
library(foreach)
library(doSNOW)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

## Import data

predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>% 
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("go_info.tsv") %>% rename(go = go_id)

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

predictions <- data.table()

for(ff in preFiles){
    tmp <- fread(ff, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    predictions <- rbind(predictions, tmp)
}

predictions <- predictions %>% 
  set_names(c("scf", "start_pos", "end_pos", "val", "V5", "V6", "V7", "V8", "V9"))

## Expand predPeaks to 5kb resolution

# predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length, win_seq)
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(predictions, "predictions_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# cmd <- paste0("bedtools intersect -a predictions_temp.tsv -b predPeaks_temp.tsv -wo > predPeaks_5kb.tsv")
# system(cmd)

predPeaks_5kb <- read_tsv("predPeaks_5kb.tsv", col_names=FALSE)

colnames(predPeaks_5kb) <- c("scf", "start_pos", "end_pos", "val", "X5", "X6", 
                            "X7", "X8", "X9", "scf_peak", "start_pos_peak", 
                            "end_pos_peak", "X13", "len", "length", "win_seq", 
                            "intersect")

predPeaks_5kb <- predPeaks_5kb %>% select(-X5:-X9, -X13)

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))
predPeaks_Hard <- predPeaks %>% filter(val=="Hard") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER) %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(scf, start_pos, end_pos, win_seq)

peak_start_end <- predPeaks_5kb %>% group_by(scf, win_seq) %>% summarise(start_pos_peak = min(start_pos_peak), end_pos_peak = max(end_pos_peak))

# RUN VIA permutation_GO_Hard_100000.R

# num_cores <- 38
# cl <- makeCluster(num_cores)
# registerDoSNOW(cl)
# 
# permutations <- foreach(j = 1:10000, .combine = "rbind", .packages = c("dplyr")) %dopar% {
#   shuffled_data <- predPeaks_Hard
#   scf_start_temp <- scf_start
#   for (i in 1:nrow(shuffled_data)) {
#     # Get peak
#     peak <- shuffled_data[i,]
#     # Move peak to a random genomic location
#     selected_scf_start <- sample_n(scf_start_temp, 1)
#     peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#     end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#     # Make sure the new genomic location is valid
#     # Check if the new genomic location is within a prediction peak or the new end_pos is still available
#     max_attempts <- 21495
#     while((selected_scf_start$end_pos > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
#       # Move peak to a random genomic location
#       selected_scf_start <- sample_n(scf_start_temp, 1)
#       peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#       end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#       # Decrement attempts counter
#       max_attempts <- max_attempts - 1
#     }
#     # Break if max_attempts
#     if (max_attempts == 0) {
#       cat("Failed to find valid random location.\n")
#       shuffled_data <- NULL
#       break
#     }
#     # Remove the randomly selected location from the pool
#     region <- scf_start_temp %>%
#       filter(scf==selected_scf_start$scf) %>%
#       filter(start_pos >= selected_scf_start$start_pos & end_pos <= selected_scf_start$end_pos)
#     scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
#     # Add random location to predPeaks
#     shuffled_data$perm[i] <- j
#     shuffled_data$scf_perm[i] <- selected_scf_start$scf
#     shuffled_data$start_pos_perm[i] <- selected_scf_start$start_pos
#     shuffled_data$end_pos_perm[i] <- selected_scf_start$end_pos
#   }
#   shuffled_data
# }
# 
# stopCluster(cl)
# 
# 
# 
# # Save file
# fileName <- "predPeaks_permutation_Hard_100000_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv"
# write_tsv(permutations, file=fileName)

permutations <- fread("predPeaks_permutation_Hard_100000_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

# ## Join GO terms for predictions peaks
# 
# predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length)
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
# system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "len", "length", "intersect")
predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

# ## Join GO terms to permutations
# 
# permutations_temp <- permutations %>% dplyr::select(scf_perm, start_pos_perm, end_pos_perm, perm, val, len, length)
# write.table(permutations_temp, "permutations_Hard_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b permutations_Hard_temp.tsv -wo > permutations_Hard_GO_merged.tsv")
# system(cmd)

permutationsGO_Hard <- fread("permutations_Hard_GO_merged.tsv", header=FALSE) 
colnames(permutationsGO_Hard) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "len", "length", "intersect")
permutationsGO_Hard <- permutationsGO_Hard %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_perm, start_pos_perm, end_pos_perm)

## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_Hard %>% 
  group_by(perm, val, go, process) %>% 
  summarise(intersect_perm = sum(intersect)) %>% 
  ungroup()

# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))

### Check that intersect is never greater than one

permutationsGO_summary %>% 
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
  filter(is.na(intersect_empirical)==FALSE) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_Hard) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
  arrange(desc(intersect_proportion_perm))

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>% 
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>% 
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/100000, 
            p_proportion = sum(test_proportion)/100000)

### Intersect null

# Load required libraries
library(data.table)
library(doParallel)

# Set the number of cores to use
num_cores <- 38 # Adjust this based on your machine's capabilities

# Register a parallel cluster
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Convert permutationsGO_summary to a data.table
setDT(permutationsGO_summary, key = "perm")

# Create a parallelized function
perm_p_parallel <- foreach(i = 1:100000, .packages = c("data.table")) %dopar% {
  cat(i, "\n")
  
  # Get focal permutation
  permutationsGO_summary_1 <- permutationsGO_summary[perm == i, .(intersect_perm_1 = intersect_perm), keyby = .(val, go)]
  
  # Calculate p-values
  permTest_intersect_null <- permutationsGO_summary[perm != i, ][permutationsGO_summary_1, 
    on = .(val, go)][!is.na(intersect_perm_1)][, test := intersect_perm >= intersect_perm_1][
    , .(p = sum(test) / 99999), by = .(val, go, process)]
  
  # Get results
  return(permTest_intersect_null$p)
}

# Stop the parallel cluster
stopCluster(cl)

# Convert the list of p-values to a data frame if needed
output <- as.data.frame(matrix(unlist(perm_p_parallel), ncol = 10, byrow = TRUE))

# Biological process

## Intersect

permTest_intersect_Hard <- permTest_intersect %>% 
  filter(val=="Hard") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_proportion)

permTest_intersect_Hard %>% ggplot(aes(x=p_proportion)) + geom_histogram(bins=50) + ggtitle("Hard")

permTest_intersect_Hard$fdr <- p.adjust(permTest_intersect_Hard$p_proportion, method="fdr")
permTest_intersect_Hard$q <- qvalue(permTest_intersect_Hard$p_proportion)$qvalues

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -q, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(fdr <= 0.05) %>% 
  head(n=68)

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(go, fdr, name_1006) %>% 
  filter(fdr <= 0.05) %>% 
  rename(p_adjusted = fdr, go_name = name_1006) %>% 
  head(n=68) %>% 
  as.data.frame()

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p_proportion))) + 
  geom_point()

# min P

library(NRejections)

p <- permTest_intersect_Hard$p_proportion



## Save

result_Hard <- permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -q, -intersect_genome, -intersect_empirical, -intersect_perm_mean)

write_csv(result_Hard, file="GO_enrichment_Hard.csv")

result_Hard <- read_csv("GO_enrichment_Hard.csv")

# ## scp
# 
# cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission
# scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/GO_enrichment_Hard.csv ./

# ## Null
# 
# permTest_intersect_Hard_null <- permTest_intersect_null %>% 
#   filter(val=="Hard") %>% 
#   filter(process.y=="P") %>% 
#   distinct() %>% 
#   arrange(p)
# 
# permTest_intersect_Hard_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("Hard (null)")
# 
# permTest_intersect_Hard_null$fdr <- p.adjust(permTest_intersect_Hard_null$p, method="fdr")
# #permTest_intersect_Hard_null$q <- qvalue(permTest_intersect_Hard_null$p)$qvalues
# 
# permTest_intersect_Hard_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   as.data.frame()
# 
# intersect_genome <- permutationsGO %>% filter(perm==1) %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_Hard <- permutationsGO %>% filter(perm==1) %>% filter(val_perm=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))
# 
# permTest_intersect_Hard_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_Hard) %>% 
#   as.data.frame()
# 
# permTest_intersect_Hard_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_Hard) %>% 
#   ggplot(aes(x=intersect_genome, y=1-log10(p))) + 
#   geom_point()
# 
# permTest_intersect_Hard_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_Hard) %>% 
#   ggplot(aes(x=p)) + 
#   geom_histogram()
# 
# permTest_intersect_Hard_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_Hard) %>% 
#   filter(intersect_genome>50000) %>% 
#   ggplot(aes(x=p)) + 
#   geom_histogram()

```

### Soft

```{r, OLD}

# library(tidyverse)
library(dplyr)
library(tidyr)
library(readr)
library(rlang)
library(magrittr)
library(ggplot2)
library(qvalue)
library(data.table)
library(foreach)
# library(doParallel)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

## Import data

predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>% 
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("go_info.tsv") %>% rename(go = go_id)

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

predictions <- data.table()

for(ff in preFiles){
    tmp <- fread(ff, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    predictions <- rbind(predictions, tmp)
}

predictions <- predictions %>% 
  set_names(c("scf", "start_pos", "end_pos", "val", "V5", "V6", "V7", "V8", "V9"))

## Expand predPeaks to 5kb resolution

# predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length, win_seq)
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(predictions, "predictions_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# cmd <- paste0("bedtools intersect -a predictions_temp.tsv -b predPeaks_temp.tsv -wo > predPeaks_5kb.tsv")
# system(cmd)

predPeaks_5kb <- read_tsv("predPeaks_5kb.tsv", col_names=FALSE)

colnames(predPeaks_5kb) <- c("scf", "start_pos", "end_pos", "val", "X5", "X6", 
                            "X7", "X8", "X9", "scf_peak", "start_pos_peak", 
                            "end_pos_peak", "X13", "len", "length", "win_seq", 
                            "intersect")

predPeaks_5kb <- predPeaks_5kb %>% select(-X5:-X9, -X13)

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))
predPeaks_Soft <- predPeaks %>% filter(val=="Soft") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER) %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(scf, start_pos, end_pos, win_seq)

peak_start_end <- predPeaks_5kb %>% group_by(scf, win_seq) %>% summarise(start_pos_peak = min(start_pos_peak), end_pos_peak = max(end_pos_peak))

# RUN VIA permutation_GO_Soft_100000.R

# library(doSNOW)
# 
# num_cores <- 38
# cl <- makeCluster(num_cores)
# registerDoSNOW(cl)
# 
# permutations <- foreach(j = 1:10000, .combine = "rbind", .packages = c("dplyr")) %dopar% {
#   shuffled_data <- predPeaks_Soft
#   scf_start_temp <- scf_start
#   for (i in 1:nrow(shuffled_data)) {
#     # Get peak
#     peak <- shuffled_data[i,]
#     # Move peak to a random genomic location
#     selected_scf_start <- sample_n(scf_start_temp, 1)
#     peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#     end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#     # Make sure the new genomic location is valid
#     # Check if the new genomic location is within a prediction peak or the new end_pos is still available
#     max_attempts <- 21495
#     while((selected_scf_start$end_pos > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
#       # Move peak to a random genomic location
#       selected_scf_start <- sample_n(scf_start_temp, 1)
#       peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#       end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#       # Decrement attempts counter
#       max_attempts <- max_attempts - 1
#     }
#     # Break if max_attempts
#     if (max_attempts == 0) {
#       cat("Failed to find valid random location.\n")
#       shuffled_data <- NULL
#       break
#     }
#     # Remove the randomly selected location from the pool
#     region <- scf_start_temp %>%
#       filter(scf==selected_scf_start$scf) %>%
#       filter(start_pos >= selected_scf_start$start_pos & end_pos <= selected_scf_start$end_pos)
#     scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
#     # Add random location to predPeaks
#     shuffled_data$perm[i] <- j
#     shuffled_data$scf_perm[i] <- selected_scf_start$scf
#     shuffled_data$start_pos_perm[i] <- selected_scf_start$start_pos
#     shuffled_data$end_pos_perm[i] <- selected_scf_start$end_pos
#   }
#   shuffled_data
# }
# 
# stopCluster(cl)
# 
# 
# 
# # Save file
# fileName <- "predPeaks_permutation_Soft_GenotypeGVCFs_Default_allSites_SoftFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv"
# write_tsv(permutations, file=fileName)

permutations <- fread("predPeaks_permutation_Soft_100000_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

# ## Join GO terms for predictions peaks
# 
# predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length)
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
# system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "len", "length", "intersect")
predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

# ## Join GO terms to permutations
# 
# permutations_temp <- permutations %>% dplyr::select(scf_perm, start_pos_perm, end_pos_perm, perm, val, len, length)
# write.table(permutations_temp, "permutations_Soft_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# ### bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b permutations_Soft_temp.tsv -wo > permutations_Soft_GO_merged.tsv")
# system(cmd)

permutationsGO_Soft <- fread("permutations_Soft_GO_merged.tsv", header=FALSE) 
colnames(permutationsGO_Soft) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "len", "length", "intersect")

# permutationsGO_Soft <- permutationsGO_Soft %>% 
#   left_join(GO_geneName, relationship = "many-to-many") %>% 
#   arrange(scf_perm, start_pos_perm, end_pos_perm)

setDT(permutationsGO_Soft)
setDT(GO_geneName)
result <- permutationsGO_Soft[GO_geneName, on = .(fbgn), nomatch = 0, allow.cartesian = TRUE]
permutationsGO_Soft <- result
rm(result)
gc()

## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

# permutationsGO_summary <- permutationsGO_Soft %>% 
#   group_by(perm, val, go, process) %>% 
#   summarise(intersect_perm = sum(intersect)) %>% 
#   ungroup()

setDT(permutationsGO_Soft)
permutationsGO_summary <- permutationsGO_Soft[, .(intersect_perm = sum(intersect)), by = .(perm, val, go, process)]

# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_Soft <- predPeaksGO %>% filter(val=="Soft") %>% group_by(go) %>% summarise(intersect_Soft = sum(intersect))

### Check that intersect is never greater than one

permutationsGO_summary %>% 
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
  filter(is.na(intersect_empirical)==FALSE) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_Soft) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
  arrange(desc(intersect_proportion_perm))

# permutationsGO_Soft %>%
#   filter(perm==7917) %>%
#   filter(go=="GO:0004019")
# 
# scf_start_temp <- scf_start
# 
# # region <- scf_start_temp %>%
# #   filter(scf==selected_scf_start$scf) %>%
# #   filter(start_pos >= selected_scf_start$start_pos & start_pos <= selected_scf_start$start_pos+peak$length)
# 
# region <- scf_start_temp %>%
#   filter(scf=="ScA8VGg_76") %>%
#   filter(start_pos >= 22402500 & start_pos <= 22402500+10000)
# 
# scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
# 
# # (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
# (22407500 %in% scf_start_temp$end_pos[scf_start_temp$scf=="ScA8VGg_76"])==FALSE
# 
# # peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
# end_not_available <- (22407500 %in% scf_start_temp$end_pos[scf_start_temp$scf=="ScA8VGg_76"])==FALSE
# 
# # Make sure the new genomic location is valid
# max_attempts <- 21495
# # while((selected_scf_start$start_pos + peak$length > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
# (22402500+10000 > 22402500 || end_not_available) && max_attempts > 0


permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>% 
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>% 
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/100000, 
            p_proportion = sum(test_proportion)/100000)

# permStats <- permutationsGO_summary %>%
#   left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
#   filter(is.na(intersect_empirical)==FALSE) %>%
#   left_join(intersect_genome) %>% 
#   mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
#   mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
#   mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
#   mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
#   mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>% 
#   mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical)
# 
# permStats_temp1 <- permutationsGO_summary %>%
#   left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
#   filter(is.na(intersect_empirical)==FALSE) %>%
#   left_join(intersect_genome) %>% 
#   filter(process=="P") %>% 
#   pivot_wider(names_from=perm, values_from=intersect_perm)
# 
# perm_proportion_mat <- matrix(nrow=nrow(permStats_temp1), ncol=1000)
# 
# for(i in 1:nrow(permStats_temp1)) {
#   for (j in 1:1000) {
#     val_temp <- permStats_temp1$val[i]
#     go_temp <- permStats_temp1$go[i]
#     empirical_proportion_temp <- permStats_temp1$intersect_empirical[i] / permStats_temp1$intersect_genome[i]
#     perm_proportion_temp <- as.numeric(permStats_temp1[i,j+5] / permStats_temp1$intersect_genome[i])
#     if(is.na(perm_proportion_temp)) { perm_proportion_temp <- 0 }
#     # stat_proportion_temp <- log2(empirical_proportion_temp/perm_proportion_temp)
#     perm_proportion_mat[i,j] <- perm_proportion_temp
#   }
# }
# 
# emp_proportion_vec <- as.matrix(permStats_temp1$intersect_empirical/permStats_temp1$intersect_genome)
# 
# permStats_temp1
# perm_proportion_mat[1:10,1:15]
# emp_proportion_vec[1:10,]
# 
# perm_stat_mat <- matrix(nrow=nrow(permStats_temp1), ncol=1000)
# 
# for(i in 1:nrow(permStats_temp1)) {
#   for (j in 1:1000) {
#     val_temp <- permStats_temp1$val[i]
#     go_temp <- permStats_temp1$go[i]
#     if(is.na(permStats_temp1[i,j+5])) { permStats_temp1[i,j+5] <- 0 }
#     empirical_proportion_temp <- permStats_temp1$intersect_empirical[i]+1 / permStats_temp1$intersect_genome[i]+1
#     perm_proportion_temp <- as.numeric(permStats_temp1[i,j+5]+1 / permStats_temp1$intersect_genome[i]+1)
#     stat_proportion_temp <- log2(empirical_proportion_temp/perm_proportion_temp)
#     perm_stat_mat[i,j] <- stat_proportion_temp
#   }
# }
# 
# # emp_proportion_vec <- as.matrix(permStats_temp1$intersect_empirical/permStats_temp1$intersect_genome)
# 
# permStats_temp1
# perm_stat_mat[1:10,1:15]
# # emp_proportion_vec[1:10,]
# 
# # sd.maxT(null=perm_proportion_mat, 
# #         obs=emp_proportion_vec, 
# #         alternative="greater", 
# #         get.cutoff=FALSE, # default is FALSE 
# #         get.cr=TRUE, # default is FALSE
# #         get.adjp=TRUE, # default is TRUE 
# #         alpha = 0.05)
# 
# permStats_temp1 %>% 
#   select(val:intersect_genome) %>% 
#   (as.data.frame(perm_stat_mat)) %>% 
#   as_tibble()


# ### Intersect null
# 
# permutationsGO_summary_1 <- permutationsGO_summary %>% 
#   filter(perm==1) %>% 
#   select(-perm) %>% 
#   rename(intersect_perm_1 = intersect_perm)
# 
# permTest_intersect_null <- permutationsGO_summary %>% 
#   filter(perm!=1) %>% 
#   left_join(permutationsGO_summary_1, by=c("val", "go")) %>% 
#   filter(is.na(intersect_perm_1)==FALSE) %>% 
#   mutate(test = intersect_perm >= intersect_perm_1) %>% 
#   group_by(val, go, process.y) %>% 
#   summarise(p = sum(test)/99999)



# Soft

## Intersect

permTest_intersect_Soft <- permTest_intersect %>% 
  filter(val=="Soft") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_proportion)

permTest_intersect_Soft %>% ggplot(aes(x=p_proportion)) + geom_histogram(bins=50) + ggtitle("Soft")

permTest_intersect_Soft$fdr <- p.adjust(permTest_intersect_Soft$p_proportion, method="fdr")
permTest_intersect_Soft$q <- qvalue(permTest_intersect_Soft$p_proportion)$qvalues

# intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_Soft <- predPeaksGO %>% filter(val=="Soft") %>% group_by(go) %>% summarise(intersect_Soft = sum(intersect))

n_genes <- predPeaksGO %>% 
  filter(val=="Soft") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes = n())

n_peaks <- predPeaksGO %>% 
  filter(val=="Soft") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks = n())

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(-p_intersect, -q, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(fdr <= 0.05) %>% 
  head(n=68)

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  left_join(n_peaks) %>% 
  select(go, fdr, name_1006) %>% 
  filter(fdr <= 0.05) %>% 
  rename(p_adjusted = fdr, go_name = name_1006) %>% 
  head(n=29) %>% 
  as.data.frame()

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_Soft) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p_proportion))) + 
  geom_point()

# ## Null
# 
# permTest_intersect_Soft_null <- permTest_intersect_null %>% 
#   filter(val=="Soft") %>% 
#   filter(process.y=="P") %>% 
#   distinct() %>% 
#   arrange(p)
# 
# permTest_intersect_Soft_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("Soft (null)")
# 
# permTest_intersect_Soft_null$fdr <- p.adjust(permTest_intersect_Soft_null$p, method="fdr")
# #permTest_intersect_Soft_null$q <- qvalue(permTest_intersect_Soft_null$p)$qvalues
# 
# permTest_intersect_Soft_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   as.data.frame()
# 
# intersect_genome <- permutationsGO %>% filter(perm==1) %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_Soft <- permutationsGO %>% filter(perm==1) %>% filter(val_perm=="Soft") %>% group_by(go) %>% summarise(intersect_Soft = sum(intersect))
# 
# permTest_intersect_Soft_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_Soft) %>% 
#   as.data.frame()
# 
# permTest_intersect_Soft_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_Soft) %>% 
#   ggplot(aes(x=intersect_genome, y=1-log10(p))) + 
#   geom_point()
# 
# permTest_intersect_Soft_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_Soft) %>% 
#   ggplot(aes(x=p)) + 
#   geom_histogram()
# 
# permTest_intersect_Soft_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_Soft) %>% 
#   filter(intersect_genome>50000) %>% 
#   ggplot(aes(x=p)) + 
#   geom_histogram()

```

### HardPartial

```{r, OLD}

# library(tidyverse)
library(dplyr)
library(tidyr)
library(readr)
library(rlang)
library(magrittr)
library(ggplot2)
library(qvalue)
library(data.table)
library(foreach)
# library(doParallel)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

## Import data

predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>% 
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("go_info.tsv") %>% rename(go = go_id)

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

predictions <- data.table()

for(ff in preFiles){
    tmp <- fread(ff, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    predictions <- rbind(predictions, tmp)
}

predictions <- predictions %>% 
  set_names(c("scf", "start_pos", "end_pos", "val", "V5", "V6", "V7", "V8", "V9"))

## Expand predPeaks to 5kb resolution

# predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length, win_seq)
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(predictions, "predictions_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# cmd <- paste0("bedtools intersect -a predictions_temp.tsv -b predPeaks_temp.tsv -wo > predPeaks_5kb.tsv")
# system(cmd)

predPeaks_5kb <- read_tsv("predPeaks_5kb.tsv", col_names=FALSE)

colnames(predPeaks_5kb) <- c("scf", "start_pos", "end_pos", "val", "X5", "X6", 
                            "X7", "X8", "X9", "scf_peak", "start_pos_peak", 
                            "end_pos_peak", "X13", "len", "length", "win_seq", 
                            "intersect")

predPeaks_5kb <- predPeaks_5kb %>% select(-X5:-X9, -X13)

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))
predPeaks_HardPartial <- predPeaks %>% filter(val=="HardPartial") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER) %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(scf, start_pos, end_pos, win_seq)

peak_start_end <- predPeaks_5kb %>% group_by(scf, win_seq) %>% summarise(start_pos_peak = min(start_pos_peak), end_pos_peak = max(end_pos_peak))

# # Set the number of cores
# num_cores <- 38
# 
# # Register parallel backend
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)
# 
# permutations <- foreach(j = 1:10000, .combine = "rbind", .packages = c("dplyr")) %dopar% {
#   shuffled_data <- predPeaks_HardPartial
#   scf_start_temp <- scf_start
#   for (i in 1:nrow(shuffled_data)) {
#     # Get peak
#     peak <- shuffled_data[i,]
#     # Move peak to a random genomic location
#     selected_scf_start <- sample_n(scf_start_temp, 1)
#     peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#     end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#     # Make sure the new genomic location is valid
#     # Check if the new genomic location is within a prediction peak or the new end_pos is still available
#     max_attempts <- 21495
#     while((selected_scf_start$start_pos + peak$length > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
#       # Move peak to a random genomic location
#       selected_scf_start <- sample_n(scf_start_temp, 1)
#       peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#       end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#       # Decrement attempts counter
#       max_attempts <- max_attempts - 1
#     }
#     # Break if max_attempts
#     if (max_attempts == 0) {
#       cat("Failed to find valid random location.\n")
#       shuffled_data <- NULL
#       break
#     }
#     # Remove the randomly selected location from the pool
#     region <- scf_start_temp %>%
#       filter(scf==selected_scf_start$scf) %>%
#       filter(start_pos >= selected_scf_start$start_pos & start_pos <= selected_scf_start$start_pos+peak$length)
#     scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
#     # Add random location to predPeaks
#     shuffled_data$perm[i] <- j
#     shuffled_data$scf_perm[i] <- selected_scf_start$scf
#     shuffled_data$start_pos_perm[i] <- selected_scf_start$start_pos
#     shuffled_data$end_pos_perm[i] <- selected_scf_start$start_pos+peak$length
#   }
#   shuffled_data
# }
# 
# # Unregister the parallel backend
# stopCluster(cl)

# # RUN VIA permutation_GO_HardPartial_100000.R
# 
# library(doSNOW)
# 
# num_cores <- 38
# cl <- makeCluster(num_cores)
# registerDoSNOW(cl)
# 
# permutations <- foreach(j = 1:10000, .combine = "rbind", .packages = c("dplyr")) %dopar% {
#   shuffled_data <- predPeaks_HardPartial
#   scf_start_temp <- scf_start
#   for (i in 1:nrow(shuffled_data)) {
#     # Get peak
#     peak <- shuffled_data[i,]
#     # Move peak to a random genomic location
#     selected_scf_start <- sample_n(scf_start_temp, 1)
#     peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#     end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#     # Make sure the new genomic location is valid
#     # Check if the new genomic location is within a prediction peak or the new end_pos is still available
#     max_attempts <- 21495
#     while((selected_scf_start$end_pos > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
#       # Move peak to a random genomic location
#       selected_scf_start <- sample_n(scf_start_temp, 1)
#       peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#       end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#       # Decrement attempts counter
#       max_attempts <- max_attempts - 1
#     }
#     # Break if max_attempts
#     if (max_attempts == 0) {
#       cat("Failed to find valid random location.\n")
#       shuffled_data <- NULL
#       break
#     }
#     # Remove the randomly selected location from the pool
#     region <- scf_start_temp %>%
#       filter(scf==selected_scf_start$scf) %>%
#       filter(start_pos >= selected_scf_start$start_pos & end_pos <= selected_scf_start$end_pos)
#     scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
#     # Add random location to predPeaks
#     shuffled_data$perm[i] <- j
#     shuffled_data$scf_perm[i] <- selected_scf_start$scf
#     shuffled_data$start_pos_perm[i] <- selected_scf_start$start_pos
#     shuffled_data$end_pos_perm[i] <- selected_scf_start$end_pos
#   }
#   shuffled_data
# }
# 
# stopCluster(cl)
# 
# 
# 
# # Save file
# fileName <- "predPeaks_permutation_HardPartial_GenotypeGVCFs_Default_allSites_HardPartialFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv"
# write_tsv(permutations, file=fileName)

permutations <- fread("predPeaks_permutation_HardPartial_100000_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

## Join GO terms for predictions peaks

predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length)
write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "len", "length", "intersect")
predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

## Join GO terms to permutations

permutations_temp <- permutations %>% dplyr::select(scf_perm, start_pos_perm, end_pos_perm, perm, val, len, length)
write.table(permutations_temp, "permutations_HardPartial_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_HardPartial_temp.tsv -wo > permutations_HardPartial_GO_merged.tsv")
system(cmd)

permutationsGO_HardPartial <- fread("permutations_HardPartial_GO_merged.tsv", header=FALSE) 
colnames(permutationsGO_HardPartial) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "len", "length", "intersect")
permutationsGO_HardPartial <- permutationsGO_HardPartial %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_perm, start_pos_perm, end_pos_perm)

## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_HardPartial %>% 
  group_by(perm, val, go, process) %>% 
  summarise(intersect_perm = sum(intersect)) %>% 
  ungroup()

# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_HardPartial <- predPeaksGO %>% filter(val=="HardPartial") %>% group_by(go) %>% summarise(intersect_HardPartial = sum(intersect))

### Check that intersect is never greater than one

permutationsGO_summary %>% 
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
  filter(is.na(intersect_empirical)==FALSE) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_HardPartial) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
  arrange(desc(intersect_proportion_perm))

# permutationsGO_HardPartial %>%
#   filter(perm==7917) %>%
#   filter(go=="GO:0004019")
# 
# scf_start_temp <- scf_start
# 
# # region <- scf_start_temp %>%
# #   filter(scf==selected_scf_start$scf) %>%
# #   filter(start_pos >= selected_scf_start$start_pos & start_pos <= selected_scf_start$start_pos+peak$length)
# 
# region <- scf_start_temp %>%
#   filter(scf=="ScA8VGg_76") %>%
#   filter(start_pos >= 22402500 & start_pos <= 22402500+10000)
# 
# scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
# 
# # (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
# (22407500 %in% scf_start_temp$end_pos[scf_start_temp$scf=="ScA8VGg_76"])==FALSE
# 
# # peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
# end_not_available <- (22407500 %in% scf_start_temp$end_pos[scf_start_temp$scf=="ScA8VGg_76"])==FALSE
# 
# # Make sure the new genomic location is valid
# max_attempts <- 21495
# # while((selected_scf_start$start_pos + peak$length > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
# (22402500+10000 > 22402500 || end_not_available) && max_attempts > 0


permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>% 
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>% 
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/10000, 
            p_proportion = sum(test_proportion)/10000)

# permStats <- permutationsGO_summary %>%
#   left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
#   filter(is.na(intersect_empirical)==FALSE) %>%
#   left_join(intersect_genome) %>% 
#   mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
#   mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
#   mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
#   mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
#   mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>% 
#   mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical)
# 
# permStats_temp1 <- permutationsGO_summary %>%
#   left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
#   filter(is.na(intersect_empirical)==FALSE) %>%
#   left_join(intersect_genome) %>% 
#   filter(process=="P") %>% 
#   pivot_wider(names_from=perm, values_from=intersect_perm)
# 
# perm_proportion_mat <- matrix(nrow=nrow(permStats_temp1), ncol=1000)
# 
# for(i in 1:nrow(permStats_temp1)) {
#   for (j in 1:1000) {
#     val_temp <- permStats_temp1$val[i]
#     go_temp <- permStats_temp1$go[i]
#     empirical_proportion_temp <- permStats_temp1$intersect_empirical[i] / permStats_temp1$intersect_genome[i]
#     perm_proportion_temp <- as.numeric(permStats_temp1[i,j+5] / permStats_temp1$intersect_genome[i])
#     if(is.na(perm_proportion_temp)) { perm_proportion_temp <- 0 }
#     # stat_proportion_temp <- log2(empirical_proportion_temp/perm_proportion_temp)
#     perm_proportion_mat[i,j] <- perm_proportion_temp
#   }
# }
# 
# emp_proportion_vec <- as.matrix(permStats_temp1$intersect_empirical/permStats_temp1$intersect_genome)
# 
# permStats_temp1
# perm_proportion_mat[1:10,1:15]
# emp_proportion_vec[1:10,]
# 
# perm_stat_mat <- matrix(nrow=nrow(permStats_temp1), ncol=1000)
# 
# for(i in 1:nrow(permStats_temp1)) {
#   for (j in 1:1000) {
#     val_temp <- permStats_temp1$val[i]
#     go_temp <- permStats_temp1$go[i]
#     if(is.na(permStats_temp1[i,j+5])) { permStats_temp1[i,j+5] <- 0 }
#     empirical_proportion_temp <- permStats_temp1$intersect_empirical[i]+1 / permStats_temp1$intersect_genome[i]+1
#     perm_proportion_temp <- as.numeric(permStats_temp1[i,j+5]+1 / permStats_temp1$intersect_genome[i]+1)
#     stat_proportion_temp <- log2(empirical_proportion_temp/perm_proportion_temp)
#     perm_stat_mat[i,j] <- stat_proportion_temp
#   }
# }
# 
# # emp_proportion_vec <- as.matrix(permStats_temp1$intersect_empirical/permStats_temp1$intersect_genome)
# 
# permStats_temp1
# perm_stat_mat[1:10,1:15]
# # emp_proportion_vec[1:10,]
# 
# # sd.maxT(null=perm_proportion_mat, 
# #         obs=emp_proportion_vec, 
# #         alternative="greater", 
# #         get.cutoff=FALSE, # default is FALSE 
# #         get.cr=TRUE, # default is FALSE
# #         get.adjp=TRUE, # default is TRUE 
# #         alpha = 0.05)
# 
# permStats_temp1 %>% 
#   select(val:intersect_genome) %>% 
#   (as.data.frame(perm_stat_mat)) %>% 
#   as_tibble()


# ### Intersect null
# 
# permutationsGO_summary_1 <- permutationsGO_summary %>% 
#   filter(perm==1) %>% 
#   select(-perm) %>% 
#   rename(intersect_perm_1 = intersect_perm)
# 
# permTest_intersect_null <- permutationsGO_summary %>% 
#   filter(perm!=1) %>% 
#   left_join(permutationsGO_summary_1, by=c("val", "go")) %>% 
#   filter(is.na(intersect_perm_1)==FALSE) %>% 
#   mutate(test = intersect_perm >= intersect_perm_1) %>% 
#   group_by(val, go, process.y) %>% 
#   summarise(p = sum(test)/99999)



# HardPartial

## Intersect

permTest_intersect_HardPartial <- permTest_intersect %>% 
  filter(val=="HardPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_proportion)

permTest_intersect_HardPartial %>% ggplot(aes(x=p_proportion)) + geom_histogram(bins=50) + ggtitle("HardPartial")

permTest_intersect_HardPartial$fdr <- p.adjust(permTest_intersect_HardPartial$p_proportion, method="fdr")
# permTest_intersect_HardPartial$q <- qvalue(permTest_intersect_HardPartial$p_proportion)$qvalues

# intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_HardPartial <- predPeaksGO %>% filter(val=="HardPartial") %>% group_by(go) %>% summarise(intersect_HardPartial = sum(intersect))

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  filter(fdr<=0.05) %>% 
  as.data.frame()

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_HardPartial) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p_proportion))) + 
  geom_point()

# ## Null
# 
# permTest_intersect_HardPartial_null <- permTest_intersect_null %>% 
#   filter(val=="HardPartial") %>% 
#   filter(process.y=="P") %>% 
#   distinct() %>% 
#   arrange(p)
# 
# permTest_intersect_HardPartial_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("HardPartial (null)")
# 
# permTest_intersect_HardPartial_null$fdr <- p.adjust(permTest_intersect_HardPartial_null$p, method="fdr")
# #permTest_intersect_HardPartial_null$q <- qvalue(permTest_intersect_HardPartial_null$p)$qvalues
# 
# permTest_intersect_HardPartial_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   as.data.frame()
# 
# intersect_genome <- permutationsGO %>% filter(perm==1) %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_HardPartial <- permutationsGO %>% filter(perm==1) %>% filter(val_perm=="HardPartial") %>% group_by(go) %>% summarise(intersect_HardPartial = sum(intersect))
# 
# permTest_intersect_HardPartial_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_HardPartial) %>% 
#   as.data.frame()
# 
# permTest_intersect_HardPartial_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_HardPartial) %>% 
#   ggplot(aes(x=intersect_genome, y=1-log10(p))) + 
#   geom_point()
# 
# permTest_intersect_HardPartial_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_HardPartial) %>% 
#   ggplot(aes(x=p)) + 
#   geom_histogram()
# 
# permTest_intersect_HardPartial_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_HardPartial) %>% 
#   filter(intersect_genome>50000) %>% 
#   ggplot(aes(x=p)) + 
#   geom_histogram()

```

### SoftPartial

```{r, OLD}

# library(tidyverse)
library(dplyr)
library(tidyr)
library(readr)
library(rlang)
library(magrittr)
library(ggplot2)
library(qvalue)
library(data.table)
library(foreach)
# library(doParallel)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

## Import data

predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_SoftFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>% 
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("go_info.tsv") %>% rename(go = go_id)

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

predictions <- data.table()

for(ff in preFiles){
    tmp <- fread(ff, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    predictions <- rbind(predictions, tmp)
}

predictions <- predictions %>% 
  set_names(c("scf", "start_pos", "end_pos", "val", "V5", "V6", "V7", "V8", "V9"))

## Expand predPeaks to 5kb resolution

# predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length, win_seq)
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(predictions, "predictions_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# cmd <- paste0("bedtools intersect -a predictions_temp.tsv -b predPeaks_temp.tsv -wo > predPeaks_5kb.tsv")
# system(cmd)

predPeaks_5kb <- read_tsv("predPeaks_5kb.tsv", col_names=FALSE)

colnames(predPeaks_5kb) <- c("scf", "start_pos", "end_pos", "val", "X5", "X6", 
                            "X7", "X8", "X9", "scf_peak", "start_pos_peak", 
                            "end_pos_peak", "X13", "len", "length", "win_seq", 
                            "intersect")

predPeaks_5kb <- predPeaks_5kb %>% select(-X5:-X9, -X13)

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))
predPeaks_SoftPartial <- predPeaks %>% filter(val=="SoftPartial") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER) %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(scf, start_pos, end_pos, win_seq)

peak_start_end <- predPeaks_5kb %>% group_by(scf, win_seq) %>% summarise(start_pos_peak = min(start_pos_peak), end_pos_peak = max(end_pos_peak))

# # Set the number of cores
# num_cores <- 38
# 
# # Register parallel backend
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)
# 
# permutations <- foreach(j = 1:10000, .combine = "rbind", .packages = c("dplyr")) %dopar% {
#   shuffled_data <- predPeaks_SoftPartial
#   scf_start_temp <- scf_start
#   for (i in 1:nrow(shuffled_data)) {
#     # Get peak
#     peak <- shuffled_data[i,]
#     # Move peak to a random genomic location
#     selected_scf_start <- sample_n(scf_start_temp, 1)
#     peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#     end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#     # Make sure the new genomic location is valid
#     # Check if the new genomic location is within a prediction peak or the new end_pos is still available
#     max_attempts <- 21495
#     while((selected_scf_start$start_pos + peak$length > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
#       # Move peak to a random genomic location
#       selected_scf_start <- sample_n(scf_start_temp, 1)
#       peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#       end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#       # Decrement attempts counter
#       max_attempts <- max_attempts - 1
#     }
#     # Break if max_attempts
#     if (max_attempts == 0) {
#       cat("Failed to find valid random location.\n")
#       shuffled_data <- NULL
#       break
#     }
#     # Remove the randomly selected location from the pool
#     region <- scf_start_temp %>%
#       filter(scf==selected_scf_start$scf) %>%
#       filter(start_pos >= selected_scf_start$start_pos & start_pos <= selected_scf_start$start_pos+peak$length)
#     scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
#     # Add random location to predPeaks
#     shuffled_data$perm[i] <- j
#     shuffled_data$scf_perm[i] <- selected_scf_start$scf
#     shuffled_data$start_pos_perm[i] <- selected_scf_start$start_pos
#     shuffled_data$end_pos_perm[i] <- selected_scf_start$start_pos+peak$length
#   }
#   shuffled_data
# }
# 
# # Unregister the parallel backend
# stopCluster(cl)

# # RUN VIA permutation_GO_SoftPartial_100000.R
# 
# library(doSNOW)
# 
# num_cores <- 38
# cl <- makeCluster(num_cores)
# registerDoSNOW(cl)
# 
# permutations <- foreach(j = 1:10000, .combine = "rbind", .packages = c("dplyr")) %dopar% {
#   shuffled_data <- predPeaks_SoftPartial
#   scf_start_temp <- scf_start
#   for (i in 1:nrow(shuffled_data)) {
#     # Get peak
#     peak <- shuffled_data[i,]
#     # Move peak to a random genomic location
#     selected_scf_start <- sample_n(scf_start_temp, 1)
#     peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#     end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#     # Make sure the new genomic location is valid
#     # Check if the new genomic location is within a prediction peak or the new end_pos is still available
#     max_attempts <- 21495
#     while((selected_scf_start$end_pos > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
#       # Move peak to a random genomic location
#       selected_scf_start <- sample_n(scf_start_temp, 1)
#       peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
#       end_not_available <- (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
#       # Decrement attempts counter
#       max_attempts <- max_attempts - 1
#     }
#     # Break if max_attempts
#     if (max_attempts == 0) {
#       cat("Failed to find valid random location.\n")
#       shuffled_data <- NULL
#       break
#     }
#     # Remove the randomly selected location from the pool
#     region <- scf_start_temp %>%
#       filter(scf==selected_scf_start$scf) %>%
#       filter(start_pos >= selected_scf_start$start_pos & end_pos <= selected_scf_start$end_pos)
#     scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
#     # Add random location to predPeaks
#     shuffled_data$perm[i] <- j
#     shuffled_data$scf_perm[i] <- selected_scf_start$scf
#     shuffled_data$start_pos_perm[i] <- selected_scf_start$start_pos
#     shuffled_data$end_pos_perm[i] <- selected_scf_start$end_pos
#   }
#   shuffled_data
# }
# 
# stopCluster(cl)
# 
# 
# 
# # Save file
# fileName <- "predPeaks_permutation_SoftPartial_GenotypeGVCFs_Default_allSites_SoftPartialFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv"
# write_tsv(permutations, file=fileName)

permutations <- fread("predPeaks_permutation_SoftPartial_100000_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

## Join GO terms for predictions peaks

predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length)
write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "len", "length", "intersect")
predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

## Join GO terms to permutations

permutations_temp <- permutations %>% dplyr::select(scf_perm, start_pos_perm, end_pos_perm, perm, val, len, length)
write.table(permutations_temp, "permutations_SoftPartial_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_SoftPartial_temp.tsv -wo > permutations_SoftPartial_GO_merged.tsv")
system(cmd)

permutationsGO_SoftPartial <- fread("permutations_SoftPartial_GO_merged.tsv", header=FALSE) 
colnames(permutationsGO_SoftPartial) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "len", "length", "intersect")
permutationsGO_SoftPartial <- permutationsGO_SoftPartial %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_perm, start_pos_perm, end_pos_perm)

## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_SoftPartial %>% 
  group_by(perm, val, go, process) %>% 
  summarise(intersect_perm = sum(intersect)) %>% 
  ungroup()

# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_SoftPartial <- predPeaksGO %>% filter(val=="SoftPartial") %>% group_by(go) %>% summarise(intersect_SoftPartial = sum(intersect))

### Check that intersect is never greater than one

permutationsGO_summary %>% 
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>% 
  filter(is.na(intersect_empirical)==FALSE) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_SoftPartial) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  select(perm, val, go, process, intersect_genome, intersect_empirical, intersect_perm, intersect_proportion_empirical, intersect_proportion_perm) %>% 
  arrange(desc(intersect_proportion_perm))

# permutationsGO_SoftPartial %>%
#   filter(perm==7917) %>%
#   filter(go=="GO:0004019")
# 
# scf_start_temp <- scf_start
# 
# # region <- scf_start_temp %>%
# #   filter(scf==selected_scf_start$scf) %>%
# #   filter(start_pos >= selected_scf_start$start_pos & start_pos <= selected_scf_start$start_pos+peak$length)
# 
# region <- scf_start_temp %>%
#   filter(scf=="ScA8VGg_76") %>%
#   filter(start_pos >= 22402500 & start_pos <= 22402500+10000)
# 
# scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("scf", "start_pos"))
# 
# # (selected_scf_start$end_pos %in% scf_start_temp$end_pos[scf_start_temp$scf==selected_scf_start$scf])==FALSE
# (22407500 %in% scf_start_temp$end_pos[scf_start_temp$scf=="ScA8VGg_76"])==FALSE
# 
# # peak_end <- selected_scf_start %>% left_join(peak_start_end, by = join_by(scf, win_seq))
# end_not_available <- (22407500 %in% scf_start_temp$end_pos[scf_start_temp$scf=="ScA8VGg_76"])==FALSE
# 
# # Make sure the new genomic location is valid
# max_attempts <- 21495
# # while((selected_scf_start$start_pos + peak$length > peak_end$end_pos_peak || end_not_available) && max_attempts > 0) {
# (22402500+10000 > 22402500 || end_not_available) && max_attempts > 0


permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>% 
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>% 
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/10000, 
            p_proportion = sum(test_proportion)/10000)

# permStats <- permutationsGO_summary %>%
#   left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
#   filter(is.na(intersect_empirical)==FALSE) %>%
#   left_join(intersect_genome) %>% 
#   mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
#   mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>% 
#   mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
#   mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
#   mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>% 
#   mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical)
# 
# permStats_temp1 <- permutationsGO_summary %>%
#   left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
#   filter(is.na(intersect_empirical)==FALSE) %>%
#   left_join(intersect_genome) %>% 
#   filter(process=="P") %>% 
#   pivot_wider(names_from=perm, values_from=intersect_perm)
# 
# perm_proportion_mat <- matrix(nrow=nrow(permStats_temp1), ncol=1000)
# 
# for(i in 1:nrow(permStats_temp1)) {
#   for (j in 1:1000) {
#     val_temp <- permStats_temp1$val[i]
#     go_temp <- permStats_temp1$go[i]
#     empirical_proportion_temp <- permStats_temp1$intersect_empirical[i] / permStats_temp1$intersect_genome[i]
#     perm_proportion_temp <- as.numeric(permStats_temp1[i,j+5] / permStats_temp1$intersect_genome[i])
#     if(is.na(perm_proportion_temp)) { perm_proportion_temp <- 0 }
#     # stat_proportion_temp <- log2(empirical_proportion_temp/perm_proportion_temp)
#     perm_proportion_mat[i,j] <- perm_proportion_temp
#   }
# }
# 
# emp_proportion_vec <- as.matrix(permStats_temp1$intersect_empirical/permStats_temp1$intersect_genome)
# 
# permStats_temp1
# perm_proportion_mat[1:10,1:15]
# emp_proportion_vec[1:10,]
# 
# perm_stat_mat <- matrix(nrow=nrow(permStats_temp1), ncol=1000)
# 
# for(i in 1:nrow(permStats_temp1)) {
#   for (j in 1:1000) {
#     val_temp <- permStats_temp1$val[i]
#     go_temp <- permStats_temp1$go[i]
#     if(is.na(permStats_temp1[i,j+5])) { permStats_temp1[i,j+5] <- 0 }
#     empirical_proportion_temp <- permStats_temp1$intersect_empirical[i]+1 / permStats_temp1$intersect_genome[i]+1
#     perm_proportion_temp <- as.numeric(permStats_temp1[i,j+5]+1 / permStats_temp1$intersect_genome[i]+1)
#     stat_proportion_temp <- log2(empirical_proportion_temp/perm_proportion_temp)
#     perm_stat_mat[i,j] <- stat_proportion_temp
#   }
# }
# 
# # emp_proportion_vec <- as.matrix(permStats_temp1$intersect_empirical/permStats_temp1$intersect_genome)
# 
# permStats_temp1
# perm_stat_mat[1:10,1:15]
# # emp_proportion_vec[1:10,]
# 
# # sd.maxT(null=perm_proportion_mat, 
# #         obs=emp_proportion_vec, 
# #         alternative="greater", 
# #         get.cutoff=FALSE, # default is FALSE 
# #         get.cr=TRUE, # default is FALSE
# #         get.adjp=TRUE, # default is TRUE 
# #         alpha = 0.05)
# 
# permStats_temp1 %>% 
#   select(val:intersect_genome) %>% 
#   (as.data.frame(perm_stat_mat)) %>% 
#   as_tibble()


# ### Intersect null
# 
# permutationsGO_summary_1 <- permutationsGO_summary %>% 
#   filter(perm==1) %>% 
#   select(-perm) %>% 
#   rename(intersect_perm_1 = intersect_perm)
# 
# permTest_intersect_null <- permutationsGO_summary %>% 
#   filter(perm!=1) %>% 
#   left_join(permutationsGO_summary_1, by=c("val", "go")) %>% 
#   filter(is.na(intersect_perm_1)==FALSE) %>% 
#   mutate(test = intersect_perm >= intersect_perm_1) %>% 
#   group_by(val, go, process.y) %>% 
#   summarise(p = sum(test)/99999)



# SoftPartial

## Intersect

permTest_intersect_SoftPartial <- permTest_intersect %>% 
  filter(val=="SoftPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_proportion)

permTest_intersect_SoftPartial %>% ggplot(aes(x=p_proportion)) + geom_histogram(bins=50) + ggtitle("SoftPartial")

permTest_intersect_SoftPartial$fdr <- p.adjust(permTest_intersect_SoftPartial$p_proportion, method="fdr")
# permTest_intersect_SoftPartial$q <- qvalue(permTest_intersect_SoftPartial$p_proportion)$qvalues

# intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_SoftPartial <- predPeaksGO %>% filter(val=="SoftPartial") %>% group_by(go) %>% summarise(intersect_SoftPartial = sum(intersect))

permTest_intersect_SoftPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  filter(fdr<=0.05) %>% 
  as.data.frame()

permTest_intersect_SoftPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  # left_join(intersect_SoftPartial) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p_proportion))) + 
  geom_point()

# ## Null
# 
# permTest_intersect_SoftPartial_null <- permTest_intersect_null %>% 
#   filter(val=="SoftPartial") %>% 
#   filter(process.y=="P") %>% 
#   distinct() %>% 
#   arrange(p)
# 
# permTest_intersect_SoftPartial_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("SoftPartial (null)")
# 
# permTest_intersect_SoftPartial_null$fdr <- p.adjust(permTest_intersect_SoftPartial_null$p, method="fdr")
# #permTest_intersect_SoftPartial_null$q <- qvalue(permTest_intersect_SoftPartial_null$p)$qvalues
# 
# permTest_intersect_SoftPartial_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   as.data.frame()
# 
# intersect_genome <- permutationsGO %>% filter(perm==1) %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
# intersect_HardPartial <- permutationsGO %>% filter(perm==1) %>% filter(val_perm=="HardPartial") %>% group_by(go) %>% summarise(intersect_HardPartial = sum(intersect))
# 
# permTest_intersect_HardPartial_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_HardPartial) %>% 
#   as.data.frame()
# 
# permTest_intersect_HardPartial_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_HardPartial) %>% 
#   ggplot(aes(x=intersect_genome, y=1-log10(p))) + 
#   geom_point()
# 
# permTest_intersect_HardPartial_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_HardPartial) %>% 
#   ggplot(aes(x=p)) + 
#   geom_histogram()
# 
# permTest_intersect_HardPartial_null %>% 
#   left_join(dplyr::select(go_info, go, name_1006)) %>% 
#   left_join(intersect_genome) %>% 
#   left_join(intersect_HardPartial) %>% 
#   filter(intersect_genome>50000) %>% 
#   ggplot(aes(x=p)) + 
#   geom_histogram()

```

```{r, import data, OLD}

# library(tidyverse)
library(dplyr)
library(tidyr)
library(readr)
library(rlang)
library(data.table)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

# predictions

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

predictions <- data.table()

for(ff in preFiles){
    tmp <- fread(ff, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    predictions <- rbind(predictions, tmp)
}

predictions <- predictions %>% 
  set_names(c("scf", "start_pos", "end_pos", "val", "V5", "V6", "V7", "V8", "V9"))

# prediction peaks

predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.tsv")

# I DON"T KNOW WHERE YIGUAN GETS THESE GO FILES FROM.
GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))
GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>% 
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

# Permutations

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/permutations")

ffs <- list.files(pattern = '*.tsv', recursive = TRUE)

permutations <- data.table()

for(ff in ffs){
    tmp <- read_tsv(ff)
    permutations <- rbind(permutations, tmp)
}

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

```

## GOSemSim and clusterProfiler

```{r, OLD}

library(GOSemSim)
# library(foreach)
# library(doParallel)
library(ggdendro)
library(dendextend)

# # Install Drosophila database
# 
# if (!require("BiocManager", quietly = TRUE))
#     install.packages("BiocManager")
# 
# BiocManager::install("org.Dm.eg.db")

# Load the database

hsGO <- godata("org.Dm.eg.db", ont="BP")



# Calculate similarity for Hard sweeps

permTest_Hard <- permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  select(-p_intersect, -q, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(fdr <= 0.05)

GO_Hard <- permTest_Hard$go[permTest_Hard$fdr<=0.05]

n <- length(GO_Hard)
w <- matrix(NA, nrow = n, ncol = n)
colnames(w) <- rownames(w) <- GO_Hard

for (i in seq_len(n)) {
  cat("\n", i, " ")
  for (j in (i + 1):n) {
    w[i, j] <- goSim(GO_Hard[i], GO_Hard[j], semData=hsGO, measure="Jiang")
    # Set the corresponding element in the lower triangle to match the upper triangle
    w[j, i] <- w[i, j]
    cat(j, ",", sep="")
  }
}

# Identify rows and columns with all NA elements
na_rows <- rowSums(is.na(w)) == ncol(w)
na_cols <- colSums(is.na(w)) == nrow(w)

# Remove rows and columns with all NA elements
w_clean_Hard <- w[!na_rows, !na_cols]

# Change row and column names to GO name
goName <- permTest_Hard %>% filter(go %in% rownames(w_clean_Hard))
goName <- goName$name_1006
colnames(w_clean_Hard) <- rownames(w_clean_Hard) <- goName

# Set the diagonal elements to 1
diag(w_clean_Hard) <- 1

# Save

saveRDS(w_clean_Hard, "GO_similarity_matrix_clean_Hard.rds")

# Convert similarities to dissimilarities (if needed)
distance_matrix <- 1 - w_clean_Hard

# Perform hierarchical clustering
hclust_result <- hclust(as.dist(distance_matrix))

# Plot the dendrogram
plot(hclust_result)

# Convert the clustering result to a dendrogram
dendrogram_Hard <- as.dendrogram(hclust_result)

# Create a ggplot2 dendrogram plot
gg_hard <- ggdendrogram(dendrogram_Hard, rotate = FALSE) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  annotate("rect", xmin = 0.5, xmax = 1.5, ymin = 0, ymax = 1, fill = "grey", alpha = .2) + 
  annotate("rect", xmin = 1.5, xmax = 4.5, ymin = 0, ymax = 1, fill = "red", alpha = .2) + 
  annotate("rect", xmin = 4.5, xmax = 6.5, ymin = 0, ymax = 1, fill = "orange", alpha = .2) + 
  annotate("rect", xmin = 38.5, xmax = 43.5, ymin = 0, ymax = 1, fill = "yellow", alpha = .2) + 
  annotate("rect", xmin = 46.5, xmax = 47.5, ymin = 0, ymax = 1, fill = "green", alpha = .2) + 
  annotate("rect", xmin = 49.5, xmax = 54.5, ymin = 0, ymax = 1, fill = "blue", alpha = .2) + 
  annotate("rect", xmin = 54.5, xmax = 55.5, ymin = 0, ymax = 1, fill = "purple", alpha = .2)

# Print the plot
print(gg)



# Calculate similarity for Soft sweeps

permTest_Soft <- permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  select(-p_intersect, -q, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(fdr <= 0.05)

GO_Soft <- permTest_Soft$go[permTest_Soft$fdr<=0.05]

n <- length(GO_Soft)
w <- matrix(NA, nrow = n, ncol = n)
colnames(w) <- rownames(w) <- GO_Soft

for (i in seq_len(n)) {
  cat("\n", i, " ")
  for (j in (i + 1):n) {
    w[i, j] <- goSim(GO_Soft[i], GO_Soft[j], semData=hsGO, measure="Jiang")
    # Set the corresponding element in the lower triangle to match the upper triangle
    w[j, i] <- w[i, j]
    cat(j, ",", sep="")
  }
}

# Identify rows and columns with all NA elements
na_rows <- rowSums(is.na(w)) == ncol(w)
na_cols <- colSums(is.na(w)) == nrow(w)

# Remove rows and columns with all NA elements
w_clean_Soft <- w[!na_rows, !na_cols]

# Change row and column names to GO name
goName <- permTest_Soft %>% filter(go %in% rownames(w_clean_Soft))
goName <- goName$name_1006
colnames(w_clean_Soft) <- rownames(w_clean_Soft) <- goName

# Set the diagonal elements to 1
diag(w_clean_Soft) <- 1

# Save

saveRDS(w_clean_Soft, "GO_similarity_matrix_clean_Soft.rds")

# Convert similarities to dissimilarities (if needed)
distance_matrix <- 1 - w_clean_Soft

# Perform hierarchical clustering
hclust_result <- hclust(as.dist(distance_matrix))

# Plot the dendrogram
plot(hclust_result)

# Convert the clustering result to a dendrogram
dendrogram_Soft <- as.dendrogram(hclust_result)

# Create a ggplot2 dendrogram plot
gg_Soft <- ggdendrogram(dendrogram_Soft, rotate = FALSE) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Print the plot
print(gg_Soft)



# Calculate similarity for HardPartial sweeps

permTest_HardPartial <- permTest_intersect %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes) %>% 
  select(-p_intersect, -q, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(fdr <= 0.05)

GO_HardPartial <- permTest_HardPartial$go[permTest_HardPartial$fdr<=0.05]

n <- length(GO_HardPartial)
w <- matrix(NA, nrow = n, ncol = n)
colnames(w) <- rownames(w) <- GO_HardPartial

for (i in seq_len(n)) {
  cat("\n", i, " ")
  for (j in (i + 1):n) {
    w[i, j] <- goSim(GO_HardPartial[i], GO_HardPartial[j], semData=hsGO, measure="Jiang")
    # Set the corresponding element in the lower triangle to match the upper triangle
    w[j, i] <- w[i, j]
    cat(j, ",", sep="")
  }
}

# Identify rows and columns with all NA elements
na_rows <- rowSums(is.na(w)) == ncol(w)
na_cols <- colSums(is.na(w)) == nrow(w)

# Remove rows and columns with all NA elements
w_clean_HardPartial <- w[!na_rows, !na_cols]

# Change row and column names to GO name
goName <- permTest_HardPartial %>% filter(go %in% rownames(w_clean_HardPartial))
goName <- goName$name_1006
colnames(w_clean_HardPartial) <- rownames(w_clean_HardPartial) <- goName

# Set the diagonal elements to 1
diag(w_clean_HardPartial) <- 1

# Save

saveRDS(w_clean_HardPartial, "GO_similarity_matrix_clean_HardPartial.rds")

# Convert similarities to dissimilarities (if needed)
distance_matrix <- 1 - w_clean_HardPartial

# Perform hierarchical clustering
hclust_result <- hclust(as.dist(distance_matrix))

# Plot the dendrogram
plot(hclust_result)

# Convert the clustering result to a dendrogram
dendrogram_HardPartial <- as.dendrogram(hclust_result)

# Create a ggplot2 dendrogram plot
gg_HardPartial <- ggdendrogram(dendrogram_HardPartial, rotate = FALSE) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plot
print(gg_HardPartial)



# Calculate similarity for SoftPartial sweeps

GO_SoftPartial <- permTest_SoftPartial$go[permTest_SoftPartial$fdr<=0.3]

n <- length(GO_SoftPartial)
w <- matrix(NA, nrow = n, ncol = n)
colnames(w) <- rownames(w) <- GO_SoftPartial

try(
  for (i in seq_len(n)) {
    cat("\n", i, " ")
    for (j in (i + 1):n) {
      w[i, j] <- goSim(GO_SoftPartial[i], GO_SoftPartial[j], semData=hsGO, measure="Jiang")
      # Set the corresponding element in the lower triangle to match the upper triangle
      w[j, i] <- w[i, j]
      cat(j, ",", sep="")
    }
  }
)

# Identify rows and columns with all NA elements
na_rows <- rowSums(is.na(w)) == ncol(w)
na_cols <- colSums(is.na(w)) == nrow(w)

# Remove rows and columns with all NA elements
w_clean_SoftPartial <- w[!na_rows, !na_cols]

# Change row and column names to GO name
goName <- permTest_SoftPartial %>% filter(go %in% rownames(w_clean_SoftPartial)) %>% left_join(go_info)
goName <- goName$name_1006
colnames(w_clean_SoftPartial) <- rownames(w_clean_SoftPartial) <- goName

# Set the diagonal elements to 1
diag(w_clean_SoftPartial) <- 1

# Save a matrix to an R Data file
save(w_clean_SoftPartial, file = "goSim_SoftPartial.RData")

# Convert similarities to dissimilarities (if needed)
distance_matrix <- 1 - w_clean_SoftPartial

# Perform hierarchical clustering
hclust_result <- hclust(as.dist(distance_matrix))

# Plot the dendrogram
plot(hclust_result)

# Convert the clustering result to a dendrogram
dendrogram_SoftPartial <- as.dendrogram(hclust_result)

# Create a ggplot2 dendrogram plot
gg_SoftPartial <- ggdendrogram(dendrogram_SoftPartial, rotate = FALSE) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Print the plot
print(gg_SoftPartial)

```

## Average size of prediction peaks?

```{r, OLD}

predPeaks %>% 
  group_by(val) %>% 
  summarise(length_mean = mean(length), 
            length_lci = length_mean - 1.96 * (sd(length)/sqrt(n())), 
            length_uci = length_mean + 1.96 * (sd(length)/sqrt(n())))

```

A tibble: 9 x 4
  val                length_mean length_lci length_uci
  <chr>                    <dbl>      <dbl>      <dbl>
1 Hard                     8681.      8044.      9318.
2 Hard-linked             10595.      9937.     11252.
3 HardPartial              5476.      5027.      5925.
4 HardPartial-linked       7117.      6257.      7977.
5 Neutral                 13586.     13002.     14170.
6 Soft                     6629.      6476.      6781.
7 Soft-linked             16453.     16022.     16884.
8 SoftPartial              5544.      5344.      5744.
9 SoftPartial-linked       7382.      7083.      7680.

## Average number of genes per prediction peak?

```{r, OLD}

for (i in 1:nrow(predPeaks)) {
  
  cat(i, "\n")
  
  scf <- predPeaks$scf[i]
  winStart <- predPeaks$start_pos[i]
  winEnd <- predPeaks$end_pos[i]
  
  n_genes <- GO_bed %>% filter(scf==scf & start_pos >= winStart & end_pos <= winEnd) %>% nrow()
  
  predPeaks$n_genes[i] <- n_genes
  
}

predPeaks %>% 
  group_by(val) %>% 
  summarise(n_genes_mean = mean(n_genes), 
            n_genes_lci = n_genes_mean - 1.96 * (sd(n_genes)/sqrt(n())), 
            n_genes_uci = n_genes_mean + 1.96 * (sd(n_genes)/sqrt(n())))

```

A tibble: 9 x 4
  val                n_genes_mean n_genes_lci n_genes_uci
  <chr>                     <dbl>       <dbl>       <dbl>
1 Hard                       1.98       1.56         2.39
2 Hard-linked                2.69       2.35         3.04
3 HardPartial                1.31       0.874        1.74
4 HardPartial-linked         1.51       1.02         2.00
5 Neutral                    3.98       3.71         4.26
6 Soft                       1.62       1.52         1.73
7 Soft-linked                4.95       4.74         5.16
8 SoftPartial                1.04       0.874        1.21
9 SoftPartial-linked         1.62       1.48         1.77

## Permutation test

### Limit to one GO term per sub-window

#### Empirical

```{r, OLD}

# # Join GO terms for predictions peaks
# 
# predPeaks_temp <- predPeaks %>% dplyr::select(scf, start_pos, end_pos, val, len, length)
# write.table(predPeaks_temp, "predPeaks_temp_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# # bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
# system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE) %>% 
  set_names("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "len", "length", "intersect") %>% 
  left_join(GO_geneName) %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

# # Join GO terms for predictions peaks at the 5kb sub-window level
# 
# predictions_temp <- predictions %>% select(scf, start_pos, end_pos, val)
# write.table(predictions_temp, "predictions_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# # bedtools
# 
# cmd <- paste0("bedtools intersect -a GO.bed -b predictions_temp.tsv -wo > predictions_GO_merged.tsv")
# system(cmd)

predictionsGO <- read_tsv("predictions_GO_merged.tsv", col_names=FALSE) %>% 
  set_names("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_pred", "start_pos_pred", "end_pos_pred", "val", "geneLength") %>% 
  mutate(subWinLength = end_pos_pred - start_pos_pred) %>% 
  arrange(scf_pred, start_pos_pred, end_pos_pred) %>% 
  left_join(GO_geneName) %>% 
  arrange(scf_pred, start_pos_pred, go)

# How often does the same GO term appear in a sub-window?

predictionsGO %>% 
  dplyr::select(-scf, -start_pos, -end_pos, -fbgn, -chrom, -geneLength, -geneName) %>% 
  group_by(scf_pred, start_pos_pred, subWinLength, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- predictionsGO %>% 
  dplyr::select(-scf, -start_pos, -end_pos, -fbgn, -chrom, -geneLength, -geneName) %>% 
  group_by(scf_pred, start_pos_pred, subWinLength, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

# Limit to one count per GO term per subwindow

predictionsGO_1perWin <- predictionsGO %>% 
  select(-scf, -start_pos, -end_pos, -fbgn, -chrom, -geneLength, -geneName) %>% 
  distinct()

predictionsGO_1perWin %>% 
  group_by(scf_pred, start_pos_pred, subWinLength, go) %>% 
  tally() %>% 
  arrange(desc(n))
  
# # Add GO terms from predictionsGO_1perWin to predPeaks
# 
# predictionsGO_temp <- predictionsGO_1perWin %>% 
#   select(scf_pred, start_pos_pred, end_pos_pred, val, go, process) %>% 
#   distinct()
# 
# predPeaks_temp <- predPeaks %>% 
#   select(scf, start_pos, end_pos, val, len, length)
# 
# write.table(predictionsGO_temp, "predictionsGO_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
# 
# cmd <- paste0("bedtools intersect -a predictionsGO_temp.tsv -b predPeaks_temp.tsv -wo > predPeaks_GO_1perSubWindow.tsv")
# system(cmd)

predPeaksGO_1perSubWin <- read_tsv("predPeaks_GO_1perSubWindow.tsv", col_names=FALSE) %>% 
  set_names("scf_pred", "start_pos_pred", "end_pos_pred", "val_pred", "go", "process", "scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "X13") %>% 
  select(-X13) %>% 
  arrange(scf_peak, start_pos_peak, go)

# How often does the same GO term appear in a peak-window?

predPeaksGO_1perSubWin %>% 
  select("scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "go", "process") %>% 
  group_by(scf_peak, start_pos_peak, val_peak, length, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- predPeaksGO_1perSubWin %>% 
  select("scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "go", "process") %>% 
  group_by(scf_peak, start_pos_peak, val_peak, length, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

# How often does the same GO term appear in a Hard peak-window?

predPeaksGO_1perSubWin %>% 
  filter(val_peak=="Hard") %>% 
  select("scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "go", "process") %>% 
  group_by(scf_peak, start_pos_peak, val_peak, length, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- predPeaksGO_1perSubWin %>% 
  filter(val_peak=="Hard") %>% 
  select("scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "go", "process") %>% 
  group_by(scf_peak, start_pos_peak, val_peak, length, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

# How often does the same GO term appear in a HardPartial peak-window?

predPeaksGO_1perSubWin %>% 
  filter(val_peak=="HardPartial") %>% 
  select("scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "go", "process") %>% 
  group_by(scf_peak, start_pos_peak, val_peak, length, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- predPeaksGO_1perSubWin %>% 
  filter(val_peak=="HardPartial") %>% 
  select("scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "go", "process") %>% 
  group_by(scf_peak, start_pos_peak, val_peak, length, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

# How often does the same GO term appear in a Soft peak-window?

predPeaksGO_1perSubWin %>% 
  filter(val_peak=="Soft") %>% 
  select("scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "go", "process") %>% 
  group_by(scf_peak, start_pos_peak, val_peak, length, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- predPeaksGO_1perSubWin %>% 
  filter(val_peak=="Soft") %>% 
  select("scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "go", "process") %>% 
  group_by(scf_peak, start_pos_peak, val_peak, length, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

# How often does the same GO term appear in a SoftPartial peak-window?

predPeaksGO_1perSubWin %>% 
  filter(val_peak=="SoftPartial") %>% 
  select("scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "go", "process") %>% 
  group_by(scf_peak, start_pos_peak, val_peak, length, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- predPeaksGO_1perSubWin %>% 
  filter(val_peak=="SoftPartial") %>% 
  select("scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "go", "process") %>% 
  group_by(scf_peak, start_pos_peak, val_peak, length, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

```

#### Permutations

```{r, OLD}

# Create bed of all possible sub-windows

subWinBed <- data.frame(
  scf = c(rep("ScA8VGg_542", 4183), 
          rep("ScA8VGg_594", 3927), 
          rep("ScA8VGg_628", 6204), 
          rep("ScA8VGg_718", 1654), 
          rep("ScA8VGg_76", 7412), 
          rep("ScA8VGg_785", 4908)), 
  start_pos = c(seq(from=82500, to=20992500, by=5000), 
                seq(from=42500, to=19672500, by=5000), 
                seq(from=207500, to=31222500, by=5000), 
                seq(from=222500, to=8487500, by=5000), 
                seq(from=22500, to=37077500, by=5000), 
                seq(from=22500, to=24557500, by=5000))
)

subWinBed <- subWinBed %>% 
  mutate(end_pos = start_pos + 5000)

# Join GO terms to subWinBed

write.table(subWinBed, "subWinBed.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

# bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b subWinBed.tsv -wo > subWinBed_GO_merged.tsv")
system(cmd)

subWinBedGO <- read_tsv("subWinBed_GO_merged.tsv", col_names=FALSE) %>% 
  set_names("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_sub", "start_pos_sub", "end_pos_sub", "geneLength") %>% 
  mutate(subWinLength = end_pos_sub - start_pos_sub) %>% 
  arrange(scf_sub, start_pos_sub, end_pos_sub) %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_sub, start_pos_sub, go)

# How often does the same GO term appear in a sub-window?

subWinBedGO %>% 
  dplyr::select(-scf, -start_pos, -end_pos, -fbgn, -chrom, -geneLength, -geneName) %>% 
  group_by(scf_sub, start_pos_sub, subWinLength, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- subWinBedGO %>% 
  dplyr::select(-scf, -start_pos, -end_pos, -fbgn, -chrom, -geneLength, -geneName) %>% 
  group_by(scf_sub, start_pos_sub, subWinLength, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

# Add GO terms from subWinBedGO to permutations

subWinBedGO_temp <- subWinBedGO %>% 
  dplyr::select(scf, start_pos, end_pos, chrom, fbgn, geneName, go, process)

premutations_temp <- permutations %>% 
  mutate(length_perm = end_pos_perm - start_pos_perm) %>% 
  dplyr::select(scf_perm, start_pos_perm, end_pos_perm, perm, length_perm, val)

write.table(subWinBedGO_temp, "subWinBedGO_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
write.table(premutations_temp, "permutations_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

cmd <- paste0("bedtools intersect -a subWinBedGO_temp.tsv -b permutations_temp.tsv -wo > permutations_GO.tsv")
system(cmd)

# permutationsGO <- read_tsv("permutations_GO.tsv", col_names=FALSE)
permutationsGO <- fread("permutations_GO.tsv", sep="\t", header=FALSE)

colnames(permutationsGO) <- c("scf", "start_pos", "end_pos", "chrom", "fbgn", "geneName", "go", "process", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "length_perm", "val_perm", "intersect")

# Limit to one count per GO term per subwindow

subWinBedGO_1perWin <- subWinBedGO %>% 
  select(-scf, -start_pos, -end_pos, -fbgn, -chrom, -geneLength, -geneName) %>% 
  distinct()

subWinBedGO_1perWin %>% 
  group_by(scf_sub, start_pos_sub, subWinLength, go) %>% 
  tally() %>% 
  arrange(desc(n))

# Add GO terms from subWinBedGO_1perWin to permutations

subWinBedGO_temp <- subWinBedGO_1perWin %>% 
  select(scf_sub, start_pos_sub, end_pos_sub, go, process) %>% 
  distinct()

premutations_temp <- permutations %>% 
  mutate(length_perm = end_pos_perm - start_pos_perm) %>% 
  select(scf_perm, start_pos_perm, end_pos_perm, perm, length_perm, val, scf, start_pos, end_pos, len, length)

write.table(subWinBedGO_temp, "subWinBedGO_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
write.table(premutations_temp, "permutations_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

cmd <- paste0("bedtools intersect -a subWinBedGO_temp.tsv -b permutations_temp.tsv -wo > permutations_GO_1perSubWindow.tsv")
system(cmd)

permutationsGO_1perSubWin <- read_tsv("permutations_GO_1perSubWindow.tsv", col_names=FALSE) %>% 
  set_names("scf_sub", "start_pos_sub", "end_pos_sub", "go", "process", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "length_perm", "val_perm", "scf", "start_pos", "end_pos", "len", "length", "X17") %>%
  select(-X17) %>% 
  arrange(perm, scf, start_pos, end_pos, go)

# How often does the same GO term appear in a perm-window?

permutationsGO_1perSubWin %>% 
  filter(perm <= 10) %>% 
  select(perm, scf_perm, start_pos_perm, end_pos_perm, val_perm, len, length, go, process) %>% 
  group_by(perm, scf_perm, start_pos_perm, val_perm, length, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- permutationsGO_1perSubWin %>% 
  filter(perm <= 10) %>% 
  select(perm, scf_perm, start_pos_perm, end_pos_perm, val_perm, len, length, go, process) %>% 
  group_by(perm, scf_perm, start_pos_perm, val_perm, length, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

# How often does the same GO term appear in a Hard perm-window?

permutationsGO_1perSubWin %>% 
  filter(perm <= 10) %>% 
  filter(val_perm=="Hard") %>% 
  select("scf_perm", "start_pos_perm", "end_pos_perm", "val_perm", "len", "length", "go", "process") %>% 
  group_by(scf_perm, start_pos_perm, val_perm, length, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- permutationsGO_1perSubWin %>% 
  filter(perm <= 10) %>% 
  filter(val_perm=="Hard") %>% 
  select("scf_perm", "start_pos_perm", "end_pos_perm", "val_perm", "len", "length", "go", "process") %>% 
  group_by(scf_perm, start_pos_perm, val_perm, length, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

# How often does the same GO term appear in a HardPartial perm-window?

permutationsGO_1perSubWin %>% 
  filter(perm <= 10) %>% 
  filter(val_perm=="HardPartial") %>% 
  select("scf_perm", "start_pos_perm", "end_pos_perm", "val_perm", "len", "length", "go", "process") %>% 
  group_by(scf_perm, start_pos_perm, val_perm, length, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- permutationsGO_1perSubWin %>% 
  filter(perm <= 10) %>% 
  filter(val_perm=="HardPartial") %>% 
  select("scf_perm", "start_pos_perm", "end_pos_perm", "val_perm", "len", "length", "go", "process") %>% 
  group_by(scf_perm, start_pos_perm, val_perm, length, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

# How often does the same GO term appear in a Soft perm-window?

permutationsGO_1perSubWin %>% 
  filter(perm <= 10) %>% 
  filter(val_perm=="Soft") %>% 
  select("scf_perm", "start_pos_perm", "end_pos_perm", "val_perm", "len", "length", "go", "process") %>% 
  group_by(scf_perm, start_pos_perm, val_perm, length, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- permutationsGO_1perSubWin %>% 
  filter(perm <= 10) %>% 
  filter(val_perm=="Soft") %>% 
  select("scf_perm", "start_pos_perm", "end_pos_perm", "val_perm", "len", "length", "go", "process") %>% 
  group_by(scf_perm, start_pos_perm, val_perm, length, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

# How often does the same GO term appear in a SoftPartial perm-window?

permutationsGO_1perSubWin %>% 
  filter(val_perm=="SoftPartial") %>% 
  select("scf_perm", "start_pos_perm", "end_pos_perm", "val_perm", "len", "length", "go", "process") %>% 
  group_by(scf_perm, start_pos_perm, val_perm, length, go) %>% 
  tally() %>% 
  arrange(desc(n))

tempData <- permutationsGO_1perSubWin %>% 
  filter(val_perm=="SoftPartial") %>% 
  select("scf_perm", "start_pos_perm", "end_pos_perm", "val_perm", "len", "length", "go", "process") %>% 
  group_by(scf_perm, start_pos_perm, val_perm, length, go) %>% 
  summarise(n_go = n()) %>% 
  ungroup()

mean(tempData$n_go)
nrow(tempData[tempData$n_go > 1,])/nrow(tempData) 

```

### Test

```{r, OLD}

# library(tidyverse)
library(dplyr)
library(tidyr)
library(ggplot2)
library(rlang)
library(readr)
library(data.table)
library(qvalue)
# library(biomaRt)

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

# Import data

predPeaksGO_1perSubWin <- read_tsv("predPeaks_GO_1perSubWindow.tsv", col_names=FALSE) %>% 
  set_names("scf_pred", "start_pos_pred", "end_pos_pred", "val_pred", "go", "process", "scf_peak", "start_pos_peak", "end_pos_peak", "val_peak", "len", "length", "X13") %>% 
  dplyr::select(-X13) %>% 
  arrange(scf_peak, start_pos_peak, go)

permutationsGO_1perSubWin <- fread("permutations_GO_1perSubWindow.tsv", sep="\t", header=FALSE)
colnames(permutationsGO_1perSubWin) <- c("scf_sub", "start_pos_sub", "end_pos_sub", "go", "process", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "length_perm", "val_perm", "scf", "start_pos", "end_pos", "len", "length", "X17")
permutationsGO_1perSubWin <- permutationsGO_1perSubWin %>%
  dplyr::select(-X17) %>% 
  arrange(perm, scf, start_pos, end_pos, go)

## I DON"T KNOW WHERE YIGUAN GETS THESE GO FILES FROM.

GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>% 
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

## Intersect

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE) %>% 
  set_names("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "len", "length", "intersect") %>% 
  left_join(GO_geneName) %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

permutationsGO <- fread("permutations_GO.tsv", sep="\t", header=FALSE)
colnames(permutationsGO) <- c("scf", "start_pos", "end_pos", "chrom", "fbgn", 
                              "geneName", "go", "process", "scf_perm", 
                              "start_pos_perm", "end_pos_perm", "perm", 
                              "length_perm", "val_perm", "intersect")

# Get GO term names and descriptions

# ensembl <- useEnsembl(biomart = "ensembl", dataset = "dmelanogaster_gene_ensembl", mirror = "asia")
# go_info <- getBM(attributes = c("go_id", "name_1006", "definition_1006"), mart = ensembl)
# write.table(go_info, "go_info.tsv", row.names = F, col.names = T, quote = F, sep = "\t")

go_info <- read_tsv("go_info.tsv") %>% rename(go = go_id)

# Empirical summary

predPeaksGO_1perSubWin_summary <- predPeaksGO_1perSubWin %>% 
  rename(val = val_peak) %>% 
  group_by(val, go, process) %>% 
  summarise(n_peak = n()) %>% 
  ungroup()

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect = sum(intersect)) %>% 
  ungroup()

# Permutation summary

permutationsGO_1perSubWin_summary <- permutationsGO_1perSubWin %>% 
  rename(val = val_perm) %>% 
  group_by(perm, val, go, process) %>% 
  summarise(n_perm = n()) %>% 
  ungroup()

permutationsGO_summary <- permutationsGO %>% 
  rename(val = val_perm) %>% 
  group_by(perm, val, go, process) %>% 
  summarise(intersect_perm = sum(intersect)) %>% 
  ungroup()

# Permutation test

## Count

permTest <- permutationsGO_1perSubWin_summary %>% 
  left_join(predPeaksGO_1perSubWin_summary, by=c("val", "go")) %>% 
  filter(is.na(n_peak)==FALSE) %>% 
  mutate(test = n_perm >= n_peak) %>% 
  group_by(val, go, process) %>% 
  summarise(p = sum(test)/1000)

## Count null

permutationsGO_1perSubWin_summary_1 <- permutationsGO_1perSubWin_summary %>% 
  filter(perm==1) %>% 
  select(-perm) %>% 
  rename(n_perm_1 = n_perm)

permTest_null <- permutationsGO_1perSubWin_summary %>% 
  filter(perm!=1) %>% 
  left_join(permutationsGO_1perSubWin_summary_1, by=c("val", "go")) %>% 
  filter(is.na(n_perm_1)==FALSE) %>% 
  mutate(test = n_perm >= n_perm_1) %>% 
  group_by(val, go, process.y) %>% 
  summarise(p = sum(test)/999)

## Intersect

permTest_intersect <- permutationsGO_summary %>% 
  left_join(predPeaksGO_summary, by=c("val", "go")) %>% 
  filter(is.na(intersect)==FALSE) %>% 
  mutate(test = intersect_perm >= intersect) %>% 
  group_by(val, go, process.y) %>% 
  summarise(p = sum(test)/1000)

### Intersect null

permutationsGO_summary_1 <- permutationsGO_summary %>% 
  filter(perm==1) %>% 
  select(-perm) %>% 
  rename(intersect_perm_1 = intersect_perm)

permTest_intersect_null <- permutationsGO_summary %>% 
  filter(perm!=1) %>% 
  left_join(permutationsGO_summary_1, by=c("val", "go")) %>% 
  filter(is.na(intersect_perm_1)==FALSE) %>% 
  mutate(test = intersect_perm >= intersect_perm_1) %>% 
  group_by(val, go, process.y) %>% 
  summarise(p = sum(test)/999)



# Hard

## Count

permTest_Hard <- permTest %>% 
  filter(val=="Hard") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_Hard %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("Hard")

permTest_Hard$fdr <- p.adjust(permTest_Hard$p, method="fdr")

permTest_Hard %>% 
  filter(fdr <= 0.3) %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

## Null

permTest_Hard_null <- permTest_null %>% 
  filter(val=="Hard") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_Hard_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("Hard (null)")

permTest_Hard_null$fdr <- p.adjust(permTest_Hard_null$p, method="fdr")
#permTest_intersect_Hard_null$q <- qvalue(permTest_intersect_Hard_null$p)$qvalues

permTest_Hard_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

## Intersect

permTest_intersect_Hard <- permTest_intersect %>% 
  filter(val=="Hard") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_intersect_Hard %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("Hard")

permTest_intersect_Hard$fdr <- p.adjust(permTest_intersect_Hard$p, method="fdr")
permTest_intersect_Hard$q <- qvalue(permTest_intersect_Hard$p)$qvalues

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Hard) %>% 
  as.data.frame()

permTest_intersect_Hard %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Hard) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p))) + 
  geom_point()

## Null

permTest_intersect_Hard_null <- permTest_intersect_null %>% 
  filter(val=="Hard") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_intersect_Hard_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("Hard (null)")

permTest_intersect_Hard_null$fdr <- p.adjust(permTest_intersect_Hard_null$p, method="fdr")
#permTest_intersect_Hard_null$q <- qvalue(permTest_intersect_Hard_null$p)$qvalues

permTest_intersect_Hard_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

intersect_genome <- permutationsGO %>% filter(perm==1) %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
intersect_Hard <- permutationsGO %>% filter(perm==1) %>% filter(val_perm=="Hard") %>% group_by(go) %>% summarise(intersect_Hard = sum(intersect))

permTest_intersect_Hard_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Hard) %>% 
  as.data.frame()

permTest_intersect_Hard_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Hard) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p))) + 
  geom_point()

permTest_intersect_Hard_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Hard) %>% 
  ggplot(aes(x=p)) + 
  geom_histogram()

permTest_intersect_Hard_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Hard) %>% 
  filter(intersect_genome>50000) %>% 
  ggplot(aes(x=p)) + 
  geom_histogram()



# Soft

## Count

permTest_Soft <- permTest %>% 
  filter(val=="Soft") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_Soft %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("Soft")

permTest_Soft$fdr <- p.adjust(permTest_Soft$p, method="fdr")

permTest_Soft %>% 
  filter(fdr <= 0.3) %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

## Null

permTest_Soft_null <- permTest_null %>% 
  filter(val=="Soft") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_Soft_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("Soft (null)")

permTest_Soft_null$fdr <- p.adjust(permTest_Soft_null$p, method="fdr")
#permTest_intersect_Soft_null$q <- qvalue(permTest_intersect_Soft_null$p)$qvalues

permTest_Soft_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

## intersect

permTest_intersect_Soft <- permTest_intersect %>% 
  filter(val=="Soft") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_intersect_Soft %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("Soft")

permTest_intersect_Soft$fdr <- p.adjust(permTest_intersect_Soft$p, method="fdr")
permTest_intersect_Soft$q <- qvalue(permTest_intersect_Soft$p)$qvalues

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
intersect_Soft <- predPeaksGO %>% filter(val=="Soft") %>% group_by(go) %>% summarise(intersect_Soft = sum(intersect))

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Soft) %>% 
  as.data.frame()

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Soft) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p))) + 
  geom_point()

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Soft) %>% 
  ggplot(aes(x=log2(intersect_genome), y=1-log10(p))) + 
  geom_point()

## Null

permTest_intersect_Soft_null <- permTest_intersect_null %>% 
  filter(val=="Soft") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_intersect_Soft_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("Soft (null)")

permTest_intersect_Soft_null$fdr <- p.adjust(permTest_intersect_Soft_null$p, method="fdr")
#permTest_intersect_Soft_null$q <- qvalue(permTest_intersect_Soft_null$p)$qvalues

permTest_intersect_Soft_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()



# HardPartial

## Count

permTest_HardPartial <- permTest %>% 
  filter(val=="HardPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_HardPartial %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("HardPartial")

permTest_HardPartial$fdr <- p.adjust(permTest_HardPartial$p, method="fdr")

permTest_HardPartial %>% 
  filter(fdr <= 0.3) %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

## Null

permTest_HardPartial_null <- permTest_null %>% 
  filter(val=="HardPartial") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_HardPartial_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("HardPartial (null)")

permTest_HardPartial_null$fdr <- p.adjust(permTest_HardPartial_null$p, method="fdr")
#permTest_intersect_HardPartial_null$q <- qvalue(permTest_intersect_HardPartial_null$p)$qvalues

permTest_HardPartial_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

## intersect

permTest_intersect_HardPartial <- permTest_intersect %>% 
  filter(val=="HardPartial") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_intersect_HardPartial %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("HardPartial")

permTest_intersect_HardPartial$fdr <- p.adjust(permTest_intersect_HardPartial$p, method="fdr")
# permTest_intersect_HardPartial$q <- qvalue(permTest_intersect_HardPartial$p)$qvalues

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
intersect_HardPartial <- predPeaksGO %>% filter(val=="HardPartial") %>% group_by(go) %>% summarise(intersect_HardPartial = sum(intersect))

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_HardPartial) %>% 
  as.data.frame()

permTest_intersect_HardPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_HardPartial) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p))) + 
  geom_point()

## Null

permTest_intersect_HardPartial_null <- permTest_intersect_null %>% 
  filter(val=="HardPartial") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_intersect_HardPartial_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("HardPartial (null)")

permTest_intersect_HardPartial_null$fdr <- p.adjust(permTest_intersect_HardPartial_null$p, method="fdr")
#permTest_intersect_HardPartial_null$q <- qvalue(permTest_intersect_HardPartial_null$p)$qvalues

permTest_intersect_HardPartial_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()



# SoftPartial

## Count

permTest_SoftPartial <- permTest %>% 
  filter(val=="SoftPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_SoftPartial %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("SoftPartial")

permTest_SoftPartial$fdr <- p.adjust(permTest_SoftPartial$p, method="fdr")

permTest_SoftPartial %>% 
  filter(fdr <= 0.3) %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

## Null

permTest_SoftPartial_null <- permTest_null %>% 
  filter(val=="SoftPartial") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_SoftPartial_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("SoftPartial (null)")

permTest_SoftPartial_null$fdr <- p.adjust(permTest_SoftPartial_null$p, method="fdr")
#permTest_intersect_SoftPartial_null$q <- qvalue(permTest_intersect_SoftPartial_null$p)$qvalues

permTest_SoftPartial_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

## intersect

permTest_intersect_SoftPartial <- permTest_intersect %>% 
  filter(val=="SoftPartial") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_intersect_SoftPartial %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("SoftPartial")

permTest_intersect_SoftPartial$fdr <- p.adjust(permTest_intersect_SoftPartial$p, method="fdr")
# permTest_intersect_SoftPartial$q <- qvalue(permTest_intersect_SoftPartial$p)$qvalues

permTest_intersect_SoftPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
intersect_SoftPartial <- predPeaksGO %>% filter(val=="SoftPartial") %>% group_by(go) %>% summarise(intersect_SoftPartial = sum(intersect))

permTest_intersect_SoftPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_SoftPartial) %>% 
  as.data.frame()

permTest_intersect_SoftPartial %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_SoftPartial) %>% 
  ggplot(aes(x=intersect_genome, y=1-log10(p))) + 
  geom_point()

## Null

permTest_intersect_SoftPartial_null <- permTest_intersect_null %>% 
  filter(val=="SoftPartial") %>% 
  filter(process.y=="P") %>% 
  distinct() %>% 
  arrange(p)

permTest_intersect_SoftPartial_null %>% ggplot(aes(x=p)) + geom_histogram() + ggtitle("SoftPartial (null)")

permTest_intersect_SoftPartial_null$fdr <- p.adjust(permTest_intersect_SoftPartial_null$p, method="fdr")
#permTest_intersect_SoftPartial_null$q <- qvalue(permTest_intersect_SoftPartial_null$p)$qvalues

permTest_intersect_SoftPartial_null %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

```

### Assess test

```{r, OLD}

# Hard 

permTest_Hard %>% 
  filter(fdr <= 0.3) %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  as.data.frame()

## GO:0097305 response to alcohol

predictionsGO %>% filter(go=="GO:0097305") %>% arrange(scf_pred, start_pos_pred)

### Only two genes with this go term.
### Both genes are fairly large, the one causing the apparent enrichment in Hard sweeps is 19,135 bp and spans 5 sub-windows.
### 3 of those sub-windows are a merged Hard peak.
### This gives us a count of 3 in the empirical that is never exceeded by the permutations.

## GO:0006955 immune response

predictionsGO %>% 
  filter(go=="GO:0006955") %>% 
  arrange(scf_pred, start_pos_pred) %>% 
  as.data.frame()

predictionsGO %>% 
  filter(go=="GO:0006955") %>% 
  group_by(fbgn) %>% 
  summarise(length = end_pos - start_pos) %>% 
  distinct()

```

## Hypergeometic

```{r, OLD}

# Hard

## GO:0097305 response to alcohol

### q: The number of white balls drawn without replacement

n_white_drawn <- predPeaksGO %>% 
  filter(go=="GO:0097305") %>% 
  group_by(val) %>% 
  summarise(n_white_drawn = sum(intersect))

q <- n_white_drawn$n_white_drawn[n_white_drawn$val=="Hard"]

### m: The number of white balls in the urn.

m <- sum(n_white_drawn$n_white_drawn)

### n: The number of black balls in the urn.

n_black <- predPeaksGO %>% 
  select(fbgn, scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks, val, length, intersect) %>% 
  distinct() %>% 
  group_by(val) %>% 
  summarise(n_black = sum(length))

n <- sum(n_black$n_black) - m

### The number of balls drawn from the urn

n_drawn <- predPeaksGO %>% 
  select(fbgn, scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks, val, length, intersect) %>% 
  distinct() %>% 
  group_by(val) %>% 
  summarise(n_drawn = sum(length))

k <- n_drawn$n_drawn[n_drawn$val=="Hard"]

### Perform the hypergeometric test

cat("n white drawn ", q, "\n")
cat("n white in urn ", m, "\n")
cat("n black in urn ", n, "\n")
cat("n drawn ", k, "\n")

phyper(q - 1, m, n, k, lower.tail = FALSE)



go_Hard <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(go) %>% 
  distinct() %>% 
  filter(is.na(go)==FALSE)

for (i in 1:nrow(go_Hard)) {
  
  ### q: The number of white balls drawn without replacement
  
  n_white_drawn <- predPeaksGO %>% 
    filter(go==go_Hard$go[i]) %>% 
    group_by(val) %>% 
    summarise(n_white_drawn = sum(intersect))
  
  q <- n_white_drawn$n_white_drawn[n_white_drawn$val=="Hard"]
  
  ### m: The number of white balls in the urn.
  
  m <- sum(n_white_drawn$n_white_drawn)
  
  ### n: The number of black balls in the urn.
  
  n_black <- predPeaksGO %>% 
    select(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks, val, length, intersect) %>% 
    distinct() %>% 
    group_by(val) %>% 
    summarise(n_black = sum(intersect))
  
  n <- sum(n_black$n_black) - m
  
  ### The number of balls drawn from the urn
  
  n_drawn <- predPeaksGO %>% 
    select(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks, val, length, intersect) %>% 
    distinct() %>% 
    group_by(val) %>% 
    summarise(n_drawn = sum(length))
  
  k <- n_drawn$n_drawn[n_drawn$val=="Hard"]
  
  ### Perform the hypergeometric tes
  
  p_value <- phyper(q - 1, m, n, k, lower.tail = FALSE)
  go_Hard$p[i] <- p_value 
  go_Hard$q[i] <- q 
  go_Hard$m[i] <- m 
  go_Hard$n[i] <- n 
  go_Hard$k[i] <- k 
  cat(go_Hard$go[i], " ", p_value, "\n")
  
}

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))
intersect_Hard <- predPeaksGO %>% filter(val=="Hard") %>% group_by(go) %>% summarise(intersect_SoftPartial = sum(intersect))

go_Hard %>% 
  mutate(ratio = (q/k)/(m/n)) %>% 
  arrange(desc(ratio), p) %>% 
  left_join(select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Hard) %>% 
  as.data.frame()

go_Hard %>% 
  mutate(ratio = (q/k)/(m/n)) %>% 
  arrange(desc(ratio), p) %>% 
  left_join(select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Hard) %>% 
  mutate(p_cat = ifelse(p < 0.5, "p_less_0.5", "p_greater_0.5")) %>% 
  ggplot(aes(x=p_cat, y=log(intersect_genome))) + 
  geom_boxplot()

go_Hard %>% 
  mutate(ratio = (q/k)/(m/n)) %>% 
  arrange(desc(ratio), p) %>% 
  left_join(select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(intersect_Hard) %>% 
  mutate(p_cat = ifelse(p < 0.5, "p_less_0.5", "p_greater_0.5")) %>% 
  group_by(p_cat) %>% 
  summarise(mean = mean(intersect_genome), 
            min = min(intersect_genome), 
            max = max(intersect_genome))



go_Hard_null <- permutationsGO %>% 
  filter(perm==1) %>% 
  filter(val_perm=="Hard") %>% 
  select(go) %>% 
  distinct() %>% 
  filter(is.na(go)==FALSE)

permutationsGO_perm_1 <- permutationsGO %>% filter(perm==1)

for (i in 1:nrow(go_Hard_null)) {
  
  ### q: The number of white balls drawn without replacement
  
  n_white_drawn <- permutationsGO_perm_1 %>% 
    filter(go==go_Hard_null$go[i]) %>% 
    group_by(val_perm) %>% 
    summarise(n_white_drawn = sum(intersect))
  
  q <- n_white_drawn$n_white_drawn[n_white_drawn$val_perm=="Hard"]
  
  ### m: The number of white balls in the urn.
  
  m <- sum(n_white_drawn$n_white_drawn)
  
  ### n: The number of black balls in the urn.
  
  n_black <- permutationsGO_perm_1 %>% 
    select(scf_perm, start_pos_perm, end_pos_perm, val_perm, length_perm, intersect) %>% 
    distinct() %>% 
    group_by(val_perm) %>% 
    summarise(n_black = sum(intersect))
  
  n <- sum(n_black$n_black) - m
  
  ### The number of balls drawn from the urn
  
  n_drawn <- permutationsGO_perm_1 %>% 
    select(scf_perm, start_pos_perm, end_pos_perm, val_perm, length_perm, intersect) %>% 
    distinct() %>% 
    group_by(val_perm) %>% 
    summarise(n_drawn = sum(length_perm))
  
  k <- n_drawn$n_drawn[n_drawn$val_perm=="Hard"]
  
  ### Perform the hypergeometric test
  
  test_statistic <- (choose(m, q) * choose(n - m, k - q)) / choose(n, k)
  p_value <- phyper(q - 1, m, n, k, lower.tail = FALSE)
  go_Hard_null$p[i] <- p_value 
  cat(go_Hard_null$go[i], " ", test_statistic, " ", p_value, "\n")
  
}



go_Soft <- predPeaksGO %>% 
  filter(val=="Soft") %>% 
  select(go) %>% 
  distinct() %>% 
  filter(is.na(go)==FALSE)

for (i in 1:nrow(go_Soft)) {
  
  ### q: The number of white balls drawn without replacement
  
  n_white_drawn <- predPeaksGO %>% 
    filter(go==go_Soft$go[i]) %>% 
    group_by(val) %>% 
    summarise(n_white_drawn = sum(intersect))
  
  q <- n_white_drawn$n_white_drawn[n_white_drawn$val=="Soft"]
  
  ### m: The number of white balls in the urn.
  
  m <- sum(n_white_drawn$n_white_drawn)
  
  ### n: The number of black balls in the urn.
  
  n_black <- predPeaksGO %>% 
    select(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks, val, length, intersect) %>% 
    distinct() %>% 
    group_by(val) %>% 
    summarise(n_black = sum(length))
  
  n <- sum(n_black$n_black) - m
  
  ### The number of balls drawn from the urn
  
  n_drawn <- predPeaksGO %>% 
    select(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks, val, length, intersect) %>% 
    distinct() %>% 
    group_by(val) %>% 
    summarise(n_drawn = sum(length))
  
  k <- n_drawn$n_drawn[n_drawn$val=="Soft"]
  
  ### Perform the hypergeometric tes
  
  p_value <- phyper(q - 1, m, n, k, lower.tail = FALSE)
  go_Soft$p[i] <- p_value 
  cat(go_Soft$go[i], " ", p_value, "\n")
  
}

go_Soft %>% 
  arrange(p) %>% 
  left_join(select(go_info, go, name_1006))


```

# ######################## Compare demography

```{bash}

cd /home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical
find ./ -maxdepth 1 -type f -name '*.pred.bed*' > filelist.txt
tar -czvf pred_files.tar.gz -T filelist.txt

cd /home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/pred_empirical
find ./ -maxdepth 1 -type f -name '*.pred.bed*' > filelist.txt
tar -czvf pred_files.tar.gz -T filelist.txt

cd /home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/pred_empirical
find ./ -maxdepth 1 -type f -name '*.pred.bed*' > filelist.txt
tar -czvf pred_files.tar.gz -T filelist.txt

cd /home/shared/Scott_Allen/partialSHIC_DsGRP/constNe/pred_empirical
find ./ -maxdepth 1 -type f -name '*.pred.bed*' > filelist.txt
tar -czvf pred_files.tar.gz -T filelist.txt

```


```{powershell}

cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\
mkdir expansionNe_intergenic
cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\expansionNe_intergenic
mkdir pred_empirical
cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\expansionNe_intergenic\pred_empirical

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/pred_files.tar.gz ./


cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\
mkdir expansionNe_intergenic_lci
cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\expansionNe_intergenic_lci
mkdir pred_empirical
cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\expansionNe_intergenic_lci\pred_empirical

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_lci/pred_empirical/pred_files.tar.gz ./

cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\
mkdir expansionNe_intergenic_uci
cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\expansionNe_intergenic_uci
mkdir pred_empirical
cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\expansionNe_intergenic_uci\pred_empirical

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic_uci/pred_empirical/pred_files.tar.gz ./

cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\
mkdir constNe
cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\constNe
mkdir pred_empirical
cd C:\Users\scott\OneDrive - The University of Queensland\Documents\Stephen_Chenoweth\partialSHIC\constNe\pred_empirical

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/pred_empirical/pred_files.tar.gz ./

```

```{r, current no constNe}

library(tidyverse)
library(data.table)
library(RColorBrewer)
library(cowplot)

# expansionNe_intergenic

setwd("C:/Users/scott/OneDrive - The University of Queensland/Documents/Stephen_Chenoweth/partialSHIC/expansionNe_intergenic")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_expansionNe <- ensemble



# expansionNe_intergenic_lci

setwd("C:/Users/scott/OneDrive - The University of Queensland/Documents/Stephen_Chenoweth/partialSHIC/expansionNe_intergenic_lci")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_expansionNe_lci <- ensemble



# expansionNe_intergenic_uci

setwd("C:/Users/scott/OneDrive - The University of Queensland/Documents/Stephen_Chenoweth/partialSHIC/expansionNe_intergenic_uci")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_expansionNe_uci <- ensemble



# constNe

setwd("C:/Users/scott/OneDrive - The University of Queensland/Documents/Stephen_Chenoweth/partialSHIC/constNe")

# beds <- list.files(pattern = 'ScA8VGg_.*\\d.bed$')
beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_constNe <- ensemble



## Wrangle

ensemble_expansionNe <- ensemble_expansionNe %>% 
  select(contig, start, end, pred_ensemble) %>% 
  mutate(train = "SP2 median")

ensemble_expansionNe_lci <- ensemble_expansionNe_lci %>% 
  select(contig, start, end, pred_ensemble) %>% 
  mutate(train = "SP2 2.5th")

ensemble_expansionNe_uci <- ensemble_expansionNe_uci %>% 
  select(contig, start, end, pred_ensemble) %>% 
  mutate(train = "SP2 97.5th")

ensemble_constNe <- ensemble_constNe %>% 
  select(contig, start, end, pred_ensemble) %>% 
  mutate(train = "constNe")

predictions <- ensemble_expansionNe %>%
  bind_rows(ensemble_expansionNe_lci) %>%
  bind_rows(ensemble_expansionNe_uci) %>%
  bind_rows(ensemble_constNe)



## Agreement across demographic assumptions

predictions_wide <- predictions %>%
  pivot_wider(names_from = train, values_from = pred_ensemble)



### Hard

library(RColorBrewer)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

predictions_wide %>% 
  filter(`SP2 median` == "Hard")

predictions_wide %>% 
  filter(`SP2 median` == "Hard") %>% 
  group_by(`SP2 2.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "Hard") %>% 
  group_by(`SP2 97.5th`) %>% 
  tally()

predictions_wide %>%
  filter(`SP2 median` == "Hard") %>%
  group_by(constNe) %>%
  tally()

temp1 <- data.frame(train = c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"), 
                    pred_ensemble = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(451, 277, 315, 266, # Hard
                              0, 11, 55, 21, # Hard-linked
                              0, 2, 4, 18, # HardPartial
                              0, 0, 0, 3, # HardPartial-linked
                              0, 2, 0, 48, # Neutral
                              0, 130, 69, 81, # Soft
                              0, 25, 8, 5, # Soft-linked
                              0, 3, 0, 5, # SoftPartial
                              0, 1, 0, 4)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"))) %>% 
  mutate(pred_ensemble = factor(pred_ensemble, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, pred_ensemble) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="SP2 median") %>% 
  group_by(pred_ensemble) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

plot_Hard <- temp1 %>% 
  filter(train!="constNe") %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) +
  geom_bar(stat = "identity") + 
  labs(title = "Hard", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")



### Hard-linked

library(RColorBrewer)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

predictions_wide %>% 
  filter(`SP2 median` == "Hard-linked")

predictions_wide %>% 
  filter(`SP2 median` == "Hard-linked") %>% 
  group_by(`SP2 2.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "Hard-linked") %>% 
  group_by(`SP2 97.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "Hard-linked") %>% 
  group_by(constNe) %>% 
  tally()

temp1 <- data.frame(train = c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"), 
                    pred_ensemble = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 113, 85, 96, # Hard
                              2919, 1553, 2354, 1716, # Hard-linked
                              0, 0, 2, 19, # HardPartial
                              0, 47, 26, 244, # HardPartial-linked
                              0, 6, 5, 320, # Neutral
                              0, 55, 18, 33, # Soft
                              0, 1078, 388, 382, # Soft-linked
                              0, 7, 2, 10, # SoftPartial
                              0, 60, 39, 99)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"))) %>% 
  mutate(pred_ensemble = factor(pred_ensemble, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, pred_ensemble) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="SP2 median") %>% 
  group_by(pred_ensemble) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

plot_Hard_linked <- temp1 %>% 
  filter(train!="constNe") %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) +
  geom_bar(stat = "identity") + 
  labs(title = "Hard-linked", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")



### Soft

library(RColorBrewer)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

predictions_wide %>% 
  filter(`SP2 median` == "Soft")

predictions_wide %>% 
  filter(`SP2 median` == "Soft") %>% 
  group_by(`SP2 2.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "Soft") %>% 
  group_by(`SP2 97.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "Soft") %>% 
  group_by(constNe) %>% 
  tally()

temp1 <- data.frame(train = c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"), 
                    pred_ensemble = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 4, 71, 91, # Hard
                              0, 0, 20, 22, # Hard-linked
                              0, 1, 9, 21, # HardPartial
                              0, 0, 3, 21, # HardPartial-linked
                              0, 30, 29, 1292, # Neutral
                              2286, 1806, 1913, 760, # Soft
                              0, 415, 187, 39, # Soft-linked
                              0, 20, 40, 20, # SoftPartial
                              0, 10, 14, 20)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"))) %>% 
  mutate(pred_ensemble = factor(pred_ensemble, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, pred_ensemble) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="SP2 median") %>% 
  group_by(pred_ensemble) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

plot_Soft <- temp1 %>% 
  filter(train!="constNe") %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) +
  geom_bar(stat = "identity") + 
  labs(title = "Soft", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")



### Soft-linked

library(RColorBrewer)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

predictions_wide %>% 
  filter(`SP2 median` == "Soft-linked")

predictions_wide %>% 
  filter(`SP2 median` == "Soft-linked") %>% 
  group_by(`SP2 2.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "Soft-linked") %>% 
  group_by(`SP2 97.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "Soft-linked") %>% 
  group_by(constNe) %>% 
  tally()

temp1 <- data.frame(train = c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"), 
                    pred_ensemble = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 2, 25, 24, # Hard
                              0, 29, 585, 332, # Hard-linked
                              0, 0, 15, 34, # HardPartial
                              0, 1, 45, 226, # HardPartial-linked
                              0, 215, 468, 9982, # Neutral
                              0, 357, 646, 144, # Soft
                              13588, 12715, 11129, 2424, # Soft-linked
                              0, 32, 85, 39, # SoftPartial
                              0, 237, 590, 383)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"))) %>% 
  mutate(pred_ensemble = factor(pred_ensemble, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, pred_ensemble) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="SP2 median") %>% 
  group_by(pred_ensemble) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

plot_Soft_linked <- temp1 %>% 
  filter(train!="constNe") %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) +
  geom_bar(stat = "identity") + 
  labs(title = "Soft-linked", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")



### HardPartial

library(RColorBrewer)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

predictions_wide %>% 
  filter(`SP2 median` == "HardPartial")

predictions_wide %>% 
  filter(`SP2 median` == "HardPartial") %>% 
  group_by(`SP2 2.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "HardPartial") %>% 
  group_by(`SP2 97.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "HardPartial") %>% 
  group_by(constNe) %>% 
  tally()

temp1 <- data.frame(train = c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"), 
                    pred_ensemble = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 4, 5, 1, # Hard
                              0, 3, 7, 4, # Hard-linked
                              67, 17, 23, 22, # HardPartial
                              0, 6, 10, 4, # HardPartial-linked
                              0, 1, 0, 19, # Neutral
                              0, 11, 6, 3, # Soft
                              0, 13, 7, 0, # Soft-linked
                              0, 8, 6, 9, # SoftPartial
                              0, 4, 3, 5)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"))) %>% 
  mutate(pred_ensemble = factor(pred_ensemble, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, pred_ensemble) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="SP2 median") %>% 
  group_by(pred_ensemble) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

plot_HardPartial <- temp1 %>% 
  filter(train!="constNe") %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) +
  geom_bar(stat = "identity") + 
  labs(title = "HardPartial", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")



### HardPartial-linked

library(RColorBrewer)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

predictions_wide %>% 
  filter(`SP2 median` == "HardPartial-linked")

predictions_wide %>% 
  filter(`SP2 median` == "HardPartial-linked") %>% 
  group_by(`SP2 2.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "HardPartial-linked") %>% 
  group_by(`SP2 97.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "HardPartial-linked") %>% 
  group_by(constNe) %>% 
  tally()

temp1 <- data.frame(train = c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"), 
                    pred_ensemble = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 4, 4, 2, # Hard
                              0, 20, 91, 18, # Hard-linked
                              0, 1, 8, 13, # HardPartial
                              340, 68, 159, 142, # HardPartial-linked
                              0, 0, 0, 51, # Neutral
                              0, 5, 4, 3, # Soft
                              0, 158, 44, 18, # Soft-linked
                              0, 5, 0, 3, # SoftPartial
                              0, 79, 30, 90)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"))) %>% 
  mutate(pred_ensemble = factor(pred_ensemble, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, pred_ensemble) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="SP2 median") %>% 
  group_by(pred_ensemble) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

plot_HardPartial_linked <- temp1 %>% 
  filter(train!="constNe") %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) +
  geom_bar(stat = "identity") + 
  labs(title = "HardPartial-linked", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")



### SoftPartial

library(RColorBrewer)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

predictions_wide %>% 
  filter(`SP2 median` == "SoftPartial")

predictions_wide %>% 
  filter(`SP2 median` == "SoftPartial") %>% 
  group_by(`SP2 2.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "SoftPartial") %>% 
  group_by(`SP2 97.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "SoftPartial") %>% 
  group_by(constNe) %>% 
  tally()

temp1 <- data.frame(train = c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"), 
                    pred_ensemble = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 0, 1, 0, # Hard
                              0, 1, 3, 0, # Hard-linked
                              0, 1, 1, 3, # HardPartial
                              0, 1, 4, 2, # HardPartial-linked
                              0, 47, 6, 180, # Neutral
                              0, 44, 40, 10, # Soft
                              0, 56, 59, 0, # Soft-linked
                              221, 59, 69, 21, # SoftPartial
                              0, 12, 28, 5)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"))) %>% 
  mutate(pred_ensemble = factor(pred_ensemble, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, pred_ensemble) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="SP2 median") %>% 
  group_by(pred_ensemble) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

plot_SoftPartial <- temp1 %>% 
  filter(train!="constNe") %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) +
  geom_bar(stat = "identity") + 
  labs(title = "SoftPartial", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")



### SoftPartial-linked

library(RColorBrewer)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

predictions_wide %>% 
  filter(`SP2 median` == "SoftPartial-linked")

predictions_wide %>% 
  filter(`SP2 median` == "SoftPartial-linked") %>% 
  group_by(`SP2 2.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "SoftPartial-linked") %>% 
  group_by(`SP2 97.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "SoftPartial-linked") %>% 
  group_by(constNe) %>% 
  tally()

temp1 <- data.frame(train = c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"), 
                    pred_ensemble = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 0, 0, 0, # Hard
                              0, 4, 20, 6, # Hard-linked
                              0, 2, 5, 4, # HardPartial
                              0, 5, 30, 41, # HardPartial-linked
                              0, 113, 15, 604, # Neutral
                              0, 7, 18, 1, # Soft
                              0, 386, 421, 27, # Soft-linked
                              0, 16, 16, 7, # SoftPartial
                              798, 265, 273, 108)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"))) %>% 
  mutate(pred_ensemble = factor(pred_ensemble, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, pred_ensemble) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="SP2 median") %>% 
  group_by(pred_ensemble) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

plot_SoftPartial_linked <- temp1 %>% 
  filter(train!="constNe") %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) +
  geom_bar(stat = "identity") + 
  labs(title = "SoftPartial-linked", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")



### Neutral

library(RColorBrewer)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

predictions_wide %>% 
  filter(`SP2 median` == "Neutral")

predictions_wide %>% 
  filter(`SP2 median` == "Neutral") %>% 
  group_by(`SP2 2.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "Neutral") %>% 
  group_by(`SP2 97.5th`) %>% 
  tally()

predictions_wide %>% 
  filter(`SP2 median` == "Neutral") %>% 
  group_by(constNe) %>% 
  tally()

temp1 <- data.frame(train = c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"), 
                    pred_ensemble = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 0, 3, 0, # Hard
                              0, 0, 4, 0, # Hard-linked
                              0, 0, 0, 0, # HardPartial
                              0, 0, 0, 1, # HardPartial-linked
                              831, 228, 264, 829, # Neutral
                              0, 24, 70, 0, # Soft
                              0, 562, 358, 0, # Soft-linked
                              0, 7, 28, 0, # SoftPartial
                              0, 10, 104, 1)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("SP2 median", "SP2 2.5th", "SP2 97.5th", "constNe"))) %>% 
  mutate(pred_ensemble = factor(pred_ensemble, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, pred_ensemble) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="SP2 median") %>% 
  group_by(pred_ensemble) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

plot_Neutral <- temp1 %>% 
  filter(train!="constNe") %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) +
  geom_bar(stat = "identity") + 
  labs(title = "Neutral", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")



P10 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme_void() + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf), fill = "white") + 
  theme(legend.position="none")

P11 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme_void() + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf), fill = "white") + 
  theme(legend.position = c(0.5, 0.5), 
        legend.text = element_text(size=8), 
        legend.title = element_blank(), 
        legend.key.width = unit(0.5, "cm"), 
        legend.key.height = unit(0.5, "cm"))

P12 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = pred_ensemble)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme_void() + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf), fill = "white") + 
  theme(legend.position="none")



plot_grid(plot_Hard, plot_Hard_linked, 
          plot_Soft, plot_Soft_linked, 
          plot_HardPartial, plot_HardPartial_linked, 
          plot_SoftPartial, plot_SoftPartial_linked, 
          plot_Neutral, 
          P11, 
          nrow=5, ncol=2)

setwd("C:/Users/scott/Dropbox/PhD_Docu_Yiguan/sweep_paper_draft/MolEcol_Submission")

ggsave('Figure_7_demography_comparison_bar_SCOTT_v4_no_constNe.pdf', width=8, height=10, dpi=300, unit='in')
ggsave('Figure_7_demography_comparison_bar_SCOTT_v4_no_constNe.png', width=8, height=10, dpi=300, unit='in')


```

```{bash}

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe

```

```{r}

library(tidyverse)
library(data.table)

## Import data

### Expansion Ne

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe")

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

predictions_expansionNe <- data.table()

for(ff in preFiles){
    tmp <- fread(ff, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    predictions_expansionNe <- rbind(predictions_expansionNe, tmp)
}

predictions_expansionNe <- predictions_expansionNe %>% 
  set_names(c("scf", "start_pos", "end_pos", "val", "V5", "V6", "V7", "V8", "V9"))

### Constant Ne

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe")

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

predictions_constNe <- data.table()

for(ff in preFiles){
    tmp <- fread(ff, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    predictions_constNe <- rbind(predictions_constNe, tmp)
}

predictions_constNe <- predictions_constNe %>% 
  set_names(c("scf", "start_pos", "end_pos", "val", "V5", "V6", "V7", "V8", "V9"))

### longShallow

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_longShallow")

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

predictions_longShallow <- data.table()

for(ff in preFiles){
    tmp <- fread(ff, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    predictions_longShallow <- rbind(predictions_longShallow, tmp)
}

predictions_longShallow <- predictions_longShallow %>% 
  set_names(c("scf", "start_pos", "end_pos", "val", "V5", "V6", "V7", "V8", "V9"))

### shortSevere

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_shortSevere")

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

predictions_shortSevere <- data.table()

for(ff in preFiles){
    tmp <- fread(ff, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    predictions_shortSevere <- rbind(predictions_shortSevere, tmp)
}

predictions_shortSevere <- predictions_shortSevere %>% 
  set_names(c("scf", "start_pos", "end_pos", "val", "V5", "V6", "V7", "V8", "V9"))

## Wrangle

predictions_expansionNe <- predictions_expansionNe %>% 
  select(scf:val) %>% 
  mutate(train = "expansionNe")

predictions_constNe <- predictions_constNe %>% 
  select(scf:val) %>% 
  mutate(train = "constNe")

predictions_longShallow <- predictions_longShallow %>% 
  select(scf:val) %>% 
  mutate(train = "longShallow")

predictions_shortSevere <- predictions_shortSevere %>% 
  select(scf:val) %>% 
  mutate(train = "shortSevere")

predictions <- predictions_expansionNe %>% 
  bind_rows(predictions_constNe) %>% 
  bind_rows(predictions_longShallow) %>% 
  bind_rows(predictions_shortSevere)

## Agreement across demographic assumptions

predictions_wide <- predictions %>%
  pivot_wider(names_from = train, values_from = val)

### Hard

predictions_wide %>%
  rowwise() %>%
  mutate(distinct_count = n_distinct(c_across(expansionNe:shortSevere))) %>% 
  ungroup() %>% 
  group_by(expansionNe, distinct_count) %>% 
  tally() %>% 
  filter(expansionNe=="Hard")

temp1 <- predictions_wide %>%
  rowwise() %>%
  mutate(distinct_count = n_distinct(c_across(expansionNe:shortSevere))) %>% 
  ungroup() %>% 
  filter(distinct_count==2) %>% 
  mutate(mismatch = ifelse(expansionNe=="Hard" & (constNe=="Hard-linked" | longShallow=="Hard-linked" | shortSevere=="Hard-linked"), TRUE, FALSE)) %>% 
  filter(expansionNe=="Hard") %>% 
  summarise(sum = sum(mismatch))

print(paste("Hard as Hard-linked", temp1$sum))

temp1 <- predictions_wide %>%
  rowwise() %>%
  mutate(distinct_count = n_distinct(c_across(expansionNe:shortSevere))) %>% 
  ungroup() %>% 
  filter(distinct_count==2) %>% 
  mutate(mismatch = ifelse(expansionNe=="Hard" & (constNe=="Soft" | longShallow=="Soft" | shortSevere=="Soft"), TRUE, FALSE)) %>% 
  filter(expansionNe=="Hard") %>% 
  summarise(sum = sum(mismatch))

print(paste("Hard as Soft", temp1$sum))

temp1 <- predictions_wide %>%
  rowwise() %>%
  mutate(distinct_count = n_distinct(c_across(expansionNe:shortSevere))) %>% 
  ungroup() %>% 
  filter(distinct_count==2) %>% 
  mutate(mismatch = ifelse(expansionNe=="Hard" & (constNe=="Soft-linked" | longShallow=="Soft-linked" | shortSevere=="Soft-linked"), TRUE, FALSE)) %>% 
  filter(expansionNe=="Hard") %>% 
  summarise(sum = sum(mismatch))

print(paste("Hard as Soft-linked", temp1$sum))

temp1 <- predictions_wide %>%
  rowwise() %>%
  mutate(distinct_count = n_distinct(c_across(expansionNe:shortSevere))) %>% 
  ungroup() %>% 
  filter(distinct_count==2) %>% 
  mutate(mismatch = ifelse(expansionNe=="Hard" & (constNe=="HardPartial" | longShallow=="HardPartial" | shortSevere=="HardPartial"), TRUE, FALSE)) %>% 
  filter(expansionNe=="Hard") %>% 
  summarise(sum = sum(mismatch))

print(paste("Hard as HardPartial", temp1$sum))

temp1 <- predictions_wide %>%
  rowwise() %>%
  mutate(distinct_count = n_distinct(c_across(expansionNe:shortSevere))) %>% 
  ungroup() %>% 
  filter(distinct_count==2) %>% 
  mutate(mismatch = ifelse(expansionNe=="Hard" & (constNe=="HardPartial-linked" | longShallow=="HardPartial-linked" | shortSevere=="HardPartial-linked"), TRUE, FALSE)) %>% 
  filter(expansionNe=="Hard") %>% 
  summarise(sum = sum(mismatch))

print(paste("Hard as HardPartial-linked", temp1$sum))

temp1 <- predictions_wide %>%
  rowwise() %>%
  mutate(distinct_count = n_distinct(c_across(expansionNe:shortSevere))) %>% 
  ungroup() %>% 
  filter(distinct_count==2) %>% 
  mutate(mismatch = ifelse(expansionNe=="Hard" & (constNe=="SoftPartial" | longShallow=="SoftPartial" | shortSevere=="SoftPartial"), TRUE, FALSE)) %>% 
  filter(expansionNe=="Hard") %>% 
  summarise(sum = sum(mismatch))

print(paste("Hard as SoftPartial", temp1$sum))

temp1 <- predictions_wide %>%
  rowwise() %>%
  mutate(distinct_count = n_distinct(c_across(expansionNe:shortSevere))) %>% 
  ungroup() %>% 
  filter(distinct_count==2) %>% 
  mutate(mismatch = ifelse(expansionNe=="Hard" & (constNe=="SoftPartial-linked" | longShallow=="SoftPartial-linked" | shortSevere=="SoftPartial-linked"), TRUE, FALSE)) %>% 
  filter(expansionNe=="Hard") %>% 
  summarise(sum = sum(mismatch))

print(paste("Hard as SoftPartial-linked", temp1$sum))

temp1 <- predictions_wide %>%
  rowwise() %>%
  mutate(distinct_count = n_distinct(c_across(expansionNe:shortSevere))) %>% 
  ungroup() %>% 
  filter(distinct_count==2) %>% 
  mutate(mismatch = ifelse(expansionNe=="Hard" & (constNe=="Neutral" | longShallow=="Neutral" | shortSevere=="Neutral"), TRUE, FALSE)) %>% 
  filter(expansionNe=="Hard") %>% 
  summarise(sum = sum(mismatch))

print(paste("Hard as Neutral", temp1$sum))

### Stacked bar chart

library(RColorBrewer)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

predictions_wide %>% 
  filter(expansionNe == "Hard")

predictions_wide %>% 
  filter(expansionNe == "Hard") %>% 
  group_by(constNe) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "Hard") %>% 
  group_by(longShallow) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "Hard") %>% 
  group_by(shortSevere) %>% 
  tally()

temp1 <- data.frame(train = c("expansionNe", "constNe", "longShallow", "shortSevere"), 
                    val = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(283, 182, 194, 161, # Hard
                              0, 24, 8, 37, # Hard-linked
                              0, 5, 0, 2, # HardPartial
                              0, 1, 1, 2, # HardPartial-linked
                              0, 1, 9, 0, # Neutral
                              0, 66, 69, 59, # Soft
                              0, 3, 2, 20, # Soft-linked
                              0, 1, 0, 2, # SoftPartial
                              0, 0, 0, 0)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("expansionNe", "constNe", "longShallow", "shortSevere"))) %>% 
  mutate(val = factor(val, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, val) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="expansionNe") %>% 
  group_by(val) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

P1 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) +
  geom_bar(stat = "identity") + 
  labs(title = "Hard", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")

## Hard-linked

### Stacked bar chart

predictions_wide %>% 
  filter(expansionNe == "Hard-linked")

predictions_wide %>% 
  filter(expansionNe == "Hard-linked") %>% 
  group_by(constNe) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "Hard-linked") %>% 
  group_by(longShallow) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "Hard-linked") %>% 
  group_by(shortSevere) %>% 
  tally()

temp1 <- data.frame(train = c("expansionNe", "constNe", "longShallow", "shortSevere"), 
                    val = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 61, 89, 16, # Hard
                              1140, 872, 807, 922, # Hard-linked
                              0, 11, 4, 10, # HardPartial
                              0, 72, 14, 35, # HardPartial-linked
                              0, 13, 35, 2, # Neutral
                              0, 7, 11, 3, # Soft
                              0, 93, 177, 148, # Soft-linked
                              0, 3, 1, 1, # SoftPartial
                              0, 8, 2, 3)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("expansionNe", "constNe", "longShallow", "shortSevere"))) %>% 
  mutate(val = factor(val, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, val) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="expansionNe") %>% 
  group_by(val) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

P2 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) +
  geom_bar(stat = "identity") + 
  labs(title = "Hard-linked", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")

## HardPartial

### Stacked bar chart

predictions_wide %>% 
  filter(expansionNe == "HardPartial")

predictions_wide %>% 
  filter(expansionNe == "HardPartial") %>% 
  group_by(constNe) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "HardPartial") %>% 
  group_by(longShallow) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "HardPartial") %>% 
  group_by(shortSevere) %>% 
  tally()

temp1 <- data.frame(train = c("expansionNe", "constNe", "longShallow", "shortSevere"), 
                    val = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 5, 5, 5, # Hard
                              0, 3, 3, 3, # Hard-linked
                              46, 17, 3, 14, # HardPartial
                              0, 5, 2, 9, # HardPartial-linked
                              0, 11, 27, 4, # Neutral
                              0, 1, 3, 6, # Soft
                              0, 2, 1, 3, # Soft-linked
                              0, 0, 2, 0, # SoftPartial
                              0, 2, 0, 2)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("expansionNe", "constNe", "longShallow", "shortSevere"))) %>% 
  mutate(val = factor(val, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, val) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="expansionNe") %>% 
  group_by(val) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

P3 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) +
  geom_bar(stat = "identity") + 
  labs(title = "HardPartial", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")

## HardPartial-linked

### Stacked bar chart

predictions_wide %>% 
  filter(expansionNe == "HardPartial-linked")

predictions_wide %>% 
  filter(expansionNe == "HardPartial-linked") %>% 
  group_by(constNe) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "HardPartial-linked") %>% 
  group_by(longShallow) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "HardPartial-linked") %>% 
  group_by(shortSevere) %>% 
  tally()

temp1 <- data.frame(train = c("expansionNe", "constNe", "longShallow", "shortSevere"), 
                    val = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 3, 5, 2, # Hard
                              0, 20, 26, 30, # Hard-linked
                              0, 19, 5, 15, # HardPartial
                              195, 102, 54, 103, # HardPartial-linked
                              0, 28, 66, 6, # Neutral
                              0, 0, 0, 4, # Soft
                              0, 6, 21, 15, # Soft-linked
                              0, 1, 4, 4, # SoftPartial
                              0, 16, 14, 16)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("expansionNe", "constNe", "longShallow", "shortSevere"))) %>% 
  mutate(val = factor(val, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, val) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="expansionNe") %>% 
  group_by(val) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

P4 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) +
  geom_bar(stat = "identity") + 
  labs(title = "HardPartial-linked", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")

### Soft

### Stacked bar chart

predictions_wide %>% 
  filter(expansionNe == "Soft")

predictions_wide %>% 
  filter(expansionNe == "Soft") %>% 
  group_by(constNe) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "Soft") %>% 
  group_by(longShallow) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "Soft") %>% 
  group_by(shortSevere) %>% 
  tally()

temp1 <- data.frame(train = c("expansionNe", "constNe", "longShallow", "shortSevere"), 
                    val = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 207, 65, 134, # Hard
                              0, 30, 7, 27, # Hard-linked
                              0, 75, 6, 84, # HardPartial
                              0, 19, 8, 18, # HardPartial-linked
                              0, 470, 769, 130, # Neutral
                              1628, 733, 744, 1008, # Soft
                              0, 50, 27, 184, # Soft-linked
                              0, 28, 1, 31, # SoftPartial
                              0, 16, 1, 12)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("expansionNe", "constNe", "longShallow", "shortSevere"))) %>% 
  mutate(val = factor(val, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, val) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="expansionNe") %>% 
  group_by(val) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

P5 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) +
  geom_bar(stat = "identity") + 
  labs(title = "Soft", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")

## Soft-linked

### Stacked bar chart

predictions_wide %>% 
  filter(expansionNe == "Soft-linked")

predictions_wide %>% 
  filter(expansionNe == "Soft-linked") %>% 
  group_by(constNe) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "Soft-linked") %>% 
  group_by(longShallow) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "Soft-linked") %>% 
  group_by(shortSevere) %>% 
  tally()

temp1 <- data.frame(train = c("expansionNe", "constNe", "longShallow", "shortSevere"), 
                    val = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 99, 62, 48, # Hard
                              0, 1068, 416, 1006, # Hard-linked
                              0, 196, 18, 221, # HardPartial
                              0, 945, 107, 684, # HardPartial-linked
                              0, 5631, 7782, 1843, # Neutral
                              0, 187, 206, 248, # Soft
                              10635, 1972, 1965, 5963, # Soft-linked
                              0, 60, 21, 98, # SoftPartial
                              0, 477, 58, 524)) # SoftPartial-linked

temp1 %>% 
  arrange(train, val) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="expansionNe") %>% 
  group_by(val) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("expansionNe", "constNe", "longShallow", "shortSevere"))) %>% 
  mutate(val = factor(val, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

P6 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) +
  geom_bar(stat = "identity") + 
  labs(title = "Soft-linked", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")

## SoftPartial

### Stacked bar chart

predictions_wide %>% 
  filter(expansionNe == "SoftPartial")

predictions_wide %>% 
  filter(expansionNe == "SoftPartial") %>% 
  group_by(constNe) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "SoftPartial") %>% 
  group_by(longShallow) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "SoftPartial") %>% 
  group_by(shortSevere) %>% 
  tally()

temp1 <- data.frame(train = c("expansionNe", "constNe", "longShallow", "shortSevere"), 
                    val = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 3, 6, 8, # Hard
                              0, 2, 2, 4, # Hard-linked
                              0, 48, 4, 63, # HardPartial
                              0, 23, 8, 21, # HardPartial-linked
                              0, 174, 258, 95, # Neutral
                              0, 17, 27, 48, # Soft
                              0, 6, 2, 22, # Soft-linked
                              316, 29, 8, 46, # SoftPartial
                              0, 14, 1, 9)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("expansionNe", "constNe", "longShallow", "shortSevere"))) %>% 
  mutate(val = factor(val, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, val) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="expansionNe") %>% 
  group_by(val) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

P7 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) +
  geom_bar(stat = "identity") + 
  labs(title = "SoftPartial", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")

## SoftPartial-linked

### Stacked bar chart

predictions_wide %>% 
  filter(expansionNe == "SoftPartial-linked")

predictions_wide %>% 
  filter(expansionNe == "SoftPartial-linked") %>% 
  group_by(constNe) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "SoftPartial-linked") %>% 
  group_by(longShallow) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "SoftPartial-linked") %>% 
  group_by(shortSevere) %>% 
  tally()

temp1 <- data.frame(train = c("expansionNe", "constNe", "longShallow", "shortSevere"), 
                    val = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 4, 10, 10, # Hard
                              0, 39, 31, 47, # Hard-linked
                              0, 87, 16, 136, # HardPartial
                              0, 308, 91, 348, # HardPartial-linked
                              0, 633, 1044, 251, # Neutral
                              0, 12, 13, 26, # Soft
                              0, 60, 84, 199, # Soft-linked
                              0, 23, 12, 56, # SoftPartial
                              1342, 176, 41, 269)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("expansionNe", "constNe", "longShallow", "shortSevere"))) %>% 
  mutate(val = factor(val, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, val) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="expansionNe") %>% 
  group_by(val) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

P8 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) +
  geom_bar(stat = "identity") + 
  labs(title = "SoftPartial-linked", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position = "none")

## Neutral

### Stacked bar chart

predictions_wide %>% 
  filter(expansionNe == "Neutral")

predictions_wide %>% 
  filter(expansionNe == "Neutral") %>% 
  group_by(constNe) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "Neutral") %>% 
  group_by(longShallow) %>% 
  tally()

predictions_wide %>% 
  filter(expansionNe == "Neutral") %>% 
  group_by(shortSevere) %>% 
  tally()

temp1 <- data.frame(train = c("expansionNe", "constNe", "longShallow", "shortSevere"), 
                    val = c(rep("Hard", 4), 
                            rep("Hard-linked", 4), 
                            rep("HardPartial", 4), 
                            rep("HardPartial-linked", 4), 
                            rep("Neutral", 4), 
                            rep("Soft", 4), 
                            rep("Soft-linked", 4), 
                            rep("SoftPartial", 4), 
                            rep("SoftPartial-linked", 4)), 
                    count = c(0, 16, 1, 27, # Hard
                              0, 36, 6, 50, # Hard-linked
                              0, 80, 0, 256, # HardPartial
                              0, 198, 6, 232, # HardPartial-linked
                              5910, 5270, 5837, 3757, # Neutral
                              0, 53, 16, 240, # Soft
                              0, 113, 30, 919, # Soft-linked
                              0, 16, 5, 155, # SoftPartial
                              0, 128, 9, 274)) # SoftPartial-linked

temp1 <- temp1 %>% 
  mutate(train = factor(train, levels=c("expansionNe", "constNe", "longShallow", "shortSevere"))) %>% 
  mutate(val = factor(val, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))))

temp1 %>% 
  arrange(train, val) %>% 
  group_by(train) %>% 
  mutate(prop = count/sum(count)) %>% 
  ungroup() %>% 
  filter(train!="expansionNe") %>% 
  group_by(val) %>% 
  summarise(mean_count = mean(count), stdev_count = sd(count), 
            mean_prop = mean(prop), stdev_prop = sd(prop))

P9 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) +
  geom_bar(stat = "identity") + 
  labs(title = "Neutral", 
       x = "Assumed demography") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme(legend.position="none")

## Combined plot

library(cowplot)

P10 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme_void() + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf), fill = "white") + 
  theme(legend.position="none")

P11 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme_void() + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf), fill = "white") + 
  theme(legend.position = c(0.5, 0.5), 
        legend.text = element_text(size=8), 
        legend.title = element_blank(), 
        legend.key.width = unit(0.5, "cm"), 
        legend.key.height = unit(0.5, "cm"))

P12 <- temp1 %>%
  ggplot(aes(x = train, y = count, fill = val)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) + 
  theme_void() + 
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf), fill = "white") + 
  theme(legend.position="none")

plot_grid(P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, nrow=4, ncol=3)

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors")

ggsave('Figure_7_demography_comparison_bar_SCOTT_v2.pdf', width=8, height=10, dpi=300, unit='in')
ggsave('Figure_7_demography_comparison_bar_SCOTT_v2.png', width=8, height=10, dpi=300, unit='in')


## Stacked bar chart (Yiguan)

library(RColorBrewer)

aa <- predictions_expansionNe %>% 
  bind_rows(predictions_constNe) %>% 
  bind_rows(predictions_longShallow) %>% 
  bind_rows(predictions_shortSevere) %>% 
  rename(istate=val, Classifier=train)

my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')

# beds <- list.files(pattern = 'ScA8VGg_[0-9]*.bed$', recursive = TRUE)
# beds <- beds[!str_detect(beds,'old')]
# 
# aa <- data.table()
# 
# for(bed in beds){
#     classifier <- str_split(bed, '_')[[1]][1]
#     tmp <- fread(bed, skip=1L, sep='\t', header=FALSE, select=1:4) %>%
#         extract(V4,'istate','(.*)_S,*') %>% 
#         mutate(Classifier = classifier)
#     aa <- rbind(aa, tmp)    
#     
# }
# 
# aa[V1 %in% c('chrScA8VGg_542','chrScA8VGg_718'), chr:='2L']
# aa[V1 == 'chrScA8VGg_785', chr:='2R']
# aa[V1 == 'chrScA8VGg_628', chr:='3L']
# aa[V1 == 'chrScA8VGg_76', chr:='3R']
# aa[V1 == 'chrScA8VGg_594', chr:='X']

a1 <- aa[,.N,by=c('istate','Classifier')]
a1 <- a1[,.(prop=N/sum(N), istate=istate), by = Classifier]
a1[istate %in% c('Hard', 'HardPartial'), istate2:='hard']
a1[istate %in% c('Hard-linked', 'HardPartial-linked'), istate2:='hard-linked']
a1[istate %in% c('Soft', 'SoftPartial'), istate2:='soft']
a1[istate %in% c('Soft-linked', 'SoftPartial-linked'), istate2:='soft-linked']
a1[istate=='Neutral', istate2:='neutral']

a1[istate2 %in% c('hard','soft'), istate3:='sweep']
a1[istate2 %in% c('hard-linked','soft-linked'), istate3:='sweep-linked']
a1[istate2 == 'neutral', istate3:='neutral']

# a1$istate <- factor(a1$istate, levels=rev(c("Hard","HardPartial", "Soft","SoftPartial",
#                                             "Hard-linked", "HardPartial-linked",
#                                             "Soft-linked", "SoftPartial-linked", "Neutral")))

a1$istate <- factor(a1$istate, levels=rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

ggplot(a1, aes(x = Classifier, y = prop, fill = istate)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "Soft-linked"=my.soft.col[4],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "Hard-linked"=my.hard.col[4],
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft"=my.soft.col[6],
                               "HardPartial"=my.partialHard.col[6],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors")

ggsave('Figure_7_demography_comparison_bar_SCOTT.pdf', width=8, height=7, dpi=300, unit='in')
ggsave('Figure_7_demography_comparison_bar_SCOTT.png', width=8, height=7, dpi=300, unit='in')

```

```{powershell, scp}

cd C:\Users\scott\Dropbox\PhD_Docu_Yiguan\sweep_paper_draft\MolEcol_Submission

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/Figure_7_demography_comparison_bar_SCOTT.p* ./

scp uqsalle3@biol-6nt80t2.staff.net.uq.edu.au:/home/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/Figure_7_demography_comparison_bar_SCOTT_v2.p* ./

```

# ######################## iHS

```{shell}

module load vcftools

cd /home/shared/Scott_Allen/partialSHIC_DsGRP/iHS

VCF='/home/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf'

vcftools --vcf $VCF --IMPUTE

mv out.impute.hap ScA8VGg_785.impute.hap
mv out.impute.hap.indv ScA8VGg_785.impute.hap.indv
mv out.impute.legend ScA8VGg_785.impute.legend
mv out.log ScA8VGg_785.log

# awk 'BEGIN {FS="\t"}; {print $1 FS "0" FS $2}' ../top6.anc.fa.fai > top6.anc.bed
# 
# awk '{print $1"\t0\t"length($2)"\t"$3}' top6.anc.single_line.fa > ancestral.bed

# awk '/^>/ {if (seq) print id "\t0\t" length(seq) "\t" seq; id = substr($0, 2); seq = ""} !/^>/ {seq = seq $0} END {if (seq) print id "\t0\t" length(seq) "\t" seq}' top6.anc.single_line.fa > top6.anc.bed

# awk '/^>/ {if (seq) for (i=1; i<=length(seq); i++) print id "\t" i "\t" substr(seq, i, 1); id = substr($0, 2); seq = ""} !/^>/ {seq = seq $0} END {if (seq) for (i=1; i<=length(seq); i++) print id "\t" i "\t" substr(seq, i, 1)}' top6.anc.single_line.fa

awk '/^>/ {if (seq) for (i=1; i<=length(seq); i++) print id "\t" i-1 "\t" i-1 "\t" substr(seq, i, 1); id = substr($0, 2); seq = ""} !/^>/ {seq = seq $0} END {if (seq) for (i=1; i<=length(seq); i++) print id "\t" i-1 "\t" i-1 "\t" substr(seq, i, 1)}' top6.anc.single_line.fa > top6.anc.bed

bedtools intersect -a $VCF -b top6.anc.bed -wa

```

```{r}

library(rehh)

cat(readLines("ScA8VGg_785.impute.hap"), sep = "\n")

hh_vcf <- data2haplohh(hap_file = "/home/shared/Scott_Allen/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf",
                       vcf_reader = "data.table")

```

# ######################## TEMP

```{shell}

tar -I "pigz" -cvf test_expansionNe_intergenic.tar.gz ./test_expansionNe_intergenic
tar -I "pigz" -cvf test_expansionNe_intergenic_lci.tar.gz ./test_expansionNe_intergenic_lci
tar -I "pigz" -cvf test_expansionNe_intergenic_uci.tar.gz ./test_expansionNe_intergenic_uci
tar -I "pigz" -cvf test_self_newDat.tar.gz ./test_self_newDat

```


