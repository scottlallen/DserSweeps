---
title: "partialS/HIC"
author: "Scott L. Allen"
output: html_document
---

# Map DsGRP 

```{shell, get DNA sequence files}

mkdir -p [PATH_TO]/partialSHIC_DsGRP
cd [PATH_TO]/partialSHIC_DsGRP
mkdir -p rawData
cd rawData
rsync -ahPv [PATH_TO]/*.fq.gz ./

```

## conda bwa 

```{shell, create conda environment for bwa}

module load anaconda3

conda create -n bwa
conda activate bwa
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

conda install qualimap multiqc bwa samtools picard r-base 

```

## index ref (READY BUT MAKE REF AVAIL)

```{shell, index the reference genome for mapping}

module load anaconda3
conda activate bwa

mkdir -p [PATH_TO]/partialSHIC_DsGRP/mapping
cd [PATH_TO]/partialSHIC_DsGRP/mapping

rsync -ahPv [PATH_TO]/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta ./
  
bwa index drosophila_06Jul2018_A8VGg_noSpecialChar.fasta

```

## bwa (READY BUT MAKE GFF AVAILABLE)

```{shell, example map DNA sequence reads to the reference genome}

module load anaconda3
conda activate bwa

cd [PATH_TO]/partialSHIC_DsGRP/mapping

refName='drosophila_06Jul2018_A8VGg_noSpecialChar.fasta'
genomeName='line100'
DNAfastqPath=$(grep '/100/' fastq_paths.txt | sed 's/100\/.*/100\//')
DNAfastq1=$(grep '/100/.*1.fq.gz' fastq_paths.txt)
DNAfastq2=$(grep '/100/.*2.fq.gz' fastq_paths.txt)
gffPath="[PATH_TO]/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_noSpecialChar.gff"
cpu=8

echo -e "\nMapping genome ${genomeName}...\n"
echo "started mapping genome ${genomeName} at: $(date)" > ${genomeName}_mapping.log
srun bwa mem -t $cpu $refName $DNAfastq1 $DNAfastq2 > ./${genomeName}.sam
srun samtools view -@ $cpu -bT $refName ${genomeName}.sam -o ${genomeName}.bam
srun samtools sort -@ $cpu -o ${genomeName}_noSpecialChar_sorted.bam ${genomeName}.bam
srun samtools index -@ $cpu ${genomeName}_noSpecialChar_sorted.bam
rm ${genomeName}.sam ${genomeName}.bam
srun qualimap bamqc -bam ${genomeName}_noSpecialChar_sorted.bam -gff ${gffPath} -nt ${cpu}
echo "ended at: $(date)" >> ${genomeName}_mapping.log
echo -e "\nMapping genome ${genomeName} complete\n"

```

```{shell, create lists for fastq files and sample names}

# get paths
find [PATH_TO]/partialSHIC_DsGRP/rawData/batch_1 -type f -name '*.fq.gz' > fastq_paths.txt

# get sample names
sed 's/.*clean_reads\///' fastq_paths.txt | 
  sed 's/\/.*//' | 
  sort |
  uniq > samples.txt

```

```{shell, template script for mapping}

module load anaconda3
conda activate bwa

cd [PATH_TO]/partialSHIC_DsGRP/mapping

refName='drosophila_06Jul2018_A8VGg_noSpecialChar.fasta'
genomeName='line[SAMPLE]'
DNAfastqPath=$(grep '/[SAMPLE]/' fastq_paths.txt | sed 's/[SAMPLE]\/.*/[SAMPLE]\//')
DNAfastq1=$(grep '/[SAMPLE]/.*1.fq.gz' fastq_paths.txt)
DNAfastq2=$(grep '/[SAMPLE]/.*2.fq.gz' fastq_paths.txt)
gffPath="[PATH_TO]/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_noSpecialChar.gff"
cpu=8

echo -e "\nMapping genome ${genomeName}...\n"
echo "started mapping genome ${genomeName} at: $(date)" > ${genomeName}_mapping.log
srun bwa mem -t $cpu $refName $DNAfastq1 $DNAfastq2 > ./${genomeName}.sam
srun samtools view -@ $cpu -bT $refName ${genomeName}.sam -o ${genomeName}.bam
srun samtools sort -@ $cpu -o ${genomeName}_noSpecialChar_sorted.bam ${genomeName}.bam
srun samtools index -@ $cpu ${genomeName}_noSpecialChar_sorted.bam
rm ${genomeName}.sam ${genomeName}.bam
srun qualimap bamqc -bam ${genomeName}_noSpecialChar_sorted.bam -gff ${gffPath} -nt ${cpu}
echo "ended at: $(date)" >> ${genomeName}_mapping.log
echo -e "\nMapping genome ${genomeName} complete\n"

```

```{shell, create scripts for mapping}

while read p; do
  echo "$p"
  cp template_bwa.txt bwa_line${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" bwa_line${p}.slurm.sh
done < samples.txt

chmod 755 ./*.slurm.sh

```

# GATK genotyping 

## conda 

```{shell, create conda environment for genotyping}

module load anaconda3
conda create -n gatk
conda activate gatk
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

conda install qualimap r-base picard gatk4 samtools vcftools bcftools bedtools

module load anaconda3
conda activate gatk

```

## data pre-processing for variant discovery 

### duplicate removal 

```{shell, example duplicate removal}

module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/MarkDuplicates

genomeName="line100"
BAMPath="[PATH_TO]/partialSHIC_DsGRP/mapping/${genomeName}_noSpecialChar_sorted.bam"
outBAM="${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outMetrics="${genomeName}_MarkDuplicatesMetrics.txt"
gffPath="[PATH_TO]/partialSHIC_DsGRP/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_noSpecialChar.gff"
cpu=8

# MarkDuplicates

echo -e "\nMarkDuplicates\n"

srun picard MarkDuplicates\
 INPUT=${BAMPath}\
 OUTPUT=./${outBAM}\
 REMOVE_DUPLICATES=TRUE\
 METRICS_FILE=./${outMetrics}

# Qualimap

srun qualimap bamqc -bam ${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam -gff ${gffPath} -nt ${cpu}

```

```{shell, template script for duplicate removal}

module load anaconda3
conda activate gatk

cd /scratch/project_mnt/S0032/partialSHIC_DsGRP/MarkDuplicates

genomeName="line[SAMPLE]"
BAMPath="[PATH_TO]/partialSHIC_DsGRP/mapping/${genomeName}_noSpecialChar_sorted.bam"
outBAM="${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outMetrics="${genomeName}_MarkDuplicatesMetrics.txt"
gffPath="[PATH_TO]/partialSHIC_DsGRP/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_noSpecialChar.gff"
cpu=8

# MarkDuplicates

echo -e "\nMarkDuplicates\n"

srun picard MarkDuplicates\
 INPUT=${BAMPath}\
 OUTPUT=./${outBAM}\
 REMOVE_DUPLICATES=TRUE\
 METRICS_FILE=./${outMetrics}

# Qualimap

srun qualimap bamqc -bam ${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam -gff ${gffPath} -nt ${cpu}

```

```{shell, create scripts for duplicate removal}

while read p; do
  echo "$p"
  cp template_MarkDuplicates.slurm.sh MarkDuplicates_line${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" MarkDuplicates_line${p}.slurm.sh
done < [PATH_TO]/partialSHIC_DsGRP/mapping/samples.txt

chmod 755 ./*.slurm.sh

```

## HaplotypeCaller in GVCF mode 

### index ref 

```{shell, index the reference genome for GATK}

# Dell 2

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

module load anaconda3
conda activate gatk

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
dict="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar"

picard CreateSequenceDictionary\
 R=$refPath\
 O=${dict}.dict

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

module load anaconda3
conda activate gatk

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
dict="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar"

picard CreateSequenceDictionary\
 R=$refPath\
 O=${dict}.dict

```

### call genotypes 

#### AddReplaceReadGroups 

```{shell, example AddOrReplaceReadGroups}

module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line100"
BAMPath1="[PATH_TO]partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="[PATH_TO]partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=8

# add read groups (2 minutes)

srun gatk AddOrReplaceReadGroups \
I=${BAMPath1} \
O=${outBAM1} \
RGLB=lib \
RGPL=ILLUMINA \
RGPU=unit1 \
RGSM=${genomeName}

# index bam file

srun samtools index -@ ${cpu} ${outBAM1}

```

```{shell, template script for AddOrReplaceReadGroups}

module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line[SAMPLE]"
BAMPath1="[PATH_TO]/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="[PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=8

# add read groups (2 minutes)

srun gatk AddOrReplaceReadGroups \
I=${BAMPath1} \
O=${outBAM1} \
RGLB=lib \
RGPL=ILLUMINA \
RGPU=unit1 \
RGSM=${genomeName}

# index bam file

srun samtools index -@ ${cpu} ${outBAM1}

```

```{shell, create scripts for AddOrReplaceReadGroups}

while read p; do
  echo "$p"
  cp template_AddOrReplaceReadGroups.txt AddOrReplaceReadGroups_line${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" AddOrReplaceReadGroups_line${p}.slurm.sh
done < [PATH_TO]/partialSHIC_DsGRP/mapping/samples.txt

chmod 755 ./*.slurm.sh

```

#### HaplotypeCaller 

##### regions for parallel 

```{shell, regions file to run HaplotypeCaller in parallel}

module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"

# create regions file for parallel processing

samtools faidx ${refPath}
awk 'BEGIN {FS="\t"}; {print $1 FS "0" FS $2}' ${refPath}.fai > ${refPath}.bed
seqnames=$(cut -f 1 ${refPath}.bed)
cut -f 1 ${refPath}.bed > seqnames.txt

```

##### genotype 

```{shell, example HaplotypeCaller}

module load parallel
module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line100"
BAMPath1="[PATH_TO]/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="[PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=32

# Parallel should launch one instances of srun per SLURM task
MY_PARALLEL_OPTS="-N 1 --delay .2 -j ${SLURM_NTASKS} --joblog ${genomeName}-parallel-${SLURM_JOBID}.log"

# srun itself should launch 1 instance of our program and not oversubscribe resources
MY_SRUN_OPTS="-N 1 -n 1 --exclusive"

# Use parallel to launch srun with these options
# parallel $MY_PARALLEL_OPTS srun $MY_SRUN_OPTS ./a.out ::: {0..1023}

# run HaplotypCaller in parallel
seqnames=$(cut -f 1 ${refPath}.bed)
parallel $MY_PARALLEL_OPTS srun $MY_SRUN_OPTS "gatk HaplotypeCaller -R ${refPath} -I ${BAMPath2} --intervals {} -O ${genomeName}_{}.g.vcf.gz --output-mode EMIT_ALL_CONFIDENT_SITES -ERC BP_RESOLUTION" ::: $seqnames

```

```{shell, template script for HaplotypeCaller}

module load parallel
module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line[SAMPLE]"
BAMPath1="[PATH_TO]/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="[PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=8

# Parallel should launch one instances of srun per SLURM task
MY_PARALLEL_OPTS="-N 1 --delay .2 -j ${SLURM_NTASKS} --joblog ${genomeName}-parallel-${SLURM_JOBID}.log"

# srun itself should launch 1 instance of our program and not oversubscribe resources
MY_SRUN_OPTS="-N 1 -n 1 --exclusive"

# run HaplotypCaller in parallel
seqnames=$(cut -f 1 ${refPath}.bed)
parallel $MY_PARALLEL_OPTS srun $MY_SRUN_OPTS "gatk HaplotypeCaller -R ${refPath} -I ${BAMPath2} --intervals {} -O ${genomeName}_{}.g.vcf.gz --output-mode EMIT_ALL_CONFIDENT_SITES -ERC BP_RESOLUTION" ::: $seqnames

```

```{shell, create scripts for HaplotypeCaller}

while read p; do
  echo "$p"
  cp template_HaplotypeCaller.txt HaplotypeCaller_line${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" HaplotypeCaller_line${p}.slurm.sh
done < [PATH_TO]/partialSHIC_DsGRP/mapping/samples.txt

chmod 755 ./*.slurm.sh

```

##### gather VCFs 

```{shell, example GatherVcfs}

# module load parallel
module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line100"
BAMPath1="[PATH_TO]/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="[PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=1

# gather vcf files

find ./ -type f -name "${genomeName}_*.g.vcf.gz" | sed 's/\.\///' > temp_${genomeName}.list
sort -V temp_${genomeName}.list > input_vcfs_${genomeName}.list
rm temp_${genomeName}.list

gatk GatherVcfs\
 -I input_vcfs_${genomeName}.list\
 -O ${genomeName}.g.vcf.gz\
 --REFERENCE_SEQUENCE ${refPath}

gatk IndexFeatureFile\
 -I ${genomeName}.g.vcf.gz

```

```{shell, template script for GatherVcfs}

module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
refName="drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
genomeName="line[SAMPLE]"
BAMPath1="[PATH_TO]/partialSHIC_DsGRP/MarkDuplicates/${genomeName}_noSpecialChar_sorted_MarkDuplicates.bam"
outBAM1="${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
BAMPath2="[PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/${genomeName}_noSpecialChar_sorted_MarkDuplicates_ReadGroups.bam"
cpu=1

# gather vcf files

find ./ -type f -name "${genomeName}_*.g.vcf.gz" | sed 's/\.\///' > temp_${genomeName}.list
sort -V temp_${genomeName}.list > input_vcfs_${genomeName}.list
rm temp_${genomeName}.list

gatk GatherVcfs\
 -I input_vcfs_${genomeName}.list\
 -O ${genomeName}.g.vcf.gz\
 --REFERENCE_SEQUENCE ${refPath}

gatk IndexFeatureFile\
 -I ${genomeName}.g.vcf.gz

```

```{shell, create scripts for GatherVcfs}

while read p; do
  echo "$p"
  cp template_GatherVcfs.txt GatherVcfs_line${p}.slurm.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" GatherVcfs_line${p}.slurm.sh
done < [PATH_TO]/partialSHIC_DsGRP/mapping/samples.txt

chmod 755 ./*.slurm.sh

```

##### CombineGVCFs 

```{shell, consolidate GVCFs}

# Create a samples mapping file
  
cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

samples_file="../../mapping/samples.txt"
samples=$(cat ../../mapping/samples.txt)
scaffolds=$(ls | grep 'line100_*' | grep -v 'tbi' | grep -v 'line100.g.vcf.gz' | sed 's/line100_//; s/.g.vcf.gz//')

for scf in $scaffolds; do
    for sample in $samples; do
        printf "line%s_${scf}\tline%s_${scf}.g.vcf.gz\tline%s_${scf}.g.vcf.gz.tbi\n" "$sample" "$sample" "$sample" >> map_${scf}_file.txt
    done
done

for scf in $scaffolds; do
    cut -f 2 map_${scf}_file.txt > variants_${scf}.list
done

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"

for scf in $scaffolds; do
    gatk CombineGVCFs\
        -R $refPath\
        --variant variants_${scf}.list\
        --convert-to-base-pair-resolution true\
        -O cohort.${scf}.basepair.vcf.gz
done

```

##### joint-call cohort 

```{shell, GenotypeGVCFs}

module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/Bunya

refPath="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"

contigs=$(find ./ -type f -name 'cohort*.vcf.gz' | sed 's/\.\/cohort\.//' | sed 's/\.basepair\.vcf\.gz//')

find ./ -type f -name 'cohort*.vcf.gz' | sed 's/\.\/cohort\.//' | sed 's/\.basepair\.vcf\.gz//' > contigs.txt

while IFS= read -r contig; do
  gatk --java-options "-Xmx120g" GenotypeGVCFs -R "$refPath" -V "cohort.${contig}.basepair.vcf.gz" --include-non-variant-sites true -O "GenotypeGVCFs_${contig}_Default_allSites.vcf.gz"
done < contigs.txt

# gather vcf files

find ./ -type f -name "GenotypeGVCFs_*.vcf.gz" | sed 's/\.\///' > temp.list
sort -V temp.list > input_vcfs.list
rm temp.list

gatk GatherVcfs\
 -I input_vcfs.list\
 -O GenotypeGVCFs_Default_allSites.vcf.gz\
 --REFERENCE_SEQUENCE ${refPath}

nohup gatk IndexFeatureFile\
 -I GenotypeGVCFs_Default_allSites.vcf.gz > nohup_IndexFeatureFile_GenotypeGVCFs_Default_allSites.log &

nohup gatk GatherVcfs\
 -I input_vcfs_top6.list\
 -O GenotypeGVCFs_Default_allSites_top6.vcf.gz\
 --REFERENCE_SEQUENCE ${refPath} > nohup_GatherVcfs_GenotypeGVCFs_Default_allSites_top6.log &

nohup gatk IndexFeatureFile\
 -I GenotypeGVCFs_Default_allSites_top6.vcf.gz > nohup_IndexFeatureFile_GenotypeGVCFs_Default_allSites_top6.log &

nohup gatk GatherVcfs\
 -I input_vcfs_NOTtop6.list\
 -O GenotypeGVCFs_Default_allSites_NOTtop6.vcf.gz\
 --REFERENCE_SEQUENCE ${refPath} > nohup_GatherVcfs_GenotypeGVCFs_Default_allSites_NOTtop6.log &

nohup gatk IndexFeatureFile\
 -I GenotypeGVCFs_Default_allSites_NOTtop6.vcf.gz > nohup_IndexFeatureFile_GenotypeGVCFs_Default_allSites_NOTtop6.log &

```

##### filter variants 

```{python, format_and_recode_biallelicNoRef.py}

import subprocess
import sys
import re

input_file = sys.argv[1]

biallelic_genotypes = ["0/0", "0|0", "0/1", "0|1", "1/1", "1|1", "./."]
biallelicNoRef_genotypes = ["1/1", "1|1", "1/2", "1|2", "2/2", "2|2", "./."]
monomorphicRef_genotype = ["0/0", "0|0", "./."]
monomorphicNoRef_genotype = ["1/1", "1|1", "./."]

def extract_genotype(genotype):
    return genotype.split(':')[0]

def recode_genotype(genotype):
    genotype = re.sub(r'(^|[^0-9/])1/1([^0-9/]|$)', r'\g<1>0/0\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9|])1\|1([^0-9|]|$)', r'\g<1>0|0\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9/])1/2([^0-9/]|$)', r'\g<1>0/1\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9|])1\|2([^0-9|]|$)', r'\g<1>0|1\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9/])2/2([^0-9/]|$)', r'\g<1>1/1\g<2>', genotype)
    genotype = re.sub(r'(^|[^0-9|])2\|2([^0-9|]|$)', r'\g<1>1|1\g<2>', genotype)
    return genotype

with subprocess.Popen(['zcat', input_file], stdout=subprocess.PIPE, universal_newlines=True) as proc:
    for line in proc.stdout:
        line = line.rstrip('\n')
        if line.startswith('#'):
            print(line)
        else:
            fields = line.split('\t')
            count = sum(field.count('./.') for field in fields[9:])
            F_MISS = count / 111
            if all(extract_genotype(genotype) in biallelic_genotypes for genotype in fields[9:]):
                if F_MISS <= 0.25:
                    fields[6] = 'Biallelic75'
                if F_MISS <= 0.1:
                    fields[6] = 'Biallelic90'
                if F_MISS <= 0.05:
                    fields[6] = 'Biallelic95'
                if F_MISS == 0:
                    fields[6] = 'Biallelic100'
            if ',' in fields[4] and all(extract_genotype(genotype) in biallelicNoRef_genotypes for genotype in fields[9:]):
                alt1 = fields[4].split(',')[0]
                alt2 = fields[4].split(',')[1]
                fields[3] = alt1
                fields[4] = alt2
                fields[9:] = [recode_genotype(genotype) for genotype in fields[9:]]
                if F_MISS <= 0.25:
                    fields[6] = 'BiallelicNoRef75'
                if F_MISS <= 0.1:
                    fields[6] = 'BiallelicNoRef90'
                if F_MISS <= 0.05:
                    fields[6] = 'BiallelicNoRef95'
                if F_MISS == 0:
                    fields[6] = 'BiallelicNoRef100'
            if '.' in fields[4] and all(extract_genotype(genotype) in monomorphicRef_genotype for genotype in fields[9:]):
                fields[6] = 'MonomorphicRef'
            if all(extract_genotype(genotype) in monomorphicNoRef_genotype for genotype in fields[9:]):
                fields[6] = 'MonomorphicAlt'
            if 'DP=' in fields[7] and count < 111:
                match = re.search(r'DP=(\d+)', fields[7])
                if match:
                    depth = int(match.group(1))/111
                    if depth < 5 or depth > 30:
                        fields[6] = 'Depth'
            if F_MISS > 0.75:
                fields[6] = 'Missing75'
            if len(fields[3]) > 1 or len(fields[4]) > 1:
                fields[6] = 'Indel'
            if ',' in fields[4]:
                fields[6] = 'Multiallelic'
            if '*' in fields[3]:
                fields[6] = 'Complex'
            print('\t'.join(fields))

```

```{shell, filter for 100% genotyped biallelic SNPs}

module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller

vcfs="GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites GenotypeGVCFs_Default_allSites"

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites.vcf.gz > GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf &

nohup python format_and_recode_biallelicNoRef.py GenotypeGVCFs_Default_allSites.vcf.gz > GenotypeGVCFs_Default_allSites_FILTERED_FORMATfilled_recode_biallelicNoRef.vcf &



nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf &

nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf &

nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf &

grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf

nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf &

nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf &

nohup grep -E '^#|Biallelic100|BiallelicNoRef100' GenotypeGVCFs_Default_allSites_FILTERED_FORMATfilled_recode_biallelicNoRef.vcf > GenotypeGVCFs_Default_allSites_FILTERED_recode_biallelicNoRef.vcf &




conda activate pigz

pigz -p 38 -v GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
pigz -p 38 -v GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
pigz -p 38 -v GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
pigz -p 38 -v GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
pigz -p 38 -v GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
pigz -p 38 -v GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FORMATfilled_recode_biallelicNoRef.vcf
pigz -p 38 -v GenotypeGVCFs_Default_allSites_FILTERED_recode_biallelicNoRef.vcf

```

##### mask 

```{shell, create mask fasta Dell 2}

module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP

# create mask bed

zcat [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/GenotypeGVCFs_Default_allSites_FILTERED_recode_biallelicNoRef.vcf |\
 awk '!/^#/ && $4 ~ /N/ { print $1"\t"$2-1"\t"$2"\t"$4"\t"$7 }' >\
 GenotypeGVCFs_Default_allSites_FILTERED_MASK.bed


# create masked reference

REF="[PATH_TO]/partialSHIC_DsGRP/mapping/drosophila_06Jul2018_A8VGg_noSpecialChar.fasta"
BED="GenotypeGVCFs_Default_allSites_FILTERED_MASK.bed"
REF_MASKED="drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta"

bedtools maskfasta -fi ${REF} -bed ${BED} -fo ${REF_MASKED}

```

# Phase 

```{shell, phase genotyping with shapeit}

# contig 542

nohup [PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

[PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

# contig 594

nohup [PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

[PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

# contig 628

nohup [PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

[PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

# contig 718

nohup [PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

[PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

# contig 76

nohup [PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

[PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

# contig 785

nohup [PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -phase\
 --input-vcf ~/shared/Scott_Allen/partialSHIC_DsGRP/HaplotypeCaller/Bunya/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf\
 --window 0.5\
 --effective-size 1000000\
 --rho 0.01\
 --output-max HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-graph HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.graphs\
 --output-log HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.log\
 --thread 38 &

[PATH_TO]/shapeit.v2.904.3.10.0-693.11.6.el7.x86_64/bin/shapeit\
 -convert\
 --input-haps HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.haps\
 --output-vcf HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf

```

# Synthetic inbreeding 

```{python, synthetic_inbreed.py}

import subprocess
import sys
import random

input_file = sys.argv[1]
# input_file = 'toy.vcf'

# Set the seed for reproducibility
seed_value = 123
random.seed(seed_value)

# Generate a random list of 111 values (0 or 1)
random_list = [random.randint(0, 1) for _ in range(111)]

with subprocess.Popen(['cat', input_file], stdout=subprocess.PIPE, universal_newlines=True) as proc:
    for line in proc.stdout:
        line = line.rstrip('\n')
        if line.startswith('#'):
            print(line)
        else:
            fields = line.split('\t')
            for i in range(111):
                haplotype = fields[i+9].split("|")[random_list[i]]
                fields[i+9] = haplotype + "|" + haplotype
            print('\t'.join(fields))

```

```{shell, run synthetic_inbreed.py}

module load anaconda3
conda activate gatk

cd [PATH_TO]/partialSHIC_DsGRP/shapeit

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

nohup python synthetic_inbreed.py\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros.vcf >\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf &

```

# ######################## partialS/HIC - empirical feature vectors

## conda 

```{shell, install requirements for partialS/HIC}

# Dell 2

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC
module load anaconda3
source ~/conda-init
conda create -n partialshic_py3
conda activate partialshic_py3
conda install pip
pip install gcc
#pip install diploshic
pip install numpy
python setup.py install
pip install h5py
pip install scikit-allel
pip install matplotlib
pip install pandas
pip install scikit-learn
pip install tensorflow # this installs keras
# pip install np_utils # NOT NEEDED

module load anaconda3
conda activate partialshic_py3

```

## Empirical feature vectors
    
### Empirical to FVs

```{shell, copy vcf files}

cd [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors

rsync -ahPv\
 [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./
rsync -ahPv\
 [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./
rsync -ahPv\
 [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./
rsync -ahPv\
 [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./
rsync -ahPv\
 [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./
rsync -ahPv\
 [PATH_TO]/partialSHIC_DsGRP/HaplotypeCaller/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz ./

```

```{shell, format vcf files to have short names}

cd [PATH_TO]/partialSHIC_DsGRP/

sed 's/_HRSCAF_.*//' drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta > drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta

cd [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors

nohup zcat GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup zcat GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef.vcf.gz |\
 sed 's/_HRSCAF_[0-9]\+//' >\
 GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf &

nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf
nohup pigz -p 38 -v GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_FILTERED_recode_biallelicNoRef_shortName.vcf

cd [PATH_TO]/partialSHIC_DsGRP/shapeit

sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > HaplotypeData_PIRsList_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_shortName.vcf

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

nohup sed 's/_HRSCAF_[0-9]\+//' HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred.vcf > HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf &

```

```{shell, format inbred line names sample_pops}

cd [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors

zcat GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_FILTERED.vcf.gz |\
 grep '#CHROM' |\
 cut -f 10-121 |\
 sed 's/\t/ dser\n/g' |\
 sed 's/line97/line97 dser/' > samples_pops.txt

```

#### Port to Python 3 and custom masking 

```{python, empirical_convert_to_FVs_python3_phased.py}

import time
#startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
#import h5py # NOT USED
import allel
from fvTools import *
import numpy as np
import math

'''usage eg:
pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
python2 empirical_convert_to_FVs.py ag1000g.phase1.ar3.haplotypes.2L.h5 2L 49364325 1 5000000 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.2L.fa samples_pops.txt AOM 2L.1.stats 2L.1.fvec
'''

if not len(sys.argv) in [15,17]:
  sys.exit("usage:\npython2 empirical_convert_to_FVs.py chrArmFileName chrArm chrLen [segmentStart segmentEnd] subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName sampleToPopFileName targetPop statFileName fvecFileName\n")

if len(sys.argv)==17:
  chrArmFileName, chrArm, chrLen, segmentStart, segmentEnd, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
else:
  chrArmFileName, chrArm, chrLen, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, sampleToPopFileName, targetPop, statFileName, fvecFileName = sys.argv[1:]
  segmentStart=None

#chrArmFile=h5py.File(chrArmFileName,"r")
#genos=allel.GenotypeChunkedArray(chrArmFile[chrArm]["calldata"]["genotype"])
#positions=allel.SortedIndex(chrArmFile["/%s/variants/POS" %(chrArm)][:])
#refAlleles=chrArmFile[chrArm]['variants']['REF']
#altAlleles=chrArmFile[chrArm]['variants']['ALT']
#samples=chrArmFile[chrArm]["samples"]

chrArmFile=allel.read_vcf(chrArmFileName) # change to scikit-allel
genos = allel.GenotypeArray(chrArmFile['calldata/GT']) # change to scikit-allel
positions=allel.SortedIndex(chrArmFile['variants/POS'], copy=False) # change to scikit-allel
refAlleles=chrArmFile['variants/REF'] # change to scikit-allel
altAlleles=chrArmFile['variants/ALT'] # change to scikit-allel
samples=chrArmFile['samples'] # change to scikit-allel

chrLen=int(chrLen)
assert chrLen>0

print(chrArm + " is " + str(chrLen) + " basepairs, " + "Genotyped " + str(len(genos))) # SCOTT troubleshooting missed regions

if segmentStart!=None:
  segmentStart,segmentEnd = int(segmentStart),int(segmentEnd)
  assert segmentStart>0 and segmentEnd>=segmentStart
  snpIndicesToKeep=[x for x in range(len(positions)) if segmentStart<=positions[x]<=segmentEnd]
  genos=allel.GenotypeArray(genos.subset(sel0=snpIndicesToKeep))
  positions=[positions[x] for x in snpIndicesToKeep]
  refAlleles=[refAlleles[x] for x in snpIndicesToKeep]
  altAlleles=[altAlleles[x] for x in snpIndicesToKeep]

subWinSize,numSubWins,unmaskedFracCutoff,pMisPol = int(subWinSize),int(numSubWins),float(unmaskedFracCutoff),float(pMisPol)

assert subWinSize>0 and numSubWins>1

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")

if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")

standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none","false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*chrLen
  sys.stderr.write("Warning: a mask.fa file for the chr arm with all masked sites N'ed out is strongly recommended (pass in the reference to remove Ns at the very least)!\n")
else:
  unmasked=readMaskDataForScan(maskFileName,chrArm)
  assert len(unmasked)==chrLen

print(chrArm + " is " + str(chrLen) + " basepairs, " + "unmasked " + str(sum(unmasked)) + " basepairs") # SCOTT troubleshooting missed regions

print("Subset samples...")

def readSampleToPopFile(sampleToPopFileName):
  table={}
  with open(sampleToPopFileName) as sampleToPopFile:
    for line in sampleToPopFile:
      sample,pop = line.strip().split()
      table[sample]=pop
  return table

sampleToPop=readSampleToPopFile(sampleToPopFileName)

sampleIndicesToKeep=[x for x in range(len(samples)) if sampleToPop.get(samples[x],"popNotFound!")==targetPop]

genos=genos.subset(sel1=sampleIndicesToKeep)

print("Create alleleCounts and isBiallelic...")

alleleCounts=genos.count_alleles() # Count the number of calls of each allele per variant.
isBiallelic=alleleCounts.is_biallelic() # Find variants biallelic for the reference (0) and first alternate (1) allele.

print("Total biallelic loci " + str(sum(isBiallelic)))

print("Biallelic loci passing filters " + str(sum(isBiallelic)))

#snpIndicesToKeep=[x for x in range(len(positions)) if genosRate100Mask_hetMask_2[positions[x]-1]] # SCOTT
snpIndicesToKeep=[x for x in range(len(positions)) if isBiallelic[x]] # SCOTT
genos_biallelic=genos.subset(sel0=snpIndicesToKeep) # SCOTT
haps=genos.to_haplotypes()
haps_biallelic=genos_biallelic.to_haplotypes() # SCOTT
# mapping_biallelic=[mapping[x] for x in snpIndicesToKeep] # SCOTT
alleleCounts_biallelic=genos_biallelic.count_alleles() # SCOTT
positions_biallelic=[positions[x] for x in snpIndicesToKeep] # SCOTT
refAlleles_biallelic=[refAlleles[x] for x in snpIndicesToKeep] # SCOTT
altAlleles_biallelic=[altAlleles[x] for x in snpIndicesToKeep] # SCOTT

##################################################################################################

# SCOTT fix problem only, still mask unpolarised

def my_polarizeSnps(unmasked, positions_biallelic, refAlleles_biallelic, altAlleles_biallelic, ancArm):
    assert len(unmasked) == len(ancArm)
    assert len(positions_biallelic) == len(refAlleles_biallelic)
    assert len(positions_biallelic) == len(altAlleles_biallelic)
    isSnp = {}
    for i in range(len(positions_biallelic)):
        isSnp[positions_biallelic[i]] = i
    mapping = []
    for i in range(len(ancArm)):
        if ancArm[i] in 'ACGT':
            if i+1 in isSnp:
                # ref, alt = refAlleles[isSnp[i+1]], altAlleles[isSnp[i+1]]
                ref, alt = refAlleles_biallelic[isSnp[i+1]], altAlleles_biallelic[isSnp[i+1]][0] # SCOTT
                if ancArm[i] == ref:
                    mapping.append([0, 1])  # no swap
                elif ancArm[i] == alt:
                    mapping.append([1, 0])  # swap
                else:
                    mapping.append([0, 1])  # no swap -- failed to polarize
                    unmasked[i] = False # mask SNP if cannot polarize
        elif ancArm[i] == "N":
            #unmasked[i] = False
            if i+1 in isSnp:
                mapping.append([0, 1])  # no swap -- failed to polarize
        else:
            sys.exit(
                "Found a character in ancestral chromosome "\
                 "that is not 'A', 'C', 'G', 'T' or 'N' (all upper case)!\n")
    assert len(mapping) == len(positions_biallelic)
    return mapping, unmasked

################################################################################################

#ancArm=readFaArm(ancestralArmFaFileName).upper()
#sys.stderr.write("polarizing SNPs\n")
#polTime=time.clock()
#mapping,unmasked = polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm)
#sys.stderr.write("took %s seconds to polarize SNPs\n" %((time.clock()-polTime)))

ancArm=readFaArm(ancestralArmFaFileName, chrArm).upper() # no change
print("polarizing SNPs\n") # changed to print but didn't need to
polTime=time.perf_counter() # Python 3
mapping,unmasked = my_polarizeSnps(unmasked,positions,refAlleles,altAlleles,ancArm) # use my_polarizeSnps
print("took %s seconds to polarize SNPs" % (time.perf_counter() - polTime)) # Python 3

print(chrArm + " is " + str(chrLen) + " basepairs, " + "unmasked " + str(sum(unmasked)) + " basepairs after polarization") # SCOTT

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

statHeader="chrom start end".split()
header="chrom classifiedWinStart classifiedWinEnd bigWinRange".split()

for statName in statNames:
  statHeader.append(statName)
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName,i))

statHeader="\t".join(statHeader)
header="\t".join(header)

precomputedStats={}

def getSubWinBounds(subWinSize,positions):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  subWinBounds=[]
  for i in range(len(positions)):
    while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if (subWinStart,subWinEnd) not in subWinBounds:
      subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,positions)

#dafs=alleleCounts[:,1]/float(len(sampleIndicesToKeep)*2)
dafs=alleleCounts_biallelic[:,1]/float(len(sampleIndicesToKeep)*2) # SCOTT

#ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
#nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
#nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
#sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))

ihsVals=allel.stats.selection.ihs(haps_biallelic,positions_biallelic,use_threads=False,include_edges=False) # SCOTT
nonNanCount=[x for x in np.isnan(ihsVals)].count(False) # SCOTT
nonInfCount=[x for x in np.isinf(ihsVals)].count(False) # SCOTT
print("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount)) # SCOTT

#if nonNanCount==0:
#  precomputedStats["iHS"]=[]
#  for subWinIndex in range(len(subWinBounds)):
#    precomputedStats["iHS"].append([])
#else:
#  ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
#  precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["iHS"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["iHS"].append([])
else:
  ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
  precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)

#nslVals=allel.stats.selection.nsl(haps,use_threads=False)
#nonNanCount=[x for x in np.isnan(nslVals)].count(False)
#sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))

nslVals=allel.stats.selection.nsl(haps_biallelic,use_threads=False) # SCOTT
nonNanCount=[x for x in np.isnan(nslVals)].count(False) # SCOTT
print("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount)) # SCOTT

#if nonNanCount==0:
#  precomputedStats["nSL"]=[]
#  for subWinIndex in range(len(subWinBounds)):
#    precomputedStats["nSL"].append([])
#else:
#  nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
#  precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)

if nonNanCount==0:
  precomputedStats["nSL"]=[]
  for subWinIndex in range(len(subWinBounds)):
    precomputedStats["nSL"].append([])
else:
  nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
  precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions_biallelic,keepNans=False,absVal=True)

#mispolarization information not used since no inference on which specific SNPs may be mispolarized, and since each block contains a relatively small number of SNPs, especially with respect to the mispolarization rate, cannot effectively use the overall mispolarization rate

def SAFEstats(hapsInSubWin,mappingDerivedInSubWin,dafsInSubWin):
  haplotypes={}
  for i in range(len(hapsInSubWin[0])):
    haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
    haplotype="".join(str(x) for x in haplotype)
    if haplotype in haplotypes:
      haplotypes[haplotype].append(i)
    else:
      haplotypes[haplotype]=[i]
  HAF=[]
  HAFunique={}
  for i in haplotypes:
    HAFunique[i]=0
    for j in range(len(hapsInSubWin)):
      if hapsInSubWin[j][haplotypes[i][0]]==mappingDerivedInSubWin[j]:
        HAFunique[i]+=len([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j])) if hapsInSubWin[j][x]==mappingDerivedInSubWin[j]])
    for j in range(len(haplotypes[i])):
      HAF.append(HAFunique[i])
  phi=[]
  kappa=[]
  SAFE=[]
  for i in range(len(hapsInSubWin)):
    phi.append(0)
    kappa.append([])
    phiDenom=0
    for j in haplotypes:
      if int(list(j)[i])==mappingDerivedInSubWin[i]:
        phi[i]+=(HAFunique[j]*len(haplotypes[j]))
        if HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
          kappa[i].append(HAFunique[j])
      phiDenom+=(HAFunique[j]*len(haplotypes[j]))
    phi[i]/=float(phiDenom)
    kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
    if dafsInSubWin[i]==0 or dafsInSubWin[i]==1:
     SAFE.append(0.0)
    else:
     SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafsInSubWin[i]*(1-dafsInSubWin[i]))))
  statVals={}
  quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}
  for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
    if i=="SFS":
      windowStats=dafsInSubWin
    elif i=="HAFunique":
      #windowStats=[eval(i)[x] for x in eval(i)]
      windowStats = list(HAFunique.values()) # SCOTT
    else:
      windowStats=eval(i)
    statVals[i+"-Mean"]=np.mean(windowStats)
    statVals[i+"-Median"]=np.median(windowStats)
    if(len(np.unique(windowStats,return_counts=True)[1])==1):
      statVals[i+"-Mode"]=windowStats[0]
    else:
      if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
        statVals[i+"-Mode"]=scipy.stats.mstats.mode(windowStats)[0][0]
      else:
        mode=min(windowStats)
        for j in range(1,51):
          if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
            mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
        statVals[i+"-Mode"]=mode+((max(windowStats)-min(windowStats))/100)
    for j in quantiles:
      statVals[i+"-"+j]=np.percentile(windowStats,quantiles[j])
    statVals[i+"-Max"]=max(windowStats)
    statVals[i+"-Var"]=np.var(windowStats)
    statVals[i+"-SD"]=np.std(windowStats)
    statVals[i+"-Skew"]=scipy.stats.skew(windowStats)
    statVals[i+"-Kurt"]=scipy.stats.kurtosis(windowStats)
  return statVals

if segmentStart==None:
  firstSubWinStart=1
  lastSubWinStart=(((chrLen-1)/subWinSize)*subWinSize)+1
else:
  firstSubWinStart=(((segmentStart-1)/subWinSize)*subWinSize)+1
  lastSubWinStart=(((segmentEnd-1)/subWinSize)*subWinSize)+1

goodSubWins=[]

for i in range(numSubWins):
  goodSubWins.append(False)

subWinIndex=-1

#def getSnpIndicesInSubWins(subWinSize,positions):
#  subWinStart=1
#  subWinEnd=subWinStart+subWinSize-1
#  snpIndicesInSubWins=None
#  for i in range(len(positions)):
#    if snpIndicesInSubWins and not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#      snpIndicesInSubWins.append([])
#    while not (positions[i]>=subWinStart and positions[i]<=subWinEnd):
#      subWinStart+=subWinSize
#      subWinEnd+=subWinSize
#    if not snpIndicesInSubWins:
#      snpIndicesInSubWins=[[]]
#    snpIndicesInSubWins[-1].append(i)
#  return snpIndicesInSubWins

def getSnpIndicesInSubWins(subWinSize,positions_biallelic):
  subWinStart=1
  subWinEnd=subWinStart+subWinSize-1
  snpIndicesInSubWins=None
  for i in range(len(positions_biallelic)):
    if snpIndicesInSubWins and not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      snpIndicesInSubWins.append([])
    while not (positions_biallelic[i]>=subWinStart and positions_biallelic[i]<=subWinEnd):
      subWinStart+=subWinSize
      subWinEnd+=subWinSize
    if not snpIndicesInSubWins:
      snpIndicesInSubWins=[[]]
    snpIndicesInSubWins[-1].append(i)
  return snpIndicesInSubWins

#snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions)
snpIndicesInSubWins=getSnpIndicesInSubWins(subWinSize,positions_biallelic) # SCOTT

statVals={}

for statName in statNames:
  statVals[statName]=[]

if statFileName.lower() in ["none","false"]:
  statFileName=None
else:
  statFile=open(statFileName,"w")
  statFile.write(statHeader+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  if segmentStart!=None:
    fvecFileName=targetPop+'.'+chrArm+'.'+segStart+'.fvec'
  else:
    fvecFileName=targetPop+'.'+chrArm+'.fvec'

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

#for subWinStart in range(firstSubWinStart,lastSubWinStart+1,subWinSize):
for subWinStart in range(int(firstSubWinStart),int(lastSubWinStart)+1,int(subWinSize)):
  subWinEnd=subWinStart+subWinSize-1
  unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
  #if len([x for x in positions if subWinStart<=x<=subWinEnd])==0:
  if len([x for x in positions_biallelic if subWinStart<=x<=subWinEnd])==0:
    #sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    print("window at positions %d-%d: number of unmasked SNPs - 0; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,unmaskedFrac))
    goodSubWins.append(False)
  else:
    subWinIndex+=1
    #sys.stderr.write("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    #print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac))
    if unmaskedFrac<unmaskedFracCutoff:
      goodSubWins.append(False)
    else:
      goodSubWins.append(True)
      #hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
      hapsInSubWin=haps_biallelic.subset(sel0=snpIndicesInSubWins[subWinIndex])
      print("window at positions %d-%d: number of unmasked SNPs - %d; fraction of sites unmasked - %f; haplotypes - %d; unique haplotypes - %d\n" %(subWinStart,subWinEnd,len(snpIndicesInSubWins[subWinIndex]),unmaskedFrac, hapsInSubWin.shape[1], np.unique(hapsInSubWin, axis=1).shape[1]))
      for statName in statNames:
        if statName!="H12" and statName!="H2/H1" and statName!="Omega" and statName!="distSkew" and statName!="distKurt" and statName[0:3]!="HAF" and statName[0:3]!="phi" and statName[0:3]!="kap" and statName[0:3]!="SFS" and statName[0:3]!="SAF":
          if statName=="fayWuH":
            #calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
            calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, (len(statVals["thetaH"])-1), hapsInSubWin, unmasked, precomputedStats)
          else:
            #calcAndAppendStatValForScan(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
            calcAndAppendStatValForScan(alleleCounts_biallelic, positions_biallelic, statName, subWinStart, subWinEnd, statVals, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
      #SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      SAFEVals=SAFEstats(hapsInSubWin,[mapping[x][1] for x in snpIndicesInSubWins[subWinIndex]],dafs[snpIndicesInSubWins[subWinIndex]])
      for statName in SAFEVals:
        statVals[statName].append(SAFEVals[statName])
      if statFileName:
        statFile.write("\t".join([chrArm, str(subWinStart), str(subWinEnd)] + [str(statVals[statName][-1]) for statName in statNames]) + "\n")
  goodSubWins=goodSubWins[1:]
  if goodSubWins.count(True)==numSubWins:
    outVec=[]
    for statName in statNames:
      outVec+=normalizeFeatureVec(statVals[statName][-numSubWins:])
    midSubWinEnd=subWinEnd-(subWinSize*(numSubWins/2))
    midSubWinStart=midSubWinEnd-subWinSize+1
    fvecFile.write("\t".join([chrArm, str(midSubWinStart), str(midSubWinEnd), str((subWinEnd-(subWinSize*numSubWins)+1))+"-"+str(subWinEnd)] + [str(x) for x in outVec]) + "\n")

if statFileName:
  statFile.close()
fvecFile.close()
#sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
print("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### RUN (MISSING CODE FOR dser_partial_stats.txt YIGUAN)

```{shell, run empirical_convert_to_FVs_python3_phased.py}

# phased / synthetic inbred / 110 lines

## scaffold 542

nohup python [PATH_TO]/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_542\
 21028053\
 1\
 21025000\
 5000\
 11\
 0.25\
 0.003\
 [PATH_TO]/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 [PATH_TO]/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 [PATH_TO]/partialSHIC_DsGRP/top6.anc.fa\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542_110Lines.phased.noPIRs.inbred.stats\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542_110Lines.phased.noPIRs.inbred.fvec >\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_542_110Lines.phased.noPIRs.inbred.log &

## scaffold 594

nohup python [PATH_TO]/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_594\
 30326886\
 1\
 30325000\
 5000\
 11\
 0.25\
 0.003\
 [PATH_TO]/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 [PATH_TO]/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 [PATH_TO]/partialSHIC_DsGRP/top6.anc.fa\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594_110Lines.phased.noPIRs.inbred.stats\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594_110Lines.phased.noPIRs.inbred.fvec >\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_594_110Lines.phased.noPIRs.inbred.log &

## scaffold 628

nohup python [PATH_TO]/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_628\
 31265526\
 1\
 31265000\
 5000\
 11\
 0.25\
 0.003\
 [PATH_TO]/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 [PATH_TO]/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 [PATH_TO]/partialSHIC_DsGRP/top6.anc.fa\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_628_110Lines.phased.noPIRs.inbred.stats\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_628_110Lines.phased.noPIRs.inbred.fvec >\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_628_110Lines.phased.noPIRs.inbred.log &

## scaffold 718

nohup python [PATH_TO]/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_718\
 8653123\
 1\
 8650000\
 5000\
 11\
 0.25\
 0.003\
 [PATH_TO]/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 [PATH_TO]/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 [PATH_TO]/partialSHIC_DsGRP/top6.anc.fa\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.noPIRs.inbred.stats\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.noPIRs.inbred.fvec >\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718_110Lines.phased.noPIRs.inbred.log &

## scaffold 76

nohup python [PATH_TO]/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_76\
 38731659\
 1\
 38730000\
 5000\
 11\
 0.25\
 0.003\
 [PATH_TO]/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 [PATH_TO]/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 [PATH_TO]/partialSHIC_DsGRP/top6.anc.fa\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_76_110Lines.phased.noPIRs.inbred.stats\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_76_110Lines.phased.noPIRs.inbred.fvec >\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_76_110Lines.phased.noPIRs.inbred.log &

## scaffold 785

nohup python [PATH_TO]/partialSHIC/empirical_convert_to_FVs_python3_phased.py\
 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 ScA8VGg_785\
 28136203\
 1\
 28135000\
 5000\
 11\
 0.25\
 0.003\
 [PATH_TO]/partialSHIC_DsGRP/Yiguan/discoal_simu_partialSHIC/empiricalDat/dser_partial_stats.txt\
 [PATH_TO]/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED_shortName.fasta\
 [PATH_TO]/partialSHIC_DsGRP/top6.anc.fa\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/samples_pops_110.txt\
 "dser"\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_785_110Lines.phased.noPIRs.inbred.stats\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_785_110Lines.phased.noPIRs.inbred.fvec >\
 [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_785_110Lines.phased.noPIRs.inbred.log &


```

#### Merge FVs 

```{bash, merge empirical feature vectors}

for ff in ScA8VGg_76 ScA8VGg_785 ScA8VGg_628 ScA8VGg_594 ScA8VGg_542 ScA8VGg_718;do
echo ${ff}
outfile=${ff}.all.fvec
[ -f "${outfile}" ] && rm ${outfile}
head -1 ${ff}.1.fvec > ${outfile}
ls -ltrh ${ff}.[0-9]*.fvec | awk '{print $9}' | xargs -I {} tail -n +2 {} >> ${outfile}
done

```

# ######################## partialSHIC - constNe Ne 1M

## Generate training data 

```{shell, install discoal}

cd [PATH_TO]/partialSHIC_DsGRP

git clone https://github.com/kr-colab/discoal.git

cd [PATH_TO]/partialSHIC_DsGRP/discoal

make discoal

```

```{shell, mkdir and load R}

mkdir -p [PATH_TO]/partialSHIC_DsGRP/constNe

cd [PATH_TO]/partialSHIC_DsGRP/constNe

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get simulation parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '[PATH_TO]/discoal/discoal' 
TIMES = 2000

Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu:
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s:
# strength of selection: alpha = 2*Ne*s
s_low = 0.00001
s_high = 0.01
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
Pu_high <- 0.01 # 0.01*4*1000000 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2

# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# sweep position (irrelevant)
cat("sweep_pos: ", sweep_pos, "\n")

```

```{shell, discoal_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1000000
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1100
Pre_upper=3300

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=220
Pt_high=3520

# Selection coefficient s
s_low=0.00001
s_high=0.01
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.01

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell, discoal_Hard_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1000000
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1100
Pre_upper=3300

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=220
Pt_high=3520

# Selection coefficient s
s_low=0.00001
s_high=0.01
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.01

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_${TIME}.msOut.gz
done

```

```{shell, discoal_Soft_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1000000
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1100
Pre_upper=3300

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=220
Pt_high=3520

# Selection coefficient s
s_low=0.00001
s_high=0.01
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.01

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_${TIME}.msOut.gz
done

```

```{shell, discoal_HardPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1000000
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1100
Pre_upper=3300

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=220
Pt_high=3520

# Selection coefficient s
s_low=0.00001
s_high=0.01
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.01

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_${TIME}.msOut.gz
done

```

```{shell, discoal_SoftPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1000000
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1100
Pre_upper=3300

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=220
Pt_high=3520

# Selection coefficient s
s_low=0.00001
s_high=0.01
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.01

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_${TIME}.msOut.gz
done

```

```{shell, create discoal scripts for linked}

cd [PATH_TO]/partialSHIC_DsGRP/constNe/

# Hard-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_10.sh

# Soft-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_10.sh

# HardPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_10.sh

# SoftPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_10.sh

```

```{shell, run discoal scripts}

cd [PATH_TO]/partialSHIC_DsGRP/constNe

find ./ -maxdepth 1 -type f -name 'discoal_*.sh' | sed 's/\.\///' | sort -V > list_discoal_files.txt

while read p; do
  echo "${p} submitted"
  nohup bash ${p}
done < list_discoal_files.txt

```

## Generate test data 

```{shell, mkdir and load R}

mkdir -p [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat

cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat

module load anaconda3
conda activate r_env_4.3.1

```

```{r, get simulation parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '[PATH_TO]/discoal/discoal' 
TIMES = 2000

Ne = 1000000
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu:
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s:
# strength of selection: alpha = 2*Ne*s
s_low = 0.00001
s_high = 0.01
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
Pu_high <- 0.01 # 0.01*4*1000000 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2

# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

# bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# sweep position (irrelevant)
cat("sweep_pos: ", sweep_pos, "\n")

```

```{shell, discoal_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1000000
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1100
Pre_upper=3300

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=220
Pt_high=3520

# Selection coefficient s
s_low=0.00001
s_high=0.01
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.01

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell, discoal_Hard_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1000000
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1100
Pre_upper=3300

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=220
Pt_high=3520

# Selection coefficient s
s_low=0.00001
s_high=0.01
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.01

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_${TIME}.msOut.gz
done

```

```{shell, discoal_Soft_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1000000
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1100
Pre_upper=3300

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=220
Pt_high=3520

# Selection coefficient s
s_low=0.00001
s_high=0.01
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.01

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_${TIME}.msOut.gz
done

```

```{shell, discoal_HardPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1000000
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1100
Pre_upper=3300

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=220
Pt_high=3520

# Selection coefficient s
s_low=0.00001
s_high=0.01
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.01

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_${TIME}.msOut.gz
done

```

```{shell, discoal_SoftPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1000000
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1100
Pre_upper=3300

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=220
Pt_high=3520

# Selection coefficient s
s_low=0.00001
s_high=0.01
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.01

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_${TIME}.msOut.gz
done

```

```{shell, create discoal scripts for linked}

cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat

# Hard-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_10.sh

# Soft-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_10.sh

# HardPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_10.sh

# SoftPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_10.sh

```

```{shell, run discoal scripts}

cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat

find ./ -maxdepth 1 -type f -name 'discoal_*.sh' | sed 's/\.\///' | sort -V > list_discoal_files.txt

while read p; do
  echo "${p} submitted"
  nohup bash ${p}
done < list_discoal_files.txt

```

## Run training

### Training - calcStatsAndDafForEachSnp 

```{python, calcStatsAndDafForEachSnpSingleMsFile_Python3}

import sys
import allel
import random
import numpy as np
from msTools import *
from fvTools import *
import time

'''usage example
python calcStatsAndDafForEachSnpSingleMsFile.py /san/data/dan/simulations/discoal_multipopStuff/spatialSVMSims/trainingSets/equilibNeut.msout.gz 110000 iHS nSL
'''

trainingDataFileName, totalPhysLen = sys.argv[1:3]
totalPhysLen = int(totalPhysLen)

hapArraysIn, positionArrays = msOutToHaplotypeArrayIn(trainingDataFileName, totalPhysLen)
numInstances = len(hapArraysIn)

header = "daf\tiHS\tnSL"
#print header # Python 2
print(header) # Python 3

dafs = []
#start = time.clock() # Python 2
start = time.perf_counter() # Python 3
numInstancesDone = 0

numSnpsDone = 0
for instanceIndex in range(numInstances):
    if instanceIndex % 10 == 0:
        sys.stderr.write("done wtih %d of %d instances (%d SNPs)\n" %(instanceIndex, numInstances, numSnpsDone))
    haps = allel.HaplotypeArray(hapArraysIn[instanceIndex], dtype='i1')
    genos = haps.to_genotypes(ploidy=2)
    ac = genos.count_alleles()
    sampleSizes = [sum(x) for x in ac]
    assert len(set(sampleSizes)) == 1
    dafs = ac[:,1]/float(sampleSizes[0])
    ihsVals = allel.stats.selection.ihs(haps, positionArrays[instanceIndex], use_threads=True, include_edges=False)
    nslVals = allel.stats.selection.nsl(haps, use_threads=True)
    assert len(dafs) == len(ihsVals) == len(nslVals)
    numSnpsDone += len(dafs)
    for i in range(len(dafs)):
        #print "%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i]) # Python 2
        print ("%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i])) #Python 3
    if numSnpsDone > 5000000:
        break

```

```{shell, template script for calcStatsAndDafForEachSnpSingleMsFile_Python3.py}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/

msOut="[SAMPLE]"

python [PATH_TO]/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, create scripts for calcStatsAndDafForEachSnpSingleMsFile_Python3.py}

cd [PATH_TO]/partialSHIC_DsGRP/constNe/

find ./ -maxdepth 1 -type f -name '*.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_calcStatsAndDafForEachSnpSingleMsFile_Python3.txt calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
done < msOut_files.txt

find ./ -maxdepth 1 -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*.sh' -print0 | xargs -0 chmod 755

ls ./calcStatsAndDafForEachSnpSingleMsFile_Python3_*.sh

```

```{shell, run calcStatsAndDafForEachSnpSingleMsFile_Python3 scripts}

cd [PATH_TO]/partialSHIC_DsGRP/constNe/

while read p; do
  echo "calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh submitted"
  nohup bash calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
  sleep 0.25
done < msOut_files.txt

```

### Training - training_convert_to_FVs 

```{python, training_convert_to_FVs_python3.py}

import time
#startTime=time.clock() # Python 2
start = time.perf_counter() # Python 3
import sys
from msTools import *
from fvTools import *
import random
import allel
import numpy as np
import math

'''usage eg:
pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
python2 training_convert_to_FVs.py spNeut.msOut.gz 2L,2R,3L,3R 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.fa ./ spNeut.msOut.fvec
'''

if len(sys.argv)!=12:
  sys.exit("usage:\npython2 training_convert_to_FVs.py trainingDataFileName chrArmsForMasking subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName statDir fvecFileName\n")
else:
  trainingDataFileName, chrArmsForMasking, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, statDir, fvecFileName = sys.argv[1:]

subWinSize,numSubWins = int(subWinSize),int(numSubWins)
assert subWinSize>0 and numSubWins>1

totalPhysLen=(subWinSize*numSubWins)

hapArraysIn,positionArrays=msOutToHaplotypeArrayIn(trainingDataFileName,totalPhysLen)

chrArmsForMasking=chrArmsForMasking.split(",")

unmaskedFracCutoff,pMisPol=float(unmaskedFracCutoff),float(pMisPol)

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")

if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")

standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none", "false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*totalPhysLen
  sys.stderr.write("Warning: not doing any masking! (i.e. mask.fa file for the chr arm with all masked sites N'ed out, or at least the reference with Ns, is not provided)\n")
  maskFileName=False
else:
  if ancestralArmFaFileName.lower() in ["none", "false"]:
    maskData=readMaskDataForTraining(maskFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  else:
    maskData=readMaskAndAncDataForTraining(maskFileName,ancestralArmFaFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  if len(maskData)<len(hapArraysIn):
    sys.stderr.write("Warning: not enough windows from masked data (needed %d; got %d); will draw with replacement!!\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=True
  else:
    sys.stderr.write("enough windows from masked data (needed %d; got %d); will draw without replacement.\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=False

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

header=[]

for statName in statNames:
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName, i))

header="\t".join(header)

statVals={}

for statName in statNames:
  statVals[statName]=[]

def getSubWinBounds(subWinSize,totalPhysLen):
  subWinStart=1-subWinSize
  subWinEnd=0
  subWinBounds=[]
  for i in range(0,(numSubWins-1)):
    subWinStart+=subWinSize
    subWinEnd+=subWinSize
    subWinBounds.append((subWinStart,subWinEnd))
  subWinStart+=subWinSize
  subWinEnd=totalPhysLen
  subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,totalPhysLen)

def getSnpIndicesInSubWins(subWinBounds,positions):
  snpIndicesInSubWins=[]
  for subWinIndex in range(len(subWinBounds)):
    snpIndicesInSubWins.append([])
  subWinIndex=0
  for i in range(len(positions)):
    while not (positions[i]>=subWinBounds[subWinIndex][0] and positions[i]<=subWinBounds[subWinIndex][1]):
      subWinIndex+=1
    snpIndicesInSubWins[subWinIndex].append(i)
  return snpIndicesInSubWins

quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

if statDir.lower() in ["none","false","default"]:
  statDir='./'

statFiles=[]

for subWinIndex in range(numSubWins):
  statFileName='/'.join(("%s/%s.subWin%d.stats" %(statDir, trainingDataFileName.split("/")[-1].replace(".msOut.gz",""), subWinIndex)).split('//'))
  statFiles.append(open(statFileName,"w"))
  statFiles[-1].write("\t".join(statNames)+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  fvecFileName="%s.fvec" %trainingDataFileName.split("/")[-1].replace(".msOut.gz","")

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

for instanceIndex in range(len(hapArraysIn)):
  for statName in statNames:
    statVals[statName].append([])
  if maskFileName:
    if drawWithReplacement:
      unmasked=random.choice(maskData)
    else:
      unmasked=maskData[instanceIndex]
    assert len(unmasked)==totalPhysLen
  snpIndicesToKeep=[x for x in range(len(positionArrays[instanceIndex])) if unmasked[positionArrays[instanceIndex][x]-1]]
  if len(snpIndicesToKeep)==0:
    for subWinIndex in range(numSubWins):
      for statName in statNames:
        appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  else:
    haps=allel.HaplotypeArray(hapArraysIn[instanceIndex],dtype='i1').subset(sel0=snpIndicesToKeep)
    if pMisPol>0:
      misPolarizeCorrectionIndex=np.random.binomial(1,pMisPol,len(haps))
      for i in range(len(misPolarizeCorrectionIndex)):
        if misPolarizeCorrectionIndex[i]==1:
          for j in range(len(haps[i])):
            if haps[i][j]==0:
              haps[i][j]=1
            else:
              haps[i][j]=0
    genos=haps.to_genotypes(ploidy=2)
    alleleCounts=genos.count_alleles()
    positions=[positionArrays[instanceIndex][x] for x in snpIndicesToKeep]
    precomputedStats={}
    sampleSizes=[sum(x) for x in alleleCounts]
    assert len(set(sampleSizes))==1
    dafs=alleleCounts[:,1]/float(sampleSizes[0])
    ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
    nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
    nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
    sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    if nonNanCount==0:
      precomputedStats["iHS"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["iHS"].append([])
    else:
      ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
      precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)
    nslVals=allel.stats.selection.nsl(haps,use_threads=False)
    nonNanCount=[x for x in np.isnan(nslVals)].count(False)
    sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    if nonNanCount==0:
      precomputedStats["nSL"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["nSL"].append([])
    else:
      nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
      precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)
    snpIndicesInSubWins=getSnpIndicesInSubWins(subWinBounds,positions)
    for subWinIndex in range(numSubWins):
      subWinStart,subWinEnd = subWinBounds[subWinIndex]
      unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
      assert unmaskedFrac>=unmaskedFracCutoff
      if len(snpIndicesInSubWins[subWinIndex])==0:
        for statName in statNames:
          appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      else:
        hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
        for statName in [x for x in statNames if x[0:3]!="HAF" and x[0:3]!="phi" and x[0:3]!="kap" and x[0:3]!="SFS" and x[0:3]!="SAF"]:
          calcAndAppendStatVal(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, instanceIndex, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        haplotypes={}
        for i in range(len(hapsInSubWin[0])):
          haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
          haplotype="".join(str(x) for x in haplotype)
          if haplotype in haplotypes:
            haplotypes[haplotype].append(i)
          else:
            haplotypes[haplotype]=[i]
        HAF=[]
        HAFunique={}
        for i in haplotypes:
          HAFunique[i]=0
          for j in range(len(hapsInSubWin)):
            if hapsInSubWin[j][haplotypes[i][0]]==1:
              HAFunique[i]+=sum([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j]))])
          for j in range(len(haplotypes[i])):
            HAF.append(HAFunique[i])
        phi=[]
        kappa=[]
        SAFE=[]
        for i in range(len(hapsInSubWin)):
          phi.append(0)
          kappa.append([])
          phiDenom=0
          for j in haplotypes:
            phi[i]+=(int(list(j)[i])*HAFunique[j]*len(haplotypes[j]))
            if int(list(j)[i])==1 and HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
              kappa[i].append(HAFunique[j])
            phiDenom+=(HAFunique[j]*len(haplotypes[j]))
          phi[i]/=float(phiDenom)
          kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
          if dafs[snpIndicesInSubWins[subWinIndex]][i]==0 or dafs[snpIndicesInSubWins[subWinIndex]][i]==1:
           SAFE.append(0.0)
          else:
           SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafs[snpIndicesInSubWins[subWinIndex]][i]*(1-dafs[snpIndicesInSubWins[subWinIndex]][i]))))
        for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
          if i=="SFS":
            windowStats=dafs[snpIndicesInSubWins[subWinIndex]]
          elif i=="HAFunique":
            #windowStats=[eval(i)[x] for x in eval(i)]
            windowStats = list(HAFunique.values()) # SCOTT
          else:
            windowStats=eval(i)
          statVals[i+"-Mean"][instanceIndex].append(np.mean(windowStats))
          statVals[i+"-Median"][instanceIndex].append(np.median(windowStats))
          if(len(np.unique(windowStats,return_counts=True)[1])==1):
            statVals[i+"-Mode"][instanceIndex].append(windowStats[0])
          else:
            if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
              statVals[i+"-Mode"][instanceIndex].append(scipy.stats.mstats.mode(windowStats)[0][0])
            else:
              mode=min(windowStats)
              for j in range(1,51):
                if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
                  mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
              statVals[i+"-Mode"][instanceIndex].append(mode+((max(windowStats)-min(windowStats))/100))
          for j in quantiles:
            statVals[i+"-"+j][instanceIndex].append(np.percentile(windowStats,quantiles[j]))
          statVals[i+"-Max"][instanceIndex].append(max(windowStats))
          statVals[i+"-Var"][instanceIndex].append(np.var(windowStats))
          statVals[i+"-SD"][instanceIndex].append(np.std(windowStats))
          statVals[i+"-Skew"][instanceIndex].append(scipy.stats.skew(windowStats))
          statVals[i+"-Kurt"][instanceIndex].append(scipy.stats.kurtosis(windowStats))
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][instanceIndex])
  fvecFile.write("\t".join([str(x) for x in outVec])+"\n")

for subWinIndex in range(numSubWins):
  statFiles[subWinIndex].close()

fvecFile.close()

#sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, training_convert_to_FVs_Python3_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat/

msOut="[SAMPLE]"

python [PATH_TO]/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spNeut_stats.txt\
  [PATH_TO]/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell, create scripts for sweeps}

cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat/
  
# Hard

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_10.sh

sed -i 's/spNeut_stats\.txt/spHard_0_stats\.txt/' training_convert_to_FVs_Python3_Hard_0.sh
sed -i 's/spNeut_stats\.txt/spHard_1_stats\.txt/' training_convert_to_FVs_Python3_Hard_1.sh
sed -i 's/spNeut_stats\.txt/spHard_2_stats\.txt/' training_convert_to_FVs_Python3_Hard_2.sh
sed -i 's/spNeut_stats\.txt/spHard_3_stats\.txt/' training_convert_to_FVs_Python3_Hard_3.sh
sed -i 's/spNeut_stats\.txt/spHard_4_stats\.txt/' training_convert_to_FVs_Python3_Hard_4.sh
sed -i 's/spNeut_stats\.txt/spHard_5_stats\.txt/' training_convert_to_FVs_Python3_Hard_5.sh
sed -i 's/spNeut_stats\.txt/spHard_6_stats\.txt/' training_convert_to_FVs_Python3_Hard_6.sh
sed -i 's/spNeut_stats\.txt/spHard_7_stats\.txt/' training_convert_to_FVs_Python3_Hard_7.sh
sed -i 's/spNeut_stats\.txt/spHard_8_stats\.txt/' training_convert_to_FVs_Python3_Hard_8.sh
sed -i 's/spNeut_stats\.txt/spHard_9_stats\.txt/' training_convert_to_FVs_Python3_Hard_9.sh
sed -i 's/spNeut_stats\.txt/spHard_10_stats\.txt/' training_convert_to_FVs_Python3_Hard_10.sh

# Soft

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_10.sh

sed -i 's/spNeut_stats\.txt/spSoft_0_stats\.txt/' training_convert_to_FVs_Python3_Soft_0.sh
sed -i 's/spNeut_stats\.txt/spSoft_1_stats\.txt/' training_convert_to_FVs_Python3_Soft_1.sh
sed -i 's/spNeut_stats\.txt/spSoft_2_stats\.txt/' training_convert_to_FVs_Python3_Soft_2.sh
sed -i 's/spNeut_stats\.txt/spSoft_3_stats\.txt/' training_convert_to_FVs_Python3_Soft_3.sh
sed -i 's/spNeut_stats\.txt/spSoft_4_stats\.txt/' training_convert_to_FVs_Python3_Soft_4.sh
sed -i 's/spNeut_stats\.txt/spSoft_5_stats\.txt/' training_convert_to_FVs_Python3_Soft_5.sh
sed -i 's/spNeut_stats\.txt/spSoft_6_stats\.txt/' training_convert_to_FVs_Python3_Soft_6.sh
sed -i 's/spNeut_stats\.txt/spSoft_7_stats\.txt/' training_convert_to_FVs_Python3_Soft_7.sh
sed -i 's/spNeut_stats\.txt/spSoft_8_stats\.txt/' training_convert_to_FVs_Python3_Soft_8.sh
sed -i 's/spNeut_stats\.txt/spSoft_9_stats\.txt/' training_convert_to_FVs_Python3_Soft_9.sh
sed -i 's/spNeut_stats\.txt/spSoft_10_stats\.txt/' training_convert_to_FVs_Python3_Soft_10.sh

# HardPartial

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_10.sh

sed -i 's/spNeut_stats\.txt/spHardPartial_0_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_0.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_1_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_1.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_2_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_2.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_3_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_3.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_4_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_4.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_5_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_5.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_6_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_6.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_7_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_7.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_8_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_8.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_9_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_9.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_10_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_10.sh

# SoftPartial

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_10.sh

sed -i 's/spNeut_stats\.txt/spSoftPartial_0_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_0.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_1_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_1.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_2_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_2.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_3_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_3.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_4_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_4.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_5_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_5.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_6_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_6.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_7_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_7.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_8_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_8.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_9_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_9.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_10_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_10.sh

```

```{shell, submit training_convert_to_FVs_Python3 scripts}

[PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat/

nohup bash training_convert_to_FVs_Python3_Neutral.sh

nohup bash training_convert_to_FVs_Python3_Hard_0.sh
nohup bash training_convert_to_FVs_Python3_Hard_1.sh
nohup bash training_convert_to_FVs_Python3_Hard_2.sh
nohup bash training_convert_to_FVs_Python3_Hard_3.sh
nohup bash training_convert_to_FVs_Python3_Hard_4.sh
nohup bash training_convert_to_FVs_Python3_Hard_5.sh
nohup bash training_convert_to_FVs_Python3_Hard_6.sh
nohup bash training_convert_to_FVs_Python3_Hard_7.sh
nohup bash training_convert_to_FVs_Python3_Hard_8.sh
nohup bash training_convert_to_FVs_Python3_Hard_9.sh
nohup bash training_convert_to_FVs_Python3_Hard_10.sh

nohup bash training_convert_to_FVs_Python3_Soft_0.sh
nohup bash training_convert_to_FVs_Python3_Soft_1.sh
nohup bash training_convert_to_FVs_Python3_Soft_2.sh
nohup bash training_convert_to_FVs_Python3_Soft_3.sh
nohup bash training_convert_to_FVs_Python3_Soft_4.sh
nohup bash training_convert_to_FVs_Python3_Soft_5.sh
nohup bash training_convert_to_FVs_Python3_Soft_6.sh
nohup bash training_convert_to_FVs_Python3_Soft_7.sh
nohup bash training_convert_to_FVs_Python3_Soft_8.sh
nohup bash training_convert_to_FVs_Python3_Soft_9.sh
nohup bash training_convert_to_FVs_Python3_Soft_10.sh

nohup bash training_convert_to_FVs_Python3_HardPartial_0.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_1.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_2.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_3.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_4.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_5.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_6.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_7.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_8.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_9.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_10.sh

nohup bash training_convert_to_FVs_Python3_SoftPartial_0.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_1.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_2.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_3.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_4.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_5.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_6.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_7.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_8.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_9.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_10.sh

```

### Training - training_sample_FVs 

```{python, training_sample_FVs.py}

# import sys,os,random
import sys # SCOTT
import os # SCOTT
import random # SCOTT

'''usage eg:
python2 training_sample_FVs.py spNeut.fvec spHard_ spSoft_ spPartialHard_ spPartialSoft_ 5 0,1,2,3,4,6,7,8,9,10 ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec
'''

if len(sys.argv)!=10:
  sys.exit("usage:\npython2 training_sample_FVs.py neutTrainingFileName hardTrainingFilesPrefix softTrainingFilesPrefix partialHardTrainingFilesPrefix partialSoftTrainingFilesPrefix sweepTrainingWindow linkedTrainingWindows sampledFVsDir sampledFVsFiles\n")
else:
  neutTrainingFileName, hardTrainingFilesPrefix, softTrainingFilesPrefix, partialHardTrainingFilesPrefix, partialSoftTrainingFilesPrefix, sweepTrainingWindow, linkedTrainingWindows, sampledFVsDir, sampledFVsFiles = sys.argv[1:]

neutTrainingFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spNeut.fvec"
hardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_"
softTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spSoft_"
partialHardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialHard_"
partialSoftTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialSoft_"
sweepTrainingWindow=5
linkedTrainingWindows=0,1,2,3,4,6,7,8,9,10
sampledFVsDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe"
sampledFVsFiles="neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec"

sweepFilePaths,linkedFilePaths = {},{}
for trainingFilePrefix in [hardTrainingFilesPrefix,softTrainingFilesPrefix,partialHardTrainingFilesPrefix,partialSoftTrainingFilesPrefix]:
  trainingSetDir="/".join(trainingFilePrefix.split("/")[:-1])
  trainingFilePrefixDirless=trainingFilePrefix.split("/")[-1]
  sweepFilePaths[trainingFilePrefix]=[]
  linkedFilePaths[trainingFilePrefix]=[]
  for fileName in os.listdir(trainingSetDir):
    if fileName.startswith(trainingFilePrefixDirless):
      winNum=int(fileName.split("_")[1].split(".")[0])
      if winNum==int(sweepTrainingWindow):
        sweepFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)
      elif winNum in [int(x) for x in linkedTrainingWindows.split(",")]:
        linkedFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)

def getExamplesFromFVFile(simFileName):
  try:
    simFile=open(simFileName)
    lines=[line.strip() for line in simFile.readlines()]
    header=lines[0]
    examples=lines[1:]
    simFile.close()
    return header,examples
  except Exception:
    return "",[]

def getExamplesFromFVFileLs(simFileLs):
  examples=[]
  keptHeader=""
  for filePath in simFileLs:
    header,currExamples=getExamplesFromFVFile(filePath)
    if header:
      keptHeader=header
    examples+=currExamples
  return keptHeader,examples

header,neutExamples=getExamplesFromFVFile(neutTrainingFileName)
hardHeader,hardExamples=getExamplesFromFVFileLs(sweepFilePaths[hardTrainingFilesPrefix])
linkedHardHeader,linkedHardExamples=getExamplesFromFVFileLs(linkedFilePaths[hardTrainingFilesPrefix])
softHeader,softExamples=getExamplesFromFVFileLs(sweepFilePaths[softTrainingFilesPrefix])
linkedSoftHeader,linkedSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[softTrainingFilesPrefix])
partialHardHeader,partialHardExamples=getExamplesFromFVFileLs(sweepFilePaths[partialHardTrainingFilesPrefix])
linkedPartialHardHeader,linkedPartialHardExamples=getExamplesFromFVFileLs(linkedFilePaths[partialHardTrainingFilesPrefix])
partialSoftHeader,partialSoftExamples=getExamplesFromFVFileLs(sweepFilePaths[partialSoftTrainingFilesPrefix])
linkedPartialSoftHeader,linkedPartialSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[partialSoftTrainingFilesPrefix])

def getMinButNonZeroExamples(lsLs):
  counts=[]
  for ls in lsLs:
    if len(ls)>0:
      counts.append(len(ls))
  if not counts:
    raise Exception
  return min(counts)

trainingSetLs=[hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
numExamplesToKeep=getMinButNonZeroExamples(trainingSetLs)
for i in range(len(trainingSetLs)):
  random.shuffle(trainingSetLs[i])
  trainingSetLs[i]=trainingSetLs[i][:numExamplesToKeep]
hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples=trainingSetLs

if sampledFVsDir.lower() in ["none","false","default"]:
  sampledFVsDir='./'
if sampledFVsFiles.lower() in ["none","false","default"]:
  sampledFVsFiles=["neut.fvec","hard.fvec","linkedHard.fvec","soft.fvec","linkedSoft.fvec","partialHard.fvec","linkedPartialHard.fvec","partialSoft.fvec","linkedPartialSoft.fvec"]
else:
  sampledFVsFiles=sampledFVsFiles.split(",")
  assert len(sampledFVsFiles)==9
outExamples=[neutExamples,hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
for i in range(len(sampledFVsFiles)):
  if outExamples[i]:
    outFile=open('/'.join((sampledFVsDir+'/'+sampledFVsFiles[i]).split('//')),"w")
    outFile.write("classLabel\t%s\n" %(hardHeader))
    for example in outExamples[i]:
      outFile.write("%s\t%s\n" %(sampledFVsFiles[i].replace(".fvec",""),example))
    outFile.close()

```

```{shell, run training_sample_FVs.py}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs

for file in *.fvec; do
  mv "$file" "${file/.fvec/.fvec}"
done

for file in spHardPartial_*.fvec; do
  mv "$file" "${file/spHardPartial_/spPartialHard_}"
done

for file in spSoftPartial_*.fvec; do
  mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
done

python [PATH_TO]/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

```

### Training 

#### Training with early stopping and 10-fold CV 

```{python, training_deep_learning_python3_earlyStopping_kFold.py}

import time
#startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import numpy as np
np.random.seed(123)
from sklearn.model_selection import train_test_split,StratifiedKFold # SCOTT
#from keras.utils import np_utils # Depricated
from keras.utils import to_categorical # SCOTT
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
from keras import optimizers
from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard # SCOTT

'''usage eg:
python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
'''

if len(sys.argv)!=9:
  sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
else:
  fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

if fvecFiles.lower() in ["none","false","default"]:
  fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
else:
  fvecFiles=fvecFiles.split(",")
  assert len(fvecFiles)==9

numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

if validationSize.lower() in ["none","false","default"]:
  validationSize=0.1
else:
  validationSize=float(validationSize)

sweeps=[]

for i in range(0,9):
 sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
 if i>0:
  assert len(sweeps[0])==len(sweeps[i])

sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))

sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)

models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))

#sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=1980)

models=to_categorical(models,9)

# Create StratifiedKFold object
n_splits = 10  # You can adjust the number of folds
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Loop over folds
for fold, (train_index, val_index) in enumerate(skf.split(sumstats, models.argmax(axis=1))):

    sumstats_train, sumstats_val = sumstats[train_index], sumstats[val_index]
    models_train, models_val = models[train_index], models[val_index]

    netlayers = Sequential()
    netlayers.add(Conv2D(256, (3, 6), padding='same', input_shape=sumstats.shape[1:]))
    netlayers.add(MaxPooling2D(pool_size=(3, 3), padding='same'))
    netlayers.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
    netlayers.add(MaxPooling2D(pool_size=(3, 3), padding='same'))
    netlayers.add(Dropout(0.25))
    netlayers.add(Flatten())
    netlayers.add(Dense(512, activation='relu'))
    netlayers.add(Dropout(0.5))
    netlayers.add(Dense(128, activation='relu'))
    netlayers.add(Dropout(0.5))
    netlayers.add(Dense(9, activation='softmax'))
    netlayers.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Modify the weights file name for each fold
    checkpoint = ModelCheckpoint(
        f'fold_{fold+1}_{weightsFileName}',
        monitor='val_accuracy',
        verbose=1,
        save_best_only=True,
        save_weights_only=True, mode='auto'
    )

    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, restore_best_weights=True, verbose=1)

    tensorboard_callback = TensorBoard(log_dir=f"./logs/fold_{fold + 1}")

    # Train the model
    netlayers.fit(
        sumstats_train,
        models_train,
        batch_size=128,
        epochs=1000,
        validation_data=(sumstats_val, models_val),
        callbacks=[checkpoint, early_stopping, tensorboard_callback],
        verbose=1
    )

    netlayers_json=netlayers.to_json()

    with open(f'fold_{fold+1}_{jsonFileName}',"w") as json_file:
        json_file.write(netlayers_json)

    netlayers.save(f'fold_{fold+1}_{npyFileName}')

#models=np_utils.to_categorical(models,9) # Depricated
#models=to_categorical(models,9) # SCOTT

#models_val=np_utils.to_categorical(models_val,9) # Depricated
#models_val=to_categorical(models_val,9) # SCOTT

#netlayers=Sequential()
#netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
#netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
#netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
#netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
#netlayers.add(Dropout(0.25))
#netlayers.add(Flatten())
#netlayers.add(Dense(512, activation='relu'))
#netlayers.add(Dropout(0.5))
#netlayers.add(Dense(128, activation='relu'))
#netlayers.add(Dropout(0.5))
#netlayers.add(Dense(9, activation='softmax'))
#netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

#checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')
#checkpoint=ModelCheckpoint(weightsFileName, monitor='val_accuracy', save_best_only=True, save_weights_only=True, mode='auto') # SCOTT

#early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.05, patience=5, restore_best_weights=True, verbose=1) # SCOTT

#tensorboard_callback = TensorBoard(log_dir="./logs") # SCOTT

#netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)
# SCOTT
#netlayers.fit(
#    sumstats, models,
#    batch_size=128, epochs=1000,  # Adjust the number of epochs as needed
#    validation_data=(sumstats_val, models_val),
#    callbacks=[checkpoint, early_stopping, tensorboard_callback],  # Include the EarlyStopping callback
#    verbose=1
#)

#netlayers_json=netlayers.to_json()

#with open(jsonFileName,"w") as json_file:
#  json_file.write(netlayers_json)

#netlayers.save(npyFileName)

#sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, run training_deep_learning_python3_earlyStopping_kFold.py}

module load anaconda3
conda activate partialshic_py3

# constNe

cd [PATH_TO]/partialSHIC_DsGRP/constNe
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
rm ./*
cp ../*.fvec ./

time python3 [PATH_TO]/partialSHIC/training_deep_learning_python3_earlyStopping_kFold.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    constNe_earlyStopping_fold.hdf5\
    constNe_earlyStopping_fold.json\
    constNe_earlyStopping_fold.npy

```

### Run testing 

```{python, testing_deep_learning_classify_python3.py}

import time
# startTime=time.clock()
startTime = time.perf_counter() # Python 3
import sys,os
import keras
import numpy as np
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import pandas as pd

'''usage eg:
python3 testing_deep_learning_classify.py training.npy ./ 11 89 ./ accuracy confusion_matrix.pdf
'''

if len(sys.argv)!=8:
  sys.exit("usage:\npython3 testing_deep_learning_classify.py classifierPickleFileName fvecDir numSubWins numSumStatsPerSubWin resultsDir accuracyFilesPrefix confusionMatrixFigFileName\n")
else:
  classifierPickleFileName, fvecDir, numSubWins, numSumStatsPerSubWin, resultsDir, accuracyFilesPrefix, confusionMatrixFigFileName = sys.argv[1:]

classifierPickleFileName='../FVs/constNe.npy'
fvecDir='./'
numSubWins='11'
numSumStatsPerSubWin='92'
resultsDir='./'
accuracyFilesPrefix='constNe_accuracy'
confusionMatrixFigFileName='constNe_confusion_matrix.pdf'

netlayers=keras.models.load_model(classifierPickleFileName)

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

testX={}

numSubWins=int(numSubWins)

# for testing for loop below
testSetFileName='spHard_0.fvec'
testExample=currTestData[0]
i=0

for testSetFileName in os.listdir(fvecDir):
  # testSetFile=open('/'.join((fvecDir+'/'+testSetFileName).split('//')))
  testSetFile=open(testSetFileName)
  currTestData=testSetFile.readlines()
  testSetFile.close()
  currTestData=currTestData[1:]
  testX[testSetFileName]=[]
  for testExample in currTestData:
    if not "nan" in testExample:
      testData=testExample.strip().split("\t")
      currVector=[]
      for i in range(len(testData)):
        currVector.append(float(testData[i]))
      testX[testSetFileName].append(currVector)
  testX[testSetFileName]=np.reshape(np.array(testX[testSetFileName]),(np.array(testX[testSetFileName]).shape[0],int(numSumStatsPerSubWin),numSubWins,1))

selVals={"Neutral":0,"Hard":1,"Soft":2,"HardPartial":3,"SoftPartial":4}

def getSelType(x):
  if "Neut" in x:
    return "Neutral"
  elif "PartialHard" in x:
    return "HardPartial"
  elif "PartialSoft" in x:
    return "SoftPartial"
  elif "Hard" in x:
    return "Hard"
  elif "Soft" in x:
    return "Soft"
  else:
    raise ValueError

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split()

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

outlinesH={}
accuracyOverall=0
accuracySpecific=0
accuracyBroad=0

# for testing loop below
testSetFileName='spNeut.fvec'
className='Neutral'
testExampleIndex=0

for testSetFileName in sorted(testX, key=lambda x: (selVals[getSelType(x)], int((x.split(".")[0]+"_0").split("_")[1]))):
  predictions=np.argmax(netlayers.predict(testX[testSetFileName]),axis=1)
  pred = netlayers.predict(testX[testSetFileName]) # SCOTT
  prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
  prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT
  prob_df['truth']=testSetFileName # SCOTT
  prob_df.to_csv(testSetFileName+'.prob.csv', index=False) # SCOTT
  currPreds={}
  for className in classOrder:
    currPreds[className]=0
  denom=float(len(testX[testSetFileName]))
  for testExampleIndex in range(len(predictions)):
    predictedClass=labelToClassName[predictions[testExampleIndex]]
    currPreds[predictedClass]+=1/denom
  if not testSetFileName=='spNeut.fvec':
    testSetFilePrefix=testSetFileName.split(".")[0].split("_")
    selType,selWin = getSelType(testSetFilePrefix[0]),testSetFilePrefix[1]
    selWin=int(selWin)
    key=(selType,selWin)
  else:
    key=('Neutral',0)
  outlinesH[key]=(testSetFileName,[currPreds[className] for className in classOrder])
  if key[0]=='Neutral' or key[1]==5:
    accuracyOverall+=currPreds[key[0]]
    accuracySpecific+=currPreds[key[0]]
  else:
    accuracyOverall+=currPreds[key[0]+'-linked']
  accuracyBroad+=currPreds[key[0]]
  if not key[0]=='Neutral':
    accuracyBroad+=currPreds[key[0]+'-linked']

if resultsDir.lower() in ["none","false","default"]:
  resultsDir='./'

if accuracyFilesPrefix.lower() in ["none","false","default"]:
  accuracyFilesPrefix='accuracy'

accuracyOverall=accuracyOverall/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_overall.txt').split('//')),'w')
fileName.write(str(accuracyOverall))
fileName.close()

accuracySpecific=accuracySpecific/5

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_specific.txt').split('//')),'w')
fileName.write(str(accuracySpecific))
fileName.close()

accuracyBroad=accuracyBroad/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_broad.txt').split('//')),'w')
fileName.write(str(accuracyBroad))
fileName.close()


rowLabels,data = [],[]

for selType in sorted(selVals, key=lambda x: selVals[x]):
 for selWin in range(numSubWins):
  if selType!="Neutral" or selWin==0:
    if "Neutral" in selType:
      rowLabels.append("Neutral")
    else:
      if selWin==5:
        rowLabels.append("%s sweep in focal window" %selType)
      else:
        diff=abs(selWin-5)
        if diff==1:
          plural=""
        else:
          plural="s"
        if selWin<5:
          direction="left"
        else:
          direction="right"
        rowLabels.append("%s sweep %s window%s to %s" %(selType, diff, plural, direction))
    vec=outlinesH[(selType,selWin)][1]
    data.append(vec)

data=np.array(data)

ax=plt.subplots()[1]
heatmap=ax.pcolor(data,cmap=plt.cm.Blues,vmin=0.0,vmax=1.0)
cbar=plt.colorbar(heatmap,cmap=plt.cm.Blues)
cbar.set_label('Fraction of simulations assigned to class',rotation=270,labelpad=20)
ax.set_xticks(np.arange(data.shape[1])+0.5,minor=False)
ax.set_yticks(np.arange(data.shape[0])+0.5,minor=False)
ax.invert_yaxis()
ax.xaxis.tick_top()
ax.axis('tight')
plt.tick_params(axis='y',which='both',right='off')
plt.tick_params(axis='x',which='both',direction='out')
ax.set_xticklabels(classOrder,minor=False,fontsize=9,rotation=45,ha="left")
ax.set_yticklabels(rowLabels,minor=False,fontsize=7)

for y in range(data.shape[0]):
  for x in range(data.shape[1]):
    val=data[y,x]
    val*=100
    if val>50:
      c='0.9'
    else:
      c='black'
    ax.text(x+0.5,y +0.5,'%.1f%%' % val,horizontalalignment='center',verticalalignment='center',color=c,fontsize=6)

if confusionMatrixFigFileName.lower() in ["none","false","default"]:
  confusionMatrixFigFileName='confusionmatrix.pdf'

plt.savefig('/'.join((resultsDir+'/'+confusionMatrixFigFileName).split('//')),bbox_inches='tight',dpi=600)

# sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(classifierPickleFileName,fvecDir,(time.clock()-startTime)))
sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### test on self-new data 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat
  mkdir -p FVs_earlyStopping && cd FVs_earlyStopping
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp ../../*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

### Predict on empirical 

```{python, empirical_deep_learning_classify_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '[PATH_TO]/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '[PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '[PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell, run empirical_deep_learning_classify_python3.py}

cd [PATH_TO]/partialSHIC_DsGRP/constNe/
mkdir -p pred_empirical && cd pred_empirical
rsync -ahPv [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/*_110Lines.phased.noPIRs.inbred.fvec ./

for CONTIG in 542 594 628 718 76 785; do
  for i in 1 2 3 4 5 6 7 8 9 10; do
    python3 [PATH_TO]/partialSHIC/empirical_deep_learning_classify_python3.py\
      [PATH_TO]/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
      ./ScA8VGg_${CONTIG}_110Lines.phased.noPIRs.inbred.fvec\
      11\
      92\
      ScA8VGg_${CONTIG}.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_fold_${i}.pred.bed >\
      empirical_deep_learning_classify_ScA8VGg_${CONTIG}_fold_${i}.log 2>&1
  done
done

```

# ######################## Demography (Stairway Plot 2)

## Intergenic region (READY BUT MAKE SHORTNAME REF AVAILABLE)

```{shell, install and run SNPdat to get intergenic}

## Install snpdat

cd [PATH_TO]/partialSHIC_DsGRP

git clone https://github.com/agdoran/snpdat.git

cd snpdat

## Create input file

### A tab-delimited text file with chromosome_id, position, mutation (no header)

cd [PATH_TO]/partialSHIC_DsGRP/shapeit/

vcf-concat\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\ 
 HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\ 
 HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\ 
 HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\ 
 HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf\
 > HaplotypeData_GenotypeGVCFs_ALL_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf

cd [PATH_TO]/partialSHIC_DsGRP/snpdat

cut -f 1,2,5 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

cut -f 1,2,5 [PATH_TO]/partialSHIC_DsGRP/shapeit/HaplotypeData_GenotypeGVCFs_ALL_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf | grep -v '^#' > input_HaplotypeData_GenotypeGVCFs_ALL_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

## Prepare genome fasta

cp [PATH_TO]/drosophila_06Jul2018_A8VGg.fasta ./
sed 's/\;HRSCAF\=.*//g' drosophila_06Jul2018_A8VGg.fasta > drosophila_06Jul2018_A8VGg_shortName.fasta
rm drosophila_06Jul2018_A8VGg.fasta

### Filter for chrom via VIM

## Prepare GTF

cp [PATH_TO]/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gff ./
sed 's/;HRSCAF=[0-9]\{1,\}\t/\t/' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gff > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName.gff

[PATH_TO]/gffread-0.11.8.Linux_x86_64/gffread GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName.gff -T -o temp.gtf

grep 'CDS' temp.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf

grep -P '_542\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_542.gtf

grep -P '_594\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_594.gtf

grep -P '_628\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_628.gtf

grep -P '_718\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_718.gtf

grep -P '_76\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_76.gtf

grep -P '_785\t' GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS.gtf > GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_785.gtf

## Run snpdat

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_542.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_542.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_542.log 2>&1 stderr_542.log &

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_594.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_594.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_594.log 2>&1 stderr_594.log &

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_628.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_628.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_628.log 2>&1 stderr_628.log &

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_718.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_718.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_718.log 2>&1 stderr_718.log &

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_76.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_76.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_76.log 2>&1 stderr_76.log &

nohup perl my_SNPdat_v1.0.5.pl\
 -i input_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt\
 -f drosophila_06Jul2018_A8VGg_shortName_785.fasta\
 -g GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted_shortName_CDS_785.gtf\
 -o SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt >\
 stdout_785.log 2>&1 stderr_785.log &

```

```{r, get genomic regions that are good for prediction (predPeaks) from constNe}

library(data.table)
library(dplyr)
library(stringr)
library(parallel)

setwd("[PATH_TO]/partialSHIC_DsGRP/constNe/pred_empirical")

preFiles <- list.files(pattern = "^ScA8VGg_[0-9]+.*.bed$")

class2peaks <- function(dat){
  r <- rle(dat$State)
  rr <- data.frame("len" = r$lengths, "val" = r$values)
  rr$endWin <- cumsum(rr$len)
  rr$startWin <- lag(rr$endWin,1,0) + 1
  rr$start_pos <- dat[rr$startWin, 2]
  rr$end_pos <- dat[rr$endWin, 3]
  return(rr)
}

x <- preFiles[1]

main <- function(x) {
    aa <- fread(x, header = FALSE, sep = "\t") %>%
        tidyr::extract('V4','State','(.*)_ScA8VGg.*') %>%
        mutate(V1=gsub('chr','', V1))
    aa <- aa[,1:4]
    aa$t1 <- lag(aa$V2,1)
    aa$t2 <- aa$V2 - aa$t1
    # count gaps between wins
    aa$wingap <- aa$t2/5000
    aa$wingap[1] <- 1
    aa$win_seq <- 0
    win_s = 1
    # gap larger than 2*5000kbp were considered separately
    for(i in 1:nrow(aa)){
        if(aa[i,7]<=2){
            aa$win_seq[i] <- win_s
            }
        else{
            win_s <- win_s + 1
            aa$win_seq[i] <- win_s
        }
    }
    # for each segmentation, run class2peaks
    merge_return <- data.frame()
    for(j in unique(aa$win_seq)){
        sub_aa <- aa[aa$win_seq==j,]
        tmp <- class2peaks(sub_aa)
        tmp$scf <- str_split(x,"\\.")[[1]][1]
        tmp <- tmp[,c("scf","len","val","start_pos","end_pos")]
        tmp$win_seq <- j
        merge_return <- rbind(merge_return,tmp)
    }
    fwrite(merge_return, paste0(x,".peak"), col.names = T, row.names = F, sep = "\t", quote = F)
}

mclapply(preFiles, main, mc.cores = 6)


for (scfid in c(542,594,628,718,76,785)) {
  peakFile <- paste0("ScA8VGg_",scfid,".train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed.peak")
  tempData <- fread(peakFile)
  if (scfid == 542) { predPeaks = tempData }
  else { predPeaks = rbind(predPeaks, tempData) }
}

predPeaks

fwrite(predPeaks, "predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv", col.names = T, row.names = F, sep = "\t", quote = F)

```

```{bash, intersect SNPdat with predPeaks to ensure the same SNPs are used}

cd [PATH_TO]/partialSHIC_DsGRP/snpdat

module load bedtools

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.bed

# 542

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_542\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_542.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_542\t' > temp_SNPdat_542.txt

sed -i 's/SCA8VGG_542/ScA8VGg_542/g' temp_SNPdat_542.txt

bedtools intersect\
 -a temp_SNPdat_542.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_542.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 594

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_594\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_594.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_594\t' > temp_SNPdat_594.txt

sed -i 's/SCA8VGG_594/ScA8VGg_594/g' temp_SNPdat_594.txt

bedtools intersect\
 -a temp_SNPdat_594.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_594.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 628

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_628\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_628.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_628\t' > temp_SNPdat_628.txt

sed -i 's/SCA8VGG_628/ScA8VGg_628/g' temp_SNPdat_628.txt

bedtools intersect\
 -a temp_SNPdat_628.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_628.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 718

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_718\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_718.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_718\t' > temp_SNPdat_718.txt

sed -i 's/SCA8VGG_718/ScA8VGg_718/g' temp_SNPdat_718.txt

bedtools intersect\
 -a temp_SNPdat_718.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_718.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 76

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_76\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_76.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_76\t' > temp_SNPdat_76.txt

sed -i 's/SCA8VGG_76/ScA8VGg_76/g' temp_SNPdat_76.txt

bedtools intersect\
 -a temp_SNPdat_76.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_76.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

# 785

awk 'BEGIN {OFS="\t"} {print $1, $4, $5, $3, $2}' [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.tsv | grep -P '_785\t' > predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_785.bed

awk 'BEGIN {OFS="\t"} {print $1, $2, $2+1, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15}' SNPdat_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt | grep -P '_785\t' > temp_SNPdat_785.txt

sed -i 's/SCA8VGG_785/ScA8VGg_785/g' temp_SNPdat_785.txt

bedtools intersect\
 -a temp_SNPdat_785.txt\
 -b predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_785.bed\
 -wa\
 -wb\
 > SNPdat_predPeaks_HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.txt

```

```{bash, install stairway_plot}

cd [PATH_TO]/partialSHIC_DsGRP

git clone https://github.com/xiaoming-liu/stairway-plot-v2.git

cd [PATH_TO]/partialSHIC_DsGRP/stairway-plot-v2

unzip stairway_plot_v2.1.2.zip

```

```{r, bed file for regions}

library(tidyverse)
library(progress)
library(data.table)

setwd("[PATH_TO]/partialSHIC_DsGRP/snpdat")

snpdat_files <- list.files(pattern = "^SNPdat_predPeaks_.*.txt$")

for (i in 1:length(snpdat_files)) {
  cat("Reading ", snpdat_files[i], "\n")
  tempData <- read_tsv(snpdat_files[i], col_names = FALSE)
  if(i==1) { snpdat <- tempData }
  if(i > 1) { snpdat <- rbind(snpdat, tempData) }
}

snpdat <- snpdat %>% mutate(X6 = as.numeric(X6))

# Intergenic 5kb

tempData <- snpdat %>% 
  select(X1, X2, X5, X6, X20) %>% 
  distinct() %>% 
  filter(X5=="Intergenic") %>% 
  filter(X6>=5000)

tempData <- tempData %>% 
  group_by(X1) %>% 
  mutate(lag_snp = X2 - lag(X2)) %>% 
  mutate(lag_snp = ifelse(is.na(lag_snp), 0, lag_snp)) %>% 
  ungroup() %>% 
  mutate(window = 0)

windowData <- rbindlist(lapply(unique(tempData$X1), function(x) {
  temp <- tempData[tempData$X1 == x, ]
  temp$window <- 1 + cumsum(temp$lag_snp != 0 & temp$lag_snp >= 5000)
  temp
}))

bedFile <- windowData %>% 
  group_by(X1, window) %>% 
  summarise(start = min(X2), end = max(X2)) %>% 
  ungroup() %>% 
  select(X1, start, end)

write_tsv(bedFile, 
          file="ScA8VGg_ALL_intergenicSNPs.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed", 
          col_names = FALSE)

```

```{bash, total loci}

cd [PATH_TO]/partialSHIC_DsGRP/snpdat

# 5kb

awk -F'\t' '{print $0, $3 - $2}' ScA8VGg_ALL_intergenicSNPs.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed | awk '{sum += $4} END {print "Sum:", sum}'

# 27,461,057

```

```{bash, filter vcfs}

cd [PATH_TO]/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2

# 5kb

bed_file="[PATH_TO]/partialSHIC_DsGRP/snpdat/ScA8VGg_ALL_intergenicSNPs.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred.pred.bed"

vcf_path="[PATH_TO]/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="[PATH_TO]/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="[PATH_TO]/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="[PATH_TO]/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="[PATH_TO]/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

vcf_path="[PATH_TO]/partialSHIC_DsGRP/shapeit/"
vcf_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName.vcf"
out_file="HaplotypeData_GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs"

vcftools \
--vcf ${vcf_path}${vcf_file} \
--bed ${bed_file} \
--out ${out_file} \
--keep-INFO-all \
--recode

```

### Estimate  SFS 

#### VCF to SFS 

```{r, vcf to sfs}

library(data.table)

setwd("[PATH_TO]/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2")

# 5kb

vcfs <- list.files(pattern = 'HaplotypeData_.*_intergenicSNPs.recode.vcf$')

st_dt <- data.table()

vcf <- vcfs[1]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[2]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[3]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[4]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[5]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

vcf <- vcfs[6]
iscf <- stringr::str_replace_all(vcf,'_HRSCAF_.*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_Dros_syntheticInbred_shortName_intergenicSNPs.recode.vcf','')
iscf <- stringr::str_replace_all(iscf,'HaplotypeData_GenotypeGVCFs_','')
aa <- fread(vcf, skip=6L, header=F)
aa <- aa[,c(1,2,10:119)]
t1 <- apply(aa[,3:112],1, function(x) grepl("1|1",x))
s1 <- apply(t1,2,sum)
t0 <- apply(aa[,3:112],1, function(x) grepl("0|0",x))
s0 <- apply(t0,2,sum)
st <- data.table("ref"=s0, "alt"=s1)
st[,hom:=ref + alt]
st[,alt_adj:=round(alt/hom*110)]
st[,scf:=iscf]

st_dt <- rbind(st_dt, st)

st_dt[alt_adj>55, alt_adj:=110-alt_adj]

library(tidyverse)

sfs <- st_dt %>% 
  filter(scf!="ScA8VGg_549") %>% 
  group_by(alt_adj) %>% 
  tally() %>% 
  filter(alt_adj!=0)

sfs_out <- paste0(as.integer(sfs$n),collapse = " ")
writeLines(sfs_out, "all_autosomal_intergenicSNPs.sfs")

```

### Predict demography 

```{shell, stairway_plot_es Stairbuilder}

# 5kb

cd [PATH_TO]/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2

cp two-epoch_Neutral_constNe.blueprint two-epoch_intergenicSNPs.blueprint

# vim two-epoch_intergenicSNPs.blueprint

# #example blueprint file
# #input setting
# popid: intergenicSNPs # id of the population (no white space)
# nseq: 110 # number of sequences
# L: 61155000 # total number of observed nucleic sites, including polymorphic and monomorphic
# whether_folded: true # whethr the SFS is folded (true or false)
# SFS: 2234234 752294 381413 238466 165399 123033 96943 79224 65694 55754 48340 42535 37939 33923 30683 27993 25838 23996 22218 20654 19495 18417 17131 16818 15685 14981 14318 13784 13377 13021 12472 12093 11749 11387 11003 10746 10637 10587 10297 9854 10004 9777 9467 9431 9467 9314 9103 9085 9265 9005 8952 8782 8759 8725 4482 # snp frequency spectrum: number of singleton, number of doubleton, etc. (separated by white space)
# #smallest_size_of_SFS_bin_used_for_estimation: 1 # default is 1; to ignore singletons, uncomment this line and change this number to 2
# #largest_size_of_SFS_bin_used_for_estimation: 29 # default is nseq/2 for folded SFS
# pct_training: 0.67 # percentage of sites for training
# nrand: 27       54      81      108 # number of random break points for each try (separated by white space)
# project_dir: intergenicSNPs # project directory
# stairway_plot_dir: stairway_plot_es # directory to the stairway plot files
# ninput: 1000 # number of input files to be created for each estimation
# random_seed: 186457
# #output setting
# mu: 2.8e-9 # assumed mutation rate per site per generation
# year_per_generation: 0.067 # assumed generation time (in years)
# #plot setting
# plot_title: intergenicSNPs # title of the plot
# xrange: 0.1,400 # Time (1k year) range; format: xmin,xmax; "0,0" for default
# yrange: 0,0 # Ne (1k individual) range; format: xmin,xmax; "0,0" for default
# xspacing: 2 # X axis spacing
# yspacing: 2 # Y axis spacing
# fontsize: 12 # Font size

java -cp stairway_plot_es Stairbuilder two-epoch_intergenicSNPs.blueprint # do no change file name
# bash two-epoch_intergenicSNPs.blueprint.sh

# Parallel

grep 'java -Xmx1g -cp stairway_plot_es/:stairway_plot_es/swarmops.jar Stairway_fold_training_testing7 intergenicSNPs/input/intergenicSNPs' two-epoch_intergenicSNPs.blueprint.sh > Stairway_fold_training_testing7_intergenicSNPs.list

parallel -j 38 -a Stairway_fold_training_testing7_intergenicSNPs.list

grep 'mv -f' two-epoch_intergenicSNPs.blueprint.sh > Stairway_fold_mv_intergenicSNPs.list

bash Stairway_fold_mv_intergenicSNPs.list

java -Xmx1g -cp stairway_plot_es/ Stairpainter two-epoch_intergenicSNPs.blueprint

bash two-epoch_intergenicSNPs.blueprint.plot.sh

cd [PATH_TO]/partialSHIC_DsGRP/stairway-plot-v2/stairway_plot_v2.1.2/intergenicSNPs

evince intergenicSNPs.final.summary.pdf

```

#### Figure 1 

* Run by Scott

* Yiguan's demography doesn't match Stairway Plot 2.

```{r}

library(tidyverse)

sp2_intergenicSNPs <- read_tsv("[PATH_TO]\\intergenicSNPs.final.summary")
sp2_intergenicSNPs %>% tail(n=20)

```

```{r}

N0 <- 1000000
constNe <- sp2_intergenicSNPs
constNe <- constNe %>% 
  mutate(Ne = N0)

p3 <- sp2_intergenicSNPs %>% 
  ggplot(aes(x=year, y=Ne_median, col="StairwayPlot2 median")) + 
  geom_line() + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_2.5%`, col="StairwayPlot2 lower quantile")) + 
  geom_line(data=sp2_intergenicSNPs, aes(x=year, y=`Ne_97.5%`, col="StairwayPlot2 upper quantile")) + 
  geom_line(data=constNe, aes(x=year, y=`Ne`, col="Constant")) + 
  xlim(c(0, 100000)) + 
  xlab("Years since present") +
  ylab("Effective population size (Ne)") +
  theme_classic()

p3

setwd("[PATH_TO]")
ggsave("Demography_for_paper.png", width=8, height=4)
ggsave("Demography_for_paper.pdf", width=8, height=4)

```

#### Save in -en format for discoal 

```{r, SP2 medain}

options(scipen = 999)

N0 <- 3607692

setwd("[PATH_TO]")

en <- sp2_intergenicSNPs %>%
  select(year, Ne_median) %>%
  mutate(Ne_change10 = Ne_median - lag(Ne_median, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = Ne_median/N0) %>%
  mutate(year_4N0 = (year*gen_year)/(4*N0)) %>%
  filter(abs(Ne_change10)>=10000) %>% 
  mutate(Ne_change1 = Ne_median - lag(Ne_median, 1)) %>%
  filter(abs(Ne_change1) > 0)

file_conn <- file("intergenicSNPs.final.en_for_discoal.txt", "w")

for (i in 1:nrow(en)) {
  cat("-en", en$year_4N0[i], "0", en$Ne_factor[i], "", file = file_conn)
}

close(file_conn)

```

```{r, SP2 2.5th}

options(scipen = 999)

setwd("[PATH_TO]")

intergenicSNPs_frac_lci <- sp2_intergenicSNPs %>%
  select(year, `Ne_2.5%`) %>%
  mutate(Ne_change10 = `Ne_2.5%` - lag(`Ne_2.5%`, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = `Ne_2.5%`/1041174) %>%
  mutate(year_4N0 = (year*gen_year)/(4*1041174)) %>%
  filter(abs(Ne_change10)>=25000) %>% 
  mutate(Ne_change1 = `Ne_2.5%` - lag(`Ne_2.5%`, 1)) %>%
  filter(abs(Ne_change1) > 0) %>% 
  mutate(Ne2 = Ne_factor*1041174) %>%
  mutate(year2 = (year_4N0*4*1041174)/gen_year)

file_conn <- file("intergenicSNPs.final.en_for_discoal_lci.txt", "w")

for (i in 1:nrow(intergenicSNPs_frac_lci)) {
  cat("-en", intergenicSNPs_frac_lci$year_4N0[i], "0", intergenicSNPs_frac_lci$Ne_factor[i], "", file = file_conn)
}

close(file_conn)

getwd()

```

```{r, SP2 97.5th}

options(scipen = 999)

setwd("[PATH_TO]")

intergenicSNPs_frac_uci <- sp2_intergenicSNPs %>%
  select(year, `Ne_97.5%`) %>%
  mutate(Ne_change10 = `Ne_97.5%` - lag(`Ne_97.5%`, 10)) %>%
  filter(Ne_change10!=0) %>%
  mutate(Ne_factor = `Ne_97.5%`/5182173) %>%
  mutate(year_4N0 = (year*gen_year)/(4*5182173)) %>%
  filter(abs(Ne_change10)>=50000) %>% 
  mutate(Ne_change1 = `Ne_97.5%` - lag(`Ne_97.5%`, 1)) %>%
  filter(abs(Ne_change1) > 0) %>% 
  mutate(Ne2 = Ne_factor*5182173) %>%
  mutate(year2 = (year_4N0*4*5182173)/gen_year)

file_conn <- file("intergenicSNPs.final.en_for_discoal_uci.txt", "w")

for (i in 1:nrow(intergenicSNPs_frac_uci)) {
  cat("-en", intergenicSNPs_frac_uci$year_4N0[i], "0", intergenicSNPs_frac_uci$Ne_factor[i], "", file = file_conn)
}

close(file_conn)

getwd()

```

# ######################## partialSHIC - SP2 median

## Generate training data 

```{shell, install discoal}

cd [PATH_TO]/partialSHIC_DsGRP

git clone https://github.com/kr-colab/discoal.git

cd [PATH_TO]/partialSHIC_DsGRP/discoal

make discoal

```

```{shell, mkdir and load R}

mkdir -p [PATH_TO]/partialSHIC_DsGRP/sp2_median

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get simulation parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '[PATH_TO]/discoal/discoal' 
TIMES = 2000

Ne = 3607692
len = 55000

# recombination rate (matches manuscript)
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3

# mutation rate mu: (matches manuscript)
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len

# selection coefficient s: 0.0001-0.1 (matches manuscript)
# strength of selection: alpha = 2*Ne*s
s_low = 0.000002771855
s_high = 0.002771855
Pa_low <- 2*Ne*s_low # 20 2*3607692*0.000002771855
Pa_high <- 2*Ne*s_high # 20000 2*3607692*0.002771855

# tau: time of fixation looking backward in time - units: 4N (matches manuscript)
Pu_low <- 0.0
Pu_high <- 0.002771855 # 0.002771855*4*3607692 ~= 40,000 generations

# for soft seletive sweep (matches manuscript)
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2

# partial sweep (matches manuscript)
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# sweep position
cat("sweep_pos: ", sweep_pos, "\n")

```

```{shell, discoal_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell, discoal_Hard_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_${TIME}.msOut.gz
done

```

```{shell, discoal_Soft_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_${TIME}.msOut.gz
done

```

```{shell, discoal_HardPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_${TIME}.msOut.gz
done

```

```{shell, discoal_SoftPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_${TIME}.msOut.gz
done

```

```{shell, create discoal scripts for linked}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/

# Hard-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_10.sh

# Soft-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_10.sh

# HardPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_10.sh

# SoftPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_10.sh

```

```{shell, run discoal scripts}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median

find ./ -maxdepth 1 -type f -name 'discoal_*.sh' | sed 's/\.\///' | sort -V > list_discoal_files.txt

while read p; do
  echo "${p} submitted"
  nohup bash ${p}
done < list_discoal_files.txt

```

## Generate test data 

```{shell, mkdir and load R}

mkdir -p [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat

module load anaconda3
conda activate r_env_4.3.1

```

```{r, get simulation parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '[PATH_TO]/discoal/discoal' 
TIMES = 2000

Ne = 3607692
len = 55000

# recombination rate (matches manuscript)
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3

# mutation rate mu: (matches manuscript)
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len

# selection coefficient s: 0.0001-0.1 (matches manuscript)
# strength of selection: alpha = 2*Ne*s
s_low = 0.000002771855
s_high = 0.002771855
Pa_low <- 2*Ne*s_low # 20 2*3607692*0.000002771855
Pa_high <- 2*Ne*s_high # 20000 2*3607692*0.002771855

# tau: time of fixation looking backward in time - units: 4N (matches manuscript)
Pu_low <- 0.0
Pu_high <- 0.002771855 # 0.002771855*4*3607692 ~= 40,000 generations

# for soft seletive sweep (matches manuscript)
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2

# partial sweep (matches manuscript)
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# sweep position
cat("sweep_pos: ", sweep_pos, "\n")

```

```{shell, discoal_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell, discoal_Hard_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_${TIME}.msOut.gz
done

```

```{shell, discoal_Soft_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_${TIME}.msOut.gz
done

```

```{shell, discoal_HardPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_${TIME}.msOut.gz
done

```

```{shell, discoal_SoftPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_${TIME}.msOut.gz
done

```

```{shell, create discoal scripts for linked}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat

# Hard-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_10.sh

# Soft-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_10.sh

# HardPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_10.sh

# SoftPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_10.sh

```

```{shell, run discoal scripts}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_mediantest_self_newDat

find ./ -maxdepth 1 -type f -name 'discoal_*.sh' | sed 's/\.\///' | sort -V > list_discoal_files.txt

while read p; do
  echo "${p} submitted"
  nohup bash ${p}
done < list_discoal_files.txt

```

## Run training

### Training - calcStatsAndDafForEachSnp 

```{python, calcStatsAndDafForEachSnpSingleMsFile_Python3}

import sys
import allel
import random
import numpy as np
from msTools import *
from fvTools import *
import time

'''usage example
python calcStatsAndDafForEachSnpSingleMsFile.py /san/data/dan/simulations/discoal_multipopStuff/spatialSVMSims/trainingSets/equilibNeut.msout.gz 110000 iHS nSL
'''

trainingDataFileName, totalPhysLen = sys.argv[1:3]
totalPhysLen = int(totalPhysLen)

hapArraysIn, positionArrays = msOutToHaplotypeArrayIn(trainingDataFileName, totalPhysLen)
numInstances = len(hapArraysIn)

header = "daf\tiHS\tnSL"
#print header # Python 2
print(header) # Python 3

dafs = []
#start = time.clock() # Python 2
start = time.perf_counter() # Python 3
numInstancesDone = 0

numSnpsDone = 0
for instanceIndex in range(numInstances):
    if instanceIndex % 10 == 0:
        sys.stderr.write("done wtih %d of %d instances (%d SNPs)\n" %(instanceIndex, numInstances, numSnpsDone))
    haps = allel.HaplotypeArray(hapArraysIn[instanceIndex], dtype='i1')
    genos = haps.to_genotypes(ploidy=2)
    ac = genos.count_alleles()
    sampleSizes = [sum(x) for x in ac]
    assert len(set(sampleSizes)) == 1
    dafs = ac[:,1]/float(sampleSizes[0])
    ihsVals = allel.stats.selection.ihs(haps, positionArrays[instanceIndex], use_threads=True, include_edges=False)
    nslVals = allel.stats.selection.nsl(haps, use_threads=True)
    assert len(dafs) == len(ihsVals) == len(nslVals)
    numSnpsDone += len(dafs)
    for i in range(len(dafs)):
        #print "%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i]) # Python 2
        print ("%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i])) #Python 3
    if numSnpsDone > 5000000:
        break

```

```{shell, template script for calcStatsAndDafForEachSnpSingleMsFile_Python3.py}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/

msOut="[SAMPLE]"

python [PATH_TO]/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, create scripts for calcStatsAndDafForEachSnpSingleMsFile_Python3.py}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/

find ./ -maxdepth 1 -type f -name '*.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_calcStatsAndDafForEachSnpSingleMsFile_Python3.txt calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
done < msOut_files.txt

find ./ -maxdepth 1 -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*.sh' -print0 | xargs -0 chmod 755

ls ./calcStatsAndDafForEachSnpSingleMsFile_Python3_*.sh

```

```{shell, run calcStatsAndDafForEachSnpSingleMsFile_Python3 scripts}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/

while read p; do
  echo "calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh submitted"
  nohup bash calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
  sleep 0.25
done < msOut_files.txt

```

### Training - training_convert_to_FVs 

```{python, training_convert_to_FVs_python3.py}

import time
#startTime=time.clock() # Python 2
start = time.perf_counter() # Python 3
import sys
from msTools import *
from fvTools import *
import random
import allel
import numpy as np
import math

'''usage eg:
pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
python2 training_convert_to_FVs.py spNeut.msOut.gz 2L,2R,3L,3R 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.fa ./ spNeut.msOut.fvec
'''

if len(sys.argv)!=12:
  sys.exit("usage:\npython2 training_convert_to_FVs.py trainingDataFileName chrArmsForMasking subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName statDir fvecFileName\n")
else:
  trainingDataFileName, chrArmsForMasking, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, statDir, fvecFileName = sys.argv[1:]

subWinSize,numSubWins = int(subWinSize),int(numSubWins)
assert subWinSize>0 and numSubWins>1

totalPhysLen=(subWinSize*numSubWins)

hapArraysIn,positionArrays=msOutToHaplotypeArrayIn(trainingDataFileName,totalPhysLen)

chrArmsForMasking=chrArmsForMasking.split(",")

unmaskedFracCutoff,pMisPol=float(unmaskedFracCutoff),float(pMisPol)

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")

if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")

standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none", "false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*totalPhysLen
  sys.stderr.write("Warning: not doing any masking! (i.e. mask.fa file for the chr arm with all masked sites N'ed out, or at least the reference with Ns, is not provided)\n")
  maskFileName=False
else:
  if ancestralArmFaFileName.lower() in ["none", "false"]:
    maskData=readMaskDataForTraining(maskFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  else:
    maskData=readMaskAndAncDataForTraining(maskFileName,ancestralArmFaFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  if len(maskData)<len(hapArraysIn):
    sys.stderr.write("Warning: not enough windows from masked data (needed %d; got %d); will draw with replacement!!\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=True
  else:
    sys.stderr.write("enough windows from masked data (needed %d; got %d); will draw without replacement.\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=False

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

header=[]

for statName in statNames:
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName, i))

header="\t".join(header)

statVals={}

for statName in statNames:
  statVals[statName]=[]

def getSubWinBounds(subWinSize,totalPhysLen):
  subWinStart=1-subWinSize
  subWinEnd=0
  subWinBounds=[]
  for i in range(0,(numSubWins-1)):
    subWinStart+=subWinSize
    subWinEnd+=subWinSize
    subWinBounds.append((subWinStart,subWinEnd))
  subWinStart+=subWinSize
  subWinEnd=totalPhysLen
  subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,totalPhysLen)

def getSnpIndicesInSubWins(subWinBounds,positions):
  snpIndicesInSubWins=[]
  for subWinIndex in range(len(subWinBounds)):
    snpIndicesInSubWins.append([])
  subWinIndex=0
  for i in range(len(positions)):
    while not (positions[i]>=subWinBounds[subWinIndex][0] and positions[i]<=subWinBounds[subWinIndex][1]):
      subWinIndex+=1
    snpIndicesInSubWins[subWinIndex].append(i)
  return snpIndicesInSubWins

quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

if statDir.lower() in ["none","false","default"]:
  statDir='./'

statFiles=[]

for subWinIndex in range(numSubWins):
  statFileName='/'.join(("%s/%s.subWin%d.stats" %(statDir, trainingDataFileName.split("/")[-1].replace(".msOut.gz",""), subWinIndex)).split('//'))
  statFiles.append(open(statFileName,"w"))
  statFiles[-1].write("\t".join(statNames)+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  fvecFileName="%s.fvec" %trainingDataFileName.split("/")[-1].replace(".msOut.gz","")

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

for instanceIndex in range(len(hapArraysIn)):
  for statName in statNames:
    statVals[statName].append([])
  if maskFileName:
    if drawWithReplacement:
      unmasked=random.choice(maskData)
    else:
      unmasked=maskData[instanceIndex]
    assert len(unmasked)==totalPhysLen
  snpIndicesToKeep=[x for x in range(len(positionArrays[instanceIndex])) if unmasked[positionArrays[instanceIndex][x]-1]]
  if len(snpIndicesToKeep)==0:
    for subWinIndex in range(numSubWins):
      for statName in statNames:
        appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  else:
    haps=allel.HaplotypeArray(hapArraysIn[instanceIndex],dtype='i1').subset(sel0=snpIndicesToKeep)
    if pMisPol>0:
      misPolarizeCorrectionIndex=np.random.binomial(1,pMisPol,len(haps))
      for i in range(len(misPolarizeCorrectionIndex)):
        if misPolarizeCorrectionIndex[i]==1:
          for j in range(len(haps[i])):
            if haps[i][j]==0:
              haps[i][j]=1
            else:
              haps[i][j]=0
    genos=haps.to_genotypes(ploidy=2)
    alleleCounts=genos.count_alleles()
    positions=[positionArrays[instanceIndex][x] for x in snpIndicesToKeep]
    precomputedStats={}
    sampleSizes=[sum(x) for x in alleleCounts]
    assert len(set(sampleSizes))==1
    dafs=alleleCounts[:,1]/float(sampleSizes[0])
    ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
    nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
    nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
    sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    if nonNanCount==0:
      precomputedStats["iHS"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["iHS"].append([])
    else:
      ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
      precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)
    nslVals=allel.stats.selection.nsl(haps,use_threads=False)
    nonNanCount=[x for x in np.isnan(nslVals)].count(False)
    sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    if nonNanCount==0:
      precomputedStats["nSL"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["nSL"].append([])
    else:
      nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
      precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)
    snpIndicesInSubWins=getSnpIndicesInSubWins(subWinBounds,positions)
    for subWinIndex in range(numSubWins):
      subWinStart,subWinEnd = subWinBounds[subWinIndex]
      unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
      assert unmaskedFrac>=unmaskedFracCutoff
      if len(snpIndicesInSubWins[subWinIndex])==0:
        for statName in statNames:
          appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      else:
        hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
        for statName in [x for x in statNames if x[0:3]!="HAF" and x[0:3]!="phi" and x[0:3]!="kap" and x[0:3]!="SFS" and x[0:3]!="SAF"]:
          calcAndAppendStatVal(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, instanceIndex, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        haplotypes={}
        for i in range(len(hapsInSubWin[0])):
          haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
          haplotype="".join(str(x) for x in haplotype)
          if haplotype in haplotypes:
            haplotypes[haplotype].append(i)
          else:
            haplotypes[haplotype]=[i]
        HAF=[]
        HAFunique={}
        for i in haplotypes:
          HAFunique[i]=0
          for j in range(len(hapsInSubWin)):
            if hapsInSubWin[j][haplotypes[i][0]]==1:
              HAFunique[i]+=sum([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j]))])
          for j in range(len(haplotypes[i])):
            HAF.append(HAFunique[i])
        phi=[]
        kappa=[]
        SAFE=[]
        for i in range(len(hapsInSubWin)):
          phi.append(0)
          kappa.append([])
          phiDenom=0
          for j in haplotypes:
            phi[i]+=(int(list(j)[i])*HAFunique[j]*len(haplotypes[j]))
            if int(list(j)[i])==1 and HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
              kappa[i].append(HAFunique[j])
            phiDenom+=(HAFunique[j]*len(haplotypes[j]))
          phi[i]/=float(phiDenom)
          kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
          if dafs[snpIndicesInSubWins[subWinIndex]][i]==0 or dafs[snpIndicesInSubWins[subWinIndex]][i]==1:
           SAFE.append(0.0)
          else:
           SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafs[snpIndicesInSubWins[subWinIndex]][i]*(1-dafs[snpIndicesInSubWins[subWinIndex]][i]))))
        for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
          if i=="SFS":
            windowStats=dafs[snpIndicesInSubWins[subWinIndex]]
          elif i=="HAFunique":
            #windowStats=[eval(i)[x] for x in eval(i)]
            windowStats = list(HAFunique.values()) # SCOTT
          else:
            windowStats=eval(i)
          statVals[i+"-Mean"][instanceIndex].append(np.mean(windowStats))
          statVals[i+"-Median"][instanceIndex].append(np.median(windowStats))
          if(len(np.unique(windowStats,return_counts=True)[1])==1):
            statVals[i+"-Mode"][instanceIndex].append(windowStats[0])
          else:
            if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
              statVals[i+"-Mode"][instanceIndex].append(scipy.stats.mstats.mode(windowStats)[0][0])
            else:
              mode=min(windowStats)
              for j in range(1,51):
                if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
                  mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
              statVals[i+"-Mode"][instanceIndex].append(mode+((max(windowStats)-min(windowStats))/100))
          for j in quantiles:
            statVals[i+"-"+j][instanceIndex].append(np.percentile(windowStats,quantiles[j]))
          statVals[i+"-Max"][instanceIndex].append(max(windowStats))
          statVals[i+"-Var"][instanceIndex].append(np.var(windowStats))
          statVals[i+"-SD"][instanceIndex].append(np.std(windowStats))
          statVals[i+"-Skew"][instanceIndex].append(scipy.stats.skew(windowStats))
          statVals[i+"-Kurt"][instanceIndex].append(scipy.stats.kurtosis(windowStats))
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][instanceIndex])
  fvecFile.write("\t".join([str(x) for x in outVec])+"\n")

for subWinIndex in range(numSubWins):
  statFiles[subWinIndex].close()

fvecFile.close()

#sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, training_convert_to_FVs_Python3_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat/

msOut="[SAMPLE]"

python [PATH_TO]/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spNeut_stats.txt\
  [PATH_TO]/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell, create scripts for sweeps}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat/
  
# Hard

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_10.sh

sed -i 's/spNeut_stats\.txt/spHard_0_stats\.txt/' training_convert_to_FVs_Python3_Hard_0.sh
sed -i 's/spNeut_stats\.txt/spHard_1_stats\.txt/' training_convert_to_FVs_Python3_Hard_1.sh
sed -i 's/spNeut_stats\.txt/spHard_2_stats\.txt/' training_convert_to_FVs_Python3_Hard_2.sh
sed -i 's/spNeut_stats\.txt/spHard_3_stats\.txt/' training_convert_to_FVs_Python3_Hard_3.sh
sed -i 's/spNeut_stats\.txt/spHard_4_stats\.txt/' training_convert_to_FVs_Python3_Hard_4.sh
sed -i 's/spNeut_stats\.txt/spHard_5_stats\.txt/' training_convert_to_FVs_Python3_Hard_5.sh
sed -i 's/spNeut_stats\.txt/spHard_6_stats\.txt/' training_convert_to_FVs_Python3_Hard_6.sh
sed -i 's/spNeut_stats\.txt/spHard_7_stats\.txt/' training_convert_to_FVs_Python3_Hard_7.sh
sed -i 's/spNeut_stats\.txt/spHard_8_stats\.txt/' training_convert_to_FVs_Python3_Hard_8.sh
sed -i 's/spNeut_stats\.txt/spHard_9_stats\.txt/' training_convert_to_FVs_Python3_Hard_9.sh
sed -i 's/spNeut_stats\.txt/spHard_10_stats\.txt/' training_convert_to_FVs_Python3_Hard_10.sh

# Soft

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_10.sh

sed -i 's/spNeut_stats\.txt/spSoft_0_stats\.txt/' training_convert_to_FVs_Python3_Soft_0.sh
sed -i 's/spNeut_stats\.txt/spSoft_1_stats\.txt/' training_convert_to_FVs_Python3_Soft_1.sh
sed -i 's/spNeut_stats\.txt/spSoft_2_stats\.txt/' training_convert_to_FVs_Python3_Soft_2.sh
sed -i 's/spNeut_stats\.txt/spSoft_3_stats\.txt/' training_convert_to_FVs_Python3_Soft_3.sh
sed -i 's/spNeut_stats\.txt/spSoft_4_stats\.txt/' training_convert_to_FVs_Python3_Soft_4.sh
sed -i 's/spNeut_stats\.txt/spSoft_5_stats\.txt/' training_convert_to_FVs_Python3_Soft_5.sh
sed -i 's/spNeut_stats\.txt/spSoft_6_stats\.txt/' training_convert_to_FVs_Python3_Soft_6.sh
sed -i 's/spNeut_stats\.txt/spSoft_7_stats\.txt/' training_convert_to_FVs_Python3_Soft_7.sh
sed -i 's/spNeut_stats\.txt/spSoft_8_stats\.txt/' training_convert_to_FVs_Python3_Soft_8.sh
sed -i 's/spNeut_stats\.txt/spSoft_9_stats\.txt/' training_convert_to_FVs_Python3_Soft_9.sh
sed -i 's/spNeut_stats\.txt/spSoft_10_stats\.txt/' training_convert_to_FVs_Python3_Soft_10.sh

# HardPartial

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_10.sh

sed -i 's/spNeut_stats\.txt/spHardPartial_0_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_0.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_1_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_1.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_2_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_2.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_3_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_3.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_4_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_4.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_5_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_5.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_6_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_6.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_7_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_7.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_8_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_8.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_9_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_9.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_10_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_10.sh

# SoftPartial

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_10.sh

sed -i 's/spNeut_stats\.txt/spSoftPartial_0_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_0.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_1_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_1.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_2_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_2.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_3_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_3.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_4_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_4.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_5_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_5.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_6_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_6.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_7_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_7.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_8_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_8.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_9_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_9.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_10_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_10.sh

```

```{shell, submit training_convert_to_FVs_Python3 scripts}

[PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat/

nohup bash training_convert_to_FVs_Python3_Neutral.sh

nohup bash training_convert_to_FVs_Python3_Hard_0.sh
nohup bash training_convert_to_FVs_Python3_Hard_1.sh
nohup bash training_convert_to_FVs_Python3_Hard_2.sh
nohup bash training_convert_to_FVs_Python3_Hard_3.sh
nohup bash training_convert_to_FVs_Python3_Hard_4.sh
nohup bash training_convert_to_FVs_Python3_Hard_5.sh
nohup bash training_convert_to_FVs_Python3_Hard_6.sh
nohup bash training_convert_to_FVs_Python3_Hard_7.sh
nohup bash training_convert_to_FVs_Python3_Hard_8.sh
nohup bash training_convert_to_FVs_Python3_Hard_9.sh
nohup bash training_convert_to_FVs_Python3_Hard_10.sh

nohup bash training_convert_to_FVs_Python3_Soft_0.sh
nohup bash training_convert_to_FVs_Python3_Soft_1.sh
nohup bash training_convert_to_FVs_Python3_Soft_2.sh
nohup bash training_convert_to_FVs_Python3_Soft_3.sh
nohup bash training_convert_to_FVs_Python3_Soft_4.sh
nohup bash training_convert_to_FVs_Python3_Soft_5.sh
nohup bash training_convert_to_FVs_Python3_Soft_6.sh
nohup bash training_convert_to_FVs_Python3_Soft_7.sh
nohup bash training_convert_to_FVs_Python3_Soft_8.sh
nohup bash training_convert_to_FVs_Python3_Soft_9.sh
nohup bash training_convert_to_FVs_Python3_Soft_10.sh

nohup bash training_convert_to_FVs_Python3_HardPartial_0.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_1.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_2.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_3.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_4.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_5.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_6.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_7.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_8.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_9.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_10.sh

nohup bash training_convert_to_FVs_Python3_SoftPartial_0.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_1.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_2.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_3.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_4.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_5.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_6.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_7.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_8.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_9.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_10.sh

```

### Training - training_sample_FVs 

```{python, training_sample_FVs.py}

# import sys,os,random
import sys # SCOTT
import os # SCOTT
import random # SCOTT

'''usage eg:
python2 training_sample_FVs.py spNeut.fvec spHard_ spSoft_ spPartialHard_ spPartialSoft_ 5 0,1,2,3,4,6,7,8,9,10 ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec
'''

if len(sys.argv)!=10:
  sys.exit("usage:\npython2 training_sample_FVs.py neutTrainingFileName hardTrainingFilesPrefix softTrainingFilesPrefix partialHardTrainingFilesPrefix partialSoftTrainingFilesPrefix sweepTrainingWindow linkedTrainingWindows sampledFVsDir sampledFVsFiles\n")
else:
  neutTrainingFileName, hardTrainingFilesPrefix, softTrainingFilesPrefix, partialHardTrainingFilesPrefix, partialSoftTrainingFilesPrefix, sweepTrainingWindow, linkedTrainingWindows, sampledFVsDir, sampledFVsFiles = sys.argv[1:]

neutTrainingFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spNeut.fvec"
hardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_"
softTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spSoft_"
partialHardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialHard_"
partialSoftTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialSoft_"
sweepTrainingWindow=5
linkedTrainingWindows=0,1,2,3,4,6,7,8,9,10
sampledFVsDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe"
sampledFVsFiles="neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec"

sweepFilePaths,linkedFilePaths = {},{}
for trainingFilePrefix in [hardTrainingFilesPrefix,softTrainingFilesPrefix,partialHardTrainingFilesPrefix,partialSoftTrainingFilesPrefix]:
  trainingSetDir="/".join(trainingFilePrefix.split("/")[:-1])
  trainingFilePrefixDirless=trainingFilePrefix.split("/")[-1]
  sweepFilePaths[trainingFilePrefix]=[]
  linkedFilePaths[trainingFilePrefix]=[]
  for fileName in os.listdir(trainingSetDir):
    if fileName.startswith(trainingFilePrefixDirless):
      winNum=int(fileName.split("_")[1].split(".")[0])
      if winNum==int(sweepTrainingWindow):
        sweepFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)
      elif winNum in [int(x) for x in linkedTrainingWindows.split(",")]:
        linkedFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)

def getExamplesFromFVFile(simFileName):
  try:
    simFile=open(simFileName)
    lines=[line.strip() for line in simFile.readlines()]
    header=lines[0]
    examples=lines[1:]
    simFile.close()
    return header,examples
  except Exception:
    return "",[]

def getExamplesFromFVFileLs(simFileLs):
  examples=[]
  keptHeader=""
  for filePath in simFileLs:
    header,currExamples=getExamplesFromFVFile(filePath)
    if header:
      keptHeader=header
    examples+=currExamples
  return keptHeader,examples

header,neutExamples=getExamplesFromFVFile(neutTrainingFileName)
hardHeader,hardExamples=getExamplesFromFVFileLs(sweepFilePaths[hardTrainingFilesPrefix])
linkedHardHeader,linkedHardExamples=getExamplesFromFVFileLs(linkedFilePaths[hardTrainingFilesPrefix])
softHeader,softExamples=getExamplesFromFVFileLs(sweepFilePaths[softTrainingFilesPrefix])
linkedSoftHeader,linkedSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[softTrainingFilesPrefix])
partialHardHeader,partialHardExamples=getExamplesFromFVFileLs(sweepFilePaths[partialHardTrainingFilesPrefix])
linkedPartialHardHeader,linkedPartialHardExamples=getExamplesFromFVFileLs(linkedFilePaths[partialHardTrainingFilesPrefix])
partialSoftHeader,partialSoftExamples=getExamplesFromFVFileLs(sweepFilePaths[partialSoftTrainingFilesPrefix])
linkedPartialSoftHeader,linkedPartialSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[partialSoftTrainingFilesPrefix])

def getMinButNonZeroExamples(lsLs):
  counts=[]
  for ls in lsLs:
    if len(ls)>0:
      counts.append(len(ls))
  if not counts:
    raise Exception
  return min(counts)

trainingSetLs=[hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
numExamplesToKeep=getMinButNonZeroExamples(trainingSetLs)
for i in range(len(trainingSetLs)):
  random.shuffle(trainingSetLs[i])
  trainingSetLs[i]=trainingSetLs[i][:numExamplesToKeep]
hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples=trainingSetLs

if sampledFVsDir.lower() in ["none","false","default"]:
  sampledFVsDir='./'
if sampledFVsFiles.lower() in ["none","false","default"]:
  sampledFVsFiles=["neut.fvec","hard.fvec","linkedHard.fvec","soft.fvec","linkedSoft.fvec","partialHard.fvec","linkedPartialHard.fvec","partialSoft.fvec","linkedPartialSoft.fvec"]
else:
  sampledFVsFiles=sampledFVsFiles.split(",")
  assert len(sampledFVsFiles)==9
outExamples=[neutExamples,hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
for i in range(len(sampledFVsFiles)):
  if outExamples[i]:
    outFile=open('/'.join((sampledFVsDir+'/'+sampledFVsFiles[i]).split('//')),"w")
    outFile.write("classLabel\t%s\n" %(hardHeader))
    for example in outExamples[i]:
      outFile.write("%s\t%s\n" %(sampledFVsFiles[i].replace(".fvec",""),example))
    outFile.close()

```

```{shell, run training_sample_FVs.py}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_median

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_median/FVs

for file in *.fvec; do
  mv "$file" "${file/.fvec/.fvec}"
done

for file in spHardPartial_*.fvec; do
  mv "$file" "${file/spHardPartial_/spPartialHard_}"
done

for file in spSoftPartial_*.fvec; do
  mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
done

python [PATH_TO]/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_median/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_median/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_median/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_median/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_median/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

```

### Training 

#### Training with early stopping and 10-fold CV 

```{python, training_deep_learning_python3_earlyStopping_kFold.py}

import time
#startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import numpy as np
np.random.seed(123)
from sklearn.model_selection import train_test_split,StratifiedKFold # SCOTT
#from keras.utils import np_utils # Depricated
from keras.utils import to_categorical # SCOTT
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
from keras import optimizers
from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard # SCOTT

'''usage eg:
python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
'''

if len(sys.argv)!=9:
  sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
else:
  fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

if fvecFiles.lower() in ["none","false","default"]:
  fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
else:
  fvecFiles=fvecFiles.split(",")
  assert len(fvecFiles)==9

numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

if validationSize.lower() in ["none","false","default"]:
  validationSize=0.1
else:
  validationSize=float(validationSize)

sweeps=[]

for i in range(0,9):
 sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
 if i>0:
  assert len(sweeps[0])==len(sweeps[i])

sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))

sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)

models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))

#sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=1980)

models=to_categorical(models,9)

# Create StratifiedKFold object
n_splits = 10  # You can adjust the number of folds
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Loop over folds
for fold, (train_index, val_index) in enumerate(skf.split(sumstats, models.argmax(axis=1))):

    sumstats_train, sumstats_val = sumstats[train_index], sumstats[val_index]
    models_train, models_val = models[train_index], models[val_index]

    netlayers = Sequential()
    netlayers.add(Conv2D(256, (3, 6), padding='same', input_shape=sumstats.shape[1:]))
    netlayers.add(MaxPooling2D(pool_size=(3, 3), padding='same'))
    netlayers.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
    netlayers.add(MaxPooling2D(pool_size=(3, 3), padding='same'))
    netlayers.add(Dropout(0.25))
    netlayers.add(Flatten())
    netlayers.add(Dense(512, activation='relu'))
    netlayers.add(Dropout(0.5))
    netlayers.add(Dense(128, activation='relu'))
    netlayers.add(Dropout(0.5))
    netlayers.add(Dense(9, activation='softmax'))
    netlayers.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Modify the weights file name for each fold
    checkpoint = ModelCheckpoint(
        f'fold_{fold+1}_{weightsFileName}',
        monitor='val_accuracy',
        verbose=1,
        save_best_only=True,
        save_weights_only=True, mode='auto'
    )

    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, restore_best_weights=True, verbose=1)

    tensorboard_callback = TensorBoard(log_dir=f"./logs/fold_{fold + 1}")

    # Train the model
    netlayers.fit(
        sumstats_train,
        models_train,
        batch_size=128,
        epochs=1000,
        validation_data=(sumstats_val, models_val),
        callbacks=[checkpoint, early_stopping, tensorboard_callback],
        verbose=1
    )

    netlayers_json=netlayers.to_json()

    with open(f'fold_{fold+1}_{jsonFileName}',"w") as json_file:
        json_file.write(netlayers_json)

    netlayers.save(f'fold_{fold+1}_{npyFileName}')

#models=np_utils.to_categorical(models,9) # Depricated
#models=to_categorical(models,9) # SCOTT

#models_val=np_utils.to_categorical(models_val,9) # Depricated
#models_val=to_categorical(models_val,9) # SCOTT

#netlayers=Sequential()
#netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
#netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
#netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
#netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
#netlayers.add(Dropout(0.25))
#netlayers.add(Flatten())
#netlayers.add(Dense(512, activation='relu'))
#netlayers.add(Dropout(0.5))
#netlayers.add(Dense(128, activation='relu'))
#netlayers.add(Dropout(0.5))
#netlayers.add(Dense(9, activation='softmax'))
#netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

#checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')
#checkpoint=ModelCheckpoint(weightsFileName, monitor='val_accuracy', save_best_only=True, save_weights_only=True, mode='auto') # SCOTT

#early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.05, patience=5, restore_best_weights=True, verbose=1) # SCOTT

#tensorboard_callback = TensorBoard(log_dir="./logs") # SCOTT

#netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)
# SCOTT
#netlayers.fit(
#    sumstats, models,
#    batch_size=128, epochs=1000,  # Adjust the number of epochs as needed
#    validation_data=(sumstats_val, models_val),
#    callbacks=[checkpoint, early_stopping, tensorboard_callback],  # Include the EarlyStopping callback
#    verbose=1
#)

#netlayers_json=netlayers.to_json()

#with open(jsonFileName,"w") as json_file:
#  json_file.write(netlayers_json)

#netlayers.save(npyFileName)

#sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, run training_deep_learning_python3_earlyStopping_kFold.py}

module load anaconda3
conda activate partialshic_py3

# sp2_median

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
rm ./*
cp ../*.fvec ./

time python3 [PATH_TO]/partialSHIC/training_deep_learning_python3_earlyStopping_kFold.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    constNe_earlyStopping_fold.hdf5\
    constNe_earlyStopping_fold.json\
    constNe_earlyStopping_fold.npy

```

### Run testing 

```{python, testing_deep_learning_classify_python3.py}

import time
# startTime=time.clock()
startTime = time.perf_counter() # Python 3
import sys,os
import keras
import numpy as np
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import pandas as pd

'''usage eg:
python3 testing_deep_learning_classify.py training.npy ./ 11 89 ./ accuracy confusion_matrix.pdf
'''

if len(sys.argv)!=8:
  sys.exit("usage:\npython3 testing_deep_learning_classify.py classifierPickleFileName fvecDir numSubWins numSumStatsPerSubWin resultsDir accuracyFilesPrefix confusionMatrixFigFileName\n")
else:
  classifierPickleFileName, fvecDir, numSubWins, numSumStatsPerSubWin, resultsDir, accuracyFilesPrefix, confusionMatrixFigFileName = sys.argv[1:]

classifierPickleFileName='../FVs/constNe.npy'
fvecDir='./'
numSubWins='11'
numSumStatsPerSubWin='92'
resultsDir='./'
accuracyFilesPrefix='constNe_accuracy'
confusionMatrixFigFileName='constNe_confusion_matrix.pdf'

netlayers=keras.models.load_model(classifierPickleFileName)

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

testX={}

numSubWins=int(numSubWins)

# for testing for loop below
testSetFileName='spHard_0.fvec'
testExample=currTestData[0]
i=0

for testSetFileName in os.listdir(fvecDir):
  # testSetFile=open('/'.join((fvecDir+'/'+testSetFileName).split('//')))
  testSetFile=open(testSetFileName)
  currTestData=testSetFile.readlines()
  testSetFile.close()
  currTestData=currTestData[1:]
  testX[testSetFileName]=[]
  for testExample in currTestData:
    if not "nan" in testExample:
      testData=testExample.strip().split("\t")
      currVector=[]
      for i in range(len(testData)):
        currVector.append(float(testData[i]))
      testX[testSetFileName].append(currVector)
  testX[testSetFileName]=np.reshape(np.array(testX[testSetFileName]),(np.array(testX[testSetFileName]).shape[0],int(numSumStatsPerSubWin),numSubWins,1))

selVals={"Neutral":0,"Hard":1,"Soft":2,"HardPartial":3,"SoftPartial":4}

def getSelType(x):
  if "Neut" in x:
    return "Neutral"
  elif "PartialHard" in x:
    return "HardPartial"
  elif "PartialSoft" in x:
    return "SoftPartial"
  elif "Hard" in x:
    return "Hard"
  elif "Soft" in x:
    return "Soft"
  else:
    raise ValueError

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split()

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

outlinesH={}
accuracyOverall=0
accuracySpecific=0
accuracyBroad=0

# for testing loop below
testSetFileName='spNeut.fvec'
className='Neutral'
testExampleIndex=0

for testSetFileName in sorted(testX, key=lambda x: (selVals[getSelType(x)], int((x.split(".")[0]+"_0").split("_")[1]))):
  predictions=np.argmax(netlayers.predict(testX[testSetFileName]),axis=1)
  pred = netlayers.predict(testX[testSetFileName]) # SCOTT
  prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
  prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT
  prob_df['truth']=testSetFileName # SCOTT
  prob_df.to_csv(testSetFileName+'.prob.csv', index=False) # SCOTT
  currPreds={}
  for className in classOrder:
    currPreds[className]=0
  denom=float(len(testX[testSetFileName]))
  for testExampleIndex in range(len(predictions)):
    predictedClass=labelToClassName[predictions[testExampleIndex]]
    currPreds[predictedClass]+=1/denom
  if not testSetFileName=='spNeut.fvec':
    testSetFilePrefix=testSetFileName.split(".")[0].split("_")
    selType,selWin = getSelType(testSetFilePrefix[0]),testSetFilePrefix[1]
    selWin=int(selWin)
    key=(selType,selWin)
  else:
    key=('Neutral',0)
  outlinesH[key]=(testSetFileName,[currPreds[className] for className in classOrder])
  if key[0]=='Neutral' or key[1]==5:
    accuracyOverall+=currPreds[key[0]]
    accuracySpecific+=currPreds[key[0]]
  else:
    accuracyOverall+=currPreds[key[0]+'-linked']
  accuracyBroad+=currPreds[key[0]]
  if not key[0]=='Neutral':
    accuracyBroad+=currPreds[key[0]+'-linked']

if resultsDir.lower() in ["none","false","default"]:
  resultsDir='./'

if accuracyFilesPrefix.lower() in ["none","false","default"]:
  accuracyFilesPrefix='accuracy'

accuracyOverall=accuracyOverall/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_overall.txt').split('//')),'w')
fileName.write(str(accuracyOverall))
fileName.close()

accuracySpecific=accuracySpecific/5

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_specific.txt').split('//')),'w')
fileName.write(str(accuracySpecific))
fileName.close()

accuracyBroad=accuracyBroad/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_broad.txt').split('//')),'w')
fileName.write(str(accuracyBroad))
fileName.close()


rowLabels,data = [],[]

for selType in sorted(selVals, key=lambda x: selVals[x]):
 for selWin in range(numSubWins):
  if selType!="Neutral" or selWin==0:
    if "Neutral" in selType:
      rowLabels.append("Neutral")
    else:
      if selWin==5:
        rowLabels.append("%s sweep in focal window" %selType)
      else:
        diff=abs(selWin-5)
        if diff==1:
          plural=""
        else:
          plural="s"
        if selWin<5:
          direction="left"
        else:
          direction="right"
        rowLabels.append("%s sweep %s window%s to %s" %(selType, diff, plural, direction))
    vec=outlinesH[(selType,selWin)][1]
    data.append(vec)

data=np.array(data)

ax=plt.subplots()[1]
heatmap=ax.pcolor(data,cmap=plt.cm.Blues,vmin=0.0,vmax=1.0)
cbar=plt.colorbar(heatmap,cmap=plt.cm.Blues)
cbar.set_label('Fraction of simulations assigned to class',rotation=270,labelpad=20)
ax.set_xticks(np.arange(data.shape[1])+0.5,minor=False)
ax.set_yticks(np.arange(data.shape[0])+0.5,minor=False)
ax.invert_yaxis()
ax.xaxis.tick_top()
ax.axis('tight')
plt.tick_params(axis='y',which='both',right='off')
plt.tick_params(axis='x',which='both',direction='out')
ax.set_xticklabels(classOrder,minor=False,fontsize=9,rotation=45,ha="left")
ax.set_yticklabels(rowLabels,minor=False,fontsize=7)

for y in range(data.shape[0]):
  for x in range(data.shape[1]):
    val=data[y,x]
    val*=100
    if val>50:
      c='0.9'
    else:
      c='black'
    ax.text(x+0.5,y +0.5,'%.1f%%' % val,horizontalalignment='center',verticalalignment='center',color=c,fontsize=6)

if confusionMatrixFigFileName.lower() in ["none","false","default"]:
  confusionMatrixFigFileName='confusionmatrix.pdf'

plt.savefig('/'.join((resultsDir+'/'+confusionMatrixFigFileName).split('//')),bbox_inches='tight',dpi=600)

# sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(classifierPickleFileName,fvecDir,(time.clock()-startTime)))
sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### test on self-new data 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/test_self_newDat
  mkdir -p FVs_earlyStopping && cd FVs_earlyStopping
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp ../../*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_median/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

### Predict on empirical 

```{python, empirical_deep_learning_classify_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '[PATH_TO]/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '[PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '[PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell, run empirical_deep_learning_classify_python3.py}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/
mkdir -p pred_empirical && cd pred_empirical
rsync -ahPv [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/*_110Lines.phased.noPIRs.inbred.fvec ./

for CONTIG in 542 594 628 718 76 785; do
  for i in 1 2 3 4 5 6 7 8 9 10; do
    python3 [PATH_TO]/partialSHIC/empirical_deep_learning_classify_python3.py\
      [PATH_TO]/partialSHIC_DsGRP/sp2_median/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
      ./ScA8VGg_${CONTIG}_110Lines.phased.noPIRs.inbred.fvec\
      11\
      92\
      ScA8VGg_${CONTIG}.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_fold_${i}.pred.bed >\
      empirical_deep_learning_classify_ScA8VGg_${CONTIG}_fold_${i}.log 2>&1
  done
done

```

# ######################## partialSHIC - SP2 2.5th (lci)

## Generate training data 

```{shell, install discoal}

cd [PATH_TO]/partialSHIC_DsGRP

git clone https://github.com/kr-colab/discoal.git

cd [PATH_TO]/partialSHIC_DsGRP/discoal

make discoal

```

```{shell, mkdir and load R}

mkdir -p [PATH_TO]/partialSHIC_DsGRP/sp2_lci

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get simulation parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '[PATH_TO]/discoal/discoal' 
TIMES = 2000

Ne = 1041174
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000009604543
# s_high = 0.01
s_high = 0.009604543
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.009604543 # 0.009604543*4*1041174 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2

# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# sweep position
cat("sweep_pos: ", sweep_pos, "\n")

```

```{shell, discoal_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.000009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_lci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell, discoal_Hard_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.000009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_lci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_${TIME}.msOut.gz
done

```

```{shell, discoal_Soft_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.000009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_lci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_${TIME}.msOut.gz
done

```

```{shell, discoal_HardPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.000009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_lci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_${TIME}.msOut.gz
done

```

```{shell, discoal_SoftPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=1041174
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=1145
Pre_upper=3436

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=229
Pt_high=3665

# Selection coefficient s
s_low=0.000009604543
s_high=0.000009604543
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.009604543

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_lci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_${TIME}.msOut.gz
done

```

```{shell, create discoal scripts for linked}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/

# Hard-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_10.sh

# Soft-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_10.sh

# HardPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_10.sh

# SoftPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_10.sh

```

```{shell, run discoal scripts}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci

find ./ -maxdepth 1 -type f -name 'discoal_*.sh' | sed 's/\.\///' | sort -V > list_discoal_files.txt

while read p; do
  echo "${p} submitted"
  nohup bash ${p}
done < list_discoal_files.txt

```

## Generate test data 

```{shell, mkdir and load R}

mkdir -p [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat

module load anaconda3
conda activate r_env_4.3.1

```

```{r, get simulation parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '[PATH_TO]/discoal/discoal' 
TIMES = 2000

Ne = 1041174
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low = 0.000009604543
# s_high = 0.01
s_high = 0.009604543
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.009604543 # 0.009604543*4*1041174 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2


# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# sweep position
cat("sweep_pos: ", sweep_pos, "\n")

```

```{shell, discoal_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell, discoal_Hard_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_${TIME}.msOut.gz
done

```

```{shell, discoal_Soft_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_${TIME}.msOut.gz
done

```

```{shell, discoal_HardPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_${TIME}.msOut.gz
done

```

```{shell, discoal_SoftPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=3607692
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=3968
Pre_upper=11905

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=794
Pt_high=12699

# Selection coefficient s
s_low=0.000002771855
s_high=0.002771855
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.002771855

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_${TIME}.msOut.gz
done

```

```{shell, create discoal scripts for linked}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat

# Hard-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_10.sh

# Soft-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_10.sh

# HardPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_10.sh

# SoftPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_10.sh

```

```{shell, run discoal scripts}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat

find ./ -maxdepth 1 -type f -name 'discoal_*.sh' | sed 's/\.\///' | sort -V > list_discoal_files.txt

while read p; do
  echo "${p} submitted"
  nohup bash ${p}
done < list_discoal_files.txt

```

## Run training

### Training - calcStatsAndDafForEachSnp 

```{python, calcStatsAndDafForEachSnpSingleMsFile_Python3}

import sys
import allel
import random
import numpy as np
from msTools import *
from fvTools import *
import time

'''usage example
python calcStatsAndDafForEachSnpSingleMsFile.py /san/data/dan/simulations/discoal_multipopStuff/spatialSVMSims/trainingSets/equilibNeut.msout.gz 110000 iHS nSL
'''

trainingDataFileName, totalPhysLen = sys.argv[1:3]
totalPhysLen = int(totalPhysLen)

hapArraysIn, positionArrays = msOutToHaplotypeArrayIn(trainingDataFileName, totalPhysLen)
numInstances = len(hapArraysIn)

header = "daf\tiHS\tnSL"
#print header # Python 2
print(header) # Python 3

dafs = []
#start = time.clock() # Python 2
start = time.perf_counter() # Python 3
numInstancesDone = 0

numSnpsDone = 0
for instanceIndex in range(numInstances):
    if instanceIndex % 10 == 0:
        sys.stderr.write("done wtih %d of %d instances (%d SNPs)\n" %(instanceIndex, numInstances, numSnpsDone))
    haps = allel.HaplotypeArray(hapArraysIn[instanceIndex], dtype='i1')
    genos = haps.to_genotypes(ploidy=2)
    ac = genos.count_alleles()
    sampleSizes = [sum(x) for x in ac]
    assert len(set(sampleSizes)) == 1
    dafs = ac[:,1]/float(sampleSizes[0])
    ihsVals = allel.stats.selection.ihs(haps, positionArrays[instanceIndex], use_threads=True, include_edges=False)
    nslVals = allel.stats.selection.nsl(haps, use_threads=True)
    assert len(dafs) == len(ihsVals) == len(nslVals)
    numSnpsDone += len(dafs)
    for i in range(len(dafs)):
        #print "%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i]) # Python 2
        print ("%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i])) #Python 3
    if numSnpsDone > 5000000:
        break

```

```{shell, template script for calcStatsAndDafForEachSnpSingleMsFile_Python3.py}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/

msOut="[SAMPLE]"

python [PATH_TO]/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, create scripts for calcStatsAndDafForEachSnpSingleMsFile_Python3.py}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/

find ./ -maxdepth 1 -type f -name '*.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_calcStatsAndDafForEachSnpSingleMsFile_Python3.txt calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
done < msOut_files.txt

find ./ -maxdepth 1 -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*.sh' -print0 | xargs -0 chmod 755

ls ./calcStatsAndDafForEachSnpSingleMsFile_Python3_*.sh

```

```{shell, run calcStatsAndDafForEachSnpSingleMsFile_Python3 scripts}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/

while read p; do
  echo "calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh submitted"
  nohup bash calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
  sleep 0.25
done < msOut_files.txt

```

### Training - training_convert_to_FVs 

```{python, training_convert_to_FVs_python3.py}

import time
#startTime=time.clock() # Python 2
start = time.perf_counter() # Python 3
import sys
from msTools import *
from fvTools import *
import random
import allel
import numpy as np
import math

'''usage eg:
pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
python2 training_convert_to_FVs.py spNeut.msOut.gz 2L,2R,3L,3R 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.fa ./ spNeut.msOut.fvec
'''

if len(sys.argv)!=12:
  sys.exit("usage:\npython2 training_convert_to_FVs.py trainingDataFileName chrArmsForMasking subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName statDir fvecFileName\n")
else:
  trainingDataFileName, chrArmsForMasking, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, statDir, fvecFileName = sys.argv[1:]

subWinSize,numSubWins = int(subWinSize),int(numSubWins)
assert subWinSize>0 and numSubWins>1

totalPhysLen=(subWinSize*numSubWins)

hapArraysIn,positionArrays=msOutToHaplotypeArrayIn(trainingDataFileName,totalPhysLen)

chrArmsForMasking=chrArmsForMasking.split(",")

unmaskedFracCutoff,pMisPol=float(unmaskedFracCutoff),float(pMisPol)

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")

if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")

standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none", "false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*totalPhysLen
  sys.stderr.write("Warning: not doing any masking! (i.e. mask.fa file for the chr arm with all masked sites N'ed out, or at least the reference with Ns, is not provided)\n")
  maskFileName=False
else:
  if ancestralArmFaFileName.lower() in ["none", "false"]:
    maskData=readMaskDataForTraining(maskFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  else:
    maskData=readMaskAndAncDataForTraining(maskFileName,ancestralArmFaFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  if len(maskData)<len(hapArraysIn):
    sys.stderr.write("Warning: not enough windows from masked data (needed %d; got %d); will draw with replacement!!\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=True
  else:
    sys.stderr.write("enough windows from masked data (needed %d; got %d); will draw without replacement.\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=False

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

header=[]

for statName in statNames:
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName, i))

header="\t".join(header)

statVals={}

for statName in statNames:
  statVals[statName]=[]

def getSubWinBounds(subWinSize,totalPhysLen):
  subWinStart=1-subWinSize
  subWinEnd=0
  subWinBounds=[]
  for i in range(0,(numSubWins-1)):
    subWinStart+=subWinSize
    subWinEnd+=subWinSize
    subWinBounds.append((subWinStart,subWinEnd))
  subWinStart+=subWinSize
  subWinEnd=totalPhysLen
  subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,totalPhysLen)

def getSnpIndicesInSubWins(subWinBounds,positions):
  snpIndicesInSubWins=[]
  for subWinIndex in range(len(subWinBounds)):
    snpIndicesInSubWins.append([])
  subWinIndex=0
  for i in range(len(positions)):
    while not (positions[i]>=subWinBounds[subWinIndex][0] and positions[i]<=subWinBounds[subWinIndex][1]):
      subWinIndex+=1
    snpIndicesInSubWins[subWinIndex].append(i)
  return snpIndicesInSubWins

quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

if statDir.lower() in ["none","false","default"]:
  statDir='./'

statFiles=[]

for subWinIndex in range(numSubWins):
  statFileName='/'.join(("%s/%s.subWin%d.stats" %(statDir, trainingDataFileName.split("/")[-1].replace(".msOut.gz",""), subWinIndex)).split('//'))
  statFiles.append(open(statFileName,"w"))
  statFiles[-1].write("\t".join(statNames)+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  fvecFileName="%s.fvec" %trainingDataFileName.split("/")[-1].replace(".msOut.gz","")

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

for instanceIndex in range(len(hapArraysIn)):
  for statName in statNames:
    statVals[statName].append([])
  if maskFileName:
    if drawWithReplacement:
      unmasked=random.choice(maskData)
    else:
      unmasked=maskData[instanceIndex]
    assert len(unmasked)==totalPhysLen
  snpIndicesToKeep=[x for x in range(len(positionArrays[instanceIndex])) if unmasked[positionArrays[instanceIndex][x]-1]]
  if len(snpIndicesToKeep)==0:
    for subWinIndex in range(numSubWins):
      for statName in statNames:
        appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  else:
    haps=allel.HaplotypeArray(hapArraysIn[instanceIndex],dtype='i1').subset(sel0=snpIndicesToKeep)
    if pMisPol>0:
      misPolarizeCorrectionIndex=np.random.binomial(1,pMisPol,len(haps))
      for i in range(len(misPolarizeCorrectionIndex)):
        if misPolarizeCorrectionIndex[i]==1:
          for j in range(len(haps[i])):
            if haps[i][j]==0:
              haps[i][j]=1
            else:
              haps[i][j]=0
    genos=haps.to_genotypes(ploidy=2)
    alleleCounts=genos.count_alleles()
    positions=[positionArrays[instanceIndex][x] for x in snpIndicesToKeep]
    precomputedStats={}
    sampleSizes=[sum(x) for x in alleleCounts]
    assert len(set(sampleSizes))==1
    dafs=alleleCounts[:,1]/float(sampleSizes[0])
    ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
    nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
    nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
    sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    if nonNanCount==0:
      precomputedStats["iHS"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["iHS"].append([])
    else:
      ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
      precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)
    nslVals=allel.stats.selection.nsl(haps,use_threads=False)
    nonNanCount=[x for x in np.isnan(nslVals)].count(False)
    sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    if nonNanCount==0:
      precomputedStats["nSL"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["nSL"].append([])
    else:
      nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
      precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)
    snpIndicesInSubWins=getSnpIndicesInSubWins(subWinBounds,positions)
    for subWinIndex in range(numSubWins):
      subWinStart,subWinEnd = subWinBounds[subWinIndex]
      unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
      assert unmaskedFrac>=unmaskedFracCutoff
      if len(snpIndicesInSubWins[subWinIndex])==0:
        for statName in statNames:
          appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      else:
        hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
        for statName in [x for x in statNames if x[0:3]!="HAF" and x[0:3]!="phi" and x[0:3]!="kap" and x[0:3]!="SFS" and x[0:3]!="SAF"]:
          calcAndAppendStatVal(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, instanceIndex, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        haplotypes={}
        for i in range(len(hapsInSubWin[0])):
          haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
          haplotype="".join(str(x) for x in haplotype)
          if haplotype in haplotypes:
            haplotypes[haplotype].append(i)
          else:
            haplotypes[haplotype]=[i]
        HAF=[]
        HAFunique={}
        for i in haplotypes:
          HAFunique[i]=0
          for j in range(len(hapsInSubWin)):
            if hapsInSubWin[j][haplotypes[i][0]]==1:
              HAFunique[i]+=sum([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j]))])
          for j in range(len(haplotypes[i])):
            HAF.append(HAFunique[i])
        phi=[]
        kappa=[]
        SAFE=[]
        for i in range(len(hapsInSubWin)):
          phi.append(0)
          kappa.append([])
          phiDenom=0
          for j in haplotypes:
            phi[i]+=(int(list(j)[i])*HAFunique[j]*len(haplotypes[j]))
            if int(list(j)[i])==1 and HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
              kappa[i].append(HAFunique[j])
            phiDenom+=(HAFunique[j]*len(haplotypes[j]))
          phi[i]/=float(phiDenom)
          kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
          if dafs[snpIndicesInSubWins[subWinIndex]][i]==0 or dafs[snpIndicesInSubWins[subWinIndex]][i]==1:
           SAFE.append(0.0)
          else:
           SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafs[snpIndicesInSubWins[subWinIndex]][i]*(1-dafs[snpIndicesInSubWins[subWinIndex]][i]))))
        for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
          if i=="SFS":
            windowStats=dafs[snpIndicesInSubWins[subWinIndex]]
          elif i=="HAFunique":
            #windowStats=[eval(i)[x] for x in eval(i)]
            windowStats = list(HAFunique.values()) # SCOTT
          else:
            windowStats=eval(i)
          statVals[i+"-Mean"][instanceIndex].append(np.mean(windowStats))
          statVals[i+"-Median"][instanceIndex].append(np.median(windowStats))
          if(len(np.unique(windowStats,return_counts=True)[1])==1):
            statVals[i+"-Mode"][instanceIndex].append(windowStats[0])
          else:
            if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
              statVals[i+"-Mode"][instanceIndex].append(scipy.stats.mstats.mode(windowStats)[0][0])
            else:
              mode=min(windowStats)
              for j in range(1,51):
                if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
                  mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
              statVals[i+"-Mode"][instanceIndex].append(mode+((max(windowStats)-min(windowStats))/100))
          for j in quantiles:
            statVals[i+"-"+j][instanceIndex].append(np.percentile(windowStats,quantiles[j]))
          statVals[i+"-Max"][instanceIndex].append(max(windowStats))
          statVals[i+"-Var"][instanceIndex].append(np.var(windowStats))
          statVals[i+"-SD"][instanceIndex].append(np.std(windowStats))
          statVals[i+"-Skew"][instanceIndex].append(scipy.stats.skew(windowStats))
          statVals[i+"-Kurt"][instanceIndex].append(scipy.stats.kurtosis(windowStats))
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][instanceIndex])
  fvecFile.write("\t".join([str(x) for x in outVec])+"\n")

for subWinIndex in range(numSubWins):
  statFiles[subWinIndex].close()

fvecFile.close()

#sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, training_convert_to_FVs_Python3_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat/

msOut="[SAMPLE]"

python [PATH_TO]/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spNeut_stats.txt\
  [PATH_TO]/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell, create scripts for sweeps}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat/
  
# Hard

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_10.sh

sed -i 's/spNeut_stats\.txt/spHard_0_stats\.txt/' training_convert_to_FVs_Python3_Hard_0.sh
sed -i 's/spNeut_stats\.txt/spHard_1_stats\.txt/' training_convert_to_FVs_Python3_Hard_1.sh
sed -i 's/spNeut_stats\.txt/spHard_2_stats\.txt/' training_convert_to_FVs_Python3_Hard_2.sh
sed -i 's/spNeut_stats\.txt/spHard_3_stats\.txt/' training_convert_to_FVs_Python3_Hard_3.sh
sed -i 's/spNeut_stats\.txt/spHard_4_stats\.txt/' training_convert_to_FVs_Python3_Hard_4.sh
sed -i 's/spNeut_stats\.txt/spHard_5_stats\.txt/' training_convert_to_FVs_Python3_Hard_5.sh
sed -i 's/spNeut_stats\.txt/spHard_6_stats\.txt/' training_convert_to_FVs_Python3_Hard_6.sh
sed -i 's/spNeut_stats\.txt/spHard_7_stats\.txt/' training_convert_to_FVs_Python3_Hard_7.sh
sed -i 's/spNeut_stats\.txt/spHard_8_stats\.txt/' training_convert_to_FVs_Python3_Hard_8.sh
sed -i 's/spNeut_stats\.txt/spHard_9_stats\.txt/' training_convert_to_FVs_Python3_Hard_9.sh
sed -i 's/spNeut_stats\.txt/spHard_10_stats\.txt/' training_convert_to_FVs_Python3_Hard_10.sh

# Soft

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_10.sh

sed -i 's/spNeut_stats\.txt/spSoft_0_stats\.txt/' training_convert_to_FVs_Python3_Soft_0.sh
sed -i 's/spNeut_stats\.txt/spSoft_1_stats\.txt/' training_convert_to_FVs_Python3_Soft_1.sh
sed -i 's/spNeut_stats\.txt/spSoft_2_stats\.txt/' training_convert_to_FVs_Python3_Soft_2.sh
sed -i 's/spNeut_stats\.txt/spSoft_3_stats\.txt/' training_convert_to_FVs_Python3_Soft_3.sh
sed -i 's/spNeut_stats\.txt/spSoft_4_stats\.txt/' training_convert_to_FVs_Python3_Soft_4.sh
sed -i 's/spNeut_stats\.txt/spSoft_5_stats\.txt/' training_convert_to_FVs_Python3_Soft_5.sh
sed -i 's/spNeut_stats\.txt/spSoft_6_stats\.txt/' training_convert_to_FVs_Python3_Soft_6.sh
sed -i 's/spNeut_stats\.txt/spSoft_7_stats\.txt/' training_convert_to_FVs_Python3_Soft_7.sh
sed -i 's/spNeut_stats\.txt/spSoft_8_stats\.txt/' training_convert_to_FVs_Python3_Soft_8.sh
sed -i 's/spNeut_stats\.txt/spSoft_9_stats\.txt/' training_convert_to_FVs_Python3_Soft_9.sh
sed -i 's/spNeut_stats\.txt/spSoft_10_stats\.txt/' training_convert_to_FVs_Python3_Soft_10.sh

# HardPartial

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_10.sh

sed -i 's/spNeut_stats\.txt/spHardPartial_0_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_0.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_1_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_1.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_2_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_2.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_3_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_3.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_4_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_4.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_5_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_5.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_6_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_6.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_7_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_7.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_8_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_8.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_9_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_9.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_10_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_10.sh

# SoftPartial

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_10.sh

sed -i 's/spNeut_stats\.txt/spSoftPartial_0_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_0.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_1_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_1.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_2_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_2.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_3_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_3.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_4_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_4.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_5_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_5.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_6_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_6.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_7_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_7.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_8_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_8.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_9_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_9.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_10_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_10.sh

```

```{shell, submit training_convert_to_FVs_Python3 scripts}

[PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat/

nohup bash training_convert_to_FVs_Python3_Neutral.sh

nohup bash training_convert_to_FVs_Python3_Hard_0.sh
nohup bash training_convert_to_FVs_Python3_Hard_1.sh
nohup bash training_convert_to_FVs_Python3_Hard_2.sh
nohup bash training_convert_to_FVs_Python3_Hard_3.sh
nohup bash training_convert_to_FVs_Python3_Hard_4.sh
nohup bash training_convert_to_FVs_Python3_Hard_5.sh
nohup bash training_convert_to_FVs_Python3_Hard_6.sh
nohup bash training_convert_to_FVs_Python3_Hard_7.sh
nohup bash training_convert_to_FVs_Python3_Hard_8.sh
nohup bash training_convert_to_FVs_Python3_Hard_9.sh
nohup bash training_convert_to_FVs_Python3_Hard_10.sh

nohup bash training_convert_to_FVs_Python3_Soft_0.sh
nohup bash training_convert_to_FVs_Python3_Soft_1.sh
nohup bash training_convert_to_FVs_Python3_Soft_2.sh
nohup bash training_convert_to_FVs_Python3_Soft_3.sh
nohup bash training_convert_to_FVs_Python3_Soft_4.sh
nohup bash training_convert_to_FVs_Python3_Soft_5.sh
nohup bash training_convert_to_FVs_Python3_Soft_6.sh
nohup bash training_convert_to_FVs_Python3_Soft_7.sh
nohup bash training_convert_to_FVs_Python3_Soft_8.sh
nohup bash training_convert_to_FVs_Python3_Soft_9.sh
nohup bash training_convert_to_FVs_Python3_Soft_10.sh

nohup bash training_convert_to_FVs_Python3_HardPartial_0.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_1.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_2.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_3.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_4.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_5.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_6.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_7.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_8.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_9.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_10.sh

nohup bash training_convert_to_FVs_Python3_SoftPartial_0.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_1.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_2.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_3.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_4.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_5.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_6.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_7.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_8.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_9.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_10.sh

```

### Training - training_sample_FVs 

```{python, training_sample_FVs.py}

# import sys,os,random
import sys # SCOTT
import os # SCOTT
import random # SCOTT

'''usage eg:
python2 training_sample_FVs.py spNeut.fvec spHard_ spSoft_ spPartialHard_ spPartialSoft_ 5 0,1,2,3,4,6,7,8,9,10 ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec
'''

if len(sys.argv)!=10:
  sys.exit("usage:\npython2 training_sample_FVs.py neutTrainingFileName hardTrainingFilesPrefix softTrainingFilesPrefix partialHardTrainingFilesPrefix partialSoftTrainingFilesPrefix sweepTrainingWindow linkedTrainingWindows sampledFVsDir sampledFVsFiles\n")
else:
  neutTrainingFileName, hardTrainingFilesPrefix, softTrainingFilesPrefix, partialHardTrainingFilesPrefix, partialSoftTrainingFilesPrefix, sweepTrainingWindow, linkedTrainingWindows, sampledFVsDir, sampledFVsFiles = sys.argv[1:]

neutTrainingFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spNeut.fvec"
hardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_"
softTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spSoft_"
partialHardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialHard_"
partialSoftTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialSoft_"
sweepTrainingWindow=5
linkedTrainingWindows=0,1,2,3,4,6,7,8,9,10
sampledFVsDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe"
sampledFVsFiles="neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec"

sweepFilePaths,linkedFilePaths = {},{}
for trainingFilePrefix in [hardTrainingFilesPrefix,softTrainingFilesPrefix,partialHardTrainingFilesPrefix,partialSoftTrainingFilesPrefix]:
  trainingSetDir="/".join(trainingFilePrefix.split("/")[:-1])
  trainingFilePrefixDirless=trainingFilePrefix.split("/")[-1]
  sweepFilePaths[trainingFilePrefix]=[]
  linkedFilePaths[trainingFilePrefix]=[]
  for fileName in os.listdir(trainingSetDir):
    if fileName.startswith(trainingFilePrefixDirless):
      winNum=int(fileName.split("_")[1].split(".")[0])
      if winNum==int(sweepTrainingWindow):
        sweepFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)
      elif winNum in [int(x) for x in linkedTrainingWindows.split(",")]:
        linkedFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)

def getExamplesFromFVFile(simFileName):
  try:
    simFile=open(simFileName)
    lines=[line.strip() for line in simFile.readlines()]
    header=lines[0]
    examples=lines[1:]
    simFile.close()
    return header,examples
  except Exception:
    return "",[]

def getExamplesFromFVFileLs(simFileLs):
  examples=[]
  keptHeader=""
  for filePath in simFileLs:
    header,currExamples=getExamplesFromFVFile(filePath)
    if header:
      keptHeader=header
    examples+=currExamples
  return keptHeader,examples

header,neutExamples=getExamplesFromFVFile(neutTrainingFileName)
hardHeader,hardExamples=getExamplesFromFVFileLs(sweepFilePaths[hardTrainingFilesPrefix])
linkedHardHeader,linkedHardExamples=getExamplesFromFVFileLs(linkedFilePaths[hardTrainingFilesPrefix])
softHeader,softExamples=getExamplesFromFVFileLs(sweepFilePaths[softTrainingFilesPrefix])
linkedSoftHeader,linkedSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[softTrainingFilesPrefix])
partialHardHeader,partialHardExamples=getExamplesFromFVFileLs(sweepFilePaths[partialHardTrainingFilesPrefix])
linkedPartialHardHeader,linkedPartialHardExamples=getExamplesFromFVFileLs(linkedFilePaths[partialHardTrainingFilesPrefix])
partialSoftHeader,partialSoftExamples=getExamplesFromFVFileLs(sweepFilePaths[partialSoftTrainingFilesPrefix])
linkedPartialSoftHeader,linkedPartialSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[partialSoftTrainingFilesPrefix])

def getMinButNonZeroExamples(lsLs):
  counts=[]
  for ls in lsLs:
    if len(ls)>0:
      counts.append(len(ls))
  if not counts:
    raise Exception
  return min(counts)

trainingSetLs=[hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
numExamplesToKeep=getMinButNonZeroExamples(trainingSetLs)
for i in range(len(trainingSetLs)):
  random.shuffle(trainingSetLs[i])
  trainingSetLs[i]=trainingSetLs[i][:numExamplesToKeep]
hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples=trainingSetLs

if sampledFVsDir.lower() in ["none","false","default"]:
  sampledFVsDir='./'
if sampledFVsFiles.lower() in ["none","false","default"]:
  sampledFVsFiles=["neut.fvec","hard.fvec","linkedHard.fvec","soft.fvec","linkedSoft.fvec","partialHard.fvec","linkedPartialHard.fvec","partialSoft.fvec","linkedPartialSoft.fvec"]
else:
  sampledFVsFiles=sampledFVsFiles.split(",")
  assert len(sampledFVsFiles)==9
outExamples=[neutExamples,hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
for i in range(len(sampledFVsFiles)):
  if outExamples[i]:
    outFile=open('/'.join((sampledFVsDir+'/'+sampledFVsFiles[i]).split('//')),"w")
    outFile.write("classLabel\t%s\n" %(hardHeader))
    for example in outExamples[i]:
      outFile.write("%s\t%s\n" %(sampledFVsFiles[i].replace(".fvec",""),example))
    outFile.close()

```

```{shell, run training_sample_FVs.py}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_lci

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_lci/FVs

for file in *.fvec; do
  mv "$file" "${file/.fvec/.fvec}"
done

for file in spHardPartial_*.fvec; do
  mv "$file" "${file/spHardPartial_/spPartialHard_}"
done

for file in spSoftPartial_*.fvec; do
  mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
done

python [PATH_TO]/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_lci/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_lci/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_lci/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_lci/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_lci/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

```

### Training 

#### Training with early stopping and 10-fold CV 

```{python, training_deep_learning_python3_earlyStopping_kFold.py}

import time
#startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import numpy as np
np.random.seed(123)
from sklearn.model_selection import train_test_split,StratifiedKFold # SCOTT
#from keras.utils import np_utils # Depricated
from keras.utils import to_categorical # SCOTT
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
from keras import optimizers
from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard # SCOTT

'''usage eg:
python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
'''

if len(sys.argv)!=9:
  sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
else:
  fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

if fvecFiles.lower() in ["none","false","default"]:
  fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
else:
  fvecFiles=fvecFiles.split(",")
  assert len(fvecFiles)==9

numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

if validationSize.lower() in ["none","false","default"]:
  validationSize=0.1
else:
  validationSize=float(validationSize)

sweeps=[]

for i in range(0,9):
 sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
 if i>0:
  assert len(sweeps[0])==len(sweeps[i])

sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))

sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)

models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))

#sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=1980)

models=to_categorical(models,9)

# Create StratifiedKFold object
n_splits = 10  # You can adjust the number of folds
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Loop over folds
for fold, (train_index, val_index) in enumerate(skf.split(sumstats, models.argmax(axis=1))):

    sumstats_train, sumstats_val = sumstats[train_index], sumstats[val_index]
    models_train, models_val = models[train_index], models[val_index]

    netlayers = Sequential()
    netlayers.add(Conv2D(256, (3, 6), padding='same', input_shape=sumstats.shape[1:]))
    netlayers.add(MaxPooling2D(pool_size=(3, 3), padding='same'))
    netlayers.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
    netlayers.add(MaxPooling2D(pool_size=(3, 3), padding='same'))
    netlayers.add(Dropout(0.25))
    netlayers.add(Flatten())
    netlayers.add(Dense(512, activation='relu'))
    netlayers.add(Dropout(0.5))
    netlayers.add(Dense(128, activation='relu'))
    netlayers.add(Dropout(0.5))
    netlayers.add(Dense(9, activation='softmax'))
    netlayers.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Modify the weights file name for each fold
    checkpoint = ModelCheckpoint(
        f'fold_{fold+1}_{weightsFileName}',
        monitor='val_accuracy',
        verbose=1,
        save_best_only=True,
        save_weights_only=True, mode='auto'
    )

    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, restore_best_weights=True, verbose=1)

    tensorboard_callback = TensorBoard(log_dir=f"./logs/fold_{fold + 1}")

    # Train the model
    netlayers.fit(
        sumstats_train,
        models_train,
        batch_size=128,
        epochs=1000,
        validation_data=(sumstats_val, models_val),
        callbacks=[checkpoint, early_stopping, tensorboard_callback],
        verbose=1
    )

    netlayers_json=netlayers.to_json()

    with open(f'fold_{fold+1}_{jsonFileName}',"w") as json_file:
        json_file.write(netlayers_json)

    netlayers.save(f'fold_{fold+1}_{npyFileName}')

#models=np_utils.to_categorical(models,9) # Depricated
#models=to_categorical(models,9) # SCOTT

#models_val=np_utils.to_categorical(models_val,9) # Depricated
#models_val=to_categorical(models_val,9) # SCOTT

#netlayers=Sequential()
#netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
#netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
#netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
#netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
#netlayers.add(Dropout(0.25))
#netlayers.add(Flatten())
#netlayers.add(Dense(512, activation='relu'))
#netlayers.add(Dropout(0.5))
#netlayers.add(Dense(128, activation='relu'))
#netlayers.add(Dropout(0.5))
#netlayers.add(Dense(9, activation='softmax'))
#netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

#checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')
#checkpoint=ModelCheckpoint(weightsFileName, monitor='val_accuracy', save_best_only=True, save_weights_only=True, mode='auto') # SCOTT

#early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.05, patience=5, restore_best_weights=True, verbose=1) # SCOTT

#tensorboard_callback = TensorBoard(log_dir="./logs") # SCOTT

#netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)
# SCOTT
#netlayers.fit(
#    sumstats, models,
#    batch_size=128, epochs=1000,  # Adjust the number of epochs as needed
#    validation_data=(sumstats_val, models_val),
#    callbacks=[checkpoint, early_stopping, tensorboard_callback],  # Include the EarlyStopping callback
#    verbose=1
#)

#netlayers_json=netlayers.to_json()

#with open(jsonFileName,"w") as json_file:
#  json_file.write(netlayers_json)

#netlayers.save(npyFileName)

#sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, run training_deep_learning_python3_earlyStopping_kFold.py}

module load anaconda3
conda activate partialshic_py3

# sp2_lci

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
rm ./*
cp ../*.fvec ./

time python3 [PATH_TO]/partialSHIC/training_deep_learning_python3_earlyStopping_kFold.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    constNe_earlyStopping_fold.hdf5\
    constNe_earlyStopping_fold.json\
    constNe_earlyStopping_fold.npy

```

### Run testing 

```{python, testing_deep_learning_classify_python3.py}

import time
# startTime=time.clock()
startTime = time.perf_counter() # Python 3
import sys,os
import keras
import numpy as np
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import pandas as pd

'''usage eg:
python3 testing_deep_learning_classify.py training.npy ./ 11 89 ./ accuracy confusion_matrix.pdf
'''

if len(sys.argv)!=8:
  sys.exit("usage:\npython3 testing_deep_learning_classify.py classifierPickleFileName fvecDir numSubWins numSumStatsPerSubWin resultsDir accuracyFilesPrefix confusionMatrixFigFileName\n")
else:
  classifierPickleFileName, fvecDir, numSubWins, numSumStatsPerSubWin, resultsDir, accuracyFilesPrefix, confusionMatrixFigFileName = sys.argv[1:]

classifierPickleFileName='../FVs/constNe.npy'
fvecDir='./'
numSubWins='11'
numSumStatsPerSubWin='92'
resultsDir='./'
accuracyFilesPrefix='constNe_accuracy'
confusionMatrixFigFileName='constNe_confusion_matrix.pdf'

netlayers=keras.models.load_model(classifierPickleFileName)

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

testX={}

numSubWins=int(numSubWins)

# for testing for loop below
testSetFileName='spHard_0.fvec'
testExample=currTestData[0]
i=0

for testSetFileName in os.listdir(fvecDir):
  # testSetFile=open('/'.join((fvecDir+'/'+testSetFileName).split('//')))
  testSetFile=open(testSetFileName)
  currTestData=testSetFile.readlines()
  testSetFile.close()
  currTestData=currTestData[1:]
  testX[testSetFileName]=[]
  for testExample in currTestData:
    if not "nan" in testExample:
      testData=testExample.strip().split("\t")
      currVector=[]
      for i in range(len(testData)):
        currVector.append(float(testData[i]))
      testX[testSetFileName].append(currVector)
  testX[testSetFileName]=np.reshape(np.array(testX[testSetFileName]),(np.array(testX[testSetFileName]).shape[0],int(numSumStatsPerSubWin),numSubWins,1))

selVals={"Neutral":0,"Hard":1,"Soft":2,"HardPartial":3,"SoftPartial":4}

def getSelType(x):
  if "Neut" in x:
    return "Neutral"
  elif "PartialHard" in x:
    return "HardPartial"
  elif "PartialSoft" in x:
    return "SoftPartial"
  elif "Hard" in x:
    return "Hard"
  elif "Soft" in x:
    return "Soft"
  else:
    raise ValueError

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split()

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

outlinesH={}
accuracyOverall=0
accuracySpecific=0
accuracyBroad=0

# for testing loop below
testSetFileName='spNeut.fvec'
className='Neutral'
testExampleIndex=0

for testSetFileName in sorted(testX, key=lambda x: (selVals[getSelType(x)], int((x.split(".")[0]+"_0").split("_")[1]))):
  predictions=np.argmax(netlayers.predict(testX[testSetFileName]),axis=1)
  pred = netlayers.predict(testX[testSetFileName]) # SCOTT
  prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
  prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT
  prob_df['truth']=testSetFileName # SCOTT
  prob_df.to_csv(testSetFileName+'.prob.csv', index=False) # SCOTT
  currPreds={}
  for className in classOrder:
    currPreds[className]=0
  denom=float(len(testX[testSetFileName]))
  for testExampleIndex in range(len(predictions)):
    predictedClass=labelToClassName[predictions[testExampleIndex]]
    currPreds[predictedClass]+=1/denom
  if not testSetFileName=='spNeut.fvec':
    testSetFilePrefix=testSetFileName.split(".")[0].split("_")
    selType,selWin = getSelType(testSetFilePrefix[0]),testSetFilePrefix[1]
    selWin=int(selWin)
    key=(selType,selWin)
  else:
    key=('Neutral',0)
  outlinesH[key]=(testSetFileName,[currPreds[className] for className in classOrder])
  if key[0]=='Neutral' or key[1]==5:
    accuracyOverall+=currPreds[key[0]]
    accuracySpecific+=currPreds[key[0]]
  else:
    accuracyOverall+=currPreds[key[0]+'-linked']
  accuracyBroad+=currPreds[key[0]]
  if not key[0]=='Neutral':
    accuracyBroad+=currPreds[key[0]+'-linked']

if resultsDir.lower() in ["none","false","default"]:
  resultsDir='./'

if accuracyFilesPrefix.lower() in ["none","false","default"]:
  accuracyFilesPrefix='accuracy'

accuracyOverall=accuracyOverall/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_overall.txt').split('//')),'w')
fileName.write(str(accuracyOverall))
fileName.close()

accuracySpecific=accuracySpecific/5

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_specific.txt').split('//')),'w')
fileName.write(str(accuracySpecific))
fileName.close()

accuracyBroad=accuracyBroad/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_broad.txt').split('//')),'w')
fileName.write(str(accuracyBroad))
fileName.close()


rowLabels,data = [],[]

for selType in sorted(selVals, key=lambda x: selVals[x]):
 for selWin in range(numSubWins):
  if selType!="Neutral" or selWin==0:
    if "Neutral" in selType:
      rowLabels.append("Neutral")
    else:
      if selWin==5:
        rowLabels.append("%s sweep in focal window" %selType)
      else:
        diff=abs(selWin-5)
        if diff==1:
          plural=""
        else:
          plural="s"
        if selWin<5:
          direction="left"
        else:
          direction="right"
        rowLabels.append("%s sweep %s window%s to %s" %(selType, diff, plural, direction))
    vec=outlinesH[(selType,selWin)][1]
    data.append(vec)

data=np.array(data)

ax=plt.subplots()[1]
heatmap=ax.pcolor(data,cmap=plt.cm.Blues,vmin=0.0,vmax=1.0)
cbar=plt.colorbar(heatmap,cmap=plt.cm.Blues)
cbar.set_label('Fraction of simulations assigned to class',rotation=270,labelpad=20)
ax.set_xticks(np.arange(data.shape[1])+0.5,minor=False)
ax.set_yticks(np.arange(data.shape[0])+0.5,minor=False)
ax.invert_yaxis()
ax.xaxis.tick_top()
ax.axis('tight')
plt.tick_params(axis='y',which='both',right='off')
plt.tick_params(axis='x',which='both',direction='out')
ax.set_xticklabels(classOrder,minor=False,fontsize=9,rotation=45,ha="left")
ax.set_yticklabels(rowLabels,minor=False,fontsize=7)

for y in range(data.shape[0]):
  for x in range(data.shape[1]):
    val=data[y,x]
    val*=100
    if val>50:
      c='0.9'
    else:
      c='black'
    ax.text(x+0.5,y +0.5,'%.1f%%' % val,horizontalalignment='center',verticalalignment='center',color=c,fontsize=6)

if confusionMatrixFigFileName.lower() in ["none","false","default"]:
  confusionMatrixFigFileName='confusionmatrix.pdf'

plt.savefig('/'.join((resultsDir+'/'+confusionMatrixFigFileName).split('//')),bbox_inches='tight',dpi=600)

# sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(classifierPickleFileName,fvecDir,(time.clock()-startTime)))
sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### test on self-new data 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/test_self_newDat
  mkdir -p FVs_earlyStopping && cd FVs_earlyStopping
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp ../../*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_lci/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

### Predict on empirical 

```{python, empirical_deep_learning_classify_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '[PATH_TO]/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '[PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '[PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell, run empirical_deep_learning_classify_python3.py}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/
mkdir -p pred_empirical && cd pred_empirical
rsync -ahPv [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/*_110Lines.phased.noPIRs.inbred.fvec ./

for CONTIG in 542 594 628 718 76 785; do
  for i in 1 2 3 4 5 6 7 8 9 10; do
    python3 [PATH_TO]/partialSHIC/empirical_deep_learning_classify_python3.py\
      [PATH_TO]/partialSHIC_DsGRP/sp2_lci/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
      ./ScA8VGg_${CONTIG}_110Lines.phased.noPIRs.inbred.fvec\
      11\
      92\
      ScA8VGg_${CONTIG}.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_fold_${i}.pred.bed >\
      empirical_deep_learning_classify_ScA8VGg_${CONTIG}_fold_${i}.log 2>&1
  done
done

```

# ######################## partialSHIC - SP2 97.5th (uci)

## Generate training data 

```{shell, install discoal}

cd [PATH_TO]/partialSHIC_DsGRP

git clone https://github.com/kr-colab/discoal.git

cd [PATH_TO]/partialSHIC_DsGRP/discoal

make discoal

```

```{shell, mkdir and load R}

mkdir -p [PATH_TO]/partialSHIC_DsGRP/sp2_uci

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci

module load anaconda3
source ~/conda-init
conda activate r_env_4.3.1

```

```{r, get simulation parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '[PATH_TO]/discoal/discoal' 
TIMES = 2000

Ne = 5182173
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low=0.000001929692
s_high=0.001929692
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

20000/(2*Ne)

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.001929692 # 0.001929692*4*5182173 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2

# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# sweep position
cat("sweep_pos: ", sweep_pos, "\n")

```

```{shell, discoal_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=[SAMPLE]
Ne=5182173
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=5700
Pre_upper=17101

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=1140
Pt_high=18241

# Selection coefficient s
s_low=0.000001929692
s_high=0.001929692
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.001929692

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_uci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell, discoal_Hard_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=[SAMPLE]
Ne=5182173
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=5700
Pre_upper=17101

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=1140
Pt_high=18241

# Selection coefficient s
s_low=0.000001929692
s_high=0.001929692
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.001929692

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_uci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_${TIME}.msOut.gz
done

```

```{shell, discoal_Soft_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=[SAMPLE]
Ne=5182173
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=5700
Pre_upper=17101

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=1140
Pt_high=18241

# Selection coefficient s
s_low=0.000001929692
s_high=0.001929692
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.001929692

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_uci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_${TIME}.msOut.gz
done

```

```{shell, discoal_HardPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=[SAMPLE]
Ne=5182173
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=5700
Pre_upper=17101

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=1140
Pt_high=18241

# Selection coefficient s
s_low=0.000001929692
s_high=0.001929692
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.001929692

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_uci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_${TIME}.msOut.gz
done

```

```{shell, discoal_SoftPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=5182173
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=5700
Pre_upper=17101

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=1140
Pt_high=18241

# Selection coefficient s
s_low=0.000001929692
s_high=0.001929692
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.001929692

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_${TIME}.msOut.gz
done

```

```{shell, create discoal scripts for linked}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/

# Hard-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_10.sh

# Soft-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_10.sh

# HardPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_10.sh

# SoftPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_10.sh

```

```{shell, run discoal scripts}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci

find ./ -maxdepth 1 -type f -name 'discoal_*.sh' | sed 's/\.\///' | sort -V > list_discoal_files.txt

while read p; do
  echo "${p} submitted"
  nohup bash ${p}
done < list_discoal_files.txt

```

## Generate test data 

```{shell, mkdir and load R}

mkdir -p [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat

module load anaconda3
conda activate r_env_4.3.1

```

```{r, get simulation parameters for bash}

options(scipen = 999)
library("parallel")
library("stringr")

DISCOAL = '[PATH_TO]/discoal/discoal' 
TIMES = 2000

Ne = 5182173
len = 55000

# recombination rate
rr = 5*10^(-9)
Pre_mean <- 4*Ne*rr*len
Pre_upper <- Pre_mean*3
# mutation rate mu: 
mu_low = 1e-9
mu_high = 1.6e-8
Pt_low <- 4*Ne*mu_low*len
Pt_high <- 4*Ne*mu_high*len
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
# s_low = 0.00001
s_low=0.000001929692
s_high=0.001929692
Pa_low <- 2*Ne*s_low # 20
Pa_high <- 2*Ne*s_high # 20000

20000/(2*Ne)

# tau: time of fixation looking backward in time - units: 4N
Pu_low <- 0.0
# Pu_high <- 0.01 # 0.01*4*3607692 ~= 144,308 generations
Pu_high <- 0.001929692 # 0.001929692*4*5182173 ~= 40,000 generations

# for soft seletive sweep
# first frequency at which selection acts on allele
Pf_low <- 0.01
Pf_high <- 0.2

# partial sweep
# frequency at sampling
Pc_low <- 0.20 
Pc_high <- 0.99

sweep_pos <- c(0.04, 0.14, 0.23, 0.32, 0.41, 0.50, 0.60, 0.68, 0.77, 0.86, 0.95)

cat("DISCOAL PATH: ", DISCOAL, "\n")
cat("Max TIMES: ", TIMES, "\n")
cat("Ne: ", Ne, "\n")
cat("len: ", len, "\n")
cat("rr: ", rr, "\n")
cat("Pre_mean: ", Pre_mean, "\n")
cat("Pre_upper: ", Pre_upper, "\n")
# mutation rate mu: 
cat("mu_low: ", mu_low, "\n")
cat("mu_high: ", mu_high, "\n")
cat("Pt_low: ", Pt_low, "\n")
cat("Pt_high: ", Pt_high, "\n")
# selection coefficient s: 0.0001-0.1
# strength of selection: alpha = 2*Ne*s
cat("s_low: ", s_low, "\n")
cat("s_high: ", s_high, "\n")
cat("Pa_low: ", Pa_low, "\n")
cat("Pa_high: ", Pa_high, "\n")
# tau: time of fixation looking backward in time - units: 4N
cat("Pu_low: ", Pu_low, "\n")
cat("Pu_high: ", Pu_high, "\n") # 0.01*4*3,600,666 = 144,000 generations
# for soft seletive sweep
# first frequency at which selection acts on allele
cat("Pf_low: ", Pf_low, "\n")
cat("Pf_high: ", Pf_high, "\n")
# partial sweep
# frequency at sampling
cat("Pc_low: ", Pc_low, "\n")
cat("Pc_high: ", Pc_high, "\n")
# sweep position
cat("sweep_pos: ", sweep_pos, "\n")

```

```{shell, discoal_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=[SAMPLE]
Ne=5182173
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=5700
Pre_upper=17101

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=1140
Pt_high=18241

# Selection coefficient s
s_low=0.000001929692
s_high=0.001929692
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.001929692

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_uci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck | gzip > spNeut_${TIME}.msOut.gz
done

```

```{shell, discoal_Hard_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=[SAMPLE]
Ne=5182173
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=5700
Pre_upper=17101

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=1140
Pt_high=18241

# Selection coefficient s
s_low=0.000001929692
s_high=0.001929692
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.001929692

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_uci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -x ${sweep_pos[5]} | gzip > spHard_5_${TIME}.msOut.gz
done

```

```{shell, discoal_Soft_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=[SAMPLE]
Ne=5182173
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=5700
Pre_upper=17101

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=1140
Pt_high=18241

# Selection coefficient s
s_low=0.000001929692
s_high=0.001929692
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.001929692

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_uci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoft_5_${TIME}.msOut.gz
done

```

```{shell, discoal_HardPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=[SAMPLE]
Ne=5182173
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=5700
Pre_upper=17101

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=1140
Pt_high=18241

# Selection coefficient s
s_low=0.000001929692
s_high=0.001929692
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.001929692

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal_uci.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -x ${sweep_pos[5]} | gzip > spHardPartial_5_${TIME}.msOut.gz
done

```

```{shell, discoal_SoftPartial_5.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat

DISCOAL="[PATH_TO]/partialSHIC_DsGRP/discoal/discoal"

TIMES=2000
Ne=5182173
len=55000

# Recombination rate
rr=0.000000005
Pre_mean=5700
Pre_upper=17101

# Mutation rate mu
mu_low=0.000000001
mu_high=0.000000016
Pt_low=1140
Pt_high=18241

# Selection coefficient s
s_low=0.000001929692
s_high=0.001929692
Pa_low=20
Pa_high=20000

# Time of fixation looking backward in time
Pu_low=0.0
Pu_high=0.001929692

# First frequency at which selection acts on allele
Pf_low=0.01
Pf_high=0.2

# Frequency at sampling
Pc_low=0.20
Pc_high=0.99

bottleneck=$(cat intergenicSNPs.final.en_for_discoal.txt)

sweep_pos=(0.04 0.14 0.23 0.32 0.41 0.50 0.60 0.68 0.77 0.86 0.95)

msOut="[SAMPLE]"

for TIME in $(seq 1 $TIMES); do
  srun $DISCOAL 110 1 $len -Pt $Pt_low $Pt_high -Pre $Pre_mean $Pre_upper $bottleneck -ws 0 -Pa $Pa_low $Pa_high -Pu $Pu_low $Pu_high -Pc $Pc_low $Pc_high -Pf $Pf_low $Pf_high -x ${sweep_pos[5]} | gzip > spSoftPartial_5_${TIME}.msOut.gz
done

```

```{shell, create discoal scripts for linked}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat

# Hard-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Hard_5.sh > discoal_Hard_10.sh

# Soft-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_Soft_5.sh > discoal_Soft_10.sh

# HardPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_HardPartial_5.sh > discoal_HardPartial_10.sh

# SoftPartial-linked

sed 's/\[5\]/\[0\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_0.sh
sed 's/\[5\]/\[1\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_1.sh
sed 's/\[5\]/\[2\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_2.sh
sed 's/\[5\]/\[3\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_3.sh
sed 's/\[5\]/\[4\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_4.sh
sed 's/\[5\]/\[6\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_6.sh
sed 's/\[5\]/\[7\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_7.sh
sed 's/\[5\]/\[8\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_8.sh
sed 's/\[5\]/\[9\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_9.sh
sed 's/\[5\]/\[10\]/g; s/TIMES=2000/TIMES=400/g;' discoal_SoftPartial_5.sh > discoal_SoftPartial_10.sh

```

```{shell, run discoal scripts}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat

find ./ -maxdepth 1 -type f -name 'discoal_*.sh' | sed 's/\.\///' | sort -V > list_discoal_files.txt

while read p; do
  echo "${p} submitted"
  nohup bash ${p}
done < list_discoal_files.txt

```

## Run training

### Training - calcStatsAndDafForEachSnp 

```{python, calcStatsAndDafForEachSnpSingleMsFile_Python3}

import sys
import allel
import random
import numpy as np
from msTools import *
from fvTools import *
import time

'''usage example
python calcStatsAndDafForEachSnpSingleMsFile.py /san/data/dan/simulations/discoal_multipopStuff/spatialSVMSims/trainingSets/equilibNeut.msout.gz 110000 iHS nSL
'''

trainingDataFileName, totalPhysLen = sys.argv[1:3]
totalPhysLen = int(totalPhysLen)

hapArraysIn, positionArrays = msOutToHaplotypeArrayIn(trainingDataFileName, totalPhysLen)
numInstances = len(hapArraysIn)

header = "daf\tiHS\tnSL"
#print header # Python 2
print(header) # Python 3

dafs = []
#start = time.clock() # Python 2
start = time.perf_counter() # Python 3
numInstancesDone = 0

numSnpsDone = 0
for instanceIndex in range(numInstances):
    if instanceIndex % 10 == 0:
        sys.stderr.write("done wtih %d of %d instances (%d SNPs)\n" %(instanceIndex, numInstances, numSnpsDone))
    haps = allel.HaplotypeArray(hapArraysIn[instanceIndex], dtype='i1')
    genos = haps.to_genotypes(ploidy=2)
    ac = genos.count_alleles()
    sampleSizes = [sum(x) for x in ac]
    assert len(set(sampleSizes)) == 1
    dafs = ac[:,1]/float(sampleSizes[0])
    ihsVals = allel.stats.selection.ihs(haps, positionArrays[instanceIndex], use_threads=True, include_edges=False)
    nslVals = allel.stats.selection.nsl(haps, use_threads=True)
    assert len(dafs) == len(ihsVals) == len(nslVals)
    numSnpsDone += len(dafs)
    for i in range(len(dafs)):
        #print "%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i]) # Python 2
        print ("%g\t%g\t%g" %(dafs[i], ihsVals[i], nslVals[i])) #Python 3
    if numSnpsDone > 5000000:
        break

```

```{shell, template script for calcStatsAndDafForEachSnpSingleMsFile_Python3.py}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/

msOut="[SAMPLE]"

python [PATH_TO]/partialSHIC/calcStatsAndDafForEachSnpSingleMsFile_Python3.py ${msOut} 55000 > ${msOut/.msOut.gz/}_stats.txt

```

```{shell, create scripts for calcStatsAndDafForEachSnpSingleMsFile_Python3.py}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/

find ./ -maxdepth 1 -type f -name '*.msOut.gz' | sed 's/\.\///' | sort -V > msOut_files.txt

while read p; do
  echo "$p"
  cp template_calcStatsAndDafForEachSnpSingleMsFile_Python3.txt calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
  replace_CMD="s/\[SAMPLE\]/${p}/g"
  perl -pi -e "$replace_CMD" calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
done < msOut_files.txt

find ./ -maxdepth 1 -type f -name 'calcStatsAndDafForEachSnpSingleMsFile_Python3_*.sh' -print0 | xargs -0 chmod 755

ls ./calcStatsAndDafForEachSnpSingleMsFile_Python3_*.sh

```

```{shell, run calcStatsAndDafForEachSnpSingleMsFile_Python3 scripts}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/

while read p; do
  echo "calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh submitted"
  nohup bash calcStatsAndDafForEachSnpSingleMsFile_Python3_${p/.msOut.gz/}.sh
  sleep 0.25
done < msOut_files.txt

```

### Training - training_convert_to_FVs 

```{python, training_convert_to_FVs_python3.py}

import time
#startTime=time.clock() # Python 2
start = time.perf_counter() # Python 3
import sys
from msTools import *
from fvTools import *
import random
import allel
import numpy as np
import math

'''usage eg:
pMisPol=`python2 stairwayPlotToPMisPol.py AOM.meru_mela.sfs.sp.summary`
python2 training_convert_to_FVs.py spNeut.msOut.gz 2L,2R,3L,3R 5000 11 0.25 $pMisPol AOM_partial_stats.txt Anopheles-gambiae-PEST_CHROMOSOMES_AgamP3.accessible.fa anc.meru_mela.fa ./ spNeut.msOut.fvec
'''

if len(sys.argv)!=12:
  sys.exit("usage:\npython2 training_convert_to_FVs.py trainingDataFileName chrArmsForMasking subWinSize numSubWins unmaskedFracCutoff pMisPol partialStatAndDafFileName maskFileName ancestralArmFaFileName statDir fvecFileName\n")
else:
  trainingDataFileName, chrArmsForMasking, subWinSize, numSubWins, unmaskedFracCutoff, pMisPol, partialStatAndDafFileName, maskFileName, ancestralArmFaFileName, statDir, fvecFileName = sys.argv[1:]

subWinSize,numSubWins = int(subWinSize),int(numSubWins)
assert subWinSize>0 and numSubWins>1

totalPhysLen=(subWinSize*numSubWins)

hapArraysIn,positionArrays=msOutToHaplotypeArrayIn(trainingDataFileName,totalPhysLen)

chrArmsForMasking=chrArmsForMasking.split(",")

unmaskedFracCutoff,pMisPol=float(unmaskedFracCutoff),float(pMisPol)

if unmaskedFracCutoff>1.0:
  sys.exit("unmaskedFracCutoff must lie within [0, 1].\n")

if pMisPol>1.0:
  sys.exit("pMisPol must lie within [0, 1].\n")

standardizationInfo=readStatsDafsComputeStandardizationBins(partialStatAndDafFileName,nBins=50,pMisPol=pMisPol)

if maskFileName.lower() in ["none", "false"]:
  unmaskedFracCutoff=1.0
  unmasked=[True]*totalPhysLen
  sys.stderr.write("Warning: not doing any masking! (i.e. mask.fa file for the chr arm with all masked sites N'ed out, or at least the reference with Ns, is not provided)\n")
  maskFileName=False
else:
  if ancestralArmFaFileName.lower() in ["none", "false"]:
    maskData=readMaskDataForTraining(maskFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  else:
    maskData=readMaskAndAncDataForTraining(maskFileName,ancestralArmFaFileName,totalPhysLen,subWinSize,chrArmsForMasking,shuffle=True,cutoff=unmaskedFracCutoff)
  if len(maskData)<len(hapArraysIn):
    sys.stderr.write("Warning: not enough windows from masked data (needed %d; got %d); will draw with replacement!!\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=True
  else:
    sys.stderr.write("enough windows from masked data (needed %d; got %d); will draw without replacement.\n" %(len(hapArraysIn), len(maskData)))
    drawWithReplacement=False

statNames=["pi", "thetaW", "tajD", "thetaH", "fayWuH", "HapCount", "H1", "H12", "H2/H1", "ZnS", "Omega", "iHSMean", "iHSMax", "iHSOutFrac", "nSLMean", "nSLMax", "nSLOutFrac", "distVar", "distSkew", "distKurt"]

for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
  for j in ["Mean", "Median", "Mode", "Lower95%", "Lower50%", "Upper50%", "Upper95%", "Max", "Var", "SD", "Skew", "Kurt"]:
    statNames.append("%s-%s" %(i, j))

header=[]

for statName in statNames:
  for i in range(numSubWins):
    header.append("%s_win%d" %(statName, i))

header="\t".join(header)

statVals={}

for statName in statNames:
  statVals[statName]=[]

def getSubWinBounds(subWinSize,totalPhysLen):
  subWinStart=1-subWinSize
  subWinEnd=0
  subWinBounds=[]
  for i in range(0,(numSubWins-1)):
    subWinStart+=subWinSize
    subWinEnd+=subWinSize
    subWinBounds.append((subWinStart,subWinEnd))
  subWinStart+=subWinSize
  subWinEnd=totalPhysLen
  subWinBounds.append((subWinStart,subWinEnd))
  return subWinBounds

subWinBounds=getSubWinBounds(subWinSize,totalPhysLen)

def getSnpIndicesInSubWins(subWinBounds,positions):
  snpIndicesInSubWins=[]
  for subWinIndex in range(len(subWinBounds)):
    snpIndicesInSubWins.append([])
  subWinIndex=0
  for i in range(len(positions)):
    while not (positions[i]>=subWinBounds[subWinIndex][0] and positions[i]<=subWinBounds[subWinIndex][1]):
      subWinIndex+=1
    snpIndicesInSubWins[subWinIndex].append(i)
  return snpIndicesInSubWins

quantiles={"Lower95%":2.5,"Lower50%":25,"Upper50%":75,"Upper95%":97.5}

if statDir.lower() in ["none","false","default"]:
  statDir='./'

statFiles=[]

for subWinIndex in range(numSubWins):
  statFileName='/'.join(("%s/%s.subWin%d.stats" %(statDir, trainingDataFileName.split("/")[-1].replace(".msOut.gz",""), subWinIndex)).split('//'))
  statFiles.append(open(statFileName,"w"))
  statFiles[-1].write("\t".join(statNames)+"\n")

if fvecFileName.lower() in ["none","false","default"]:
  fvecFileName="%s.fvec" %trainingDataFileName.split("/")[-1].replace(".msOut.gz","")

fvecFile=open(fvecFileName,"w")
fvecFile.write(header+"\n")

for instanceIndex in range(len(hapArraysIn)):
  for statName in statNames:
    statVals[statName].append([])
  if maskFileName:
    if drawWithReplacement:
      unmasked=random.choice(maskData)
    else:
      unmasked=maskData[instanceIndex]
    assert len(unmasked)==totalPhysLen
  snpIndicesToKeep=[x for x in range(len(positionArrays[instanceIndex])) if unmasked[positionArrays[instanceIndex][x]-1]]
  if len(snpIndicesToKeep)==0:
    for subWinIndex in range(numSubWins):
      for statName in statNames:
        appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  else:
    haps=allel.HaplotypeArray(hapArraysIn[instanceIndex],dtype='i1').subset(sel0=snpIndicesToKeep)
    if pMisPol>0:
      misPolarizeCorrectionIndex=np.random.binomial(1,pMisPol,len(haps))
      for i in range(len(misPolarizeCorrectionIndex)):
        if misPolarizeCorrectionIndex[i]==1:
          for j in range(len(haps[i])):
            if haps[i][j]==0:
              haps[i][j]=1
            else:
              haps[i][j]=0
    genos=haps.to_genotypes(ploidy=2)
    alleleCounts=genos.count_alleles()
    positions=[positionArrays[instanceIndex][x] for x in snpIndicesToKeep]
    precomputedStats={}
    sampleSizes=[sum(x) for x in alleleCounts]
    assert len(set(sampleSizes))==1
    dafs=alleleCounts[:,1]/float(sampleSizes[0])
    ihsVals=allel.stats.selection.ihs(haps,positions,use_threads=False,include_edges=False)
    nonNanCount=[x for x in np.isnan(ihsVals)].count(False)
    nonInfCount=[x for x in np.isinf(ihsVals)].count(False)
    sys.stderr.write("number of iHS scores: %d (%d non-nan; %d non-inf)\n" %(len(ihsVals),nonNanCount,nonInfCount))
    if nonNanCount==0:
      precomputedStats["iHS"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["iHS"].append([])
    else:
      ihsVals=standardize_by_allele_count_from_precomp_bins(ihsVals,dafs,standardizationInfo["iHS"])
      precomputedStats["iHS"]=windowVals(ihsVals,subWinBounds,positions,keepNans=False,absVal=True)
    nslVals=allel.stats.selection.nsl(haps,use_threads=False)
    nonNanCount=[x for x in np.isnan(nslVals)].count(False)
    sys.stderr.write("number of nSL scores: %d (%d non-nan)\n" %(len(nslVals),nonNanCount))
    if nonNanCount==0:
      precomputedStats["nSL"]=[]
      for subWinIndex in range(numSubWins):
        precomputedStats["nSL"].append([])
    else:
      nslVals=standardize_by_allele_count_from_precomp_bins(nslVals,dafs,standardizationInfo["nSL"])
      precomputedStats["nSL"]=windowVals(nslVals,subWinBounds,positions,keepNans=False,absVal=True)
    snpIndicesInSubWins=getSnpIndicesInSubWins(subWinBounds,positions)
    for subWinIndex in range(numSubWins):
      subWinStart,subWinEnd = subWinBounds[subWinIndex]
      unmaskedFrac=unmasked[subWinStart-1:subWinEnd].count(True)/float(subWinSize)
      assert unmaskedFrac>=unmaskedFracCutoff
      if len(snpIndicesInSubWins[subWinIndex])==0:
        for statName in statNames:
          appendStatValsForMonomorphic(statName,statVals,instanceIndex,subWinIndex)
      else:
        hapsInSubWin=haps.subset(sel0=snpIndicesInSubWins[subWinIndex])
        for statName in [x for x in statNames if x[0:3]!="HAF" and x[0:3]!="phi" and x[0:3]!="kap" and x[0:3]!="SFS" and x[0:3]!="SAF"]:
          calcAndAppendStatVal(alleleCounts, positions, statName, subWinStart, subWinEnd, statVals, instanceIndex, subWinIndex, hapsInSubWin, unmasked, precomputedStats)
        haplotypes={}
        for i in range(len(hapsInSubWin[0])):
          haplotype=[hapsInSubWin[x][i] for x in range(len(hapsInSubWin))]
          haplotype="".join(str(x) for x in haplotype)
          if haplotype in haplotypes:
            haplotypes[haplotype].append(i)
          else:
            haplotypes[haplotype]=[i]
        HAF=[]
        HAFunique={}
        for i in haplotypes:
          HAFunique[i]=0
          for j in range(len(hapsInSubWin)):
            if hapsInSubWin[j][haplotypes[i][0]]==1:
              HAFunique[i]+=sum([hapsInSubWin[j][x] for x in range(len(hapsInSubWin[j]))])
          for j in range(len(haplotypes[i])):
            HAF.append(HAFunique[i])
        phi=[]
        kappa=[]
        SAFE=[]
        for i in range(len(hapsInSubWin)):
          phi.append(0)
          kappa.append([])
          phiDenom=0
          for j in haplotypes:
            phi[i]+=(int(list(j)[i])*HAFunique[j]*len(haplotypes[j]))
            if int(list(j)[i])==1 and HAFunique[j] not in kappa[i] and HAFunique[j]!=0:
              kappa[i].append(HAFunique[j])
            phiDenom+=(HAFunique[j]*len(haplotypes[j]))
          phi[i]/=float(phiDenom)
          kappa[i]=len(kappa[i])/float(len(set([HAFunique[x] for x in HAFunique if HAFunique[x]!=0])))
          if dafs[snpIndicesInSubWins[subWinIndex]][i]==0 or dafs[snpIndicesInSubWins[subWinIndex]][i]==1:
           SAFE.append(0.0)
          else:
           SAFE.append((phi[i]-kappa[i])/float(math.sqrt(dafs[snpIndicesInSubWins[subWinIndex]][i]*(1-dafs[snpIndicesInSubWins[subWinIndex]][i]))))
        for i in ["HAF", "HAFunique", "phi", "kappa", "SFS", "SAFE"]:
          if i=="SFS":
            windowStats=dafs[snpIndicesInSubWins[subWinIndex]]
          elif i=="HAFunique":
            #windowStats=[eval(i)[x] for x in eval(i)]
            windowStats = list(HAFunique.values()) # SCOTT
          else:
            windowStats=eval(i)
          statVals[i+"-Mean"][instanceIndex].append(np.mean(windowStats))
          statVals[i+"-Median"][instanceIndex].append(np.median(windowStats))
          if(len(np.unique(windowStats,return_counts=True)[1])==1):
            statVals[i+"-Mode"][instanceIndex].append(windowStats[0])
          else:
            if(sorted(np.unique(windowStats,return_counts=True)[1])[-1]!=sorted(np.unique(windowStats,return_counts=True)[1])[-2]):
              statVals[i+"-Mode"][instanceIndex].append(scipy.stats.mstats.mode(windowStats)[0][0])
            else:
              mode=min(windowStats)
              for j in range(1,51):
                if len([x for x in windowStats if x >= (min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))) and x < (min(windowStats)+((j+1)*((max(windowStats)-min(windowStats))/50)))]) >= len([x for x in windowStats if x >= mode and x < (mode+((max(windowStats)-min(windowStats))/50))]):
                  mode=min(windowStats)+(j*((max(windowStats)-min(windowStats))/50))
              statVals[i+"-Mode"][instanceIndex].append(mode+((max(windowStats)-min(windowStats))/100))
          for j in quantiles:
            statVals[i+"-"+j][instanceIndex].append(np.percentile(windowStats,quantiles[j]))
          statVals[i+"-Max"][instanceIndex].append(max(windowStats))
          statVals[i+"-Var"][instanceIndex].append(np.var(windowStats))
          statVals[i+"-SD"][instanceIndex].append(np.std(windowStats))
          statVals[i+"-Skew"][instanceIndex].append(scipy.stats.skew(windowStats))
          statVals[i+"-Kurt"][instanceIndex].append(scipy.stats.kurtosis(windowStats))
      statFiles[subWinIndex].write("\t".join([str(statVals[statName][instanceIndex][subWinIndex]) for statName in statNames])+"\n")
  outVec=[]
  for statName in statNames:
    outVec+=normalizeFeatureVec(statVals[statName][instanceIndex])
  fvecFile.write("\t".join([str(x) for x in outVec])+"\n")

for subWinIndex in range(numSubWins):
  statFiles[subWinIndex].close()

fvecFile.close()

#sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent calculating summary statistics and generating feature vectors: %g secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, training_convert_to_FVs_Python3_Neutral.sh}

module load anaconda3
conda activate partialshic_py3

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat/

msOut="[SAMPLE]"

python [PATH_TO]/partialSHIC/training_convert_to_FVs_Python3.py\
  ${msOut}\
  ScA8VGg_542_HRSCAF_776,ScA8VGg_594_HRSCAF_845,ScA8VGg_628_HRSCAF_890,ScA8VGg_718_HRSCAF_1046,ScA8VGg_76_HRSCAF_120,ScA8VGg_785_HRSCAF_1140\
  5000\
  11\
  0.25\
  0.003\
  spNeut_stats.txt\
  [PATH_TO]/partialSHIC_DsGRP/drosophila_06Jul2018_A8VGg_noSpecialChar_MASKED.fasta\
  none\
  [PATH_TO]/partialSHIC_DsGRP/constNe/test_self_newDat/\
  ${msOut/.msOut.gz/}.fvec

```

```{shell, create scripts for sweeps}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat/
  
# Hard

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Hard_10.sh

sed -i 's/spNeut_stats\.txt/spHard_0_stats\.txt/' training_convert_to_FVs_Python3_Hard_0.sh
sed -i 's/spNeut_stats\.txt/spHard_1_stats\.txt/' training_convert_to_FVs_Python3_Hard_1.sh
sed -i 's/spNeut_stats\.txt/spHard_2_stats\.txt/' training_convert_to_FVs_Python3_Hard_2.sh
sed -i 's/spNeut_stats\.txt/spHard_3_stats\.txt/' training_convert_to_FVs_Python3_Hard_3.sh
sed -i 's/spNeut_stats\.txt/spHard_4_stats\.txt/' training_convert_to_FVs_Python3_Hard_4.sh
sed -i 's/spNeut_stats\.txt/spHard_5_stats\.txt/' training_convert_to_FVs_Python3_Hard_5.sh
sed -i 's/spNeut_stats\.txt/spHard_6_stats\.txt/' training_convert_to_FVs_Python3_Hard_6.sh
sed -i 's/spNeut_stats\.txt/spHard_7_stats\.txt/' training_convert_to_FVs_Python3_Hard_7.sh
sed -i 's/spNeut_stats\.txt/spHard_8_stats\.txt/' training_convert_to_FVs_Python3_Hard_8.sh
sed -i 's/spNeut_stats\.txt/spHard_9_stats\.txt/' training_convert_to_FVs_Python3_Hard_9.sh
sed -i 's/spNeut_stats\.txt/spHard_10_stats\.txt/' training_convert_to_FVs_Python3_Hard_10.sh

# Soft

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_Soft_10.sh

sed -i 's/spNeut_stats\.txt/spSoft_0_stats\.txt/' training_convert_to_FVs_Python3_Soft_0.sh
sed -i 's/spNeut_stats\.txt/spSoft_1_stats\.txt/' training_convert_to_FVs_Python3_Soft_1.sh
sed -i 's/spNeut_stats\.txt/spSoft_2_stats\.txt/' training_convert_to_FVs_Python3_Soft_2.sh
sed -i 's/spNeut_stats\.txt/spSoft_3_stats\.txt/' training_convert_to_FVs_Python3_Soft_3.sh
sed -i 's/spNeut_stats\.txt/spSoft_4_stats\.txt/' training_convert_to_FVs_Python3_Soft_4.sh
sed -i 's/spNeut_stats\.txt/spSoft_5_stats\.txt/' training_convert_to_FVs_Python3_Soft_5.sh
sed -i 's/spNeut_stats\.txt/spSoft_6_stats\.txt/' training_convert_to_FVs_Python3_Soft_6.sh
sed -i 's/spNeut_stats\.txt/spSoft_7_stats\.txt/' training_convert_to_FVs_Python3_Soft_7.sh
sed -i 's/spNeut_stats\.txt/spSoft_8_stats\.txt/' training_convert_to_FVs_Python3_Soft_8.sh
sed -i 's/spNeut_stats\.txt/spSoft_9_stats\.txt/' training_convert_to_FVs_Python3_Soft_9.sh
sed -i 's/spNeut_stats\.txt/spSoft_10_stats\.txt/' training_convert_to_FVs_Python3_Soft_10.sh

# HardPartial

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_HardPartial_10.sh

sed -i 's/spNeut_stats\.txt/spHardPartial_0_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_0.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_1_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_1.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_2_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_2.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_3_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_3.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_4_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_4.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_5_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_5.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_6_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_6.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_7_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_7.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_8_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_8.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_9_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_9.sh
sed -i 's/spNeut_stats\.txt/spHardPartial_10_stats\.txt/' training_convert_to_FVs_Python3_HardPartial_10.sh

# SoftPartial

cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_0.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_1.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_2.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_3.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_4.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_5.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_6.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_7.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_8.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_9.sh
cp training_convert_to_FVs_Python3_Neutral.sh training_convert_to_FVs_Python3_SoftPartial_10.sh

sed -i 's/spNeut_stats\.txt/spSoftPartial_0_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_0.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_1_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_1.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_2_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_2.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_3_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_3.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_4_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_4.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_5_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_5.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_6_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_6.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_7_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_7.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_8_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_8.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_9_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_9.sh
sed -i 's/spNeut_stats\.txt/spSoftPartial_10_stats\.txt/' training_convert_to_FVs_Python3_SoftPartial_10.sh

```

```{shell, submit training_convert_to_FVs_Python3 scripts}

[PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat/

nohup bash training_convert_to_FVs_Python3_Neutral.sh

nohup bash training_convert_to_FVs_Python3_Hard_0.sh
nohup bash training_convert_to_FVs_Python3_Hard_1.sh
nohup bash training_convert_to_FVs_Python3_Hard_2.sh
nohup bash training_convert_to_FVs_Python3_Hard_3.sh
nohup bash training_convert_to_FVs_Python3_Hard_4.sh
nohup bash training_convert_to_FVs_Python3_Hard_5.sh
nohup bash training_convert_to_FVs_Python3_Hard_6.sh
nohup bash training_convert_to_FVs_Python3_Hard_7.sh
nohup bash training_convert_to_FVs_Python3_Hard_8.sh
nohup bash training_convert_to_FVs_Python3_Hard_9.sh
nohup bash training_convert_to_FVs_Python3_Hard_10.sh

nohup bash training_convert_to_FVs_Python3_Soft_0.sh
nohup bash training_convert_to_FVs_Python3_Soft_1.sh
nohup bash training_convert_to_FVs_Python3_Soft_2.sh
nohup bash training_convert_to_FVs_Python3_Soft_3.sh
nohup bash training_convert_to_FVs_Python3_Soft_4.sh
nohup bash training_convert_to_FVs_Python3_Soft_5.sh
nohup bash training_convert_to_FVs_Python3_Soft_6.sh
nohup bash training_convert_to_FVs_Python3_Soft_7.sh
nohup bash training_convert_to_FVs_Python3_Soft_8.sh
nohup bash training_convert_to_FVs_Python3_Soft_9.sh
nohup bash training_convert_to_FVs_Python3_Soft_10.sh

nohup bash training_convert_to_FVs_Python3_HardPartial_0.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_1.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_2.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_3.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_4.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_5.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_6.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_7.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_8.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_9.sh
nohup bash training_convert_to_FVs_Python3_HardPartial_10.sh

nohup bash training_convert_to_FVs_Python3_SoftPartial_0.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_1.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_2.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_3.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_4.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_5.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_6.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_7.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_8.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_9.sh
nohup bash training_convert_to_FVs_Python3_SoftPartial_10.sh

```

### Training - training_sample_FVs 

```{python, training_sample_FVs.py}

# import sys,os,random
import sys # SCOTT
import os # SCOTT
import random # SCOTT

'''usage eg:
python2 training_sample_FVs.py spNeut.fvec spHard_ spSoft_ spPartialHard_ spPartialSoft_ 5 0,1,2,3,4,6,7,8,9,10 ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec
'''

if len(sys.argv)!=10:
  sys.exit("usage:\npython2 training_sample_FVs.py neutTrainingFileName hardTrainingFilesPrefix softTrainingFilesPrefix partialHardTrainingFilesPrefix partialSoftTrainingFilesPrefix sweepTrainingWindow linkedTrainingWindows sampledFVsDir sampledFVsFiles\n")
else:
  neutTrainingFileName, hardTrainingFilesPrefix, softTrainingFilesPrefix, partialHardTrainingFilesPrefix, partialSoftTrainingFilesPrefix, sweepTrainingWindow, linkedTrainingWindows, sampledFVsDir, sampledFVsFiles = sys.argv[1:]

neutTrainingFileName="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spNeut.fvec"
hardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spHard_"
softTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spSoft_"
partialHardTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialHard_"
partialSoftTrainingFilesPrefix="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe/spPartialSoft_"
sweepTrainingWindow=5
linkedTrainingWindows=0,1,2,3,4,6,7,8,9,10
sampledFVsDir="/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/constNe"
sampledFVsFiles="neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec"

sweepFilePaths,linkedFilePaths = {},{}
for trainingFilePrefix in [hardTrainingFilesPrefix,softTrainingFilesPrefix,partialHardTrainingFilesPrefix,partialSoftTrainingFilesPrefix]:
  trainingSetDir="/".join(trainingFilePrefix.split("/")[:-1])
  trainingFilePrefixDirless=trainingFilePrefix.split("/")[-1]
  sweepFilePaths[trainingFilePrefix]=[]
  linkedFilePaths[trainingFilePrefix]=[]
  for fileName in os.listdir(trainingSetDir):
    if fileName.startswith(trainingFilePrefixDirless):
      winNum=int(fileName.split("_")[1].split(".")[0])
      if winNum==int(sweepTrainingWindow):
        sweepFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)
      elif winNum in [int(x) for x in linkedTrainingWindows.split(",")]:
        linkedFilePaths[trainingFilePrefix].append(trainingSetDir+"/"+fileName)

def getExamplesFromFVFile(simFileName):
  try:
    simFile=open(simFileName)
    lines=[line.strip() for line in simFile.readlines()]
    header=lines[0]
    examples=lines[1:]
    simFile.close()
    return header,examples
  except Exception:
    return "",[]

def getExamplesFromFVFileLs(simFileLs):
  examples=[]
  keptHeader=""
  for filePath in simFileLs:
    header,currExamples=getExamplesFromFVFile(filePath)
    if header:
      keptHeader=header
    examples+=currExamples
  return keptHeader,examples

header,neutExamples=getExamplesFromFVFile(neutTrainingFileName)
hardHeader,hardExamples=getExamplesFromFVFileLs(sweepFilePaths[hardTrainingFilesPrefix])
linkedHardHeader,linkedHardExamples=getExamplesFromFVFileLs(linkedFilePaths[hardTrainingFilesPrefix])
softHeader,softExamples=getExamplesFromFVFileLs(sweepFilePaths[softTrainingFilesPrefix])
linkedSoftHeader,linkedSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[softTrainingFilesPrefix])
partialHardHeader,partialHardExamples=getExamplesFromFVFileLs(sweepFilePaths[partialHardTrainingFilesPrefix])
linkedPartialHardHeader,linkedPartialHardExamples=getExamplesFromFVFileLs(linkedFilePaths[partialHardTrainingFilesPrefix])
partialSoftHeader,partialSoftExamples=getExamplesFromFVFileLs(sweepFilePaths[partialSoftTrainingFilesPrefix])
linkedPartialSoftHeader,linkedPartialSoftExamples=getExamplesFromFVFileLs(linkedFilePaths[partialSoftTrainingFilesPrefix])

def getMinButNonZeroExamples(lsLs):
  counts=[]
  for ls in lsLs:
    if len(ls)>0:
      counts.append(len(ls))
  if not counts:
    raise Exception
  return min(counts)

trainingSetLs=[hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
numExamplesToKeep=getMinButNonZeroExamples(trainingSetLs)
for i in range(len(trainingSetLs)):
  random.shuffle(trainingSetLs[i])
  trainingSetLs[i]=trainingSetLs[i][:numExamplesToKeep]
hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples=trainingSetLs

if sampledFVsDir.lower() in ["none","false","default"]:
  sampledFVsDir='./'
if sampledFVsFiles.lower() in ["none","false","default"]:
  sampledFVsFiles=["neut.fvec","hard.fvec","linkedHard.fvec","soft.fvec","linkedSoft.fvec","partialHard.fvec","linkedPartialHard.fvec","partialSoft.fvec","linkedPartialSoft.fvec"]
else:
  sampledFVsFiles=sampledFVsFiles.split(",")
  assert len(sampledFVsFiles)==9
outExamples=[neutExamples,hardExamples,linkedHardExamples,softExamples,linkedSoftExamples,partialHardExamples,linkedPartialHardExamples,partialSoftExamples,linkedPartialSoftExamples]
for i in range(len(sampledFVsFiles)):
  if outExamples[i]:
    outFile=open('/'.join((sampledFVsDir+'/'+sampledFVsFiles[i]).split('//')),"w")
    outFile.write("classLabel\t%s\n" %(hardHeader))
    for example in outExamples[i]:
      outFile.write("%s\t%s\n" %(sampledFVsFiles[i].replace(".fvec",""),example))
    outFile.close()

```

```{shell, run training_sample_FVs.py}

cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_uci

mkdir -p FVs
mv ./*.fvec ./FVs/
cd /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_uci/FVs

for file in *.fvec; do
  mv "$file" "${file/.fvec/.fvec}"
done

for file in spHardPartial_*.fvec; do
  mv "$file" "${file/spHardPartial_/spPartialHard_}"
done

for file in spSoftPartial_*.fvec; do
  mv "$file" "${file/spSoftPartial_/spPartialSoft_}"
done

python [PATH_TO]/partialSHIC/training_sample_FVs.py\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_uci/FVs/spNeut.fvec\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_uci/FVs/spHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_uci/FVs/spSoft_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_uci/FVs/spPartialHard_\
 /home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_uci/FVs/spPartialSoft_\
 5 0,1,2,3,4,6,7,8,9,10\
 ./\
 neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec

```

### Training 

#### Training with early stopping and 10-fold CV 

```{python, training_deep_learning_python3_earlyStopping_kFold.py}

import time
#startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import numpy as np
np.random.seed(123)
from sklearn.model_selection import train_test_split,StratifiedKFold # SCOTT
#from keras.utils import np_utils # Depricated
from keras.utils import to_categorical # SCOTT
from keras.models import Sequential
from keras.layers import Conv2D,MaxPooling2D,Activation,Dropout,Flatten,Dense
from keras import optimizers
from keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard # SCOTT

'''usage eg:
python3 training_deep_learning.py ./ neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec 11 89 0.1 training_weights.hdf5 training.json training.npy
'''

if len(sys.argv)!=9:
  sys.exit("usage:\npython3 training_deep_learning.py fvecDir fvecFiles numSubWins numSumStatsPerSubWin validationSize weightsFileName jsonFileName npyFileName\n")
else:
  fvecDir, fvecFiles, numSubWins, numSumStatsPerSubWin, validationSize, weightsFileName, jsonFileName, npyFileName = sys.argv[1:]

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

if fvecFiles.lower() in ["none","false","default"]:
  fvecFiles=['neut.fvec','hard.fvec','linkedHard.fvec','soft.fvec','linkedSoft.fvec','partialHard.fvec','linkedPartialHard.fvec','partialSoft.fvec','linkedPartialSoft.fvec']
else:
  fvecFiles=fvecFiles.split(",")
  assert len(fvecFiles)==9

numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

if validationSize.lower() in ["none","false","default"]:
  validationSize=0.1
else:
  validationSize=float(validationSize)

sweeps=[]

for i in range(0,9):
 sweeps.append(np.loadtxt('/'.join((fvecDir+'/'+fvecFiles[i]).split('//')),skiprows=1,usecols=list(range(1,((numSubWins*numSumStatsPerSubWin)+1)))))
 if i>0:
  assert len(sweeps[0])==len(sweeps[i])

sumstats=np.concatenate((sweeps[0],sweeps[1],sweeps[2],sweeps[3],sweeps[4],sweeps[5],sweeps[6],sweeps[7],sweeps[8]))

sumstats=sumstats.reshape(sumstats.shape[0],numSumStatsPerSubWin,numSubWins,1)

models=np.concatenate((np.repeat(0,len(sweeps[0])),np.repeat(1,len(sweeps[0])),np.repeat(2,len(sweeps[0])),np.repeat(3,len(sweeps[0])),np.repeat(4,len(sweeps[0])),np.repeat(5,len(sweeps[0])),np.repeat(6,len(sweeps[0])),np.repeat(7,len(sweeps[0])),np.repeat(8,len(sweeps[0]))))

#sumstats,sumstats_val,models,models_val = train_test_split(sumstats,models,test_size=validationSize,random_state=1980)

models=to_categorical(models,9)

# Create StratifiedKFold object
n_splits = 10  # You can adjust the number of folds
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Loop over folds
for fold, (train_index, val_index) in enumerate(skf.split(sumstats, models.argmax(axis=1))):

    sumstats_train, sumstats_val = sumstats[train_index], sumstats[val_index]
    models_train, models_val = models[train_index], models[val_index]

    netlayers = Sequential()
    netlayers.add(Conv2D(256, (3, 6), padding='same', input_shape=sumstats.shape[1:]))
    netlayers.add(MaxPooling2D(pool_size=(3, 3), padding='same'))
    netlayers.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
    netlayers.add(MaxPooling2D(pool_size=(3, 3), padding='same'))
    netlayers.add(Dropout(0.25))
    netlayers.add(Flatten())
    netlayers.add(Dense(512, activation='relu'))
    netlayers.add(Dropout(0.5))
    netlayers.add(Dense(128, activation='relu'))
    netlayers.add(Dropout(0.5))
    netlayers.add(Dense(9, activation='softmax'))
    netlayers.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Modify the weights file name for each fold
    checkpoint = ModelCheckpoint(
        f'fold_{fold+1}_{weightsFileName}',
        monitor='val_accuracy',
        verbose=1,
        save_best_only=True,
        save_weights_only=True, mode='auto'
    )

    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, restore_best_weights=True, verbose=1)

    tensorboard_callback = TensorBoard(log_dir=f"./logs/fold_{fold + 1}")

    # Train the model
    netlayers.fit(
        sumstats_train,
        models_train,
        batch_size=128,
        epochs=1000,
        validation_data=(sumstats_val, models_val),
        callbacks=[checkpoint, early_stopping, tensorboard_callback],
        verbose=1
    )

    netlayers_json=netlayers.to_json()

    with open(f'fold_{fold+1}_{jsonFileName}',"w") as json_file:
        json_file.write(netlayers_json)

    netlayers.save(f'fold_{fold+1}_{npyFileName}')

#models=np_utils.to_categorical(models,9) # Depricated
#models=to_categorical(models,9) # SCOTT

#models_val=np_utils.to_categorical(models_val,9) # Depricated
#models_val=to_categorical(models_val,9) # SCOTT

#netlayers=Sequential()
#netlayers.add(Conv2D(256,(3, 6),padding='same',input_shape=sumstats.shape[1:]))
#netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
#netlayers.add(Conv2D(256,(3, 3),padding='same',activation='relu'))
#netlayers.add(MaxPooling2D(pool_size=(3,3),padding='same'))
#netlayers.add(Dropout(0.25))
#netlayers.add(Flatten())
#netlayers.add(Dense(512, activation='relu'))
#netlayers.add(Dropout(0.5))
#netlayers.add(Dense(128, activation='relu'))
#netlayers.add(Dropout(0.5))
#netlayers.add(Dense(9, activation='softmax'))
#netlayers.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

#checkpoint=ModelCheckpoint(weightsFileName,monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=True,mode='auto')
#checkpoint=ModelCheckpoint(weightsFileName, monitor='val_accuracy', save_best_only=True, save_weights_only=True, mode='auto') # SCOTT

#early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.05, patience=5, restore_best_weights=True, verbose=1) # SCOTT

#tensorboard_callback = TensorBoard(log_dir="./logs") # SCOTT

#netlayers.fit(sumstats,models,batch_size=32,epochs=20,validation_data=(sumstats_val,models_val),callbacks=[checkpoint],verbose=1)
# SCOTT
#netlayers.fit(
#    sumstats, models,
#    batch_size=128, epochs=1000,  # Adjust the number of epochs as needed
#    validation_data=(sumstats_val, models_val),
#    callbacks=[checkpoint, early_stopping, tensorboard_callback],  # Include the EarlyStopping callback
#    verbose=1
#)

#netlayers_json=netlayers.to_json()

#with open(jsonFileName,"w") as json_file:
#  json_file.write(netlayers_json)

#netlayers.save(npyFileName)

#sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.clock()-startTime))
sys.stderr.write("total time spent fitting and validating convolutional neural network for deep learning: %f secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

```{shell, run training_deep_learning_python3_earlyStopping_kFold.py}

module load anaconda3
conda activate partialshic_py3

# sp2_uci

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci
mkdir -p FVs && cd ./FVs
mkdir -p earlyStopping && cd ./earlyStopping
rm ./*
cp ../*.fvec ./

time python3 [PATH_TO]/partialSHIC/training_deep_learning_python3_earlyStopping_kFold.py\
    ./\
    neut.fvec,hard.fvec,linkedHard.fvec,soft.fvec,linkedSoft.fvec,partialHard.fvec,linkedPartialHard.fvec,partialSoft.fvec,linkedPartialSoft.fvec\
    11\
    92\
    0.1\
    constNe_earlyStopping_fold.hdf5\
    constNe_earlyStopping_fold.json\
    constNe_earlyStopping_fold.npy

```

### Run testing 

```{python, testing_deep_learning_classify_python3.py}

import time
# startTime=time.clock()
startTime = time.perf_counter() # Python 3
import sys,os
import keras
import numpy as np
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import pandas as pd

'''usage eg:
python3 testing_deep_learning_classify.py training.npy ./ 11 89 ./ accuracy confusion_matrix.pdf
'''

if len(sys.argv)!=8:
  sys.exit("usage:\npython3 testing_deep_learning_classify.py classifierPickleFileName fvecDir numSubWins numSumStatsPerSubWin resultsDir accuracyFilesPrefix confusionMatrixFigFileName\n")
else:
  classifierPickleFileName, fvecDir, numSubWins, numSumStatsPerSubWin, resultsDir, accuracyFilesPrefix, confusionMatrixFigFileName = sys.argv[1:]

classifierPickleFileName='../FVs/constNe.npy'
fvecDir='./'
numSubWins='11'
numSumStatsPerSubWin='92'
resultsDir='./'
accuracyFilesPrefix='constNe_accuracy'
confusionMatrixFigFileName='constNe_confusion_matrix.pdf'

netlayers=keras.models.load_model(classifierPickleFileName)

if fvecDir.lower() in ["none","false","default"]:
  fvecDir='./'

testX={}

numSubWins=int(numSubWins)

# for testing for loop below
testSetFileName='spHard_0.fvec'
testExample=currTestData[0]
i=0

for testSetFileName in os.listdir(fvecDir):
  # testSetFile=open('/'.join((fvecDir+'/'+testSetFileName).split('//')))
  testSetFile=open(testSetFileName)
  currTestData=testSetFile.readlines()
  testSetFile.close()
  currTestData=currTestData[1:]
  testX[testSetFileName]=[]
  for testExample in currTestData:
    if not "nan" in testExample:
      testData=testExample.strip().split("\t")
      currVector=[]
      for i in range(len(testData)):
        currVector.append(float(testData[i]))
      testX[testSetFileName].append(currVector)
  testX[testSetFileName]=np.reshape(np.array(testX[testSetFileName]),(np.array(testX[testSetFileName]).shape[0],int(numSumStatsPerSubWin),numSubWins,1))

selVals={"Neutral":0,"Hard":1,"Soft":2,"HardPartial":3,"SoftPartial":4}

def getSelType(x):
  if "Neut" in x:
    return "Neutral"
  elif "PartialHard" in x:
    return "HardPartial"
  elif "PartialSoft" in x:
    return "SoftPartial"
  elif "Hard" in x:
    return "Hard"
  elif "Soft" in x:
    return "Soft"
  else:
    raise ValueError

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split()

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

outlinesH={}
accuracyOverall=0
accuracySpecific=0
accuracyBroad=0

# for testing loop below
testSetFileName='spNeut.fvec'
className='Neutral'
testExampleIndex=0

for testSetFileName in sorted(testX, key=lambda x: (selVals[getSelType(x)], int((x.split(".")[0]+"_0").split("_")[1]))):
  predictions=np.argmax(netlayers.predict(testX[testSetFileName]),axis=1)
  pred = netlayers.predict(testX[testSetFileName]) # SCOTT
  prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
  prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT
  prob_df['truth']=testSetFileName # SCOTT
  prob_df.to_csv(testSetFileName+'.prob.csv', index=False) # SCOTT
  currPreds={}
  for className in classOrder:
    currPreds[className]=0
  denom=float(len(testX[testSetFileName]))
  for testExampleIndex in range(len(predictions)):
    predictedClass=labelToClassName[predictions[testExampleIndex]]
    currPreds[predictedClass]+=1/denom
  if not testSetFileName=='spNeut.fvec':
    testSetFilePrefix=testSetFileName.split(".")[0].split("_")
    selType,selWin = getSelType(testSetFilePrefix[0]),testSetFilePrefix[1]
    selWin=int(selWin)
    key=(selType,selWin)
  else:
    key=('Neutral',0)
  outlinesH[key]=(testSetFileName,[currPreds[className] for className in classOrder])
  if key[0]=='Neutral' or key[1]==5:
    accuracyOverall+=currPreds[key[0]]
    accuracySpecific+=currPreds[key[0]]
  else:
    accuracyOverall+=currPreds[key[0]+'-linked']
  accuracyBroad+=currPreds[key[0]]
  if not key[0]=='Neutral':
    accuracyBroad+=currPreds[key[0]+'-linked']

if resultsDir.lower() in ["none","false","default"]:
  resultsDir='./'

if accuracyFilesPrefix.lower() in ["none","false","default"]:
  accuracyFilesPrefix='accuracy'

accuracyOverall=accuracyOverall/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_overall.txt').split('//')),'w')
fileName.write(str(accuracyOverall))
fileName.close()

accuracySpecific=accuracySpecific/5

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_specific.txt').split('//')),'w')
fileName.write(str(accuracySpecific))
fileName.close()

accuracyBroad=accuracyBroad/((4*numSubWins)+1)

fileName=open('/'.join((resultsDir+'/'+accuracyFilesPrefix+'_broad.txt').split('//')),'w')
fileName.write(str(accuracyBroad))
fileName.close()


rowLabels,data = [],[]

for selType in sorted(selVals, key=lambda x: selVals[x]):
 for selWin in range(numSubWins):
  if selType!="Neutral" or selWin==0:
    if "Neutral" in selType:
      rowLabels.append("Neutral")
    else:
      if selWin==5:
        rowLabels.append("%s sweep in focal window" %selType)
      else:
        diff=abs(selWin-5)
        if diff==1:
          plural=""
        else:
          plural="s"
        if selWin<5:
          direction="left"
        else:
          direction="right"
        rowLabels.append("%s sweep %s window%s to %s" %(selType, diff, plural, direction))
    vec=outlinesH[(selType,selWin)][1]
    data.append(vec)

data=np.array(data)

ax=plt.subplots()[1]
heatmap=ax.pcolor(data,cmap=plt.cm.Blues,vmin=0.0,vmax=1.0)
cbar=plt.colorbar(heatmap,cmap=plt.cm.Blues)
cbar.set_label('Fraction of simulations assigned to class',rotation=270,labelpad=20)
ax.set_xticks(np.arange(data.shape[1])+0.5,minor=False)
ax.set_yticks(np.arange(data.shape[0])+0.5,minor=False)
ax.invert_yaxis()
ax.xaxis.tick_top()
ax.axis('tight')
plt.tick_params(axis='y',which='both',right='off')
plt.tick_params(axis='x',which='both',direction='out')
ax.set_xticklabels(classOrder,minor=False,fontsize=9,rotation=45,ha="left")
ax.set_yticklabels(rowLabels,minor=False,fontsize=7)

for y in range(data.shape[0]):
  for x in range(data.shape[1]):
    val=data[y,x]
    val*=100
    if val>50:
      c='0.9'
    else:
      c='black'
    ax.text(x+0.5,y +0.5,'%.1f%%' % val,horizontalalignment='center',verticalalignment='center',color=c,fontsize=6)

if confusionMatrixFigFileName.lower() in ["none","false","default"]:
  confusionMatrixFigFileName='confusionmatrix.pdf'

plt.savefig('/'.join((resultsDir+'/'+confusionMatrixFigFileName).split('//')),bbox_inches='tight',dpi=600)

# sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(classifierPickleFileName,fvecDir,(time.clock()-startTime)))
sys.stderr.write("total time spent testing classifier stored in %s on feature vector sets in %s: %f secs\n" %(time.perf_counter()-startTime)) # SCOTT

```

#### test on self-new data 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/test_self_newDat
  mkdir -p FVs_earlyStopping && cd FVs_earlyStopping
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp ../../*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_uci/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

### Predict on empirical 

```{python, empirical_deep_learning_classify_python3.py}

import time
# startTime=time.clock() # Python 2
startTime = time.perf_counter() # Python 3
import sys
import keras
import numpy as np

'''usage eg:
python3 empirical_deep_learning_classify.py training.npy AOM.2L.fvec 11 89 AOM.2L.bed
'''

if len(sys.argv)!=6:
  sys.exit("usage:\npython3 empirical_deep_learning_classify.py classifierPickleFileName fvecFileName numSubWins numSumStatsPerSubWin bedFileName\n")
else:
  classifierPickleFileName, fvecFileName, numSubWins, numSumStatsPerSubWin, bedFileName = sys.argv[1:]

classifierPickleFileName = '[PATH_TO]/partialSHIC_DsGRP/constNe/FVs/constNe.npy'
fvecFileName = '[PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/ScA8VGg_718.regular.fvec'
numSubWins = 11
numSumStatsPerSubWin = 92
bedFileName = '[PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_constNe/ScA8VGg_718.regular.pred.bed'

netlayers=keras.models.load_model(classifierPickleFileName)
fvecFile=open(fvecFileName)
fvec=fvecFile.readlines()
fvecFile.close()
fvec=fvec[1:]
coords,fvecData=[],[]
numSubWins,numSumStatsPerSubWin = int(numSubWins),int(numSumStatsPerSubWin)

for example in fvec:
  if not "nan" in example:
    coords.append(example.strip().split("\t")[:-(numSubWins*numSumStatsPerSubWin)])
    exampleData=example.strip().split("\t")[-(numSubWins*numSumStatsPerSubWin):]
    currVector=[]
    for i in range(len(exampleData)):
      currVector.append(float(exampleData[i]))
    fvecData.append(currVector)

if not fvecData:
  sys.exit("Weird: no nan-less features in input file. Terminating...\n")

fvecData=np.reshape(np.array(fvecData),(np.array(fvecData).shape[0],numSumStatsPerSubWin,numSubWins,1))

predictions=np.argmax(netlayers.predict(fvecData),axis=1)

labelToClassName={0:"Neutral",1:"Hard",2:"Hard-linked",3:"Soft",4:"Soft-linked",5:"HardPartial",6:"HardPartial-linked",7:"SoftPartial",8:"SoftPartial-linked"}

classOrder="Neutral Hard Hard-linked Soft Soft-linked HardPartial HardPartial-linked SoftPartial SoftPartial-linked".split() # SCOTT
pred = netlayers.predict(fvecData) # SCOTT
prob = pred / pred.sum(axis=1, keepdims=True) # SCOTT
prob_df = pd.DataFrame(prob, columns=classOrder) # SCOTT

predClass = []
for i in range(len(predictions)):
  predClass.append(labelToClassName[predictions[i]])

prob_df['pred'] = predClass # SCOTT
prob_df.to_csv(bedFileName+'.prob.csv', index=False) # SCOTT

predictionCounts={}
for i in range(9):
  predictionCounts[labelToClassName[i]]=0

outlines=["track name=sweep_classifications description=\"Classification from sweeps inference tool\" visibility=2 itemRgb=On"]

classToColorStr={"Neutral": "0,0,0", "Hard":"255,0,0", "Hard-linked":"255,150,150", "Soft":"0,0,255", "Soft-linked":"150,150,255", "HardPartial":"255,120,0", "HardPartial-linked":"255,200,100", "SoftPartial":"175,0,255", "SoftPartial-linked":"200,120,230"}

if bedFileName.lower() in ["none","false","default"]:
  bedFileName=fvecFileName.split('/')[-1].replace(".fvec","")+".bed"

bedFile=open(bedFileName,"w")

for i in range(len(predictions)):
  chr,start,end=coords[i][:3]
  # start,end = int(start),int(end)
  start,end = int(float(start)),int(float(end))
  predictedClass=labelToClassName[predictions[i]]
  predictionCounts[predictedClass]+=1
  outlines.append("chr%s\t%d\t%d\t%s_%s_%s\t0\t.\t%d\t%d\t%s" %(chr, (start-1), end, predictedClass, chr, predictionCounts[predictedClass], (start-1), end, classToColorStr[predictedClass]))
  bedFile.write(outlines[i]+"\n")

bedFile.close()

sys.stderr.write("made predictions for %s total instances\n" %len(predictions))

sys.stderr.write("predicted %d neutral regions (%f of all classified regions)\n" %(predictionCounts["Neutral"],(predictionCounts["Neutral"]/float(len(predictions)))))

for i in range(1,9):
  sys.stderr.write("predicted %d %s sweep regions (%f of all classified regions)\n" %(predictionCounts[labelToClassName[i]],labelToClassName[i],(predictionCounts[labelToClassName[i]]/float(len(predictions)))))

# sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.clock()-startTime))) # Python 2
sys.stderr.write("total time spent classifying data in %s with convolutional neural network stored in %s : %f secs\n" %(fvecFileName,classifierPickleFileName,(time.perf_counter()-startTime))) # Python 3

```

```{shell, run empirical_deep_learning_classify_python3.py}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/
mkdir -p pred_empirical && cd pred_empirical
rsync -ahPv [PATH_TO]/partialSHIC_DsGRP/Empirical_Feature_Vectors/*_110Lines.phased.noPIRs.inbred.fvec ./

for CONTIG in 542 594 628 718 76 785; do
  for i in 1 2 3 4 5 6 7 8 9 10; do
    python3 [PATH_TO]/partialSHIC/empirical_deep_learning_classify_python3.py\
      [PATH_TO]/partialSHIC_DsGRP/sp2_uci/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
      ./ScA8VGg_${CONTIG}_110Lines.phased.noPIRs.inbred.fvec\
      11\
      92\
      ScA8VGg_${CONTIG}.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_fold_${i}.pred.bed >\
      empirical_deep_learning_classify_ScA8VGg_${CONTIG}_fold_${i}.log 2>&1
  done
done

```

# ######################## test on mismatched demography

## train constNe 

#### test on SP2 median 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/constNe/
  mkdir -p test_sp2_median && cd test_sp2_median
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/sp2_median/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    sp2_median_accuracy_${i}\
    sp2_median_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

#### test on SP2 2.5th 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  
  cd [PATH_TO]/partialSHIC_DsGRP/constNe/
  mkdir -p test_sp2_lci && cd test_sp2_lci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/sp2_lci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    sp2_lci_accuracy_${i}\
    sp2_lci_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
  
done

```

#### test on SP2 97.5th 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/constNe/
  mkdir -p test_sp2_uci && cd test_sp2_uci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/sp2_uci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/constNe/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    sp2_uci_accuracy_${i}\
    sp2_uci_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

## train SP2 median 

#### test on constNe 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/
  mkdir -p test_constNe && cd test_constNe
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/constNe/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_median/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

#### test on SP2 2.5th 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/
  mkdir -p test_sp2_lci && cd test_sp2_lci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/sp2_lci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_median/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    sp2_lci_accuracy_${i}\
    sp2_lci_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

#### test on SP2 97.5th 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/
  mkdir -p test_sp2_uci && cd test_sp2_uci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/sp2_uci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_median/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    sp2_uci_accuracy_${i}\
    sp2_uci_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

## train SP2 2.5th 

#### test on constNe 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/
  mkdir -p test_constNe && cd test_constNe
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/constNe/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_lci/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

#### test on SP2 median 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/
  mkdir -p test_sp2_median && cd test_sp2_median
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/sp2_median/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_lci/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    sp2_median_accuracy_${i}\
    sp2_median_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

#### test on SP2 97.5th 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_lci/
  mkdir -p test_sp2_uci && cd test_sp2_uci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/sp2_uci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_lci/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    sp2_uci_accuracy_${i}\
    sp2_uci_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

## train SP2 97.5th 

#### test on constNe 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/
  mkdir -p test_constNe && cd test_constNe
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/constNe/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_uci/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    constNe_accuracy_${i}\
    constNe_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

#### test on SP2 median 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/
  mkdir -p test_sp2_median && cd test_sp2_median
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/sp2_median/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_uci/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    sp2_median_accuracy_${i}\
    sp2_median_confusion_matrix_${i}.pdf
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

#### test on SP2 2.5th 

```{shell}

for i in 1 2 3 4 5 6 7 8 9 10; do
  cd [PATH_TO]/partialSHIC_DsGRP/sp2_uci/
  mkdir -p test_sp2_lci && cd test_sp2_lci
  mkdir -p FVs && cd FVs
  mkdir -p train_${i} && cd train_${i}
  rm ./*
  cp [PATH_TO]/partialSHIC_DsGRP/sp2_lci/FVs/*.fvec ./
  rm hard.fvec linkedHard.fvec soft.fvec linkedSoft.fvec partialHard.fvec linkedPartialHard.fvec partialSoft.fvec linkedPartialSoft.fvec neut.fvec
  
  python [PATH_TO]/partialSHIC/testing_deep_learning_classify_python3.py\
    [PATH_TO]/partialSHIC_DsGRP/sp2_uci/FVs/earlyStopping/fold_${i}_constNe_earlyStopping_fold.npy\
    ./\
    11\
    92\
    ./\
    sp2_lci_accuracy_${i}\
    sp2_lci_confusion_matrix_${i}.pdf
  
  for file in *.fvec.prob.csv; do
    mv "$file" "${file/.fvec.prob.csv/_${i}.prob.csv}"
  done
done

```

# ######################## Figure 2 ROC sweep vs other

```{shell}

module load anaconda3
conda activate r_env_4.3.1

cd [PATH_TO]/partialSHIC_DsGRP/

```

## train constNe 

```{r}

library(data.table)
library(stringr)
library(dplyr)
library(tidyr)
library(readr)
library(plotROC)
library(pROC)
library(cowplot)

setwd("[PATH_TO]/partialSHIC_DsGRP/constNe/")

ffs <- list.files(pattern = '*.prob.csv', recursive = TRUE)
ffs_test_self_newDat <- ffs[grep("test_self_newDat/FVs_earlyStopping/", ffs)]
ffs_test_sp2_median <- ffs[grep("test_sp2_median/FVs/", ffs)]
ffs_test_sp2_lci <- ffs[grep("test_sp2_lci/FVs/", ffs)]
ffs_test_sp2_uci <- ffs[grep("test_sp2_uci/FVs/", ffs)]



constNe_test_self_newDat <- data.table()

for(ff in ffs_test_self_newDat){
  fold <- gsub("test_self_newDat/FVs_earlyStopping/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs_earlyStopping/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "constNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  constNe_test_self_newDat <- rbind(constNe_test_self_newDat, tmp)
}

ensemble_test_self_newDat <- constNe_test_self_newDat %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_self_newDat$pred_ensemble <- apply(ensemble_test_self_newDat[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_self_newDat %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)




constNe_test_sp2_median <- data.table()

for(ff in ffs_test_sp2_median){
  fold <- gsub("test_sp2_median/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "constNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  constNe_test_sp2_median <- rbind(constNe_test_sp2_median, tmp)
}

ensemble_test_sp2_median <- constNe_test_sp2_median %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_sp2_median$pred_ensemble <- apply(ensemble_test_sp2_median[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_sp2_median %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



constNe_test_sp2_lci <- data.table()

for(ff in ffs_test_sp2_lci){
  fold <- gsub("test_sp2_lci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "constNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  constNe_test_sp2_lci <- rbind(constNe_test_sp2_lci, tmp)
}

ensemble_test_sp2_lci <- constNe_test_sp2_lci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_sp2_lci$pred_ensemble <- apply(ensemble_test_sp2_lci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_sp2_lci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



constNe_test_sp2_uci <- data.table()

for(ff in ffs_test_sp2_uci){
  fold <- gsub("test_sp2_uci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "constNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  constNe_test_sp2_uci <- rbind(constNe_test_sp2_uci, tmp)
}

ensemble_test_sp2_uci <- constNe_test_sp2_uci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_sp2_uci$pred_ensemble <- apply(ensemble_test_sp2_uci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_sp2_uci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



constNe <- rbind(
  ensemble_test_self_newDat,
  ensemble_test_sp2_median, 
  ensemble_test_sp2_lci, 
  ensemble_test_sp2_uci)

constNe <- constNe %>% 
  filter(ivali != "test_self") %>% 
  mutate(ivali = case_when(
    ivali=="test_self_newDat" ~ "constNe/constNe", 
    ivali=="test_sp2_median" ~ "constNe/expansionNe", 
    ivali=="test_sp2_lci" ~ "constNe/expansionNe_lci", 
    ivali=="test_sp2_uci" ~ "constNe/expansionNe_uci"
  ))

constNe <- data.table(constNe)

constNe[truth %like% '5', predClass:='sweep']
constNe[! truth %like% '5', predClass:='other']
constNe[,probSweep:=Hard+Soft+HardPartial+SoftPartial]
constNe <- constNe %>% mutate(probSweep = probSweep/10)

constNe <- constNe %>% 
  mutate(true_labels = ifelse(predClass=="sweep", 1, 0))

auc(roc(constNe$true_labels[constNe$ivali=="constNe/constNe"], constNe$probSweep[constNe$ivali=="constNe/constNe"]))
auc(roc(constNe$true_labels[constNe$ivali=="constNe/expansionNe"], constNe$probSweep[constNe$ivali=="constNe/expansionNe"]))
auc(roc(constNe$true_labels[constNe$ivali=="constNe/expansionNe_lci"], constNe$probSweep[constNe$ivali=="constNe/expansionNe_lci"]))
auc(roc(constNe$true_labels[constNe$ivali=="constNe/expansionNe_uci"], constNe$probSweep[constNe$ivali=="constNe/expansionNe_uci"]))

P1 <- ggplot(data=constNe, aes(d=predClass, m=probSweep, color=ivali)) + 
    geom_roc(labels=FALSE, pointsize=0) + 
    geom_abline(intercept=0, slope=1, lty="dashed", colour="grey") + 
    xlab("False positive rate") + 
    ylab("True positive rate") +
    scale_color_discrete(name='Test demogrpahy (AUC)', labels=c(
        'constNe (90.9%)',
        'SP2 median (89.8%)',
        'SP2 2.5th (88.7%)',
        'SP2 97.5th (89.6%)'
    )) +
    theme_classic() + 
    theme(legend.position = c(0.7, 0.2), 
          axis.text=element_text(size=10), 
          legend.title.align=0.1,
          axis.title=element_text(size=10), 
          legend.text=element_text(size=9), 
          legend.title=element_text(size=10), 
          plot.margin = margin(t = 20, b = 20))

setwd("[PATH_TO]/partialSHIC_DsGRP/")
write_tsv(constNe, "ensemble_train_constNe_test_ALL.tsv")

```

## train SP2 median 

```{r}

setwd("/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/sp2_median/")

ffs <- list.files(pattern = '*.prob.csv', recursive = TRUE)
ffs_test_self_newDat <- ffs[grep("test_self_newDat/FVs/", ffs)]
ffs_test_constNe <- ffs[grep("test_constNe/FVs/", ffs)]
ffs_test_sp2_lci <- ffs[grep("test_sp2_lci/FVs/", ffs)]
ffs_test_sp2_uci <- ffs[grep("test_sp2_uci/FVs/", ffs)]



sp2_median_test_self_newDat <- data.table()

for(ff in ffs_test_self_newDat){
  fold <- gsub("test_self_newDat/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  sp2_median_test_self_newDat <- rbind(sp2_median_test_self_newDat, tmp)
}

ensemble_test_self_newDat <- sp2_median_test_self_newDat %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_self_newDat$pred_ensemble <- apply(ensemble_test_self_newDat[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_self_newDat %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



sp2_median_test_constNe <- data.table()

for(ff in ffs_test_constNe){
  fold <- gsub("test_constNe/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  sp2_median_test_constNe <- rbind(sp2_median_test_constNe, tmp)
}

ensemble_test_constNe <- sp2_median_test_constNe %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_constNe$pred_ensemble <- apply(ensemble_test_constNe[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_constNe %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



sp2_median_test_sp2_lci <- data.table()

for(ff in ffs_test_sp2_lci){
  fold <- gsub("test_sp2_lci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  sp2_median_test_sp2_lci <- rbind(sp2_median_test_sp2_lci, tmp)
}

ensemble_test_sp2_lci <- sp2_median_test_sp2_lci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_sp2_lci$pred_ensemble <- apply(ensemble_test_sp2_lci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_sp2_lci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



sp2_median_test_sp2_uci <- data.table()

for(ff in ffs_test_sp2_uci){
  fold <- gsub("test_sp2_uci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  sp2_median_test_sp2_uci <- rbind(sp2_median_test_sp2_uci, tmp)
}

ensemble_test_sp2_uci <- sp2_median_test_sp2_uci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_sp2_uci$pred_ensemble <- apply(ensemble_test_sp2_uci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_sp2_uci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe <- rbind(
  ensemble_test_self_newDat,
  ensemble_test_constNe, 
  ensemble_test_sp2_lci, 
  ensemble_test_sp2_uci)

expansionNe <- expansionNe %>% 
  filter(ivali != "test_self") %>% 
  mutate(ivali = case_when(
    ivali=="test_self_newDat" ~ "expansionNe/expansionNe", 
    ivali=="test_constNe" ~ "expansionNe/constNe", 
    ivali=="test_sp2_lci" ~ "expansionNe/expansionNe_lci", 
    ivali=="test_sp2_uci" ~ "expansionNe/expansionNe_uci"
  ))

expansionNe <- data.table(expansionNe)

expansionNe[truth %like% '5', predClass:='sweep']
expansionNe[! truth %like% '5', predClass:='other']
expansionNe[,probSweep:=Hard+Soft+HardPartial+SoftPartial]
expansionNe <- expansionNe %>% mutate(probSweep = probSweep/10)

expansionNe <- expansionNe %>% 
  mutate(true_labels = ifelse(predClass=="sweep", 1, 0))



auc(roc(expansionNe$true_labels[expansionNe$ivali=="expansionNe/constNe"], 
        expansionNe$probSweep[expansionNe$ivali=="expansionNe/constNe"]))

auc(roc(expansionNe$true_labels[expansionNe$ivali=="expansionNe/expansionNe"], 
        expansionNe$probSweep[expansionNe$ivali=="expansionNe/expansionNe"]))

auc(roc(expansionNe$true_labels[expansionNe$ivali=="expansionNe/expansionNe_lci"], 
        expansionNe$probSweep[expansionNe$ivali=="expansionNe/expansionNe_lci"]))

auc(roc(expansionNe$true_labels[expansionNe$ivali=="expansionNe/expansionNe_uci"], 
        expansionNe$probSweep[expansionNe$ivali=="expansionNe/expansionNe_uci"]))

P2 <- ggplot(data=expansionNe, aes(d=predClass, m=probSweep, color=ivali)) + 
    geom_roc(labels=FALSE, pointsize=0) + 
    geom_abline(intercept=0, slope=1, lty="dashed", colour="grey") + 
    xlab("False positive rate") + 
    ylab("True positive rate") +
    scale_color_discrete(name='Test demography (AUC)', labels=c(
        'constNe (89.3%)',
        'SP2 median (95.7%)',
        'SP2 2.5th (92.6%)',
        'SP2 97.5th (96.1%)'
    )) +
    theme_classic() + 
    theme(legend.position = c(0.7, 0.2), 
          axis.text=element_text(size=10), 
          legend.title.align=0.1,
          axis.title=element_text(size=10), 
          legend.text=element_text(size=9), 
          legend.title=element_text(size=10), 
          plot.margin = margin(t = 20, b = 20))

setwd("[PATH_TO]/partialSHIC_DsGRP/")
write_tsv(expansionNe, "ensemble_train_sp2_median_test_ALL.tsv")

```

## train SP2 2.5th 

```{r}

setwd("[PATH_TO]/partialSHIC_DsGRP/expansionNe_intergenic_lci/")

ffs <- list.files(pattern = '*.prob.csv', recursive = TRUE)
ffs_test_self_newDat <- ffs[grep("test_self_newDat/FVs/", ffs)]
ffs_test_constNe <- ffs[grep("test_constNe/FVs/", ffs)]
ffs_test_sp2_median <- ffs[grep("test_sp2_median/FVs/", ffs)]
ffs_test_sp2_uci <- ffs[grep("test_sp2_uci/FVs/", ffs)]



expansionNe_lci_intergenic_test_self_newDat <- data.table()

for(ff in ffs_test_self_newDat){
  fold <- gsub("test_self_newDat/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_lci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_lci_intergenic_test_self_newDat <- rbind(expansionNe_lci_intergenic_test_self_newDat, tmp)
}

ensemble_test_self_newDat <- expansionNe_lci_intergenic_test_self_newDat %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_self_newDat$pred_ensemble <- apply(ensemble_test_self_newDat[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_self_newDat %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



sp2_lci_test_constNe <- data.table()

for(ff in ffs_test_constNe){
  fold <- gsub("test_constNe/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_lci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  sp2_lci_test_constNe <- rbind(sp2_lci_test_constNe, tmp)
}

ensemble_test_constNe <- sp2_lci_test_constNe %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_constNe$pred_ensemble <- apply(ensemble_test_constNe[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_constNe %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



sp2_lci_test_sp2_median <- data.table()

for(ff in ffs_test_sp2_median){
  fold <- gsub("test_sp2_median/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_lci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  sp2_lci_test_sp2_median <- rbind(sp2_lci_test_sp2_median, tmp)
}

ensemble_test_sp2_median <- sp2_lci_test_sp2_median %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_sp2_median$pred_ensemble <- apply(ensemble_test_sp2_median[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_sp2_median %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



sp2_lci_test_sp2_uci <- data.table()

for(ff in ffs_test_sp2_uci){
  fold <- gsub("test_sp2_uci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  sp2_lci_test_sp2_uci <- rbind(sp2_lci_test_sp2_uci, tmp)
}

ensemble_test_sp2_uci <- sp2_lci_test_sp2_uci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_sp2_uci$pred_ensemble <- apply(ensemble_test_sp2_uci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_sp2_uci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_lci <- rbind(
  ensemble_test_self_newDat,
  ensemble_test_constNe, 
  ensemble_test_sp2_median, 
  ensemble_test_sp2_uci)

expansionNe_lci <- expansionNe_lci %>% 
  filter(ivali != "test_self") %>% 
  mutate(ivali = case_when(
    ivali=="test_self_newDat" ~ "expansionNe_lci/expansionNe_lci", 
    ivali=="test_constNe" ~ "expansionNe_lci/constNe", 
    ivali=="test_sp2_median" ~ "expansionNe_lci/expansionNe", 
    ivali=="test_sp2_uci" ~ "expansionNe_lci/expansionNe_uci"
  ))

expansionNe_lci <- data.table(expansionNe_lci)

expansionNe_lci[truth %like% '5', predClass:='sweep']
expansionNe_lci[! truth %like% '5', predClass:='other']
expansionNe_lci[,probSweep:=Hard+Soft+HardPartial+SoftPartial]
expansionNe_lci <- expansionNe_lci %>% mutate(probSweep = probSweep/10)

expansionNe_lci <- expansionNe_lci %>% 
  mutate(true_labels = ifelse(predClass=="sweep", 1, 0))



auc(roc(expansionNe_lci$true_labels[expansionNe_lci$ivali=="expansionNe_lci/constNe"], 
        expansionNe_lci$probSweep[expansionNe_lci$ivali=="expansionNe_lci/constNe"]))

auc(roc(expansionNe_lci$true_labels[expansionNe_lci$ivali=="expansionNe_lci/expansionNe"], 
        expansionNe_lci$probSweep[expansionNe_lci$ivali=="expansionNe_lci/expansionNe"]))

auc(roc(expansionNe_lci$true_labels[expansionNe_lci$ivali=="expansionNe_lci/expansionNe_lci"], 
        expansionNe_lci$probSweep[expansionNe_lci$ivali=="expansionNe_lci/expansionNe_lci"]))

auc(roc(expansionNe_lci$true_labels[expansionNe_lci$ivali=="expansionNe_lci/expansionNe_uci"], 
        expansionNe_lci$probSweep[expansionNe_lci$ivali=="expansionNe_lci/expansionNe_uci"]))

P3 <- ggplot(data=expansionNe_lci, aes(d=predClass, m=probSweep, color=ivali)) + 
    geom_roc(labels=FALSE, pointsize=0) + 
    geom_abline(intercept=0, slope=1, lty="dashed", colour="grey") + 
    xlab("False positive rate") + 
    ylab("True positive rate") +
    scale_color_discrete(name='Test demography (AUC)', labels=c(
        'constNe (89.6%)',
        'SP2 median (95.3%)',
        'SP2 2.5th (92.5%)',
        'SP2 97.5th (95.5%)'
    )) +
    theme_classic() + 
    theme(legend.position = c(0.7, 0.2), 
          axis.text=element_text(size=10), 
          legend.title.align=0.1,
          axis.title=element_text(size=10), 
          legend.text=element_text(size=9), 
          legend.title=element_text(size=10), 
          plot.margin = margin(t = 20, b = 20))

setwd("[PATH_TO]/partialSHIC_DsGRP/")
write_tsv(expansionNe_lci, "ensemble_train_sp2_lci_test_ALL.tsv")

```

## train SP2 97.5th 

```{r}

setwd("[PATH_TO]/partialSHIC_DsGRP/expansionNe_intergenic_uci/")

ffs <- list.files(pattern = '*.prob.csv', recursive = TRUE)
ffs_test_self_newDat <- ffs[grep("test_self_newDat/FVs/", ffs)]
ffs_test_constNe <- ffs[grep("test_constNe/FVs/", ffs)]
ffs_test_sp2_median <- ffs[grep("test_sp2_median/FVs/", ffs)]
ffs_test_sp2_lci <- ffs[grep("test_sp2_lci/FVs/", ffs)]



expansionNe_uci_intergenic_test_self_newDat <- data.table()

for(ff in ffs_test_self_newDat){
  fold <- gsub("test_self_newDat/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_uci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  expansionNe_uci_intergenic_test_self_newDat <- rbind(expansionNe_uci_intergenic_test_self_newDat, tmp)
}

ensemble_test_self_newDat <- expansionNe_uci_intergenic_test_self_newDat %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_self_newDat$pred_ensemble <- apply(ensemble_test_self_newDat[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_self_newDat %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



sp2_uci_test_constNe <- data.table()

for(ff in ffs_test_constNe){
  fold <- gsub("test_constNe/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_uci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  sp2_uci_test_constNe <- rbind(sp2_uci_test_constNe, tmp)
}

ensemble_test_constNe <- sp2_uci_test_constNe %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_constNe$pred_ensemble <- apply(ensemble_test_constNe[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_constNe %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



sp2_uci_test_sp2_median <- data.table()

for(ff in ffs_test_sp2_median){
  fold <- gsub("test_sp2_median/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_uci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  sp2_uci_test_sp2_median <- rbind(sp2_uci_test_sp2_median, tmp)
}

ensemble_test_sp2_median <- sp2_uci_test_sp2_median %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_sp2_median$pred_ensemble <- apply(ensemble_test_sp2_median[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_sp2_median %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



sp2_uci_test_sp2_lci <- data.table()

for(ff in ffs_test_sp2_lci){
  fold <- gsub("test_sp2_lci/FVs/", "", ff)
  fold <- gsub("/.*.prob.csv", "", fold)
  Vali <- gsub("/FVs/train_\\d+/sp.*.prob.csv", "", ff)
  tmp <- fread(ff, header = TRUE, sep = ',') %>%
    mutate(iclassifier = "expansionNe_uci") %>%
    mutate(ivali = Vali) %>% 
    mutate(fold = fold) %>% 
    mutate(truth = str_replace(truth, ".fvec", ""))
  tmp$sim <- c(1:nrow(tmp))
  sp2_uci_test_sp2_lci <- rbind(sp2_uci_test_sp2_lci, tmp)
}

ensemble_test_sp2_lci <- sp2_uci_test_sp2_lci %>% 
  group_by(truth, iclassifier, ivali, sim) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble_test_sp2_lci$pred_ensemble <- apply(ensemble_test_sp2_lci[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

ensemble_test_sp2_lci %>% group_by(truth, iclassifier, ivali) %>% tally() %>% print(n=45)



expansionNe_uci <- rbind(
  ensemble_test_self_newDat,
  ensemble_test_constNe, 
  ensemble_test_sp2_median, 
  ensemble_test_sp2_lci)

expansionNe_uci <- expansionNe_uci %>% 
  filter(ivali != "test_self") %>% 
  mutate(ivali = case_when(
    ivali=="test_self_newDat" ~ "expansionNe_uci/expansionNe_uci", 
    ivali=="test_constNe" ~ "expansionNe_uci/constNe", 
    ivali=="test_sp2_median" ~ "expansionNe_uci/expansionNe", 
    ivali=="test_sp2_lci" ~ "expansionNe_uci/expansionNe_lci"
  ))

expansionNe_uci <- data.table(expansionNe_uci)

expansionNe_uci[truth %like% '5', predClass:='sweep']
expansionNe_uci[! truth %like% '5', predClass:='other']
expansionNe_uci[,probSweep:=Hard+Soft+HardPartial+SoftPartial]
expansionNe_uci <- expansionNe_uci %>% mutate(probSweep = probSweep/10)

expansionNe_uci <- expansionNe_uci %>% 
  mutate(true_labels = ifelse(predClass=="sweep", 1, 0))



auc(roc(expansionNe_uci$true_labels[expansionNe_uci$ivali=="expansionNe_uci/constNe"], 
        expansionNe_uci$probSweep[expansionNe_uci$ivali=="expansionNe_uci/constNe"]))

auc(roc(expansionNe_uci$true_labels[expansionNe_uci$ivali=="expansionNe_uci/expansionNe"], 
        expansionNe_uci$probSweep[expansionNe_uci$ivali=="expansionNe_uci/expansionNe"]))

auc(roc(expansionNe_uci$true_labels[expansionNe_uci$ivali=="expansionNe_uci/expansionNe_lci"], 
        expansionNe_uci$probSweep[expansionNe_uci$ivali=="expansionNe_uci/expansionNe_lci"]))

auc(roc(expansionNe_uci$true_labels[expansionNe_uci$ivali=="expansionNe_uci/expansionNe_uci"], 
        expansionNe_uci$probSweep[expansionNe_uci$ivali=="expansionNe_uci/expansionNe_uci"]))

P4 <- ggplot(data=expansionNe_uci, aes(d=predClass, m=probSweep, color=ivali)) + 
    geom_roc(labels=FALSE, pointsize=0) + 
    geom_abline(intercept=0, slope=1, lty="dashed", colour="grey") + 
    xlab("False positive rate") + 
    ylab("True positive rate") +
    scale_color_discrete(name='Test demography (AUC)', labels=c(
        'constNe (88.5%)',
        'SP2 median (95.4%)',
        'SP2 2.5th (91.9%)',
        'SP2 97.5th (96.4%)'
    )) +
    theme_classic() + 
    theme(legend.position = c(0.7, 0.2), 
          axis.text=element_text(size=10), 
          legend.title.align=0.1,
          axis.title=element_text(size=10), 
          legend.text=element_text(size=9), 
          legend.title=element_text(size=10), 
          plot.margin = margin(t = 20, b = 20))

setwd("[PATH_TO]/partialSHIC_DsGRP/")
write_tsv(expansionNe_uci, "ensemble_train_sp2_uci_test_ALL.tsv")

```

## plot grid

```{r}

# plot grid

plot_grid(P1, P2, P3, P4, 
          nrow=2, 
          labels=c('A) Train demography constNe',
                   'B) Train demography SP2 median', 
                   'C) Train demography SP2 2.5th quantile',
                   'D) Train demography SP2 97.5th quantile'), 
          label_size = 10, 
          hjust = 0, 
          vjust = 1)

# save pdf

setwd("[PATH_TO]/partialSHIC_DsGRP/")

ggsave('Figure_2_ROC_SCOTT_4.pdf', width=10, height=9, unit='in', dpi=600)

# save png

setwd("[PATH_TO]/partialSHIC_DsGRP/")

ggsave('Figure_2_ROC_SCOTT_4.png', width=10, height=9, unit='in', dpi=600)

```

# ######################## Figure 3 Confusion matrix Analysis

```{shell}

module load anaconda3
conda activate r_env_4.3.1

cd [PATH_TO]/partialSHIC_DsGRP/

```

```{r}

library(tidyverse)
library(data.table)

setwd("[PATH_TO]/partialSHIC_DsGRP/")

ensemble_constNe <- fread("ensemble_train_constNe_test_ALL.tsv")
ensemble_sp2_median <- fread("ensemble_train_sp2_median_test_ALL.tsv")
ensemble_sp2_lci <- fread("ensemble_train_sp2_lci_test_ALL.tsv")
ensemble_sp2_uci <- fread("ensemble_train_sp2_uci_test_ALL.tsv")

probs <- rbind(
  ensemble_constNe, 
  ensemble_sp2_median, 
  ensemble_sp2_lci, 
  ensemble_sp2_uci
  )

probs <- probs %>% mutate(window = gsub(".*_", "", truth))

# fix Neut window
probs <- probs %>% mutate(window = ifelse(truth=="spNeut", 5, window))

# create truth2 to match predicted class
probs <- probs %>% 
  mutate(truth2 = case_when(
    str_detect(truth, "^spHard_") & window!=5 ~ "Hard-linked", 
    str_detect(truth, "^spHard_") & window==5 ~ "Hard", 
    str_detect(truth, "^spSoft_") & window!=5 ~ "Soft-linked", 
    str_detect(truth, "^spSoft_") & window==5 ~ "Soft",
    str_detect(truth, "^spPartialHard_") & window!=5 ~ "HardPartial-linked", 
    str_detect(truth, "^spPartialHard_") & window==5 ~ "HardPartial", 
    str_detect(truth, "^spPartialSoft_") & window!=5 ~ "SoftPartial-linked", 
    str_detect(truth, "^spPartialSoft_") & window==5 ~ "SoftPartial", 
    str_detect(truth, "^spNeut") ~ "Neutral"
  )) 

# Create a new column "pred" with the highest probability class
probs$pred <- apply(probs[,5:13], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})

# select needed columns
probs <- probs %>% 
  rename(train = iclassifier, test = ivali) %>% 
  select(train, test, truth, truth2, pred, window)

# standardise test name
probs <- probs %>% 
  mutate(test = ifelse(test=="test_expansion", "test_sp2_median", test))

# standardise window
probs <- probs %>% 
  mutate(window = case_when(
    window==5 ~ 100,
    window==0 ~ 5, 
    window==1 ~ 4, 
    window==2 ~ 3, 
    window==3 ~ 2, 
    window==4 ~ 1, 
    window==10 ~ 5, 
    window==9 ~ 4, 
    window==8 ~ 3, 
    window==7 ~ 2, 
    window==6 ~ 1, 
  ))

# calculate accuracy

probs_OG <- probs

probs_windows <- probs %>% 
  filter(window!=100) %>% 
  group_by(train, test, truth2, pred, window) %>%
  summarise(n=n()) %>% 
  mutate(accuracy=n/800)

probs_windows <- probs_windows %>% ungroup()

probs <- probs %>% 
  group_by(train, test, truth2, pred) %>% 
  summarise(n=n())

temp <- probs %>% 
  group_by(train, test, truth2) %>% 
  summarise(accuracy=n/sum(n))

probs$accuracy <- temp$accuracy

probs <- probs %>% ungroup()

```

## Hard sweeps 

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialSoft.col <- RColorBrewer::brewer.pal(9,"Purples")
my.hard.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHard.col <- RColorBrewer::brewer.pal(9, 'Greens')

# across matched/mismatched

probs %>%
  dplyr::filter(truth2=="Hard") %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Hard" ~ "True positive",
    TRUE ~ "False positive")) %>% 
  mutate(pred = case_when(
    pred=="Hard" ~ "H",
    pred=="Hard-linked" ~ "HL",
    pred=="HardPartial" ~ "HP",
    pred=="HardPartial-linked" ~ "HPL",
    pred=="Neutral" ~ "N",
    pred=="Soft" ~ "S",
    pred=="Soft-linked" ~ "SL",
    pred=="SoftPartial" ~ "SP",
    pred=="SoftPartial-linked" ~ "SPL"
  )) %>%
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  scale_fill_manual(values=c(my.soft.col[6], my.hard.col[6])) +
  theme_classic() +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Hard",
       x = "Prediction",
       y = "Proportion (median +/- IQR)",
       fill = "") ->
hard_error

hard_error

```

```{r}

probs %>%
  filter(truth2=="Hard") %>% 
  select(test, pred, n) %>% 
  pivot_wider(names_from=pred, values_from=n, values_fill=0)

probs %>%
  filter(truth2=="Hard") %>% 
  select(test, pred, accuracy) %>% 
  pivot_wider(names_from=pred, values_from=accuracy, values_fill=0)

anova_results_all %>% filter(truth=="Hard") %>% arrange(pred, model)
anova_results_matched %>% filter(truth=="Hard") %>% arrange(pred, model)

pred_of_interest <- "Hard-linked"

probs %>% 
  filter(truth2=="Hard") %>% 
  filter(pred==pred_of_interest)  %>% 
  mutate(test = str_replace(test, "constNe/", "")) %>% 
  mutate(test = str_replace(test, "sp2_median/", "")) %>% 
  mutate(test = str_replace(test, "sp2_lci/", "")) %>% 
  mutate(test = str_replace(test, "sp2_uci/", "")) %>% 
  mutate(matched = ifelse(train==test, "matched", "mismatched")) %>% 
  ggplot(aes(x=test, y=n)) + 
  geom_boxplot() + 
  geom_point()

probs %>% 
  filter(truth2=="Hard") %>% 
  filter(pred==pred_of_interest)  %>% 
  mutate(test = str_replace(test, "constNe/", "")) %>% 
  mutate(test = str_replace(test, "sp2_median/", "")) %>% 
  mutate(test = str_replace(test, "sp2_lci/", "")) %>% 
  mutate(test = str_replace(test, "sp2_uci/", "")) %>% 
  mutate(matched = ifelse(train==test, "matched", "mismatched")) %>% 
  group_by(test) %>% 
  skimr::skim()

probs %>% 
  filter(truth2=="Hard") %>% 
  filter(pred=="Soft")  %>% 
  mutate(test = str_replace(test, "constNe/", "")) %>% 
  mutate(test = str_replace(test, "sp2_median/", "")) %>% 
  mutate(test = str_replace(test, "sp2_lci/", "")) %>% 
  mutate(test = str_replace(test, "sp2_uci/", "")) %>% 
  mutate(matched = ifelse(train==test, "matched", "mismatched")) %>% 
  group_by(matched) %>% 
  skimr::skim()

probs %>% 
  dplyr::filter(truth2=="Hard") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Hard" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

```

## Soft sweeps 

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialSoft.col <- RColorBrewer::brewer.pal(9,"Purples")
my.Soft.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialSoft.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>%
  dplyr::filter(truth2=="Soft") %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Soft" ~ "True positive",
    TRUE ~ "False positive")) %>%
  mutate(pred = case_when(
    pred=="Hard" ~ "H",
    pred=="Hard-linked" ~ "HL",
    pred=="HardPartial" ~ "HP",
    pred=="HardPartial-linked" ~ "HPL",
    pred=="Neutral" ~ "N",
    pred=="Soft" ~ "S",
    pred=="Soft-linked" ~ "SL",
    pred=="SoftPartial" ~ "SP",
    pred=="SoftPartial-linked" ~ "SPL"
  )) %>%
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  scale_fill_manual(values=c(my.soft.col[6], my.Soft.col[6])) +
  theme_classic() +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Soft",
       x = "Prediction",
       y = "Proportion (median +/- IQR)",
       fill = "") ->
soft_error

soft_error

```

```{r}

probs %>%
  filter(truth2=="Soft") %>% 
  select(test, pred, n) %>% 
  pivot_wider(names_from=pred, values_from=n, values_fill=0)

probs %>%
  filter(truth2=="Soft") %>% 
  select(test, pred, accuracy) %>% 
  pivot_wider(names_from=pred, values_from=accuracy, values_fill=0)

pred_of_interest <- "Hard-linked"

probs %>% 
  filter(truth2=="Hard") %>% 
  filter(pred==pred_of_interest)  %>% 
  mutate(test = str_replace(test, "constNe/", "")) %>% 
  mutate(test = str_replace(test, "sp2_median/", "")) %>% 
  mutate(test = str_replace(test, "sp2_lci/", "")) %>% 
  mutate(test = str_replace(test, "sp2_uci/", "")) %>% 
  mutate(matched = ifelse(train==test, "matched", "mismatched")) %>% 
  ggplot(aes(x=test, y=n)) + 
  geom_boxplot() + 
  geom_point()

probs %>% 
  filter(truth2=="Hard") %>% 
  filter(pred==pred_of_interest)  %>% 
  mutate(test = str_replace(test, "constNe/", "")) %>% 
  mutate(test = str_replace(test, "sp2_median/", "")) %>% 
  mutate(test = str_replace(test, "sp2_lci/", "")) %>% 
  mutate(test = str_replace(test, "sp2_uci/", "")) %>% 
  mutate(matched = ifelse(train==test, "matched", "mismatched")) %>% 
  group_by(test) %>% 
  skimr::skim()

probs %>% 
  filter(truth2=="Hard") %>% 
  filter(pred=="Soft")  %>% 
  mutate(test = str_replace(test, "constNe/", "")) %>% 
  mutate(test = str_replace(test, "sp2_median/", "")) %>% 
  mutate(test = str_replace(test, "sp2_lci/", "")) %>% 
  mutate(test = str_replace(test, "sp2_uci/", "")) %>% 
  mutate(matched = ifelse(train==test, "matched", "mismatched")) %>% 
  group_by(matched) %>% 
  skimr::skim()

probs %>% 
  dplyr::filter(truth2=="Soft") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Soft" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

```

## HardPartial 

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>%
  dplyr::filter(truth2=="HardPartial") %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="HardPartial" ~ "True positive",
    TRUE ~ "False positive")) %>%
  mutate(pred = case_when(
    pred=="Hard" ~ "H",
    pred=="Hard-linked" ~ "HL",
    pred=="HardPartial" ~ "HP",
    pred=="HardPartial-linked" ~ "HPL",
    pred=="Neutral" ~ "N",
    pred=="Soft" ~ "S",
    pred=="Soft-linked" ~ "SL",
    pred=="SoftPartial" ~ "SP",
    pred=="SoftPartial-linked" ~ "SPL"
  )) %>%
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  scale_fill_manual(values=c(my.soft.col[6], my.HardPartial.col[6])) +
  theme_classic() +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "HardPartial",
       x = "Prediction",
       y = "Proportion (median +/- IQR)",
       fill = "") ->
hardpartial_error

hardpartial_error

```

```{r}

probs %>%
  filter(truth2=="HardPartial") %>% 
  select(test, pred, n) %>% 
  pivot_wider(names_from=pred, values_from=n, values_fill=0)

probs %>%
  filter(truth2=="HardPartial") %>% 
  select(test, pred, accuracy) %>% 
  pivot_wider(names_from=pred, values_from=accuracy, values_fill=0)

pred_of_interest <- "Hard-linked"

probs %>% 
  filter(truth2=="Hard") %>% 
  filter(pred==pred_of_interest)  %>% 
  mutate(test = str_replace(test, "constNe/", "")) %>% 
  mutate(test = str_replace(test, "sp2_median/", "")) %>% 
  mutate(test = str_replace(test, "sp2_lci/", "")) %>% 
  mutate(test = str_replace(test, "sp2_uci/", "")) %>% 
  mutate(matched = ifelse(train==test, "matched", "mismatched")) %>% 
  ggplot(aes(x=test, y=n)) + 
  geom_boxplot() + 
  geom_point()

probs %>% 
  filter(truth2=="Hard") %>% 
  filter(pred==pred_of_interest)  %>% 
  mutate(test = str_replace(test, "constNe/", "")) %>% 
  mutate(test = str_replace(test, "sp2_median/", "")) %>% 
  mutate(test = str_replace(test, "sp2_lci/", "")) %>% 
  mutate(test = str_replace(test, "sp2_uci/", "")) %>% 
  mutate(matched = ifelse(train==test, "matched", "mismatched")) %>% 
  group_by(test) %>% 
  skimr::skim()

probs %>% 
  dplyr::filter(truth2=="HardPartial") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="HardPartial" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

```

## SoftPartial 

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialSoftPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.SoftPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialSoftPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>%
  dplyr::filter(truth2=="SoftPartial") %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="SoftPartial" ~ "True positive",
    TRUE ~ "False positive")) %>%
   mutate(pred = case_when(
    pred=="Hard" ~ "H",
    pred=="Hard-linked" ~ "HL",
    pred=="HardPartial" ~ "HP",
    pred=="HardPartial-linked" ~ "HPL",
    pred=="Neutral" ~ "N",
    pred=="Soft" ~ "S",
    pred=="Soft-linked" ~ "SL",
    pred=="SoftPartial" ~ "SP",
    pred=="SoftPartial-linked" ~ "SPL"
  )) %>%
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  scale_fill_manual(values=c(my.soft.col[6], my.SoftPartial.col[6])) +
  theme_classic() +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "SoftPartial",
       x = "Prediction",
       y = "Proportion (median +/- IQR)",
       fill = "") ->
softpartial_error

softpartial_error

```

```{r}

probs %>%
  filter(truth2=="SoftPartial") %>% 
  select(test, pred, n) %>% 
  pivot_wider(names_from=pred, values_from=n, values_fill=0)

probs %>%
  filter(truth2=="SoftPartial") %>% 
  select(test, pred, accuracy) %>% 
  pivot_wider(names_from=pred, values_from=accuracy, values_fill=0)

pred_of_interest <- "Hard-linked"

probs %>% 
  filter(truth2=="Hard") %>% 
  filter(pred==pred_of_interest)  %>% 
  mutate(test = str_replace(test, "constNe/", "")) %>% 
  mutate(test = str_replace(test, "sp2_median/", "")) %>% 
  mutate(test = str_replace(test, "sp2_lci/", "")) %>% 
  mutate(test = str_replace(test, "sp2_uci/", "")) %>% 
  mutate(matched = ifelse(train==test, "matched", "mismatched")) %>% 
  ggplot(aes(x=test, y=n)) + 
  geom_boxplot() + 
  geom_point()

probs %>% 
  filter(truth2=="Hard") %>% 
  filter(pred==pred_of_interest)  %>% 
  mutate(test = str_replace(test, "constNe/", "")) %>% 
  mutate(test = str_replace(test, "sp2_median/", "")) %>% 
  mutate(test = str_replace(test, "sp2_lci/", "")) %>% 
  mutate(test = str_replace(test, "sp2_uci/", "")) %>% 
  mutate(matched = ifelse(train==test, "matched", "mismatched")) %>% 
  group_by(test) %>% 
  skimr::skim()

probs %>% 
  dplyr::filter(truth2=="SoftPartial") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="SoftPartial" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

```

## Hard-linked 

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

# Initialize a list to store the plot objects
hardlinked_error_list <- list()

# Initialise list for distince
dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")

# Loop through the window values and create the plots
for (i in 1:5) {
  plot <- probs_windows %>%
    dplyr::filter(truth2 == "Hard-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "Hard-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    mutate(pred = case_when(
      pred=="Hard" ~ "H",
      pred=="Hard-linked" ~ "HL",
      pred=="HardPartial" ~ "HP",
      pred=="HardPartial-linked" ~ "HPL",
      pred=="Neutral" ~ "N",
      pred=="Soft" ~ "S",
      pred=="Soft-linked" ~ "SL",
      pred=="SoftPartial" ~ "SP",
      pred=="SoftPartial-linked" ~ "SPL"
    )) %>%
    ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
    geom_bar(stat = "identity", position = "identity") +
    geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
    scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
    scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
    theme_classic() +
    theme(
      legend.position = "bottom",
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      strip.text = element_text(size = 8)
    ) +
    guides(fill = guide_legend(reverse = TRUE)) +
    labs(
      title = paste("Hard-linked", dist_list[i], "from target", sep=" "),
      x = "Prediction",
      y = "Proportion (median +/- IQR)",
      fill = ""
    )
  
  # Add the plot to the list
  hardlinked_error_list[[i]] <- plot
}

```

```{r, fig.width=4, fig.height=4}

# Access the individual plots using plot_list[[1]], plot_list[[2]], etc.
hardlinked_error_list

```

```{r}

for (i in 1:5) {
  print(probs_windows %>%
    dplyr::filter(truth2=="Hard-linked") %>% 
    dplyr::filter(window == i) %>% 
    select(test, pred, n) %>% 
    pivot_wider(names_from=pred, values_from=n, values_fill=0))
}

for (i in 1:5) {
  print(probs_windows %>%
    dplyr::filter(truth2=="Hard-linked") %>% 
    dplyr::filter(window == i) %>% 
    select(test, pred, accuracy) %>% 
    pivot_wider(names_from=pred, values_from=accuracy, values_fill=0))
}

tbl_list = list()

for (i in 1:5) {
  tbl <- probs_windows %>%
    dplyr::filter(truth2 == "Hard-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "Hard-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    dplyr::arrange(desc(median))
  tbl_list[[i]] <- tbl
}

tbl_list

```

```{r, fig.width=7, fig.height=7}

probs_windows %>%
  dplyr::filter(truth2=="Hard-linked") %>%
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive",
    pred!=truth2 ~ "False positive"
  )) %>%
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>%
  ggplot2::ggplot(aes(x=pred, y=accuracy, fill=window, group=window)) +
  geom_bar(stat="identity", position="identity") +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  facet_wrap(~window) +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Simulated Hard-linked",
       x = "Prediction",
       y = "Proportion",
       fill = "")

```

```{r}

probs_windows %>%
  dplyr::filter(truth2=="Hard-linked") %>%
  dplyr::filter(window==1) %>%
  # dplyr::filter(pred!="Hard-linked") %>%
  dplyr::filter(train=="constNe") %>%
  dplyr::filter(test=="test_constNe") %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  skimr::skim() %>%
  as.data.frame() %>%
  arrange(desc(numeric.mean))

```

```{r}

probs %>% 
  dplyr::filter(truth2=="Hard-linked") %>% 
  # dplyr::filter(pred!="Hard-linked") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  skimr::skim() %>% 
  as.data.frame() %>% 
  arrange(desc(numeric.mean))

```

## Soft-linked 

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

# Initialize a list to store the plot objects
softlinked_error_list <- list()

# Initialise list for distince
dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")

# Loop through the window values and create the plots
for (i in 1:5) {
  plot <- probs_windows %>%
    dplyr::filter(truth2 == "Soft-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "Soft-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    mutate(pred = case_when(
      pred=="Hard" ~ "H",
      pred=="Hard-linked" ~ "HL",
      pred=="HardPartial" ~ "HP",
      pred=="HardPartial-linked" ~ "HPL",
      pred=="Neutral" ~ "N",
      pred=="Soft" ~ "S",
      pred=="Soft-linked" ~ "SL",
      pred=="SoftPartial" ~ "SP",
      pred=="SoftPartial-linked" ~ "SPL"
    )) %>%
    ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
    geom_bar(stat = "identity", position = "identity") +
    geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
    scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
    scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
    theme_classic() +
    theme(
      legend.position = "bottom",
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      strip.text = element_text(size = 8)
    ) +
    guides(fill = guide_legend(reverse = TRUE)) +
    labs(
      title = paste("Soft-linked", dist_list[i], "from target", sep=" "),
      x = "Prediction",
      y = "Proportion (median +/- IQR)",
      fill = ""
    )

  # Add the plot to the list
  softlinked_error_list[[i]] <- plot
}

```

```{r, fig.width=4, fig.height=4}

# Access the individual plots using plot_list[[1]], plot_list[[2]], etc.
softlinked_error_list

```

```{r}

for (i in 1:5) {
  print(probs_windows %>%
    dplyr::filter(truth2=="Soft-linked") %>% 
    dplyr::filter(window == i) %>% 
    select(test, pred, n) %>% 
    pivot_wider(names_from=pred, values_from=n, values_fill=0) %>% 
    select(test, Hard, `Hard-linked`, HardPartial, `HardPartial-linked`, Neutral, Soft, `Soft-linked`, SoftPartial, `SoftPartial-linked`))
}

for (i in 1:5) {
  print(probs_windows %>%
    dplyr::filter(truth2=="Soft-linked") %>% 
    dplyr::filter(window == i) %>% 
    select(test, pred, accuracy) %>% 
    pivot_wider(names_from=pred, values_from=accuracy, values_fill=0))
}

tbl_list = list()

for (i in 1:5) {
  tbl <- probs_windows %>%
    dplyr::filter(truth2 == "Soft-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "HardPartial-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    dplyr::arrange(desc(median))
  tbl_list[[i]] <- tbl
}
tbl_list

```

## HardPartial-linked 

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

# Initialize a list to store the plot objects
hardpartiallinked_error_list <- list()

# Initialise list for distince
dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")

# Loop through the window values and create the plots
for (i in 1:5) {
  plot <- probs_windows %>%
    dplyr::filter(truth2 == "HardPartial-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "HardPartial-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    mutate(pred = case_when(
      pred=="Hard" ~ "H",
      pred=="Hard-linked" ~ "HL",
      pred=="HardPartial" ~ "HP",
      pred=="HardPartial-linked" ~ "HPL",
      pred=="Neutral" ~ "N",
      pred=="Soft" ~ "S",
      pred=="Soft-linked" ~ "SL",
      pred=="SoftPartial" ~ "SP",
      pred=="SoftPartial-linked" ~ "SPL"
    )) %>%
    ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
    geom_bar(stat = "identity", position = "identity") +
    geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
    scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
    scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
    theme_classic() +
    theme(
      legend.position = "bottom",
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      strip.text = element_text(size = 8)
    ) +
    guides(fill = guide_legend(reverse = TRUE)) +
    labs(
      title = paste("HardPartial-linked", dist_list[i], "from target", sep=" "),
      x = "Prediction",
      y = "Proportion (median +/- IQR)",
      fill = ""
    )

  # Add the plot to the list
  hardpartiallinked_error_list[[i]] <- plot
}

```

```{r, fig.width=4, fig.height=4}

# Access the individual plots using plot_list[[1]], plot_list[[2]], etc.
hardpartiallinked_error_list

```

```{r}

for (i in 1:5) {
  print(probs_windows %>%
    dplyr::filter(truth2=="HardPartial-linked") %>% 
    dplyr::filter(window == i) %>% 
    select(test, pred, n) %>% 
    pivot_wider(names_from=pred, values_from=n, values_fill=0) %>% 
    select(test, Hard, `Hard-linked`, HardPartial, `HardPartial-linked`, Neutral, Soft, `Soft-linked`, SoftPartial, `SoftPartial-linked`))
}

for (i in 1:5) {
  print(probs_windows %>%
    dplyr::filter(truth2=="HardPartial-linked") %>% 
    dplyr::filter(window == i) %>% 
    select(test, pred, accuracy) %>% 
    pivot_wider(names_from=pred, values_from=accuracy))
}

tbl_list = list()

for (i in 1:5) {
  tbl <- probs_windows %>%
    dplyr::filter(truth2 == "HardPartial-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "HardPartial-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    dplyr::arrange(desc(median))
  tbl_list[[i]] <- tbl
}
tbl_list

```

## SoftPartial-linked 

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9,"Purples")
my.HardPartial.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialHardPartial.col <- RColorBrewer::brewer.pal(9, 'Greens')

# Initialize a list to store the plot objects
softpartiallinked_error_list <- list()

# Initialise list for distince
dist_list <- c("5kb", "10kb", "15kb", "20kb", "25kb")

# Loop through the window values and create the plots
for (i in 1:5) {
  plot <- probs_windows %>%
    dplyr::filter(truth2 == "SoftPartial-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "SoftPartial-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    mutate(pred = case_when(
      pred=="Hard" ~ "H",
      pred=="Hard-linked" ~ "HL",
      pred=="HardPartial" ~ "HP",
      pred=="HardPartial-linked" ~ "HPL",
      pred=="Neutral" ~ "N",
      pred=="Soft" ~ "S",
      pred=="Soft-linked" ~ "SL",
      pred=="SoftPartial" ~ "SP",
      pred=="SoftPartial-linked" ~ "SPL"
    )) %>%
    ggplot2::ggplot(aes(x = pred, y = median, fill = Classification)) +
    geom_bar(stat = "identity", position = "identity") +
    geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
    scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +
    scale_fill_manual(values = c(my.soft.col[6], my.HardPartial.col[6])) +
    theme_classic() +
    theme(
      legend.position = "bottom",
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
      strip.text = element_text(size = 8)
    ) +
    guides(fill = guide_legend(reverse = TRUE)) +
    labs(
      title = paste("SoftPartial-linked", dist_list[i], "from target", sep=" "),
      x = "Prediction",
      y = "Proportion (median +/- IQR)",
      fill = ""
    )

  # Add the plot to the list
  softpartiallinked_error_list[[i]] <- plot
}

```

```{r, fig.width=4, fig.height=4}

# Access the individual plots using plot_list[[1]], plot_list[[2]], etc.
softpartiallinked_error_list

```

```{r}

for (i in 1:5) {
  print(probs_windows %>%
    dplyr::filter(truth2=="SoftPartial-linked") %>% 
    dplyr::filter(window == i) %>% 
    select(test, pred, n) %>% 
    pivot_wider(names_from=pred, values_from=n, values_fill=0) %>% 
    select(test, Hard, `Hard-linked`, HardPartial, `HardPartial-linked`, Neutral, Soft, `Soft-linked`, SoftPartial, `SoftPartial-linked`))
}

for (i in 1:5) {
  print(probs_windows %>%
    dplyr::filter(truth2=="SoftPartial-linked") %>% 
    dplyr::filter(window == i) %>% 
    select(test, pred, accuracy) %>% 
    pivot_wider(names_from=pred, values_from=accuracy))
}

tbl_list = list()

for (i in 1:5) {
  tbl <- probs_windows %>%
    dplyr::filter(truth2 == "SoftPartial-linked") %>%
    dplyr::filter(window == i) %>%
    dplyr::select(pred, accuracy) %>%
    dplyr::group_by(pred) %>%
    dplyr::summarise(median = median(accuracy),
                     iqr = IQR(accuracy),
                     lowerbar = median - iqr,
                     upperbar = median + iqr) %>%
    dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                  upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
    dplyr::mutate(Classification = dplyr::case_when(
      pred == "SoftPartial-linked" ~ "True positive",
      TRUE ~ "False positive"
    )) %>%
    dplyr::arrange(desc(median))
  tbl_list[[i]] <- tbl
}
tbl_list

```

## Neutral 

```{r, fig.width=4, fig.height=4}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialNeutral.col <- RColorBrewer::brewer.pal(9,"Purples")
my.Neutral.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialNeutral.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>%
  dplyr::filter(truth2=="Neutral") %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Neutral" ~ "True positive",
    TRUE ~ "False positive")) %>%
  mutate(pred = case_when(
      pred=="Hard" ~ "H",
      pred=="Hard-linked" ~ "HL",
      pred=="HardPartial" ~ "HP",
      pred=="HardPartial-linked" ~ "HPL",
      pred=="Neutral" ~ "N",
      pred=="Soft" ~ "S",
      pred=="Soft-linked" ~ "SL",
      pred=="SoftPartial" ~ "SP",
      pred=="SoftPartial-linked" ~ "SPL"
    )) %>%
  ggplot2::ggplot(aes(x=pred, y=median, fill=Classification)) +
  geom_bar(stat="identity", position="identity") +
  geom_errorbar(aes(ymin = lowerbar, ymax = upperbar, width = 0.2)) +
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) +
  scale_fill_manual(values=c(my.soft.col[6], my.Neutral.col[6])) +
  theme_classic() +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle = 45, hjust = 1, size=8),
        strip.text = element_text(size = 8)) +
  guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "Neutral",
       x = "Prediction",
       y = "Proportion (median +/- IQR)",
       fill = "") ->
neutral_error

neutral_error

```

```{r}

probs %>%
  filter(truth2=="Neutral") %>% 
  select(test, pred, n) %>% 
  pivot_wider(names_from=pred, values_from=n, values_fill=0)

probs %>%
  filter(truth2=="Neutral") %>% 
  select(test, pred, accuracy) %>% 
  pivot_wider(names_from=pred, values_from=accuracy, values_fill=0)

probs %>% 
  dplyr::filter(truth2=="Neutral") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Hard" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

```

## Stats 

```{r}

## Sweeps

probs %>% 
  dplyr::filter(truth2=="Neutral") %>% 
  dplyr::select(pred, accuracy) %>% 
  dplyr::group_by(pred) %>% 
  dplyr::summarise(median = median(accuracy), 
                   iqr = IQR(accuracy), 
                   lowerbar = median - iqr, 
                   upperbar = median + iqr) %>% 
  dplyr::mutate(Classification = dplyr::case_when(
    pred=="Neutral" ~ "True positive", 
    TRUE ~ "False positive")) %>% 
  dplyr::arrange(desc(median))

## Hard/HardPartial-Linked

probs_windows %>%
  dplyr::filter(truth2 == "HardPartial-linked") %>%
  dplyr::select(pred, window, accuracy) %>%
  dplyr::group_by(pred, window) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "HardPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>% 
  as.data.frame()

probs_windows_noExp %>%
  dplyr::filter(truth2 == "HardPartial-linked") %>%
  dplyr::select(pred, window, accuracy) %>%
  dplyr::group_by(pred, window) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "HardPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>% 
  as.data.frame()


probs_windows %>%
  dplyr::filter(truth2 == "Hard-linked") %>%
  dplyr::filter(window == 1) %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "Hard-linked" ~ "True positive",
    TRUE ~ "False positive"
  ))

probs_windows %>%
  dplyr::filter(truth2 == "Hard-linked") %>%
  dplyr::filter(window == 5) %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "Hard-linked" ~ "True positive",
    TRUE ~ "False positive"
  ))

## Soft/SoftPartial-Linked

probs_windows %>%
  dplyr::filter(truth2 == "Soft-linked") %>%
  dplyr::select(pred, window, accuracy) %>%
  dplyr::group_by(pred, window) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "Soft-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>% 
  as.data.frame()

probs_windows %>%
  dplyr::filter(truth2 == "SoftPartial-linked") %>%
  dplyr::select(pred, window, accuracy) %>%
  dplyr::group_by(pred, window) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>% 
  as.data.frame()

probs_windows_noExp %>%
  dplyr::filter(truth2 == "SoftPartial-linked") %>%
  dplyr::select(pred, window, accuracy) %>%
  dplyr::group_by(pred, window) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  )) %>% 
  as.data.frame()

probs_windows %>%
  dplyr::filter(truth2 == "SoftPartial-linked") %>%
  dplyr::filter(window == 1) %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  ))

probs_windows %>%
  dplyr::filter(truth2 == "SoftPartial-linked") %>%
  dplyr::filter(window == 3) %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  ))

probs_windows %>%
  dplyr::filter(truth2 == "SoftPartial-linked") %>%
  dplyr::filter(window == 5) %>%
  dplyr::select(pred, accuracy) %>%
  dplyr::group_by(pred) %>%
  dplyr::summarise(median = median(accuracy),
                   iqr = IQR(accuracy),
                   lowerbar = median - iqr,
                   upperbar = median + iqr) %>%
  dplyr::mutate(lowerbar = base::ifelse(lowerbar < 0, 0, lowerbar),
                upperbar = base::ifelse(upperbar > 1, 1, upperbar)) %>%
  dplyr::mutate(Classification = dplyr::case_when(
    pred == "SoftPartial-linked" ~ "True positive",
    TRUE ~ "False positive"
  ))

```

## All error bar 

```{r, fig.width=7, fig.height=9}

# Extract the legend from one plot
legend_plot <- hard_error + 
  theme(legend.position = "bottom", 
        legend.text = element_text(size=6), 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust = 0.5)) + 
  labs(title = "Hard")



# Remove the legend from the other plots
hard_error_x <- hard_error + 
  theme(legend.position = "none", 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard")
soft_error_x <- soft_error + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft")
hardpartial_error_x <- hardpartial_error + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial")
softpartial_error_x <- softpartial_error + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial")
neutral_error_x <- neutral_error + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Neutral")



hardlinked_error_1 <- hardlinked_error_list[[1]] + 
  theme(legend.position = "none", 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard-linked 5kb")
hardlinked_error_2 <- hardlinked_error_list[[2]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard-linked 10kb")
hardlinked_error_3 <- hardlinked_error_list[[3]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard-linked 15kb")
hardlinked_error_4 <- hardlinked_error_list[[4]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard-linked 20kb")
hardlinked_error_5 <- hardlinked_error_list[[5]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Hard-linked 25kb")



softlinked_error_1 <- softlinked_error_list[[1]] + 
  theme(legend.position = "none", 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft-linked 5kb")
softlinked_error_2 <- softlinked_error_list[[2]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft-linked 10kb")
softlinked_error_3 <- softlinked_error_list[[3]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft-linked 15kb")
softlinked_error_4 <- softlinked_error_list[[4]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft-linked 20kb")
softlinked_error_5 <- softlinked_error_list[[5]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "Soft-linked 25kb")



hardpartiallinked_error_1 <- hardpartiallinked_error_list[[1]] + 
  theme(legend.position = "none", 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial-linked 5kb")
hardpartiallinked_error_2 <- hardpartiallinked_error_list[[2]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial-linked 10kb")
hardpartiallinked_error_3 <- hardpartiallinked_error_list[[3]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial-linked 15kb")
hardpartiallinked_error_4 <- hardpartiallinked_error_list[[4]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial-linked 20kb")
hardpartiallinked_error_5 <- hardpartiallinked_error_list[[5]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(), 
        axis.title.x = element_blank(), 
        # axis.text.x = element_blank(), 
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "HardPartial-linked 25kb")



softpartiallinked_error_1 <- softpartiallinked_error_list[[1]] + 
  theme(legend.position = "none", 
        axis.title.y = element_text(size=8), 
        axis.text.y = element_text(size=8),  
        axis.title.x = element_text(size=8), 
        # axis.text.x = element_text(size=6),
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial-linked 5kb")
softpartiallinked_error_2 <- softpartiallinked_error_list[[2]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),   
        axis.title.x = element_text(size=8), 
        # axis.text.x = element_text(size=6),
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial-linked 10kb")
softpartiallinked_error_3 <- softpartiallinked_error_list[[3]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),   
        axis.title.x = element_text(size=8), 
        # axis.text.x = element_text(size=6),
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial-linked 15kb")
softpartiallinked_error_4 <- softpartiallinked_error_list[[4]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),   
        axis.title.x = element_text(size=8), 
        # axis.text.x = element_text(size=6),
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial-linked 20kb")
softpartiallinked_error_5 <- softpartiallinked_error_list[[5]] + 
  theme(legend.position = "none", 
        axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),   
        axis.title.x = element_text(size=8), 
        # axis.text.x = element_text(size=6),
        axis.text.x = element_text(size=7), 
        plot.title = element_text(size = 7, hjust=0.5)) + 
  labs(title = "SoftPartial-linked 25kb")



ggpubr::ggarrange(legend_plot, 
                  soft_error_x, 
                  hardpartial_error_x, 
                  softpartial_error_x, 
                  neutral_error_x, 
                  hardlinked_error_1, 
                  hardlinked_error_2, 
                  hardlinked_error_3, 
                  hardlinked_error_4, 
                  hardlinked_error_5, 
                  softlinked_error_1, 
                  softlinked_error_2, 
                  softlinked_error_3, 
                  softlinked_error_4, 
                  softlinked_error_5, 
                  hardpartiallinked_error_1, 
                  hardpartiallinked_error_2, 
                  hardpartiallinked_error_3, 
                  hardpartiallinked_error_4, 
                  hardpartiallinked_error_5, 
                  softpartiallinked_error_1, 
                  softpartiallinked_error_2, 
                  softpartiallinked_error_3, 
                  softpartiallinked_error_4, 
                  softpartiallinked_error_5, 
                  ncol=5, 
                  nrow=5, 
                  labels="AUTO", 
                  font.label = list(size = 7, face = "bold"),
                  common.legend = TRUE, 
                  legend="bottom", 
                  widths=c(1, 0.75, 0.75, 0.75, 0.75),
                  heights=c(0.9, 0.9, 0.9, 0.9, 1))

# save pdf

setwd("[PATH_TO]/partialSHIC_DsGRP/")

ggsave('Figure_X_confusionMatrix_analysis_SCOTT_4.pdf', width=7, height=9, unit='in', dpi=300)

# save png

setwd("[PATH_TO]/partialSHIC_DsGRP/")

ggsave('Figure_X_confusionMatrix_analysis_SCOTT_4.png', width=7, height=9, unit='in', dpi=300)

```

# ######################## Figure 4

## Neutral 

```{r, fig.width=7, fig.height=7}

my.soft.col <- RColorBrewer::brewer.pal(9,"Reds")
my.partialNeutral.col <- RColorBrewer::brewer.pal(9,"Purples")
my.Neutral.col <- RColorBrewer::brewer.pal(9,"Blues")
my.partialNeutral.col <- RColorBrewer::brewer.pal(9, 'Greens')

probs %>% 
  dplyr::filter(truth2=="Neutral") %>% 
  dplyr::mutate(Classification = case_when(
    pred==truth2 ~ "True positive", 
    pred!=truth2 ~ "False positive"
  )) %>% 
  dplyr::mutate(train_test = paste(train, test, sep="_")) %>% 
  ggplot2::ggplot(aes(x=pred, y=accuracy, fill=Classification)) + 
  geom_bar(stat="identity", position="identity") + 
  scale_y_continuous(labels = scales::percent_format(), limits = c(0,1)) + 
  scale_fill_manual(values=c(my.soft.col[6], my.Neutral.col[6])) + 
  facet_wrap(~test) + 
  theme_classic() + 
  theme(legend.position="bottom", 
        axis.text.x = element_text(angle = 45, hjust = 1, size=8), 
        strip.text = element_text(size = 8)) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  labs(title = "Simulated Neutral", 
       x = "Prediction", 
       y = "Proportion", 
       fill = "")

probs %>%
  filter(truth2=="Neutral") %>% 
  select(test, pred, accuracy) %>% 
  pivot_wider(names_from=pred, values_from=accuracy)

# save pdf

setwd("[PATH_TO]/partialSHIC_DsGRP/")

ggsave('Figure_4_confusionMatrix_analysis_SCOTT_NEUTRAL.pdf', width=7, height=9, unit='in', dpi=300)

# save png

setwd("[PATH_TO]/partialSHIC_DsGRP/")

ggsave('Figure_4_confusionMatrix_analysis_SCOTT_NEUTRAL.png', width=7, height=9, unit='in', dpi=300)

# save svg

setwd("[PATH_TO]/partialSHIC_DsGRP/")

ggsave('Figure_4_confusionMatrix_analysis_SCOTT_NEUTRAL.svg', width=7, height=9, unit='in', dpi=300)

```

# ######################## Figure 5 SP2 median confusion matrix

* Output from partialS/HIC

# ######################## Figure 6 Pie and Bar

```{r}

library(tidyverse)
library(data.table)
library(RColorBrewer)
library(cowplot)

setwd("[PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical")

beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})



a1_df <- data.table(table(ensemble$pred_ensemble))
a1_df$pct <- a1_df$N/sum(a1_df$N)



my.soft.col <- brewer.pal(9,"Reds")
my.partialSoft.col <- brewer.pal(9,"Purples")
my.hard.col <- brewer.pal(9,"Blues")
my.partialHard.col <- brewer.pal(9, 'Greens')


aplot <- as.data.table(a1_df)
aplot$V1 <- factor(aplot$V1, levels=c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                      "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral"))
aplot <- aplot %>% arrange(desc(V1)) %>% mutate(prop = round(pct*100,2)) %>%
    mutate(lab_ypos = cumsum(prop) - 0.5*prop) %>%
    mutate(lbl=sprintf("%s: %.1f%%",V1,pct*100)) %>%
    mutate(lbl2=sprintf("%.1f%%",pct*100))

PA <- ggplot(aplot,aes(x="",y=prop, fill=V1)) +
  geom_bar(width=1, stat="identity", color="white") +
  coord_polar("y", start=0) +
  scale_fill_manual(values=c(my.hard.col[6],my.hard.col[4],my.partialHard.col[6],my.partialHard.col[4],
                             my.soft.col[6],my.soft.col[4],my.partialSoft.col[6],my.partialSoft.col[4],
                             "grey60")) +
  theme_void() + 
  theme(legend.position = "none")



a1 <- data.table(ensemble)

a1[,chr:="xxxx"][contig=="chrScA8VGg_76",chr:="3R"][contig=="chrScA8VGg_718",chr:="2L"][contig=="chrScA8VGg_542",chr:="2L"][contig=="chrScA8VGg_594",chr:="X"][contig=="chrScA8VGg_628",chr:="3L"][contig=="chrScA8VGg_785",chr:="2R"]
a1[chr!="xxxx"] -> a2
data.frame(table(a2$chr, a2$pred_ensemble)) -> a3


a3 <- as.data.table(a3)
a3 <- a3[,.(Freq2=Freq/sum(.SD[,Freq]), Var2=Var2),by=Var1]
a3$Var2 <- factor(a3$Var2,rev(c("Hard","Hard-linked","HardPartial","HardPartial-linked",
                                "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked", "Neutral")))

PB <- ggplot(a3, aes(x = Var1, y = Freq2, fill = Var2)) + 
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("Neutral"="grey60", 
                               "SoftPartial-linked"=my.partialSoft.col[4], 
                               "SoftPartial"=my.partialSoft.col[6],
                               "Soft-linked"=my.soft.col[4],
                               "Soft"=my.soft.col[6],
                               "HardPartial-linked"=my.partialHard.col[4], 
                               "HardPartial"=my.partialHard.col[6],
                               "Hard-linked"=my.hard.col[4],
                               "Hard"=my.hard.col[6])) +
    xlab("") + ylab("partialS/HIC sub-windows proportion") + theme_classic() + 
    labs(fill = "Genomic class") + 
    theme(axis.text = element_text(size=12),axis.title = element_text(size=13), 
          legend.text = element_text(size=11))


PP <- plot_grid(PA,PB, ncol=2, labels = c("(a)","(b)"), label_x = c(0.2,0.75), 
                label_y=c(0.95,0.95),rel_widths = c(0.4,0.6))


setwd("[PATH_TO]/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_3_intergenic.pdf', width=8, height=4, unit='in', dpi=300)

# save png

setwd("[PATH_TO]/partialSHIC_DsGRP/")

ggsave('Figure_3_pie_and_bar_SCOTT_3_intergenic.png', width=8, height=4, unit='in', dpi=300)

# save svg

setwd("[PATH_TO]/partialSHIC_DsGRP/")

ggsave("Figure_3_pie_and_bar_SCOTT_3_intergenic.svg", device = "svg", width=8, height=4, unit='in', dpi=300)

```

# ######################## Figure 7 snpEff/snpSift

### Install 

```{shell}

cd [PATH_TO]/Programs
  
wget https://snpeff.blob.core.windows.net/versions/snpEff_latest_core.zip

unzip snpEff_latest_core.zip

rm snpEff_latest_core.zip

```

### Build database for HiC 

#### Step 1 Configure snpEff.config 

Configure a new genome in SnpEff's config file snpEff.config.

a) Add genome entry to snpEff's configuration
b) If the genome uses a non-standard codon table: Add codon table parameter

```{shell}

vim snpEff.config

# Drosophila serrata genome, version drosophila_06Jul2018_A8VGg
Dserrata_HiC.genome : Dserrata_HiC

# # change data.dir at the top of the file
# data.dir = [PATH_TO]/Programs/snpEff/data/

```

#### Step 2 Build database (NEED A LOT OF FILES)

Build using gene annotations and reference sequences

Option 1: Building a database from GTF files (recommended for large genomes)
Option 2: Building a database from GenBank files (recommended for small genomes)
Option 3: Building a database from GFF files
Option 4: Building a database from RefSeq table from UCSC

To build a database, SnpEff needs:

The reference genome sequence: This is the sequence of all chromosomes in the genome, typically in a FASTA file

Gene annotations files: This is the information on where the genes, transcripts and exons are in the genome. Typically, from files in GTF, GeneBank, GFF, or RefSeq formats

Sequences of CDS or Proteins from the genome. These are used to check that SnpEff's database is consistent and doesn't have errors (see sections Checking CDS sequences and Checking Protein sequences)

```{shell}

# Create directory for this new genome
cd [PATH_TO]/Programs/snpEff
mkdir -p ./data
cd [PATH_TO]/Programs/snpEff/data
mkdir -p Dserrata_HiC
cd Dserrata_HiC

# Get annotation files GTF
rsync -ahPv [PATH_TO]/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver.gtf ./
mv GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver.gtf genes.gtf

# Get annotation files GFF
rsync -ahPv [PATH_TO]/GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gff ./
mv GCF_002093755.1_Dser1.0_genomic_OGcontigs_NOregion_HiC_liftOver_sorted.gff genes.gff

# Get the genome reference sequence file
cd [PATH_TO]/Programs/snpEff/data/Dserrata_HiC
rsync -ahPv [PATH_TO]/drosophila_06Jul2018_A8VGg.fasta ./
mv drosophila_06Jul2018_A8VGg.fasta sequences.fa

# Get the CDS sequence file
cd [PATH_TO]/Programs/snpEff/data/Dserrata_HiC
rsync -ahPv [PATH_TO]/GCF_002093755.1_Dser1.0_rna_from_genomic.fna ./
mv GCF_002093755.1_Dser1.0_rna_from_genomic.fna cds.fa
sed 's/.*\[transcript_id=/>/' cds.fa > temp1.fa
sed 's/\].*//' temp1.fa > cds.fa
rm temp1.fa

# Get the protein sequence file
cd [PATH_TO]/Programs/snpEff/data/Dserrata_HiC
rsync -ahPv [PATH_TO]/GCF_002093755.1_Dser1.0_protein.faa.gz ./
gzip -d GCF_002093755.1_Dser1.0_protein.faa.gz
mv GCF_002093755.1_Dser1.0_protein.faa protein.fa

awk -F '\t' '{ split($9, a, "; "); protein_id=""; transcript_id=""; for (i in a) { split(a[i], b, " "); gsub(/"/, "", b[2]); if (b[1]=="protein_id") protein_id=b[2]; else if (b[1]=="transcript_id") transcript_id=b[2]; } if (protein_id!="" && transcript_id!="") print protein_id, transcript_id }' genes.gtf | uniq > protein2transcript_key.txt

module load python
python replace_protein_ids.py protein.fa protein2transcript_key.txt protein2.fa
mv protein2.fa protein.fa

# Create database

module load java

cd [PATH_TO]/Programs/snpEff

# # GTF
java -jar snpEff.jar build -gtf22 -v -noCheckProtein Dserrata_HiC 2>&1 | tee Dserrata_HiC.build

# #GFF
# java -jar snpEff.jar build -gff3 -v Dserrata_HiC 2>&1 | tee Dserrata_HiC.build

```

#### Step 3 Annotate VCF 

```{shell, annotate vcf Bunya}

module load java

# 542

## convert to original scaffold names
mkdir -p [PATH_TO]/Programs/snpEff/data/vcf
cd [PATH_TO]/Programs/snpEff/data/vcf
rsync -ahPv [PATH_TO]/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf ./
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_542.vcf

# Annotate
cd [PATH_TO]/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC [PATH_TO]/Programs/snpEff/data/vcf/min_542.vcf > ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l



# 594

## convert to original scaffold names
mkdir -p [PATH_TO]/Programs/snpEff/data/vcf
cd [PATH_TO]/Programs/snpEff/data/vcf
rsync -ahPv [PATH_TO]/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf ./
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_594.vcf

# Annotate
cd [PATH_TO]/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC [PATH_TO]/Programs/snpEff/data/vcf/min_594.vcf > ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l



# Get VCF GATK 628

## convert to original scaffold names
mkdir -p [PATH_TO]/Programs/snpEff/data/vcf
cd [PATH_TO]/Programs/snpEff/data/vcf
rsync -ahPv ./GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf ./
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_628.vcf

# Annotate
cd [PATH_TO]/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC [PATH_TO]/Programs/snpEff/data/vcf/min_628.vcf > ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l



# 718

## convert to original scaffold names
mkdir -p [PATH_TO]/Programs/snpEff/data/vcf
cd [PATH_TO]/Programs/snpEff/data/vcf
rsync -ahPv ./GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf ./
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_718.vcf

# Annotate
cd [PATH_TO]/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC [PATH_TO]/Programs/snpEff/data/vcf/min_718.vcf > ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l



# Get VCF GATK 76

## convert to original scaffold names
mkdir -p [PATH_TO]/Programs/snpEff/data/vcf
cd [PATH_TO]/Programs/snpEff/data/vcf
rsync -ahPv ./GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf ./
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_76.vcf

# Annotate
cd [PATH_TO]/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC [PATH_TO]/Programs/snpEff/data/vcf/min_76.vcf > ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l



# 785

## convert to original scaffold names
mkdir -p [PATH_TO]/Programs/snpEff/data/vcf
cd [PATH_TO]/Programs/snpEff/data/vcf
rsync -ahPv ./GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf ./
sed -i 's/_HRSCAF_/;HRSCAF=/g' GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf
cut -f 1,2,3,4,5,6,7,8 GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.vcf > min_785.vcf

# Annotate
cd [PATH_TO]/Programs/snpEff
java -Xmx8g -jar snpEff.jar Dserrata_HiC [PATH_TO]/Programs/snpEff/data/vcf/min_785.vcf > ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf

grep 'MODIFIER' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'LOW' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'MODERATE' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l
grep 'HIGH' ./results/GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf | wc -l

```

#### Step 4 parse 

```{shell, parse}

module load java

cd [PATH_TO]/Programs/snpEff/results

# 542 

cat GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_542_HRSCAF_776_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

# 594 

cat GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_594_HRSCAF_845_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

# 628 

cat GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_628_HRSCAF_890_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

# 718

cat GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_718_HRSCAF_1046_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

# 76

cat GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_76_HRSCAF_120_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

# 785

cat GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.vcf |\
 ../scripts/vcfEffOnePerLine.pl |\
 java -jar ../SnpSift.jar extractFields - CHROM POS REF ALT "ANN[*].ALLELE" "ANN[*].EFFECT" "ANN[*].IMPACT" "ANN[*].GENE" "ANN[*].FEATURE" "ANN[*].FEATUREID" "ANN[*].BIOTYPE" "ANN[*].DISTANCE" "ANN[*].ERRORS" > GenotypeGVCFs_ScA8VGg_785_HRSCAF_1140_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv

```

#### Step 5 analyse

```{shell}

cd [PATH_TO]/partialSHIC_DsGRP/snpEff

```

##### EDA 

```{r}

library(tidyverse)
library(data.table)

# 100% genotyped (the same SNPs as used for partialS/HIC)
setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")

ffs <- list.files(pattern = '*_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef.ann.tsv', recursive = TRUE)

snpEff100 <- data.table()

for(ff in ffs){
    # tmp <- fread(ff, header = TRUE, sep = '\t')
    tmp <- read_tsv(ff)
    snpEff100 <- rbind(snpEff100, tmp)
}

```

##### Keep most severe 

```{r}

snpEff100 %>% 
  filter(`ANN[*].IMPACT`=="HIGH") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_HIGH = TRUE) %>% 
  distinct() -> 
high

snpEff100 %>% 
  filter(`ANN[*].IMPACT`=="MODERATE") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_MODERATE = TRUE) %>% 
  distinct() -> 
moderate

snpEff100 %>% 
  filter(`ANN[*].IMPACT`=="LOW") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_LOW = TRUE) %>% 
  distinct() -> 
low

snpEff100 %>% 
  filter(`ANN[*].IMPACT`=="MODIFIER") %>% 
  select(CHROM, POS, REF, ALT) %>% 
  mutate(IMPACT_MODIFIER = TRUE) %>% 
  distinct() -> 
modifier

moderate %>% 
  anti_join(high) -> 
moderate2

low %>% 
  anti_join(moderate) %>% 
  anti_join(high) -> 
low2

modifier %>% 
  anti_join(high) %>% 
  anti_join(moderate) %>% 
  anti_join(low) -> 
modifier2

high %>% 
  bind_rows(moderate2) %>% 
  bind_rows(low2) %>% 
  bind_rows(modifier2) -> 
snpEff100_severe

snpEff100_severe %>% 
  pivot_longer(IMPACT_HIGH:IMPACT_MODIFIER, names_to="IMPACT", values_to="value") %>% 
  filter(value==TRUE) %>% 
  mutate(IMPACT = str_replace(IMPACT, "IMPACT_", "")) %>% 
  select(-value) %>% 
  arrange(CHROM, POS, IMPACT) -> 
snpEff100_severe

write_tsv(snpEff100_severe, file="GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")

```

##### Convert windows to peaks 

* written by Yiguan, rewritten by me.
* I added ensemble stuff

```{r}

library(data.table)
library(dplyr)
library(stringr)
library(parallel)

setwd("[PATH_TO]/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical")

beds <- list.files(pattern = 'ScA8VGg_.*[0-9].pred.bed$')
probs_files <- list.files(pattern = ".*.prob.csv")

prob_pred <- data.table()

for(bed in beds){
    experiment <- gsub("ScA8VGg_\\d+.train_geonsRateMask_110Haplotypes.pred_genosRateMask_110Haplotypes_phased_noPIRs_synthInbred_", "", bed)
    experiment <- gsub(".pred.bed", "", experiment)
    tmp1 <- fread(bed, header = FALSE, sep = "\t")
    prob_file <- paste(bed, ".prob.csv", sep="")
    tmp2 <- fread(prob_file, header = TRUE, sep = ",")
    tmp <- cbind(tmp1, tmp2)
    tmp$experiment <- experiment
    prob_pred <- rbind(prob_pred, tmp)
}

prob_pred <- prob_pred %>% 
  select(V1:V4, Neutral:experiment) %>% 
  rename(prob_max=pred, contig=V1, start=V2, end=V3, pred=V4) %>% 
  mutate(pred = str_replace(pred, "_ScA8VGg_.*", ""))

ensemble <- prob_pred %>% 
  group_by(contig, start, end) %>% 
  summarise(Neutral=sum(Neutral), 
            Hard=sum(Hard), 
            `Hard-linked`=sum(`Hard-linked`), 
            Soft=sum(Soft), 
            `Soft-linked`=sum(`Soft-linked`), 
           HardPartial=sum(HardPartial), 
            `HardPartial-linked`=sum(`HardPartial-linked`), 
           SoftPartial=sum(SoftPartial), 
            `SoftPartial-linked`=sum(`SoftPartial-linked`))

ensemble$pred_ensemble <- apply(ensemble[,4:12], 1, function(row) {
  sorted_probs <- sort(row, decreasing = TRUE)
  names(sorted_probs)[1]
})



contigNames <- c("chrScA8VGg_542", "chrScA8VGg_594", "chrScA8VGg_628", "chrScA8VGg_718", "chrScA8VGg_76", "chrScA8VGg_785")

for (contigName in contigNames) {
  cat("Merging windows for", contigName, "\n")
  tempData <- ensemble %>% filter(contig==contigName)
  tempData$merged <- 1
  m <- 1
  for (i in 2:nrow(tempData)) {
    if (tempData$start[i]==tempData$end[i-1] & tempData$pred_ensemble[i]==tempData$pred_ensemble[i-1]) {
      tempData$merged[i] <- m
    } else {
      m <- m+1
      tempData$merged[i] <- m
    }
  }
  assign(contigName, tempData)
}

ensemble <- bind_rows(chrScA8VGg_542, chrScA8VGg_594, chrScA8VGg_628, chrScA8VGg_718, chrScA8VGg_76, chrScA8VGg_785)

readr::write_tsv(ensemble, "ensemble_predictions_merged_windows.tsv")

```

##### Add impact to merged windows 

```{r}

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")

snpEff100_severe <- readr::read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")

snpEff100_severe <- snpEff100_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))
setDT(snpEff100_severe)
setkey(snpEff100_severe, CHROM)

predPeaks <- ensemble
predPeaks <- predPeaks %>% mutate(contig = str_replace(contig, "chr", "")) %>% rename(CHROM = contig)

predPeaks$HIGH <- NA
predPeaks$MODERATE <- NA
predPeaks$LOW <- NA
predPeaks$MODIFIER <- NA

for (i in 1:nrow(predPeaks)) {
  cat(i, "\n")
  snpEff100_severe[predPeaks$CHROM[i], 
              .(CHROM, POS, REF, ALT, IMPACT), 
              nomatch = 0,
              on = "CHROM"][POS >= predPeaks$start[i] & POS <= predPeaks$end[i]] %>% 
    group_by(IMPACT) %>% 
    summarise(n=n()) %>% 
    tidyr::pivot_wider(names_from=IMPACT, values_from=n) -> 
  tempData
  if ("HIGH" %in% colnames(tempData)) { predPeaks$HIGH[i] <- tempData$HIGH }
  if ("MODERATE" %in% colnames(tempData)) { predPeaks$MODERATE[i] <- tempData$MODERATE }
  if ("LOW" %in% colnames(tempData)) { predPeaks$LOW[i] <- tempData$LOW }
  if ("MODIFIER" %in% colnames(tempData)) { predPeaks$MODIFIER[i] <- tempData$MODIFIER }
}

predPeaks100 <- predPeaks

predPeaks100 %>% 
  mutate(length=end-start) -> 
predPeaks100

tot <- sum(predPeaks100$length)

predPeaks100
tot

write_tsv(predPeaks100, file="predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

```

##### Permutation 

```{r, R template for permutation}

library(tidyverse)

setwd("[PATH_TO]/partialSHIC_DsGRP/permutations_intergenic_2/")



## Import and wrangle data

snpEff_severe <- read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")
snpEff_severe <- snpEff_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))

predPeaks_5kb <- read_tsv(
  "predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv"
)

predPeaks <- predPeaks_5kb %>%
  group_by(CHROM, merged, pred_ensemble) %>%
  summarise(start = min(start),
            end = max(end),
            HIGH = sum(HIGH, na.rm=TRUE),
            MODERATE = sum(MODERATE, na.rm=TRUE),
            LOW = sum(LOW, na.rm=TRUE),
            MODIFIER = sum(MODIFIER, na.rm=TRUE)) %>%
  ungroup() %>%
  mutate(length = end - start)

predPeaks_5kb$run <- 1
g <- 1

for (i in 2:nrow(predPeaks_5kb)) {
  if (predPeaks_5kb$start[i] == predPeaks_5kb$end[i-1]) {
    predPeaks_5kb$run[i] <- g
  } else {
    g <- g+1
    predPeaks_5kb$run[i] <-g
  }
}

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(CHROM, start, end, merged, run)

peak_start_end <- scf_start %>% group_by(CHROM, run) %>% summarise(start = min(start), end = max(end))


# Permutation number
permutation <- [PERM]

shuffled_data <- predPeaks
scf_start_temp <- scf_start
peak_start_end_temp <- peak_start_end

for (i in 1:nrow(shuffled_data)) {
  start_time <- Sys.time()
  # Get peak
  peak <- shuffled_data[i,]
  # Move peak to a random genomic location
  selected_scf_start <- sample_n(scf_start_temp, 1)
  peak_end <- peak_start_end_temp %>% filter(CHROM==selected_scf_start$CHROM & start <= selected_scf_start$start & end >= selected_scf_start$start + peak$length)
  end_not_available <- nrow(peak_end) < 1
  # Make sure the new genomic location is valid
  # Check if the new genomic location is within a prediction peak or the new end_pos is still available
  scf_start_temp2 <- scf_start_temp
  int_max_attempts <- nrow(scf_start_temp2)
  max_attempts <- nrow(scf_start_temp2)
  while(end_not_available && max_attempts > 0) {
    # Move peak to a random genomic location
    selected_scf_start <- sample_n(scf_start_temp, 1)
    peak_end <- peak_start_end_temp %>% filter(CHROM==selected_scf_start$CHROM & start <= selected_scf_start$start & end >= selected_scf_start$start + peak$length)
    end_not_available <- nrow(peak_end) < 1
    # Decrement attempts counter
    max_attempts <- max_attempts - 1
    scf_start_temp2 <- scf_start_temp2 %>% anti_join(selected_scf_start, by = c("CHROM", "start"))
  }
  # Break if max_attempts
  if (max_attempts == 0) {
    cat("Failed to find valid random location.\n")
    shuffled_data <- NULL
    break
  }
  cat("perm", permutation, "row", i, "location found after", int_max_attempts-max_attempts, "attempts\n")
  # Remove the randomly selected location from the pool
  region <- scf_start_temp %>%
    filter(CHROM==selected_scf_start$CHROM) %>%
    filter(start >= selected_scf_start$start & end <= selected_scf_start$start + peak$length)
  scf_start_temp <- scf_start_temp %>% anti_join(region, by = c("CHROM", "start"))

  peak_start_end_temp <- scf_start_temp
  peak_start_end_temp$run <- 1
  g <- 1

  if (nrow(peak_start_end_temp)>=2) {
    for (k in 2:nrow(peak_start_end_temp)) {
      if (peak_start_end_temp$start[k] == peak_start_end_temp$end[k-1]) {
        peak_start_end_temp$run[k] <- g
      } else {
        g <- g+1
        peak_start_end_temp$run[k] <-g
      }
    }
  }

  peak_start_end_temp <- peak_start_end_temp %>% group_by(CHROM, run) %>% summarise(start = min(start), end = max(end)) %>% ungroup()

  # Add random location to predPeaks
  shuffled_data$perm[i] <- [PERM]
  shuffled_data$scf_perm[i] <- selected_scf_start$CHROM
  shuffled_data$start_pos_perm[i] <- selected_scf_start$start
  shuffled_data$end_pos_perm[i] <- selected_scf_start$start+peak$length
  # get impact of region
  tempData <- snpEff_severe %>%
    filter(CHROM == selected_scf_start$CHROM,
           POS >= selected_scf_start$start & POS <= (selected_scf_start$start + peak$length)) %>%
    group_by(IMPACT) %>%
    summarise(n = n()) %>%
    pivot_wider(names_from = IMPACT, values_from = n, values_fill = 0)
  if ("HIGH" %in% colnames(tempData)) { shuffled_data$HIGH_perm[i] <- tempData$HIGH } else { shuffled_data$HIGH_perm[i] <- NA }
  if ("MODERATE" %in% colnames(tempData)) { shuffled_data$MODERATE_perm[i] <- tempData$MODERATE } else { shuffled_data$MODERATE_perm[i] <- NA }
  if ("LOW" %in% colnames(tempData)) { shuffled_data$LOW_perm[i] <- tempData$LOW } else { shuffled_data$LOW_perm[i] <- NA }
  if ("MODIFIER" %in% colnames(tempData)) { shuffled_data$MODIFIER_perm[i] <- tempData$MODIFIER } else { shuffled_data$MODIFIER_perm[i] <- NA }
}

# Save file
if (!is.null(shuffled_data)) {
  fileName <- "predPeaks_permutation_[PERM]_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv"
  write_tsv(shuffled_data, file=fileName)
}

# Run time
end_time <- Sys.time()
print(start_time, "\n")
print(end_time, "\n")

# Quit
quit(save="no")

```

```{shell, create R scripts Bunya}

for p in {1..5000}; do
  echo "$p"
  cp permutation_template_2ndAttempt.txt permutation_${p}_2ndAttempt.R
  replace_CMD="s/\[PERM\]/${p}/g"
  perl -pi -e "$replace_CMD" permutation_${p}_2ndAttempt.R
done

chmod 755 ./*.R

```

```{shell, slurm template}

#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=16G
#SBATCH --job-name=perm_[PERM]
#SBATCH --time=24:00:00
#SBATCH --partition=general
#SBATCH --account=[REDACTED]
#SBATCH -o [PATH_TO]/partialSHIC_DsGRP/permutations_intergenic_2/slurm_perm_[PERM].output
#SBATCH -e [PATH_TO]/partialSHIC_DsGRP/permutations_intergenic_2/slurm_perm_[PERM].error

module load r
module load anaconda3
source ~/conda-init

cd [PATH_TO]/partialSHIC_DsGRP/permutations_intergenic_2/

srun R CMD BATCH permutation_[PERM]_2ndAttempt.R

```

```{shell, create and submit slurm scripts}

for p in {1..5000}; do
  echo "$p"
  cp permutation_template_slurm_2ndAttempt.txt permutation_${p}_2ndAttempt.slurm.sh
  replace_CMD="s/\[PERM\]/${p}/g"
  perl -pi -e "$replace_CMD" permutation_${p}_2ndAttempt.slurm.sh
done

chmod 755 ./*_2ndAttempt.slurm.sh

for p in {1..5000}; do
  echo "permutation_${p}_2ndAttempt.slurm.sh submitted"
  sbatch permutation_${p}_2ndAttempt.slurm.sh
  sleep 0.05
done

# rerun

## look for missing files

echo predPeaks_permutation_{1..5000}_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv | xargs ls | grep 'error'

## look for files with not enough rows

# wc predPeaks_permutation_*_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv | grep -v -P '7624'
find . -name "predPeaks_permutation_*_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv" -print0 | xargs -0 wc | grep -v -P '7624'

find ./ -maxdepth 1 -type f -name '*_2ndAttempt.tsv' -exec wc {} \; | sort -V | grep -v '7751'

while read p; do
  echo "$p"
  cp permutation_template_slurm_2ndAttempt.txt permutation_${p}_2ndAttempt.slurm.sh
  replace_CMD="s/\[PERM\]/${p}/g"
  perl -pi -e "$replace_CMD" permutation_${p}_2ndAttempt.slurm.sh
done < rerun2.txt

chmod 755 ./*.slurm.sh

while read p; do
  echo "permutation_${p}_2ndAttempt.slurm.sh submitted"
  sbatch permutation_${p}_2ndAttempt.slurm.sh
  sleep 0.5
done < rerun2.txt

```

```{shell}

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_SIFT

rsync\
 -ahPv\
 [PATH_TO]/partialSHIC_DsGRP/permutations_intergenic_2/ /predPeaks_permutation_*\
 ./

```

##### Permutation test 

```{bash}

module load anaconda3
conda activate r_env_4.3.1

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_SIFT

```

```{r, permutations test}

library(tidyverse)
library(data.table)

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("[PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_SIFT")

ffs <- list.files(pattern = '*_2ndAttempt.tsv', recursive = TRUE)

permutations <- rbindlist(lapply(ffs, fread))

setorder(permutations, perm, CHROM, start)



# Observed

permutations %>% 
  filter(perm==1) %>% 
  group_by(pred_ensemble) %>% 
  summarise(basePairs=sum(length, na.rm=TRUE), 
            high=sum(HIGH, na.rm=TRUE), 
            moderate=sum(MODERATE, na.rm=TRUE), 
            low=sum(LOW, na.rm=TRUE), 
            modifier=sum(MODIFIER, na.rm=TRUE)) %>% 
  ungroup() %>% 
  mutate(high_prop = high/(high+moderate+low+modifier), 
         moderate_prop = moderate/(high+moderate+low+modifier), 
         low_prop = low/(high+moderate+low+modifier), 
         modifier_prop = modifier/(high+moderate+low+modifier))



logOdds <- permutations %>% 
  mutate(HIGH = ifelse(is.na(HIGH), 1, HIGH+1), 
         MODERATE = ifelse(is.na(MODERATE), 1, MODERATE+1), 
         LOW = ifelse(is.na(LOW), 1, LOW+1), 
         MODIFIER = ifelse(is.na(MODIFIER), 1, MODIFIER+1), 
         HIGH_perm = ifelse(is.na(HIGH_perm), 1, HIGH_perm+1), 
         MODERATE_perm = ifelse(is.na(MODERATE_perm), 1, MODERATE_perm+1), 
         LOW_perm = ifelse(is.na(LOW_perm), 1, LOW_perm+1), 
         MODIFIER_perm = ifelse(is.na(MODIFIER_perm), 1, MODIFIER_perm+1)) %>% 
  group_by(pred_ensemble, perm) %>% 
  summarise(basePairs=sum(length), 
            high=sum(HIGH), 
            moderate=sum(MODERATE), 
            low=sum(LOW), 
            modifier=sum(MODIFIER), 
            high_perm=sum(HIGH_perm), 
            moderate_perm=sum(MODERATE_perm), 
            low_perm=sum(LOW_perm), 
            modifier_perm=sum(MODIFIER_perm)) %>% 
  ungroup() %>% 
  mutate(high_prop = high/(high+moderate+low+modifier), 
         moderate_prop = moderate/(high+moderate+low+modifier), 
         low_prop = low/(high+moderate+low+modifier), 
         modifier_prop = modifier/(high+moderate+low+modifier), 
         high_prop_perm = high_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
         moderate_prop_perm = moderate_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
         low_prop_perm = low_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
         modifier_prop_perm = modifier_perm/(high_perm+moderate_perm+low_perm+modifier_perm), 
         high_lodds = log2(high_prop/high_prop_perm), 
         moderate_lodds = log2(moderate_prop/moderate_prop_perm), 
         low_lodds = log2(low_prop/low_prop_perm), 
         modifier_lodds = log2(modifier_prop/modifier_prop_perm)) %>% 
  select(pred_ensemble, perm, high_lodds:modifier_lodds) %>% 
  ungroup() %>% 
  group_by(pred_ensemble) %>% 
  summarise(high_lci = quantile(high_lodds, 0.025), 
            high_lodds_mean=mean(high_lodds), 
            high_uci = quantile(high_lodds, 0.975), 
            moderate_lci = quantile(moderate_lodds, 0.025), 
            moderate_lodds_mean=mean(moderate_lodds), 
            moderate_uci = quantile(moderate_lodds, 0.975), 
            low_lci = quantile(low_lodds, 0.025), 
            low_lodds_mean=mean(low_lodds), 
            low_uci = quantile(low_lodds, 0.975), 
            modifier_lci = quantile(modifier_lodds, 0.025), 
            modifier_lodds_mean=mean(modifier_lodds), 
            modifier_uci = quantile(modifier_lodds, 0.975))

# dot chart

logOdds_avg <- logOdds %>% 
  select(pred_ensemble, high_lodds_mean, moderate_lodds_mean, low_lodds_mean, modifier_lodds_mean) %>% 
  set_names(c("genome_class", "HIGH", "MODERATE", "LOW", "MODIFIER")) %>% 
  pivot_longer(HIGH:MODIFIER, names_to="impact", values_to="avg") %>% 
  as.data.frame()

logOdds_avg$impact <- factor(logOdds_avg$impact, c("HIGH","MODERATE","LOW","MODIFIER"))
logOdds_avg$genome_class <- factor(logOdds_avg$genome_class, c(
  "Hard", "Hard-linked", "HardPartial", "HardPartial-linked",
  "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked",
  "Neutral"))
logOdds_avg <- logOdds_avg[order(logOdds_avg$genome_class,logOdds_avg$impact, decreasing = T),]

pdf("Figure_4_sweepImpact_dotplot_9states_SCOTT_snpEff_log2_proportion_intergenic_3.pdf", width=6, height = 8)
png("Figure_4_sweepImpact_dotplot_9states_SCOTT_snpEff_log2_proportion_intergenic_3.png", width=6, height = 8, units="in", res=300)

dotchart(logOdds_avg$avg,
         labels = logOdds_avg$impact,
         groups = logOdds_avg$genome_class, 
         xlab = "SNP enrichment log2 fold",
         cex= 0.7, 
         xlim = c(-2, 2))
abline(v=0, col="red")

# add 95%CI line

logOdds_lci <- logOdds %>% 
  select(pred_ensemble, high_lci, moderate_lci, low_lci, modifier_lci) %>% 
  set_names(c("genome_class", "HIGH", "MODERATE", "LOW", "MODIFIER")) %>% 
  pivot_longer(HIGH:MODIFIER, names_to="impact", values_to="lci") %>% 
  as.data.frame()

logOdds_lci$impact <- factor(logOdds_lci$impact, c("HIGH","MODERATE","LOW","MODIFIER"))
logOdds_lci$genome_class <- factor(logOdds_lci$genome_class, c(
  "Hard", "Hard-linked", "HardPartial", "HardPartial-linked",
  "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked",
  "Neutral"))
logOdds_lci <- logOdds_lci[order(logOdds_lci$genome_class,logOdds_lci$impact, decreasing = T),]

logOdds_uci <- logOdds %>% 
  select(pred_ensemble, high_uci, moderate_uci, low_uci, modifier_uci) %>% 
  set_names(c("genome_class", "HIGH", "MODERATE", "LOW", "MODIFIER")) %>% 
  pivot_longer(HIGH:MODIFIER, names_to="impact", values_to="uci") %>% 
  as.data.frame()

logOdds_uci$impact <- factor(logOdds_uci$impact, c("HIGH","MODERATE","LOW","MODIFIER"))
logOdds_uci$genome_class <- factor(logOdds_uci$genome_class, c(
  "Hard", "Hard-linked", "HardPartial", "HardPartial-linked",
  "Soft", "Soft-linked", "SoftPartial", "SoftPartial-linked",
  "Neutral"))
logOdds_uci <- logOdds_uci[order(logOdds_uci$genome_class,logOdds_uci$impact, decreasing = T),]

j = 1
for(i in 1:nrow(logOdds_lci)){
    if(i%%4!=0){
        if(logOdds_lci$lci[i]!=0){segments(logOdds_lci$lci[i],j,logOdds_uci$uci[i],j)}
        j=j+1
    }else{
        if(logOdds_lci$lci[i]!=0){segments(logOdds_lci$lci[i],j,logOdds_uci$uci[i],j)}
        j=j+3
    }
}

dev.off()

```

# ######################## Empirical stats 

```{r}

library(tidyverse)
library(data.table)

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")
predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

tally_5kb <- predPeaks %>% 
  group_by(pred_ensemble) %>% 
  tally() %>% 
  mutate(percentage = n/21501)

tally_merged <- predPeaks %>% 
  group_by(CHROM, pred_ensemble, merged) %>% 
  summarise(start = min(start), end = max(end)) %>% 
  ungroup() %>% 
  arrange(CHROM, start, end) %>% 
  mutate(length = end - start) %>% 
  group_by(pred_ensemble) %>% 
  tally() %>% 
  rename(n_merged = n)

tally_5kb %>% 
  select(-percentage) %>% 
  left_join(tally_merged) %>% 
  mutate(reduction = 1 - n_merged/n) %>% 
  arrange(desc(reduction))

predPeaks %>% 
  group_by(CHROM, pred_ensemble, merged) %>% 
  summarise(start = min(start), end = max(end)) %>% 
  ungroup() %>% 
  arrange(CHROM, start, end) %>% 
  mutate(length = end - start) %>% 
  group_by(pred_ensemble) %>% 
  summarise(length = mean(length))

# percent genome impacted directly

(451+2286+67+221)/21501

# precent genome indirectly

(13588+2919+340+798)/21501

# soft vs hard

1818/336

# complete vs partial

(1818+336)/(221+67)

sum(predPeaks$HIGH, na.rm=TRUE)
sum(predPeaks$MODERATE, na.rm=TRUE)
sum(predPeaks$LOW, na.rm=TRUE)
sum(predPeaks$MODIFIER, na.rm=TRUE)

```


# ######################## GO term enrichment

```{bash}

module load bedtools

module load anaconda3
conda activate r_env_4.3.1

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class

```

## Permutation

### Hard 

```{r}

library(tidyverse)
library(foreach)
library(doSNOW)
library(data.table)

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

## Import and wrangle data

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff/")
snpEff_severe <- read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")
snpEff_severe <- snpEff_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")
predPeaks_5kb <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

predPeaks <- predPeaks_5kb %>%
  group_by(CHROM, merged, pred_ensemble) %>%
  summarise(start = min(start),
            end = max(end),
            HIGH = sum(HIGH, na.rm=TRUE),
            MODERATE = sum(MODERATE, na.rm=TRUE),
            LOW = sum(LOW, na.rm=TRUE),
            MODIFIER = sum(MODIFIER, na.rm=TRUE)) %>%
  ungroup() %>%
  mutate(length = end - start)

predPeaks_5kb$run <- 1
g <- 1

for (i in 2:nrow(predPeaks_5kb)) {
  if (predPeaks_5kb$start[i] == predPeaks_5kb$end[i-1]) {
    predPeaks_5kb$run[i] <- g
  } else {
    g <- g+1
    predPeaks_5kb$run[i] <-g
  }
}

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(CHROM, start, end, merged, run)

peak_start_end <- scf_start %>% group_by(CHROM, run) %>% summarise(start = min(start), end = max(end))

setwd("[PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class")

## Permutate regions

scf_start_temp <- scf_start
peak_start_end_temp <- peak_start_end

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

permutations <- foreach(j = 1:500000, .combine = "rbind", .packages = c("dplyr")) %dopar% {

  shuffled_data <- predPeaks %>% filter(pred_ensemble=="Hard") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER)
  shuffled_data <- shuffled_data %>% arrange(desc(length))

  # Randomly select loci
  selected_scf_start <- sample_n(scf_start_temp, nrow(shuffled_data))
  shuffled_data2 <- shuffled_data %>%
    mutate(CHROM_perm = selected_scf_start$CHROM,
           start_perm = selected_scf_start$start,
           end_perm = start_perm + length)

  # Check if end of new loci's region is available
  shuffled_data2$end_not_available <- NA
  for (i in 1:nrow(shuffled_data2)) {
    peak_end <- peak_start_end_temp %>%
      filter(CHROM==shuffled_data2$CHROM_perm[i] & start <= shuffled_data2$start_perm[i] & end >= shuffled_data2$end_perm[i])
    shuffled_data2$end_not_available[i] <- nrow(peak_end) < 1
  }

  # Check if any randomly selected loci overlap
  shuffled_data2 <- shuffled_data2 %>%
    group_by(CHROM_perm) %>%
    arrange(CHROM_perm, start_perm, end_perm) %>%
    mutate(
      overlap_next_start = lag(end_perm) >= start_perm,
      overlap_prev_end = lead(start_perm) <= end_perm,
      overlap = (overlap_next_start | overlap_prev_end)
    ) %>%
    ungroup()

  # How many permutation failed?
  failed_permutations_n <- shuffled_data2 %>%
    filter(end_not_available | overlap) %>%
    nrow()

  # Which permutation failed?
  failed_permutations <- shuffled_data2 %>%
    filter(end_not_available | overlap)

  # cat("Failed permutations on attempt 1:", failed_permutations_n, "\n")

  # If no permutations failed, print and end iteration
  if (failed_permutations_n < 1) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }

  # If some permutations failed, try and fix them

  attempt <- 2

  while(failed_permutations_n > 0 & attempt <= 100) {

    # Remove used loci from the pool of options
    scf_start_temp2 <- scf_start_temp %>% anti_join(shuffled_data2, by=c("CHROM", "start", "end"))

    # Randomly select loci
    selected_scf_start2 <- sample_n(scf_start_temp2, nrow(failed_permutations))
    failed_permutations <- failed_permutations %>%
      mutate(CHROM_perm = selected_scf_start2$CHROM,
             start_perm = selected_scf_start2$start,
             end_perm = start_perm + length)

    # Check if end of new loci's region is available
    for (i in 1:nrow(failed_permutations)) {
      peak_end <- peak_start_end_temp %>%
        filter(CHROM==failed_permutations$CHROM_perm[i] & start <= failed_permutations$start_perm[i] & end >= failed_permutations$end_perm[i])
      failed_permutations$end_not_available[i] <- nrow(peak_end) < 1

    }

    # Check if any randomly selected loci overlap
    failed_permutations_OG <- shuffled_data2 %>% filter(end_not_available | overlap)
    shuffled_data3 <- shuffled_data2 %>% anti_join(failed_permutations_OG)
    shuffled_data3 <- bind_rows(shuffled_data3, failed_permutations)
    shuffled_data3 <- shuffled_data3 %>%
      group_by(CHROM_perm) %>%
      arrange(CHROM_perm, start_perm, end_perm) %>%
      mutate(
        overlap_next_start = lag(end_perm) >= start_perm,
        overlap_prev_end = lead(start_perm) <= end_perm,
        overlap = (overlap_next_start | overlap_prev_end)
      ) %>%
      ungroup()

    # How many permutation failed?
    failed_permutations_n <- shuffled_data3 %>%
      filter(end_not_available | overlap) %>%
      nrow()

    # Which permutation failed?
    failed_permutations <- shuffled_data3 %>%
      filter(end_not_available | overlap)

    # cat("Failed permutations on attempt", attempt, ":", failed_permutations_n, "\n")

    # Overwrite shuffled_data

    shuffled_data2 <- shuffled_data3

    attempt <- attempt+1

  }

  if (attempt >=3 & attempt <= 100) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }

  shuffled_data_final

}

stopCluster(cl)

# Save file

fileName <- "predPeaks_permutation_HARD_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_1-500k.tsv"
fwrite(permutations, file=fileName, sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)

```

### Soft 

```{r}

library(tidyverse)
library(foreach)
library(doSNOW)
library(data.table)

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

## Import and wrangle data

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff/")
snpEff_severe <- read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")
snpEff_severe <- snpEff_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")
predPeaks_5kb <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

predPeaks <- predPeaks_5kb %>%
  group_by(CHROM, merged, pred_ensemble) %>%
  summarise(start = min(start),
            end = max(end),
            HIGH = sum(HIGH, na.rm=TRUE),
            MODERATE = sum(MODERATE, na.rm=TRUE),
            LOW = sum(LOW, na.rm=TRUE),
            MODIFIER = sum(MODIFIER, na.rm=TRUE)) %>%
  ungroup() %>%
  mutate(length = end - start)

predPeaks_5kb$run <- 1
g <- 1

for (i in 2:nrow(predPeaks_5kb)) {
  if (predPeaks_5kb$start[i] == predPeaks_5kb$end[i-1]) {
    predPeaks_5kb$run[i] <- g
  } else {
    g <- g+1
    predPeaks_5kb$run[i] <-g
  }
}

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(CHROM, start, end, merged, run)

peak_start_end <- scf_start %>% group_by(CHROM, run) %>% summarise(start = min(start), end = max(end))

setwd("[PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class")

## Permutate regions

scf_start_temp <- scf_start
peak_start_end_temp <- peak_start_end

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

permutations <- foreach(j = 1:500000, .combine = "rbind", .packages = c("dplyr")) %dopar% {

  shuffled_data <- predPeaks %>% filter(pred_ensemble=="Soft") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER)
  shuffled_data <- shuffled_data %>% arrange(desc(length))

  # Randomly select loci
  selected_scf_start <- sample_n(scf_start_temp, nrow(shuffled_data))
  shuffled_data2 <- shuffled_data %>%
    mutate(CHROM_perm = selected_scf_start$CHROM,
           start_perm = selected_scf_start$start,
           end_perm = start_perm + length)

  # Check if end of new loci's region is available
  shuffled_data2$end_not_available <- NA
  for (i in 1:nrow(shuffled_data2)) {
    peak_end <- peak_start_end_temp %>%
      filter(CHROM==shuffled_data2$CHROM_perm[i] & start <= shuffled_data2$start_perm[i] & end >= shuffled_data2$end_perm[i])
    shuffled_data2$end_not_available[i] <- nrow(peak_end) < 1
  }

  # Check if any randomly selected loci overlap
  shuffled_data2 <- shuffled_data2 %>%
    group_by(CHROM_perm) %>%
    arrange(CHROM_perm, start_perm, end_perm) %>%
    mutate(
      overlap_next_start = lag(end_perm) >= start_perm,
      overlap_prev_end = lead(start_perm) <= end_perm,
      overlap = (overlap_next_start | overlap_prev_end)
    ) %>%
    ungroup()

  # How many permutation failed?
  failed_permutations_n <- shuffled_data2 %>%
    filter(end_not_available | overlap) %>%
    nrow()

  # Which permutation failed?
  failed_permutations <- shuffled_data2 %>%
    filter(end_not_available | overlap)

  # cat("Failed permutations on attempt 1:", failed_permutations_n, "\n")

  # If no permutations failed, print and end iteration
  if (failed_permutations_n < 1) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }

  # If some permutations failed, try and fix them

  attempt <- 2

  while(failed_permutations_n > 0 & attempt <= 100) {

    # Remove used loci from the pool of options
    scf_start_temp2 <- scf_start_temp %>% anti_join(shuffled_data2, by=c("CHROM", "start", "end"))

    # Randomly select loci
    selected_scf_start2 <- sample_n(scf_start_temp2, nrow(failed_permutations))
    failed_permutations <- failed_permutations %>%
      mutate(CHROM_perm = selected_scf_start2$CHROM,
             start_perm = selected_scf_start2$start,
             end_perm = start_perm + length)

    # Check if end of new loci's region is available
    for (i in 1:nrow(failed_permutations)) {
      peak_end <- peak_start_end_temp %>%
        filter(CHROM==failed_permutations$CHROM_perm[i] & start <= failed_permutations$start_perm[i] & end >= failed_permutations$end_perm[i])
      failed_permutations$end_not_available[i] <- nrow(peak_end) < 1

    }

    # Check if any randomly selected loci overlap
    failed_permutations_OG <- shuffled_data2 %>% filter(end_not_available | overlap)
    shuffled_data3 <- shuffled_data2 %>% anti_join(failed_permutations_OG)
    shuffled_data3 <- bind_rows(shuffled_data3, failed_permutations)
    shuffled_data3 <- shuffled_data3 %>%
      group_by(CHROM_perm) %>%
      arrange(CHROM_perm, start_perm, end_perm) %>%
      mutate(
        overlap_next_start = lag(end_perm) >= start_perm,
        overlap_prev_end = lead(start_perm) <= end_perm,
        overlap = (overlap_next_start | overlap_prev_end)
      ) %>%
      ungroup()

    # How many permutation failed?
    failed_permutations_n <- shuffled_data3 %>%
      filter(end_not_available | overlap) %>%
      nrow()

    # Which permutation failed?
    failed_permutations <- shuffled_data3 %>%
      filter(end_not_available | overlap)

    # cat("Failed permutations on attempt", attempt, ":", failed_permutations_n, "\n")

    # Overwrite shuffled_data

    shuffled_data2 <- shuffled_data3

    attempt <- attempt+1

  }

  if (attempt >=3 & attempt <= 100) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }

  shuffled_data_final

}

stopCluster(cl)

# Save file

fileName <- "predPeaks_permutation_SOFT_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_1-500k.tsv"
fwrite(permutations, file=fileName, sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)

```

### HardPartial 

```{r}

library(tidyverse)
library(foreach)
library(doSNOW)
library(data.table)

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

## Import and wrangle data

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff/")
snpEff_severe <- read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")
snpEff_severe <- snpEff_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")
predPeaks_5kb <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

predPeaks <- predPeaks_5kb %>%
  group_by(CHROM, merged, pred_ensemble) %>%
  summarise(start = min(start),
            end = max(end),
            HIGH = sum(HIGH, na.rm=TRUE),
            MODERATE = sum(MODERATE, na.rm=TRUE),
            LOW = sum(LOW, na.rm=TRUE),
            MODIFIER = sum(MODIFIER, na.rm=TRUE)) %>%
  ungroup() %>%
  mutate(length = end - start)

predPeaks_5kb$run <- 1
g <- 1

for (i in 2:nrow(predPeaks_5kb)) {
  if (predPeaks_5kb$start[i] == predPeaks_5kb$end[i-1]) {
    predPeaks_5kb$run[i] <- g
  } else {
    g <- g+1
    predPeaks_5kb$run[i] <-g
  }
}

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(CHROM, start, end, merged, run)

peak_start_end <- scf_start %>% group_by(CHROM, run) %>% summarise(start = min(start), end = max(end))

setwd("[PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class")

## Permutate regions

scf_start_temp <- scf_start
peak_start_end_temp <- peak_start_end

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

permutations <- foreach(j = 1:500000, .combine = "rbind", .packages = c("dplyr")) %dopar% {

  shuffled_data <- predPeaks %>% filter(pred_ensemble=="HardPartial") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER)
  shuffled_data <- shuffled_data %>% arrange(desc(length))

  # Randomly select loci
  selected_scf_start <- sample_n(scf_start_temp, nrow(shuffled_data))
  shuffled_data2 <- shuffled_data %>%
    mutate(CHROM_perm = selected_scf_start$CHROM,
           start_perm = selected_scf_start$start,
           end_perm = start_perm + length)

  # Check if end of new loci's region is available
  shuffled_data2$end_not_available <- NA
  for (i in 1:nrow(shuffled_data2)) {
    peak_end <- peak_start_end_temp %>%
      filter(CHROM==shuffled_data2$CHROM_perm[i] & start <= shuffled_data2$start_perm[i] & end >= shuffled_data2$end_perm[i])
    shuffled_data2$end_not_available[i] <- nrow(peak_end) < 1
  }

  # Check if any randomly selected loci overlap
  shuffled_data2 <- shuffled_data2 %>%
    group_by(CHROM_perm) %>%
    arrange(CHROM_perm, start_perm, end_perm) %>%
    mutate(
      overlap_next_start = lag(end_perm) >= start_perm,
      overlap_prev_end = lead(start_perm) <= end_perm,
      overlap = (overlap_next_start | overlap_prev_end)
    ) %>%
    ungroup()

  # How many permutation failed?
  failed_permutations_n <- shuffled_data2 %>%
    filter(end_not_available | overlap) %>%
    nrow()

  # Which permutation failed?
  failed_permutations <- shuffled_data2 %>%
    filter(end_not_available | overlap)

  # cat("Failed permutations on attempt 1:", failed_permutations_n, "\n")

  # If no permutations failed, print and end iteration
  if (failed_permutations_n < 1) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }

  # If some permutations failed, try and fix them

  attempt <- 2

  while(failed_permutations_n > 0 & attempt <= 100) {

    # Remove used loci from the pool of options
    scf_start_temp2 <- scf_start_temp %>% anti_join(shuffled_data2, by=c("CHROM", "start", "end"))

    # Randomly select loci
    selected_scf_start2 <- sample_n(scf_start_temp2, nrow(failed_permutations))
    failed_permutations <- failed_permutations %>%
      mutate(CHROM_perm = selected_scf_start2$CHROM,
             start_perm = selected_scf_start2$start,
             end_perm = start_perm + length)

    # Check if end of new loci's region is available
    for (i in 1:nrow(failed_permutations)) {
      peak_end <- peak_start_end_temp %>%
        filter(CHROM==failed_permutations$CHROM_perm[i] & start <= failed_permutations$start_perm[i] & end >= failed_permutations$end_perm[i])
      failed_permutations$end_not_available[i] <- nrow(peak_end) < 1

    }

    # Check if any randomly selected loci overlap
    failed_permutations_OG <- shuffled_data2 %>% filter(end_not_available | overlap)
    shuffled_data3 <- shuffled_data2 %>% anti_join(failed_permutations_OG)
    shuffled_data3 <- bind_rows(shuffled_data3, failed_permutations)
    shuffled_data3 <- shuffled_data3 %>%
      group_by(CHROM_perm) %>%
      arrange(CHROM_perm, start_perm, end_perm) %>%
      mutate(
        overlap_next_start = lag(end_perm) >= start_perm,
        overlap_prev_end = lead(start_perm) <= end_perm,
        overlap = (overlap_next_start | overlap_prev_end)
      ) %>%
      ungroup()

    # How many permutation failed?
    failed_permutations_n <- shuffled_data3 %>%
      filter(end_not_available | overlap) %>%
      nrow()

    # Which permutation failed?
    failed_permutations <- shuffled_data3 %>%
      filter(end_not_available | overlap)

    # cat("Failed permutations on attempt", attempt, ":", failed_permutations_n, "\n")

    # Overwrite shuffled_data

    shuffled_data2 <- shuffled_data3

    attempt <- attempt+1

  }

  if (attempt >=3 & attempt <= 100) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }

  shuffled_data_final

}

stopCluster(cl)

# Save file

fileName <- "predPeaks_permutation_HARDPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_1-500k.tsv"
fwrite(permutations, file=fileName, sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)

```

### SoftPartial 

```{r}

library(tidyverse)
library(foreach)
library(doSNOW)
library(data.table)

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

## Import and wrangle data

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff/")
snpEff_severe <- read_tsv("GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE.ann.tsv")
snpEff_severe <- snpEff_severe %>% mutate(CHROM = str_replace(CHROM, ";HRSCAF=\\d+", ""))

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")
predPeaks_5kb <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

predPeaks <- predPeaks_5kb %>%
  group_by(CHROM, merged, pred_ensemble) %>%
  summarise(start = min(start),
            end = max(end),
            HIGH = sum(HIGH, na.rm=TRUE),
            MODERATE = sum(MODERATE, na.rm=TRUE),
            LOW = sum(LOW, na.rm=TRUE),
            MODIFIER = sum(MODIFIER, na.rm=TRUE)) %>%
  ungroup() %>%
  mutate(length = end - start)

predPeaks_5kb$run <- 1
g <- 1

for (i in 2:nrow(predPeaks_5kb)) {
  if (predPeaks_5kb$start[i] == predPeaks_5kb$end[i-1]) {
    predPeaks_5kb$run[i] <- g
  } else {
    g <- g+1
    predPeaks_5kb$run[i] <-g
  }
}

## Arrange predPeaks by peak length

predPeaks <- predPeaks %>% arrange(desc(length))

scf_start <- predPeaks_5kb %>% select(CHROM, start, end, merged, run)

peak_start_end <- scf_start %>% group_by(CHROM, run) %>% summarise(start = min(start), end = max(end))

setwd("[PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class")

## Permutate regions

scf_start_temp <- scf_start
peak_start_end_temp <- peak_start_end

num_cores <- 38
cl <- makeCluster(num_cores)
registerDoSNOW(cl)

permutations <- foreach(j = 1:500000, .combine = "rbind", .packages = c("dplyr")) %dopar% {

  shuffled_data <- predPeaks %>% filter(pred_ensemble=="SoftPartial") %>% select(-HIGH, -MODERATE, -LOW, -MODIFIER)
  shuffled_data <- shuffled_data %>% arrange(desc(length))

  # Randomly select loci
  selected_scf_start <- sample_n(scf_start_temp, nrow(shuffled_data))
  shuffled_data2 <- shuffled_data %>%
    mutate(CHROM_perm = selected_scf_start$CHROM,
           start_perm = selected_scf_start$start,
           end_perm = start_perm + length)

  # Check if end of new loci's region is available
  shuffled_data2$end_not_available <- NA
  for (i in 1:nrow(shuffled_data2)) {
    peak_end <- peak_start_end_temp %>%
      filter(CHROM==shuffled_data2$CHROM_perm[i] & start <= shuffled_data2$start_perm[i] & end >= shuffled_data2$end_perm[i])
    shuffled_data2$end_not_available[i] <- nrow(peak_end) < 1
  }

  # Check if any randomly selected loci overlap
  shuffled_data2 <- shuffled_data2 %>%
    group_by(CHROM_perm) %>%
    arrange(CHROM_perm, start_perm, end_perm) %>%
    mutate(
      overlap_next_start = lag(end_perm) >= start_perm,
      overlap_prev_end = lead(start_perm) <= end_perm,
      overlap = (overlap_next_start | overlap_prev_end)
    ) %>%
    ungroup()

  # How many permutation failed?
  failed_permutations_n <- shuffled_data2 %>%
    filter(end_not_available | overlap) %>%
    nrow()

  # Which permutation failed?
  failed_permutations <- shuffled_data2 %>%
    filter(end_not_available | overlap)

  # cat("Failed permutations on attempt 1:", failed_permutations_n, "\n")

  # If no permutations failed, print and end iteration
  if (failed_permutations_n < 1) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }

  # If some permutations failed, try and fix them

  attempt <- 2

  while(failed_permutations_n > 0 & attempt <= 100) {

    # Remove used loci from the pool of options
    scf_start_temp2 <- scf_start_temp %>% anti_join(shuffled_data2, by=c("CHROM", "start", "end"))

    # Randomly select loci
    selected_scf_start2 <- sample_n(scf_start_temp2, nrow(failed_permutations))
    failed_permutations <- failed_permutations %>%
      mutate(CHROM_perm = selected_scf_start2$CHROM,
             start_perm = selected_scf_start2$start,
             end_perm = start_perm + length)

    # Check if end of new loci's region is available
    for (i in 1:nrow(failed_permutations)) {
      peak_end <- peak_start_end_temp %>%
        filter(CHROM==failed_permutations$CHROM_perm[i] & start <= failed_permutations$start_perm[i] & end >= failed_permutations$end_perm[i])
      failed_permutations$end_not_available[i] <- nrow(peak_end) < 1

    }

    # Check if any randomly selected loci overlap
    failed_permutations_OG <- shuffled_data2 %>% filter(end_not_available | overlap)
    shuffled_data3 <- shuffled_data2 %>% anti_join(failed_permutations_OG)
    shuffled_data3 <- bind_rows(shuffled_data3, failed_permutations)
    shuffled_data3 <- shuffled_data3 %>%
      group_by(CHROM_perm) %>%
      arrange(CHROM_perm, start_perm, end_perm) %>%
      mutate(
        overlap_next_start = lag(end_perm) >= start_perm,
        overlap_prev_end = lead(start_perm) <= end_perm,
        overlap = (overlap_next_start | overlap_prev_end)
      ) %>%
      ungroup()

    # How many permutation failed?
    failed_permutations_n <- shuffled_data3 %>%
      filter(end_not_available | overlap) %>%
      nrow()

    # Which permutation failed?
    failed_permutations <- shuffled_data3 %>%
      filter(end_not_available | overlap)

    # cat("Failed permutations on attempt", attempt, ":", failed_permutations_n, "\n")

    # Overwrite shuffled_data

    shuffled_data2 <- shuffled_data3

    attempt <- attempt+1

  }

  if (attempt >=3 & attempt <= 100) {
    shuffled_data2$perm <- j
    shuffled_data_final <- shuffled_data2
  }

  shuffled_data_final

}

stopCluster(cl)

# Save file

fileName <- "predPeaks_permutation_SOFTPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt_1-500k.tsv"
fwrite(permutations, file=fileName, sep="\t", row.names=FALSE, col.names=TRUE, quote=FALSE)

```

## Permutation test

### Hard 

```{shell}

module load bedtools

module load anaconda3
conda activate r_env_4.3.1

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class

```

```{r, Hard}

library(tidyverse)
library(data.table)
library(qvalue)

## Import data

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")

predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("[PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class")

permutations <- read_tsv(
  "predPeaks_permutation_HARD_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv"
  )

GO_geneName <- read_tsv("Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("go_info.tsv") %>% rename(go = go_id)



## Join GO terms to prediction regions

predPeaks_temp <- predPeaks %>%
  dplyr::select(CHROM, start, end, pred_ensemble, length)

write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "merged", "intersect")

predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)


## Join GO terms to permutations

permutations_temp <- permutations %>%
  dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length)
fwrite(permutations_temp, "permutations_temp_HARD.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp_HARD.tsv -wo > permutations_GO_merged_HARD.tsv")
system(cmd)

permutationsGO_Hard <- fread("permutations_GO_merged_HARD.tsv", header=FALSE)
colnames(permutationsGO_Hard) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")



## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_Hard %>%
  group_by(perm, val, go, process) %>%
  summarise(intersect_perm = sum(intersect)) %>%
  ungroup()


# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))

### Check that intersect is never greater than one and perform permutation test

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/500000,
            p_proportion = sum(test_proportion)/500000)



# Biological process (Hard)

## Intersect

permTest_intersect_Hard <- permTest_intersect %>% 
  filter(val=="Hard") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permTest_intersect_Hard %>% ggplot(aes(x=p_intersect)) + geom_histogram(bins=50) + ggtitle("Hard")

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes_sweep <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes_sweep = n())

n_genes_genome <- predPeaksGO %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes_genome = n())

n_peaks_sweep <- predPeaksGO %>% 
  filter(val=="Hard") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks_sweep = n())

n_peaks_genome <- predPeaksGO %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks_genome = n())

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes_sweep) %>% 
  left_join(n_peaks_sweep) %>% 
  left_join(n_genes_genome) %>% 
  left_join(n_peaks_genome) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(q <= 0.05) %>% 
  print(n=40)

results <- permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes_sweep) %>% 
  left_join(n_peaks_sweep) %>% 
  left_join(n_genes_genome) %>% 
  left_join(n_peaks_genome)

write_tsv(results, file="go_hard.tsv")

```

### Soft 

```{shell}

module load bedtools

module load anaconda3
conda activate r_env_4.3.1

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class

```

```{r, Soft}

library(tidyverse)
library(data.table)
library(qvalue)

## Import data

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")

predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("[PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class")

permutations <- read_tsv(
  "predPeaks_permutation_SOFT_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv"
  )

GO_geneName <- read_tsv("Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("go_info.tsv") %>% rename(go = go_id)



## Join GO terms to prediction regions

predPeaks_temp <- predPeaks %>%
  dplyr::select(CHROM, start, end, pred_ensemble, length)

write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "merged", "intersect")

predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)


## Join GO terms to permutations

permutations_temp <- permutations %>%
  dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length)
fwrite(permutations_temp, "permutations_temp_SOFT.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp_SOFT.tsv -wo > permutations_GO_merged_SOFT.tsv")
system(cmd)

permutationsGO_Soft <- fread("permutations_GO_merged_SOFT.tsv", header=FALSE)
colnames(permutationsGO_Soft) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")



## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_Soft %>%
  group_by(perm, val, go, process) %>%
  summarise(intersect_perm = sum(intersect)) %>%
  ungroup()


# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))

### Check that intersect is never greater than one and perform permutation test

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/500000,
            p_proportion = sum(test_proportion)/500000)



# Biological process (Soft)

## Intersect

permTest_intersect_Soft <- permTest_intersect %>% 
  filter(val=="Soft") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes_sweep <- predPeaksGO %>% 
  filter(val=="Soft") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes_sweep = n())

n_genes_genome <- predPeaksGO %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes_genome = n())

n_peaks_sweep <- predPeaksGO %>% 
  filter(val=="Soft") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks_sweep = n())

n_peaks_genome <- predPeaksGO %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks_genome = n())

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes_sweep) %>% 
  left_join(n_peaks_sweep) %>% 
  left_join(n_genes_genome) %>% 
  left_join(n_peaks_genome) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(q <= 0.05) %>% 
  print(n=40)

results <- permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes_sweep) %>% 
  left_join(n_peaks_sweep) %>% 
  left_join(n_genes_genome) %>% 
  left_join(n_peaks_genome)

write_tsv(results, file="go_soft.tsv")

```

### HardPartial 

```{shell}

module load bedtools

module load anaconda3
conda activate r_env_4.3.1

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class

```

```{r, HardPartial}

library(tidyverse)
library(data.table)
library(qvalue)

## Import data

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")

predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("[PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class")

permutations <- read_tsv(
  "predPeaks_permutation_HARDPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv"
  )

GO_geneName <- read_tsv("Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("go_info.tsv") %>% rename(go = go_id)



## Join GO terms to prediction regions

predPeaks_temp <- predPeaks %>%
  dplyr::select(CHROM, start, end, pred_ensemble, length)

write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "merged", "intersect")

predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)


## Join GO terms to permutations

permutations_temp <- permutations %>%
  dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length)
fwrite(permutations_temp, "permutations_temp_HARDPARTIAL.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp_HARDPARTIAL.tsv -wo > permutations_GO_merged_HARDPARTIAL.tsv")
system(cmd)

permutationsGO_HardPartial <- fread("permutations_GO_merged_HARDPARTIAL.tsv", header=FALSE)
colnames(permutationsGO_HardPartial) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")



## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_HardPartial %>%
  group_by(perm, val, go, process) %>%
  summarise(intersect_perm = sum(intersect)) %>%
  ungroup()


# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))

### Check that intersect is never greater than one and perform permutation test

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/500000,
            p_proportion = sum(test_proportion)/500000)



# Biological process (HardPartial)

## Intersect

permTest_intersect_HardPartial <- permTest_intersect %>% 
  filter(val=="HardPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes_sweep <- predPeaksGO %>% 
  filter(val=="HardPartial") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes_sweep = n())

n_genes_genome <- predPeaksGO %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes_genome = n())

n_peaks_sweep <- predPeaksGO %>% 
  filter(val=="HardPartial") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks_sweep = n())

n_peaks_genome <- predPeaksGO %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks_genome = n())

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes_sweep) %>% 
  left_join(n_peaks_sweep) %>% 
  left_join(n_genes_genome) %>% 
  left_join(n_peaks_genome) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(q <= 0.05) %>% 
  print(n=40)

results <- permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes_sweep) %>% 
  left_join(n_peaks_sweep) %>% 
  left_join(n_genes_genome) %>% 
  left_join(n_peaks_genome)

write_tsv(results, file="go_hardpartial.tsv")

```

### SoftPartial 

```{shell}

module load bedtools

module load anaconda3
conda activate r_env_4.3.1

cd [PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class

```

```{r, HardPartial}

library(tidyverse)
library(data.table)
library(qvalue)

## Import data

setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")

predPeaks <- read_tsv("predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv")

setwd("[PATH_TO]/partialSHIC_DsGRP/sp2_median/pred_empirical/permutations_GO_per_class")

permutations <- read_tsv(
  "predPeaks_permutation_HARDPARTIAL_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_2ndAttempt.tsv"
  )

GO_geneName <- read_tsv("Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

GO_bed <- read_tsv("FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

go_info <- read_tsv("go_info.tsv") %>% rename(go = go_id)



## Join GO terms to prediction regions

predPeaks_temp <- predPeaks %>%
  dplyr::select(CHROM, start, end, pred_ensemble, length)

write.table(predPeaks_temp, "predPeaks_temp.tsv", row.names = F, col.names = F, quote = F, sep = "\t")
write.table(GO_bed, "GO.bed", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b predPeaks_temp.tsv -wo > predPeaks_GO_merged.tsv")
system(cmd)

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)
colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_predPeaks", "start_pos_predPeaks", "end_pos_predPeaks", "val", "length", "merged", "intersect")

predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)


## Join GO terms to permutations

permutations_temp <- permutations %>%
  dplyr::select(CHROM_perm, start_perm, end_perm, perm, pred_ensemble, length)
fwrite(permutations_temp, "permutations_temp_SOFTPARTIAL.tsv", row.names = F, col.names = F, quote = F, sep = "\t")

### bedtools

cmd <- paste0("bedtools intersect -a GO.bed -b permutations_temp_SOFTPARTIAL.tsv -wo > permutations_GO_merged_SOFTPARTIAL.tsv")
system(cmd)

permutationsGO_SoftPartial <- fread("permutations_GO_merged_SOFTPARTIAL.tsv", header=FALSE)
colnames(permutationsGO_SoftPartial) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")



## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect)) %>% 
  ungroup()

## Permutation summary

permutationsGO_summary <- permutationsGO_SoftPartial %>%
  group_by(perm, val, go, process) %>%
  summarise(intersect_perm = sum(intersect)) %>%
  ungroup()


# Permutation test

## Intersect

intersect_genome <- predPeaksGO %>% group_by(go) %>% summarise(intersect_genome = sum(intersect))

### Check that intersect is never greater than one and perform permutation test

permTest_intersect <- permutationsGO_summary %>%
  left_join(select(predPeaksGO_summary, -process), by=c("val", "go")) %>%
  filter(is.na(intersect_empirical)==FALSE) %>%
  left_join(intersect_genome) %>%
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>%
  mutate(intersect_proportion_perm = intersect_perm/intersect_genome) %>%
  mutate(stat_intersect = intersect_empirical - intersect_perm) %>%
  mutate(test_intersect = intersect_perm >= intersect_empirical) %>%
  mutate(stat_proportion = intersect_proportion_empirical - intersect_proportion_perm) %>%
  mutate(test_proportion = intersect_proportion_perm >= intersect_proportion_empirical) %>%
  group_by(val, go, process) %>%
  summarise(p_intersect = sum(test_intersect)/500000,
            p_proportion = sum(test_proportion)/500000)



# Biological process (SoftPartial)

## Intersect

permTest_intersect_SoftPartial <- permTest_intersect %>% 
  filter(val=="SoftPartial") %>% 
  filter(process=="P") %>% 
  distinct() %>% 
  arrange(p_intersect)

permutationsGO_summary_mean <- permutationsGO_summary %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_perm_mean = mean(intersect_perm))

n_genes_sweep <- predPeaksGO %>% 
  filter(val=="SoftPartial") %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes_sweep = n())

n_genes_genome <- predPeaksGO %>% 
  select(fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_genes_genome = n())

n_peaks_sweep <- predPeaksGO %>% 
  filter(val=="SoftPartial") %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks_sweep = n())

n_peaks_genome <- predPeaksGO %>% 
  select(scf, start_pos_predPeaks, end_pos_predPeaks, fbgn, go) %>% 
  distinct() %>% 
  group_by(go) %>% 
  summarise(n_peaks_genome = n())

permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes_sweep) %>% 
  left_join(n_peaks_sweep) %>% 
  left_join(n_genes_genome) %>% 
  left_join(n_peaks_genome) %>% 
  select(-p_intersect, -intersect_genome, -intersect_empirical, -intersect_perm_mean) %>% 
  filter(q <= 0.05) %>% 
  print(n=40)

results <- permTest_intersect_Soft %>% 
  left_join(dplyr::select(go_info, go, name_1006)) %>% 
  left_join(intersect_genome) %>% 
  left_join(predPeaksGO_summary) %>% 
  left_join(permutationsGO_summary_mean) %>% 
  mutate(intersect_proportion_empirical = intersect_empirical/intersect_genome) %>% 
  mutate(intersect_proportion_perm_mean = intersect_perm_mean/intersect_genome) %>% 
  left_join(n_genes_sweep) %>% 
  left_join(n_peaks_sweep) %>% 
  left_join(n_genes_genome) %>% 
  left_join(n_peaks_genome)

write_tsv(results, file="go_softpartial.tsv")

```

## nonparametric fdr

### Hard 

```{r, Hard nonparametric fdr}

library(tidyverse)
library(data.table)
library(qvalue)

## Import data

### Empirical predictions
setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")

predPeaks <- read_tsv(
  "predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv"
)

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")

### GO to gene name key
GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

### gene locations
GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

### GO names and descriptions
go_info <- read_tsv(
  "/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv"
  ) %>% rename(go = go_id)

## Join GO terms for predictions peaks

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)

colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", 
                           "scf_predPeaks", "start_pos_predPeaks", 
                           "end_pos_predPeaks", "val", "length", "merged", 
                           "intersect")

predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

predPeaksGO <- predPeaksGO %>% mutate(gene_length=end_pos-start_pos)

## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect), 
            gene_length_sum_hit = sum(gene_length), 
            gene_length_mean_hit = mean(gene_length)) %>% 
  ungroup()

## Permutation summary

permutationsGO_Hard <- fread("permutations_GO_merged_HARD.tsv", header=FALSE)
colnames(permutationsGO_Hard) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")

permutationsGO_summary <- permutationsGO_Hard %>%
  group_by(perm, val, go, process) %>%
  summarise(intersect_perm = sum(intersect)) %>%
  ungroup()
 
# Permutation

# Loop

GOs <- predPeaksGO_summary %>% 
  filter(val=="Hard") %>% 
  filter(go!="NA") %>% 
  filter(process=="P") %>% 
  select(go) %>% 
  distinct()

# Convert predPeaksGO_summary and permutationsGO_summary to data.tables
setDT(predPeaksGO_summary, key = "go")
setDT(permutationsGO_summary, key = "go")

# Initialize an empty data.table to store results
results <- data.table()

# Loop through each row of GOs
for (i in 1:nrow(GOs)) {
  
  intersect_empirical <- predPeaksGO_summary[go == GOs$go[i] & val == "Hard"]
  
  list_genes_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="Hard") %>% 
    select(fbgn) %>% 
    distinct() %>% 
    as.list()
  
  n_genes_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="Hard") %>% 
    select(fbgn) %>% 
    distinct() %>% 
    nrow()
  
  n_merged_windows_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="Hard") %>%
    select(scf, merged) %>% 
    distinct() %>% 
    nrow()
  
  n_sub_windows_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="Hard") %>%
    select(scf, start_pos_predPeaks, end_pos_predPeaks) %>%
    distinct() %>%
    nrow()
  
  n_genes_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>% 
    select(fbgn) %>% 
    distinct() %>% 
    nrow()
  
  n_merged_windows_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    select(scf, merged, val) %>% 
    distinct() %>% 
    nrow()
  
  n_sub_windows_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    select(scf, start_pos_predPeaks, end_pos_predPeaks) %>%
    distinct() %>%
    nrow()
  
  clustering_hit <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    filter(val=="Hard") %>%
    select(scf, fbgn, merged) %>%
    distinct() %>%
    group_by(scf, merged) %>%
    tally()
  
  clustering_hit_flag <- sum(clustering_hit$n)/nrow(clustering_hit)!=1
  
  clustering_hit <- sum(clustering_hit$n)/nrow(clustering_hit)
  
  bigGene_hit <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    filter(val=="Hard") %>%
    select(scf, fbgn, merged, start_pos_predPeaks, end_pos_predPeaks) %>%
    group_by(fbgn) %>%
    tally()
  
  bigGene_hit_flag <- sum(bigGene_hit$n)/nrow(bigGene_hit)!=1
  
  bigGene_hit <- sum(bigGene_hit$n)/nrow(bigGene_hit)
  
  clustering_genome <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    select(scf, fbgn, merged, val) %>%
    distinct() %>%
    group_by(scf, merged, val) %>%
    tally()
  
  clustering_genome_flag <- sum(clustering_genome$n)/nrow(clustering_genome)!=1
  
  clustering_genome <- sum(clustering_genome$n)/nrow(clustering_genome)
  
  bigGene_genome <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    select(scf, fbgn, merged, start_pos_predPeaks, end_pos_predPeaks, val) %>%
    group_by(fbgn, val) %>%
    tally()
  
  bigGene_genome_flag <- sum(bigGene_genome$n)/nrow(bigGene_genome)!=1
  
  bigGene_genome <- sum(bigGene_genome$n)/nrow(bigGene_genome)
  
  perm_temp <- permutationsGO_summary[go == GOs$go[i], ]
  
  intersect_perm_mean <- mean(perm_temp$intersect_perm)
  
  n_perm_hit <- nrow(perm_temp)
  
  prob_hit <- n_perm_hit/500000
  
  n_perm_hit_grt_eq_emp <- perm_temp[intersect_perm >= intersect_empirical$intersect_empirical, .N]
  
  prob_overlap_all <- n_perm_hit_grt_eq_emp / 500000
  prob_overlap_hit <- n_perm_hit_grt_eq_emp / n_perm_hit
  
  dt <- data.table(go = GOs$go[i],
                   intersect_empirical = intersect_empirical$intersect_empirical,
                   intersect_perm_mean = intersect_perm_mean,
                   n_perm_hit = n_perm_hit,
                   prob_hit = prob_hit,
                   n_perm_hit_grt_eq_emp = n_perm_hit_grt_eq_emp,
                   prob_overlap_all = prob_overlap_all, 
                   prob_overlap_hit = prob_overlap_hit, 
                   n_genes_hit = n_genes_hit, 
                   n_sub_windows_hit = n_sub_windows_hit, 
                   n_merged_windows_hit = n_merged_windows_hit, 
                   clustering_hit_flag = clustering_hit_flag, 
                   clustering_hit = clustering_hit, 
                   bigGene_hit_flag = bigGene_hit_flag, 
                   bigGene_hit = bigGene_hit, 
                   n_genes_genome = n_genes_genome, 
                   n_sub_windows_genome = n_sub_windows_genome, 
                   n_merged_windows_genome = n_merged_windows_genome, 
                   clustering_genome_flag = clustering_genome_flag, 
                   clustering_genome = clustering_genome, 
                   bigGene_genome_flag = bigGene_genome_flag, 
                   bigGene_genome = bigGene_genome, 
                   genes_hit = list_genes_hit)
  
  results <- rbindlist(list(results, dt))
  
  cat(i, GOs$go[i], "\n")
  
}

# view results
results %>% left_join(select(go_info, -definition_1006)) %>%  arrange(prob_overlap_hit)


### non-parametric FDR

### FDR is defined as the expected value of the ratio of false discoveries (V) 
### to the total number of rejections (R). Mathematically, it's expressed as 
### FDR = E(V/R). This means that on average, what proportion of the rejections 
### are expected to be false discoveries.

### non-parametric FDR

### FDR is defined as the expected value of the ratio of false discoveries (V)
### to the total number of rejections (R). Mathematically, it's expressed as
### FDR = E(V/R). This means that on average, what proportion of the rejections
### are expected to be false discoveries.

results <- results %>% arrange(prob_overlap_hit)
results_perm_fdr <- results %>% arrange(prob_overlap_hit)

n_GOs_emp <- nrow(GOs)

set.seed(19800607)
perm_nums <- permutationsGO_summary %>%
  select(perm) %>%
  distinct() %>%
  sample_n(1000)
perm_nums <- perm_nums$perm

counter <- 1

for (perm_num in perm_nums) {

  cat(counter, "perm", perm_num, "\n")

  GOs_perm <- permutationsGO_summary %>%
    filter(perm==perm_num) %>%
    filter(val=="Hard") %>%
    filter(go!="NA") %>%
    filter(process=="P") %>%
    select(go) %>%
    distinct()

  n_GOs_perm <- nrow(GOs_perm)

  # Initialize an empty data.table to store results
  results_perm <- data.table()

  # Loop through each row of GOs
  for (i in 1:nrow(GOs_perm)) {

    intersect_perm_num <- permutationsGO_summary[go == GOs_perm$go[i]]
    intersect_perm_num <- intersect_perm_num %>% filter(perm==perm_num)

    perm_temp <- permutationsGO_summary[go == GOs_perm$go[i], ]
    perm_temp <- perm_temp %>% filter(perm!=perm_num)

    intersect_perm_mean <- mean(perm_temp$intersect_perm)

    n_perm_hit <- nrow(perm_temp)

    n_perm_hit_grt_eq_perm_num <- perm_temp[intersect_perm >= intersect_perm_num$intersect_perm, .N]

    p <- n_perm_hit_grt_eq_perm_num / n_perm_hit

    dt <- data.table(go = GOs_perm$go[i],
                     intersect_perm_num = intersect_perm_num$intersect_perm,
                     intersect_perm_mean = intersect_perm_mean,
                     n_perm_hit = n_perm_hit,
                     prob_perm_hit = n_perm_hit/499999,
                     n_perm_hit_grt_eq_perm_num = n_perm_hit_grt_eq_perm_num,
                     p = p)

    results_perm <- rbindlist(list(results_perm, dt))

    # cat(i, GOs_perm$go[i], "p", p, "prob_perm_hit", n_perm_hit/499999, "\n")

  }

  results_perm <- results_perm %>% arrange(p)

  prob_overlap_hit <- results %>% select(prob_overlap_hit)

  for (i in 1:nrow(results)) {

    p_emp <- results$prob_overlap_hit[i]

    V <- results_perm %>% filter(p<=p_emp) %>% nrow()
    V_scaled <- round(V * (n_GOs_emp/n_GOs_perm))

    R <- i

    fdr <- V/R
    fdr_scaled <- V_scaled/R

    prob_overlap_hit$R[i] <- R
    prob_overlap_hit$V[i] <- V
    prob_overlap_hit$fdr[i] <- fdr
    prob_overlap_hit$V_scaled[i] <- V_scaled
    prob_overlap_hit$fdr_scaled[i] <- fdr_scaled

    # cat(i, "p", p_emp, "V", V, "V_scaled", V_scaled, "FDR", fdr, "FDR_scaled", fdr_scaled, "\n")

  }

  results_perm_fdr$V <- prob_overlap_hit$V
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("V", perm_num) := V)
  results_perm_fdr$fdr <- prob_overlap_hit$fdr
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("fdr", perm_num) := fdr)
  results_perm_fdr$V_scaled <- prob_overlap_hit$V_scaled
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("V_scaled", perm_num) := V_scaled)
  results_perm_fdr$fdr_scaled <- prob_overlap_hit$fdr_scaled
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("fdr_scaled", perm_num) := fdr_scaled)

  counter <- counter + 1

}

write_tsv(results_perm_fdr, file="nonparametric_fdr_HARD.tsv")

results_perm_fdr <- read_tsv("nonparametric_fdr_HARD.tsv")

# arithmatic and geometric mean fdr
results_perm_fdr <- results_perm_fdr %>%
  select(go:prob_hit, prob_overlap_hit, contains("fdr_scaled")) %>%
  rowwise() %>%
  mutate(
    mean_fdr_arithmetic = mean(c_across(fdr_scaled100121:fdr_scaled98810)),
    mean_fdr_geometric = exp(mean(log(c_across(fdr_scaled100121:fdr_scaled98810)))), 
    mean_fdr_geometric2 = exp(mean(log(ifelse(c_across(fdr_scaled100121:fdr_scaled98810) == 0, 1e-10, c_across(fdr_scaled100121:fdr_scaled98810)))))
  )

# output for supplementary table
output <- results_perm_fdr  %>% 
  select(-contains("fdr_scaled")) %>% 
  left_join(select(results, go, prob_overlap_hit, n_genes_hit, n_genes_genome, genes_hit)) %>% 
  left_join(select(go_info, -definition_1006)) %>% 
  select(-prob_hit, -mean_fdr_arithmetic, -mean_fdr_geometric) %>% 
  select(go:mean_fdr_geometric2, name_1006, n_genes_hit, n_genes_genome, genes_hit) 

output$genes_hit <- sapply(output$genes_hit, function(x) paste(x, collapse = ","))

write_tsv(output, file = "GO_nonparametric_fdr_HARD_supp_table.tsv")

```

### Soft 

```{r, Soft nonparametric fdr}

library(tidyverse)
library(data.table)
library(qvalue)

## Import data

### Empirical predictions
setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")

predPeaks <- read_tsv(
  "predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv"
)

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")

### GO to gene name key
GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

### gene locations
GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

### GO names and descriptions
go_info <- read_tsv(
  "/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv"
  ) %>% rename(go = go_id)

## Join GO terms for predictions peaks

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)

colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", 
                           "scf_predPeaks", "start_pos_predPeaks", 
                           "end_pos_predPeaks", "val", "length", "merged", 
                           "intersect")

predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

predPeaksGO <- predPeaksGO %>% mutate(gene_length=end_pos-start_pos)

## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect), 
            gene_length_sum_hit = sum(gene_length), 
            gene_length_mean_hit = mean(gene_length)) %>% 
  ungroup()

## Permutation summary

permutationsGO_Soft <- fread("permutations_GO_merged_SOFT.tsv", header=FALSE)
colnames(permutationsGO_Soft) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")

permutationsGO_summary <- permutationsGO_Soft %>%
  group_by(perm, val, go, process) %>%
  summarise(intersect_perm = sum(intersect)) %>%
  ungroup()
 
# Permutation

# Loop

GOs <- predPeaksGO_summary %>% 
  filter(val=="Soft") %>% 
  filter(go!="NA") %>% 
  filter(process=="P") %>% 
  select(go) %>% 
  distinct()

# Convert predPeaksGO_summary and permutationsGO_summary to data.tables
setDT(predPeaksGO_summary, key = "go")
setDT(permutationsGO_summary, key = "go")

# Initialize an empty data.table to store results
results <- data.table()

# Loop through each row of GOs
for (i in 1:nrow(GOs)) {
  
  intersect_empirical <- predPeaksGO_summary[go == GOs$go[i] & val == "Soft"]
  
  list_genes_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="Soft") %>% 
    select(fbgn) %>% 
    distinct() %>% 
    as.list()
  
  n_genes_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="Soft") %>% 
    select(fbgn) %>% 
    distinct() %>% 
    nrow()
  
  n_merged_windows_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="Soft") %>%
    select(scf, merged) %>% 
    distinct() %>% 
    nrow()
  
  n_sub_windows_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="Soft") %>%
    select(scf, start_pos_predPeaks, end_pos_predPeaks) %>%
    distinct() %>%
    nrow()
  
  n_genes_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>% 
    select(fbgn) %>% 
    distinct() %>% 
    nrow()
  
  n_merged_windows_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    select(scf, merged, val) %>% 
    distinct() %>% 
    nrow()
  
  n_sub_windows_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    select(scf, start_pos_predPeaks, end_pos_predPeaks) %>%
    distinct() %>%
    nrow()
  
  clustering_hit <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    filter(val=="Soft") %>%
    select(scf, fbgn, merged) %>%
    distinct() %>%
    group_by(scf, merged) %>%
    tally()
  
  clustering_hit_flag <- sum(clustering_hit$n)/nrow(clustering_hit)!=1
  
  clustering_hit <- sum(clustering_hit$n)/nrow(clustering_hit)
  
  bigGene_hit <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    filter(val=="Soft") %>%
    select(scf, fbgn, merged, start_pos_predPeaks, end_pos_predPeaks) %>%
    group_by(fbgn) %>%
    tally()
  
  bigGene_hit_flag <- sum(bigGene_hit$n)/nrow(bigGene_hit)!=1
  
  bigGene_hit <- sum(bigGene_hit$n)/nrow(bigGene_hit)
  
  clustering_genome <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    select(scf, fbgn, merged, val) %>%
    distinct() %>%
    group_by(scf, merged, val) %>%
    tally()
  
  clustering_genome_flag <- sum(clustering_genome$n)/nrow(clustering_genome)!=1
  
  clustering_genome <- sum(clustering_genome$n)/nrow(clustering_genome)
  
  bigGene_genome <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    select(scf, fbgn, merged, start_pos_predPeaks, end_pos_predPeaks, val) %>%
    group_by(fbgn, val) %>%
    tally()
  
  bigGene_genome_flag <- sum(bigGene_genome$n)/nrow(bigGene_genome)!=1
  
  bigGene_genome <- sum(bigGene_genome$n)/nrow(bigGene_genome)
  
  perm_temp <- permutationsGO_summary[go == GOs$go[i], ]
  
  intersect_perm_mean <- mean(perm_temp$intersect_perm)
  
  n_perm_hit <- nrow(perm_temp)
  
  prob_hit <- n_perm_hit/500000
  
  n_perm_hit_grt_eq_emp <- perm_temp[intersect_perm >= intersect_empirical$intersect_empirical, .N]
  
  prob_overlap_all <- n_perm_hit_grt_eq_emp / 500000
  prob_overlap_hit <- n_perm_hit_grt_eq_emp / n_perm_hit
  
  dt <- data.table(go = GOs$go[i],
                   intersect_empirical = intersect_empirical$intersect_empirical,
                   intersect_perm_mean = intersect_perm_mean,
                   n_perm_hit = n_perm_hit,
                   prob_hit = prob_hit,
                   n_perm_hit_grt_eq_emp = n_perm_hit_grt_eq_emp,
                   prob_overlap_all = prob_overlap_all, 
                   prob_overlap_hit = prob_overlap_hit, 
                   n_genes_hit = n_genes_hit, 
                   n_sub_windows_hit = n_sub_windows_hit, 
                   n_merged_windows_hit = n_merged_windows_hit, 
                   clustering_hit_flag = clustering_hit_flag, 
                   clustering_hit = clustering_hit, 
                   bigGene_hit_flag = bigGene_hit_flag, 
                   bigGene_hit = bigGene_hit, 
                   n_genes_genome = n_genes_genome, 
                   n_sub_windows_genome = n_sub_windows_genome, 
                   n_merged_windows_genome = n_merged_windows_genome, 
                   clustering_genome_flag = clustering_genome_flag, 
                   clustering_genome = clustering_genome, 
                   bigGene_genome_flag = bigGene_genome_flag, 
                   bigGene_genome = bigGene_genome, 
                   genes_hit = list_genes_hit)
  
  results <- rbindlist(list(results, dt))
  
  cat(i, GOs$go[i], "\n")
  
}

# view results
results %>% left_join(select(go_info, -definition_1006)) %>%  arrange(prob_overlap_hit)


### non-parametric FDR

### FDR is defined as the expected value of the ratio of false discoveries (V) 
### to the total number of rejections (R). Mathematically, it's expressed as 
### FDR = E(V/R). This means that on average, what proportion of the rejections 
### are expected to be false discoveries.

### non-parametric FDR

### FDR is defined as the expected value of the ratio of false discoveries (V)
### to the total number of rejections (R). Mathematically, it's expressed as
### FDR = E(V/R). This means that on average, what proportion of the rejections
### are expected to be false discoveries.

results <- results %>% arrange(prob_overlap_hit)
results_perm_fdr <- results %>% arrange(prob_overlap_hit)

n_GOs_emp <- nrow(GOs)

set.seed(19800607)
perm_nums <- permutationsGO_summary %>%
  select(perm) %>%
  distinct() %>%
  sample_n(1000)
perm_nums <- perm_nums$perm

counter <- 1

for (perm_num in perm_nums) {

  cat(counter, "perm", perm_num, "\n")

  GOs_perm <- permutationsGO_summary %>%
    filter(perm==perm_num) %>%
    filter(val=="Soft") %>%
    filter(go!="NA") %>%
    filter(process=="P") %>%
    select(go) %>%
    distinct()

  n_GOs_perm <- nrow(GOs_perm)

  # Initialize an empty data.table to store results
  results_perm <- data.table()

  # Loop through each row of GOs
  for (i in 1:nrow(GOs_perm)) {

    intersect_perm_num <- permutationsGO_summary[go == GOs_perm$go[i]]
    intersect_perm_num <- intersect_perm_num %>% filter(perm==perm_num)

    perm_temp <- permutationsGO_summary[go == GOs_perm$go[i], ]
    perm_temp <- perm_temp %>% filter(perm!=perm_num)

    intersect_perm_mean <- mean(perm_temp$intersect_perm)

    n_perm_hit <- nrow(perm_temp)

    n_perm_hit_grt_eq_perm_num <- perm_temp[intersect_perm >= intersect_perm_num$intersect_perm, .N]

    p <- n_perm_hit_grt_eq_perm_num / n_perm_hit

    dt <- data.table(go = GOs_perm$go[i],
                     intersect_perm_num = intersect_perm_num$intersect_perm,
                     intersect_perm_mean = intersect_perm_mean,
                     n_perm_hit = n_perm_hit,
                     prob_perm_hit = n_perm_hit/499999,
                     n_perm_hit_grt_eq_perm_num = n_perm_hit_grt_eq_perm_num,
                     p = p)

    results_perm <- rbindlist(list(results_perm, dt))

    # cat(i, GOs_perm$go[i], "p", p, "prob_perm_hit", n_perm_hit/499999, "\n")

  }

  results_perm <- results_perm %>% arrange(p)

  prob_overlap_hit <- results %>% select(prob_overlap_hit)

  for (i in 1:nrow(results)) {

    p_emp <- results$prob_overlap_hit[i]

    V <- results_perm %>% filter(p<=p_emp) %>% nrow()
    V_scaled <- round(V * (n_GOs_emp/n_GOs_perm))

    R <- i

    fdr <- V/R
    fdr_scaled <- V_scaled/R

    prob_overlap_hit$R[i] <- R
    prob_overlap_hit$V[i] <- V
    prob_overlap_hit$fdr[i] <- fdr
    prob_overlap_hit$V_scaled[i] <- V_scaled
    prob_overlap_hit$fdr_scaled[i] <- fdr_scaled

    # cat(i, "p", p_emp, "V", V, "V_scaled", V_scaled, "FDR", fdr, "FDR_scaled", fdr_scaled, "\n")

  }

  results_perm_fdr$V <- prob_overlap_hit$V
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("V", perm_num) := V)
  results_perm_fdr$fdr <- prob_overlap_hit$fdr
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("fdr", perm_num) := fdr)
  results_perm_fdr$V_scaled <- prob_overlap_hit$V_scaled
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("V_scaled", perm_num) := V_scaled)
  results_perm_fdr$fdr_scaled <- prob_overlap_hit$fdr_scaled
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("fdr_scaled", perm_num) := fdr_scaled)

  counter <- counter + 1

}

write_tsv(results_perm_fdr, file="nonparametric_fdr_SOFT.tsv")

results_perm_fdr <- read_tsv("nonparametric_fdr_SOFT.tsv")

# arithmatic and geometric mean fdr
results_perm_fdr <- results_perm_fdr %>%
  select(go:prob_hit, prob_overlap_hit, contains("fdr_scaled")) %>%
  rowwise() %>%
  mutate(
    mean_fdr_arithmetic = mean(c_across(fdr_scaled100121:fdr_scaled98810)),
    mean_fdr_geometric = exp(mean(log(c_across(fdr_scaled100121:fdr_scaled98810)))), 
    mean_fdr_geometric2 = exp(mean(log(ifelse(c_across(fdr_scaled100121:fdr_scaled98810) == 0, 1e-10, c_across(fdr_scaled100121:fdr_scaled98810)))))
  )

# output for supplementary table
output <- results_perm_fdr  %>% 
  select(-contains("fdr_scaled")) %>% 
  left_join(select(results, go, prob_overlap_hit, n_genes_hit, n_genes_genome, genes_hit)) %>% 
  left_join(select(go_info, -definition_1006)) %>% 
  select(-prob_hit, -mean_fdr_arithmetic, -mean_fdr_geometric) %>% 
  select(go:mean_fdr_geometric2, name_1006, n_genes_hit, n_genes_genome, genes_hit) 

output$genes_hit <- sapply(output$genes_hit, function(x) paste(x, collapse = ","))

write_tsv(output, file = "GO_nonparametric_fdr_SOFT_supp_table.tsv")

```

### HardPartial 

```{r, HardPartial nonparametric fdr}

library(tidyverse)
library(data.table)
library(qvalue)

## Import data

### Empirical predictions
setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")

predPeaks <- read_tsv(
  "predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv"
)

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")

### GO to gene name key
GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

### gene locations
GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

### GO names and descriptions
go_info <- read_tsv(
  "/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv"
  ) %>% rename(go = go_id)

## Join GO terms for predictions peaks

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)

colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", 
                           "scf_predPeaks", "start_pos_predPeaks", 
                           "end_pos_predPeaks", "val", "length", "merged", 
                           "intersect")

predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

predPeaksGO <- predPeaksGO %>% mutate(gene_length=end_pos-start_pos)

## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect), 
            gene_length_sum_hit = sum(gene_length), 
            gene_length_mean_hit = mean(gene_length)) %>% 
  ungroup()

## Permutation summary

permutationsGO_HardPartial <- fread("permutations_GO_merged_HARDPARTIAL.tsv", header=FALSE)
colnames(permutationsGO_HardPartial) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")

permutationsGO_summary <- permutationsGO_HardPartial %>%
  group_by(perm, val, go, process) %>%
  summarise(intersect_perm = sum(intersect)) %>%
  ungroup()
 
# Permutation

# Loop

GOs <- predPeaksGO_summary %>% 
  filter(val=="HardPartial") %>% 
  filter(go!="NA") %>% 
  filter(process=="P") %>% 
  select(go) %>% 
  distinct()

# Convert predPeaksGO_summary and permutationsGO_summary to data.tables
setDT(predPeaksGO_summary, key = "go")
setDT(permutationsGO_summary, key = "go")

# Initialize an empty data.table to store results
results <- data.table()

# Loop through each row of GOs
for (i in 1:nrow(GOs)) {
  
  intersect_empirical <- predPeaksGO_summary[go == GOs$go[i] & val == "HardPartial"]
  
  list_genes_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="HardPartial") %>% 
    select(fbgn) %>% 
    distinct() %>% 
    as.list()
  
  n_genes_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="HardPartial") %>% 
    select(fbgn) %>% 
    distinct() %>% 
    nrow()
  
  n_merged_windows_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="HardPartial") %>%
    select(scf, merged) %>% 
    distinct() %>% 
    nrow()
  
  n_sub_windows_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="HardPartial") %>%
    select(scf, start_pos_predPeaks, end_pos_predPeaks) %>%
    distinct() %>%
    nrow()
  
  n_genes_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>% 
    select(fbgn) %>% 
    distinct() %>% 
    nrow()
  
  n_merged_windows_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    select(scf, merged, val) %>% 
    distinct() %>% 
    nrow()
  
  n_sub_windows_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    select(scf, start_pos_predPeaks, end_pos_predPeaks) %>%
    distinct() %>%
    nrow()
  
  clustering_hit <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    filter(val=="HardPartial") %>%
    select(scf, fbgn, merged) %>%
    distinct() %>%
    group_by(scf, merged) %>%
    tally()
  
  clustering_hit_flag <- sum(clustering_hit$n)/nrow(clustering_hit)!=1
  
  clustering_hit <- sum(clustering_hit$n)/nrow(clustering_hit)
  
  bigGene_hit <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    filter(val=="HardPartial") %>%
    select(scf, fbgn, merged, start_pos_predPeaks, end_pos_predPeaks) %>%
    group_by(fbgn) %>%
    tally()
  
  bigGene_hit_flag <- sum(bigGene_hit$n)/nrow(bigGene_hit)!=1
  
  bigGene_hit <- sum(bigGene_hit$n)/nrow(bigGene_hit)
  
  clustering_genome <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    select(scf, fbgn, merged, val) %>%
    distinct() %>%
    group_by(scf, merged, val) %>%
    tally()
  
  clustering_genome_flag <- sum(clustering_genome$n)/nrow(clustering_genome)!=1
  
  clustering_genome <- sum(clustering_genome$n)/nrow(clustering_genome)
  
  bigGene_genome <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    select(scf, fbgn, merged, start_pos_predPeaks, end_pos_predPeaks, val) %>%
    group_by(fbgn, val) %>%
    tally()
  
  bigGene_genome_flag <- sum(bigGene_genome$n)/nrow(bigGene_genome)!=1
  
  bigGene_genome <- sum(bigGene_genome$n)/nrow(bigGene_genome)
  
  perm_temp <- permutationsGO_summary[go == GOs$go[i], ]
  
  intersect_perm_mean <- mean(perm_temp$intersect_perm)
  
  n_perm_hit <- nrow(perm_temp)
  
  prob_hit <- n_perm_hit/500000
  
  n_perm_hit_grt_eq_emp <- perm_temp[intersect_perm >= intersect_empirical$intersect_empirical, .N]
  
  prob_overlap_all <- n_perm_hit_grt_eq_emp / 500000
  prob_overlap_hit <- n_perm_hit_grt_eq_emp / n_perm_hit
  
  dt <- data.table(go = GOs$go[i],
                   intersect_empirical = intersect_empirical$intersect_empirical,
                   intersect_perm_mean = intersect_perm_mean,
                   n_perm_hit = n_perm_hit,
                   prob_hit = prob_hit,
                   n_perm_hit_grt_eq_emp = n_perm_hit_grt_eq_emp,
                   prob_overlap_all = prob_overlap_all, 
                   prob_overlap_hit = prob_overlap_hit, 
                   n_genes_hit = n_genes_hit, 
                   n_sub_windows_hit = n_sub_windows_hit, 
                   n_merged_windows_hit = n_merged_windows_hit, 
                   clustering_hit_flag = clustering_hit_flag, 
                   clustering_hit = clustering_hit, 
                   bigGene_hit_flag = bigGene_hit_flag, 
                   bigGene_hit = bigGene_hit, 
                   n_genes_genome = n_genes_genome, 
                   n_sub_windows_genome = n_sub_windows_genome, 
                   n_merged_windows_genome = n_merged_windows_genome, 
                   clustering_genome_flag = clustering_genome_flag, 
                   clustering_genome = clustering_genome, 
                   bigGene_genome_flag = bigGene_genome_flag, 
                   bigGene_genome = bigGene_genome, 
                   genes_hit = list_genes_hit)
  
  results <- rbindlist(list(results, dt))
  
  cat(i, GOs$go[i], "\n")
  
}

# view results
results %>% left_join(select(go_info, -definition_1006)) %>%  arrange(prob_overlap_hit)


### non-parametric FDR

### FDR is defined as the expected value of the ratio of false discoveries (V) 
### to the total number of rejections (R). Mathematically, it's expressed as 
### FDR = E(V/R). This means that on average, what proportion of the rejections 
### are expected to be false discoveries.

### non-parametric FDR

### FDR is defined as the expected value of the ratio of false discoveries (V)
### to the total number of rejections (R). Mathematically, it's expressed as
### FDR = E(V/R). This means that on average, what proportion of the rejections
### are expected to be false discoveries.

results <- results %>% arrange(prob_overlap_hit)
results_perm_fdr <- results %>% arrange(prob_overlap_hit)

n_GOs_emp <- nrow(GOs)

set.seed(19800607)
perm_nums <- permutationsGO_summary %>%
  select(perm) %>%
  distinct() %>%
  sample_n(1000)
perm_nums <- perm_nums$perm

counter <- 1

for (perm_num in perm_nums) {

  cat(counter, "perm", perm_num, "\n")

  GOs_perm <- permutationsGO_summary %>%
    filter(perm==perm_num) %>%
    filter(val=="HardPartial") %>%
    filter(go!="NA") %>%
    filter(process=="P") %>%
    select(go) %>%
    distinct()

  n_GOs_perm <- nrow(GOs_perm)

  # Initialize an empty data.table to store results
  results_perm <- data.table()

  # Loop through each row of GOs
  for (i in 1:nrow(GOs_perm)) {

    intersect_perm_num <- permutationsGO_summary[go == GOs_perm$go[i]]
    intersect_perm_num <- intersect_perm_num %>% filter(perm==perm_num)

    perm_temp <- permutationsGO_summary[go == GOs_perm$go[i], ]
    perm_temp <- perm_temp %>% filter(perm!=perm_num)

    intersect_perm_mean <- mean(perm_temp$intersect_perm)

    n_perm_hit <- nrow(perm_temp)

    n_perm_hit_grt_eq_perm_num <- perm_temp[intersect_perm >= intersect_perm_num$intersect_perm, .N]

    p <- n_perm_hit_grt_eq_perm_num / n_perm_hit

    dt <- data.table(go = GOs_perm$go[i],
                     intersect_perm_num = intersect_perm_num$intersect_perm,
                     intersect_perm_mean = intersect_perm_mean,
                     n_perm_hit = n_perm_hit,
                     prob_perm_hit = n_perm_hit/499999,
                     n_perm_hit_grt_eq_perm_num = n_perm_hit_grt_eq_perm_num,
                     p = p)

    results_perm <- rbindlist(list(results_perm, dt))

    # cat(i, GOs_perm$go[i], "p", p, "prob_perm_hit", n_perm_hit/499999, "\n")

  }

  results_perm <- results_perm %>% arrange(p)

  prob_overlap_hit <- results %>% select(prob_overlap_hit)

  for (i in 1:nrow(results)) {

    p_emp <- results$prob_overlap_hit[i]

    V <- results_perm %>% filter(p<=p_emp) %>% nrow()
    V_scaled <- round(V * (n_GOs_emp/n_GOs_perm))

    R <- i

    fdr <- V/R
    fdr_scaled <- V_scaled/R

    prob_overlap_hit$R[i] <- R
    prob_overlap_hit$V[i] <- V
    prob_overlap_hit$fdr[i] <- fdr
    prob_overlap_hit$V_scaled[i] <- V_scaled
    prob_overlap_hit$fdr_scaled[i] <- fdr_scaled

    # cat(i, "p", p_emp, "V", V, "V_scaled", V_scaled, "FDR", fdr, "FDR_scaled", fdr_scaled, "\n")

  }

  results_perm_fdr$V <- prob_overlap_hit$V
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("V", perm_num) := V)
  results_perm_fdr$fdr <- prob_overlap_hit$fdr
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("fdr", perm_num) := fdr)
  results_perm_fdr$V_scaled <- prob_overlap_hit$V_scaled
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("V_scaled", perm_num) := V_scaled)
  results_perm_fdr$fdr_scaled <- prob_overlap_hit$fdr_scaled
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("fdr_scaled", perm_num) := fdr_scaled)

  counter <- counter + 1

}

write_tsv(results_perm_fdr, file="nonparametric_fdr_HARDPARTIAL.tsv")

results_perm_fdr <- read_tsv("nonparametric_fdr_HARDPARTIAL.tsv")

# arithmatic and geometric mean fdr
results_perm_fdr <- results_perm_fdr %>%
  select(go:prob_hit, prob_overlap_hit, contains("fdr_scaled")) %>%
  rowwise() %>%
  mutate(
    mean_fdr_arithmetic = mean(c_across(fdr_scaled100121:fdr_scaled98810)),
    mean_fdr_geometric = exp(mean(log(c_across(fdr_scaled100121:fdr_scaled98810)))), 
    mean_fdr_geometric2 = exp(mean(log(ifelse(c_across(fdr_scaled100121:fdr_scaled98810) == 0, 1e-10, c_across(fdr_scaled100121:fdr_scaled98810)))))
  )

# output for supplementary table
output <- results_perm_fdr  %>% 
  select(-contains("fdr_scaled")) %>% 
  left_join(select(results, go, prob_overlap_hit, n_genes_hit, n_genes_genome, genes_hit)) %>% 
  left_join(select(go_info, -definition_1006)) %>% 
  select(-prob_hit, -mean_fdr_arithmetic, -mean_fdr_geometric) %>% 
  select(go:mean_fdr_geometric2, name_1006, n_genes_hit, n_genes_genome, genes_hit) 

output$genes_hit <- sapply(output$genes_hit, function(x) paste(x, collapse = ","))

write_tsv(output, file = "GO_nonparametric_fdr_HARDPARTIAL_supp_table.tsv")

```

### SoftPartial 

```{r, SoftPartial nonparametric fdr}

library(tidyverse)
library(data.table)
library(qvalue)

## Import data

### Empirical predictions
setwd("[PATH_TO]/partialSHIC_DsGRP/snpEff")

predPeaks <- read_tsv(
  "predPeaks_GenotypeGVCFs_Default_allSites_hardFilteredBiallelic_100precentGenotyped_recode_biallelicNoRef_SEVERE_intergenic.tsv"
)

setwd("/home/shared/Scott_Allen/partialSHIC_DsGRP/expansionNe_intergenic/pred_empirical/permutations_GO_per_class")

### GO to gene name key
GO_geneName <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/Fbgn_GO_geneName.txt", col_names=FALSE) %>% 
  set_names(c("fbgn", "geneName", "go", "process"))

### gene locations
GO_bed <- read_tsv("/home/shared/Scott_Allen/partialSHIC_DsGRP/Yiguan/FBgn.bed", col_names=FALSE) %>%
  set_names(c("scf", "start_pos", "end_pos", "fbgn", "chrom"))

### GO names and descriptions
go_info <- read_tsv(
  "/home/uqsalle3/shared/Scott_Allen/partialSHIC_DsGRP/Empirical_Feature_Vectors/pred_expansionNe/go_info.tsv"
  ) %>% rename(go = go_id)

## Join GO terms for predictions peaks

predPeaksGO <- read_tsv("predPeaks_GO_merged.tsv", col_names=FALSE)

colnames(predPeaksGO) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", 
                           "scf_predPeaks", "start_pos_predPeaks", 
                           "end_pos_predPeaks", "val", "length", "merged", 
                           "intersect")

predPeaksGO <- predPeaksGO %>% 
  left_join(GO_geneName, relationship = "many-to-many") %>% 
  arrange(scf_predPeaks, start_pos_predPeaks, end_pos_predPeaks)

predPeaksGO <- predPeaksGO %>% mutate(gene_length=end_pos-start_pos)

## Empirical summary

predPeaksGO_summary <- predPeaksGO %>% 
  group_by(val, go, process) %>% 
  summarise(intersect_empirical = sum(intersect), 
            gene_length_sum_hit = sum(gene_length), 
            gene_length_mean_hit = mean(gene_length)) %>% 
  ungroup()

## Permutation summary

permutationsGO_SoftPartial <- fread("permutations_GO_merged_SOFTPARTIAL.tsv", header=FALSE)
colnames(permutationsGO_SoftPartial) <- c("scf", "start_pos", "end_pos", "fbgn", "chrom", "scf_perm", "start_pos_perm", "end_pos_perm", "perm", "val", "length", "intersect")

permutationsGO_summary <- permutationsGO_SoftPartial %>%
  group_by(perm, val, go, process) %>%
  summarise(intersect_perm = sum(intersect)) %>%
  ungroup()
 
# Permutation

# Loop

GOs <- predPeaksGO_summary %>% 
  filter(val=="SoftPartial") %>% 
  filter(go!="NA") %>% 
  filter(process=="P") %>% 
  select(go) %>% 
  distinct()

# Convert predPeaksGO_summary and permutationsGO_summary to data.tables
setDT(predPeaksGO_summary, key = "go")
setDT(permutationsGO_summary, key = "go")

# Initialize an empty data.table to store results
results <- data.table()

# Loop through each row of GOs
for (i in 1:nrow(GOs)) {
  
  intersect_empirical <- predPeaksGO_summary[go == GOs$go[i] & val == "SoftPartial"]
  
  list_genes_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="SoftPartial") %>% 
    select(fbgn) %>% 
    distinct() %>% 
    as.list()
  
  n_genes_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="SoftPartial") %>% 
    select(fbgn) %>% 
    distinct() %>% 
    nrow()
  
  n_merged_windows_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="SoftPartial") %>%
    select(scf, merged) %>% 
    distinct() %>% 
    nrow()
  
  n_sub_windows_hit <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    filter(val=="SoftPartial") %>%
    select(scf, start_pos_predPeaks, end_pos_predPeaks) %>%
    distinct() %>%
    nrow()
  
  n_genes_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>% 
    select(fbgn) %>% 
    distinct() %>% 
    nrow()
  
  n_merged_windows_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    select(scf, merged, val) %>% 
    distinct() %>% 
    nrow()
  
  n_sub_windows_genome <- predPeaksGO %>%
    filter(go==GOs$go[i]) %>%
    select(scf, start_pos_predPeaks, end_pos_predPeaks) %>%
    distinct() %>%
    nrow()
  
  clustering_hit <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    filter(val=="SoftPartial") %>%
    select(scf, fbgn, merged) %>%
    distinct() %>%
    group_by(scf, merged) %>%
    tally()
  
  clustering_hit_flag <- sum(clustering_hit$n)/nrow(clustering_hit)!=1
  
  clustering_hit <- sum(clustering_hit$n)/nrow(clustering_hit)
  
  bigGene_hit <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    filter(val=="SoftPartial") %>%
    select(scf, fbgn, merged, start_pos_predPeaks, end_pos_predPeaks) %>%
    group_by(fbgn) %>%
    tally()
  
  bigGene_hit_flag <- sum(bigGene_hit$n)/nrow(bigGene_hit)!=1
  
  bigGene_hit <- sum(bigGene_hit$n)/nrow(bigGene_hit)
  
  clustering_genome <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    select(scf, fbgn, merged, val) %>%
    distinct() %>%
    group_by(scf, merged, val) %>%
    tally()
  
  clustering_genome_flag <- sum(clustering_genome$n)/nrow(clustering_genome)!=1
  
  clustering_genome <- sum(clustering_genome$n)/nrow(clustering_genome)
  
  bigGene_genome <- predPeaksGO %>% 
    filter(go==GOs$go[i]) %>%
    select(scf, fbgn, merged, start_pos_predPeaks, end_pos_predPeaks, val) %>%
    group_by(fbgn, val) %>%
    tally()
  
  bigGene_genome_flag <- sum(bigGene_genome$n)/nrow(bigGene_genome)!=1
  
  bigGene_genome <- sum(bigGene_genome$n)/nrow(bigGene_genome)
  
  perm_temp <- permutationsGO_summary[go == GOs$go[i], ]
  
  intersect_perm_mean <- mean(perm_temp$intersect_perm)
  
  n_perm_hit <- nrow(perm_temp)
  
  prob_hit <- n_perm_hit/500000
  
  n_perm_hit_grt_eq_emp <- perm_temp[intersect_perm >= intersect_empirical$intersect_empirical, .N]
  
  prob_overlap_all <- n_perm_hit_grt_eq_emp / 500000
  prob_overlap_hit <- n_perm_hit_grt_eq_emp / n_perm_hit
  
  dt <- data.table(go = GOs$go[i],
                   intersect_empirical = intersect_empirical$intersect_empirical,
                   intersect_perm_mean = intersect_perm_mean,
                   n_perm_hit = n_perm_hit,
                   prob_hit = prob_hit,
                   n_perm_hit_grt_eq_emp = n_perm_hit_grt_eq_emp,
                   prob_overlap_all = prob_overlap_all, 
                   prob_overlap_hit = prob_overlap_hit, 
                   n_genes_hit = n_genes_hit, 
                   n_sub_windows_hit = n_sub_windows_hit, 
                   n_merged_windows_hit = n_merged_windows_hit, 
                   clustering_hit_flag = clustering_hit_flag, 
                   clustering_hit = clustering_hit, 
                   bigGene_hit_flag = bigGene_hit_flag, 
                   bigGene_hit = bigGene_hit, 
                   n_genes_genome = n_genes_genome, 
                   n_sub_windows_genome = n_sub_windows_genome, 
                   n_merged_windows_genome = n_merged_windows_genome, 
                   clustering_genome_flag = clustering_genome_flag, 
                   clustering_genome = clustering_genome, 
                   bigGene_genome_flag = bigGene_genome_flag, 
                   bigGene_genome = bigGene_genome, 
                   genes_hit = list_genes_hit)
  
  results <- rbindlist(list(results, dt))
  
  cat(i, GOs$go[i], "\n")
  
}

# view results
results %>% left_join(select(go_info, -definition_1006)) %>%  arrange(prob_overlap_hit)


### non-parametric FDR

### FDR is defined as the expected value of the ratio of false discoveries (V) 
### to the total number of rejections (R). Mathematically, it's expressed as 
### FDR = E(V/R). This means that on average, what proportion of the rejections 
### are expected to be false discoveries.

### non-parametric FDR

### FDR is defined as the expected value of the ratio of false discoveries (V)
### to the total number of rejections (R). Mathematically, it's expressed as
### FDR = E(V/R). This means that on average, what proportion of the rejections
### are expected to be false discoveries.

results <- results %>% arrange(prob_overlap_hit)
results_perm_fdr <- results %>% arrange(prob_overlap_hit)

n_GOs_emp <- nrow(GOs)

set.seed(19800607)
perm_nums <- permutationsGO_summary %>%
  select(perm) %>%
  distinct() %>%
  sample_n(1000)
perm_nums <- perm_nums$perm

counter <- 1

for (perm_num in perm_nums) {

  cat(counter, "perm", perm_num, "\n")

  GOs_perm <- permutationsGO_summary %>%
    filter(perm==perm_num) %>%
    filter(val=="SoftPartial") %>%
    filter(go!="NA") %>%
    filter(process=="P") %>%
    select(go) %>%
    distinct()

  n_GOs_perm <- nrow(GOs_perm)

  # Initialize an empty data.table to store results
  results_perm <- data.table()

  # Loop through each row of GOs
  for (i in 1:nrow(GOs_perm)) {

    intersect_perm_num <- permutationsGO_summary[go == GOs_perm$go[i]]
    intersect_perm_num <- intersect_perm_num %>% filter(perm==perm_num)

    perm_temp <- permutationsGO_summary[go == GOs_perm$go[i], ]
    perm_temp <- perm_temp %>% filter(perm!=perm_num)

    intersect_perm_mean <- mean(perm_temp$intersect_perm)

    n_perm_hit <- nrow(perm_temp)

    n_perm_hit_grt_eq_perm_num <- perm_temp[intersect_perm >= intersect_perm_num$intersect_perm, .N]

    p <- n_perm_hit_grt_eq_perm_num / n_perm_hit

    dt <- data.table(go = GOs_perm$go[i],
                     intersect_perm_num = intersect_perm_num$intersect_perm,
                     intersect_perm_mean = intersect_perm_mean,
                     n_perm_hit = n_perm_hit,
                     prob_perm_hit = n_perm_hit/499999,
                     n_perm_hit_grt_eq_perm_num = n_perm_hit_grt_eq_perm_num,
                     p = p)

    results_perm <- rbindlist(list(results_perm, dt))

    # cat(i, GOs_perm$go[i], "p", p, "prob_perm_hit", n_perm_hit/499999, "\n")

  }

  results_perm <- results_perm %>% arrange(p)

  prob_overlap_hit <- results %>% select(prob_overlap_hit)

  for (i in 1:nrow(results)) {

    p_emp <- results$prob_overlap_hit[i]

    V <- results_perm %>% filter(p<=p_emp) %>% nrow()
    V_scaled <- round(V * (n_GOs_emp/n_GOs_perm))

    R <- i

    fdr <- V/R
    fdr_scaled <- V_scaled/R

    prob_overlap_hit$R[i] <- R
    prob_overlap_hit$V[i] <- V
    prob_overlap_hit$fdr[i] <- fdr
    prob_overlap_hit$V_scaled[i] <- V_scaled
    prob_overlap_hit$fdr_scaled[i] <- fdr_scaled

    # cat(i, "p", p_emp, "V", V, "V_scaled", V_scaled, "FDR", fdr, "FDR_scaled", fdr_scaled, "\n")

  }

  results_perm_fdr$V <- prob_overlap_hit$V
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("V", perm_num) := V)
  results_perm_fdr$fdr <- prob_overlap_hit$fdr
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("fdr", perm_num) := fdr)
  results_perm_fdr$V_scaled <- prob_overlap_hit$V_scaled
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("V_scaled", perm_num) := V_scaled)
  results_perm_fdr$fdr_scaled <- prob_overlap_hit$fdr_scaled
  results_perm_fdr <- results_perm_fdr %>% rename(!!paste0("fdr_scaled", perm_num) := fdr_scaled)

  counter <- counter + 1

}

write_tsv(results_perm_fdr, file="nonparametric_fdr_SOFTPARTIAL.tsv")

results_perm_fdr <- read_tsv("nonparametric_fdr_SOFTPARTIAL.tsv")

# arithmatic and geometric mean fdr
results_perm_fdr <- results_perm_fdr %>%
  select(go:prob_hit, prob_overlap_hit, contains("fdr_scaled")) %>%
  rowwise() %>%
  mutate(
    mean_fdr_arithmetic = mean(c_across(fdr_scaled100121:fdr_scaled98810)),
    mean_fdr_geometric = exp(mean(log(c_across(fdr_scaled100121:fdr_scaled98810)))), 
    mean_fdr_geometric2 = exp(mean(log(ifelse(c_across(fdr_scaled100121:fdr_scaled98810) == 0, 1e-10, c_across(fdr_scaled100121:fdr_scaled98810)))))
  )

# output for supplementary table
output <- results_perm_fdr  %>% 
  select(-contains("fdr_scaled")) %>% 
  left_join(select(results, go, prob_overlap_hit, n_genes_hit, n_genes_genome, genes_hit)) %>% 
  left_join(select(go_info, -definition_1006)) %>% 
  select(-prob_hit, -mean_fdr_arithmetic, -mean_fdr_geometric) %>% 
  select(go:mean_fdr_geometric2, name_1006, n_genes_hit, n_genes_genome, genes_hit) 

output$genes_hit <- sapply(output$genes_hit, function(x) paste(x, collapse = ","))

write_tsv(output, file = "GO_nonparametric_fdr_SOFTPARTIAL_supp_table.tsv")

```
